% This is part of Mes notes de mathématique
% Copyright (C) 2010-2013,2016
%   Laurent Claessens
% See the file LICENCE.txt for copying conditions.

D'autres lectures agréables dans \cite{GianlucaB}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Conditionnement et stabilité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
	Soit $F$ une fonction à valeurs réelles définie sur $X\times D$ où $X$ et $D$ sont des espaces vectoriels réels normés. Le problème de la recherche des solutions de
	\begin{equation}
		F(x,d)=0
	\end{equation}
	est dit \defe{stable}{stable} si
	\begin{enumerate}
		\item
			la solution $x=x(d)$ existe et est unique pour tout $d$;
		\item \label{ItemProbStableB}
			Pour tout $\eta>0$, et pour tout $d_0$, il existe un nombre $K>0$ tel que $|d-d_0|<\eta$ entraine $|x(d)-x(d_0)|\;\leq\;K\;|d-d_0|$.
	\end{enumerate}
\end{definition}
Le nombre
\begin{equation}        \label{EqDefAABSOLU}
	K_{abs}(d_0,\eta):=\sup_{d\text{ tel que $|d_0-d|<\eta$}}\frac{|x(d)-x(d_0)|}{|d-d_0|}
\end{equation}
est appelé le \defe{conditionnement absolu}{conditionnement!absolu} du problème autour de $d_0$.

\begin{definition}
	Soit $F(x,d)=0$ un problème stable de conditionnement absolu $K_{\text{abs}}(d,\eta)$.  Le conditionnement relatif est défini par
	\begin{equation}
		K_{\text{rel}}(d,\eta):=K_{\text{abs}}(d,\eta)\frac{|d|}{|x(d)|}.
	\end{equation}
	Le problème est dit \defe{bien conditionné}{bien!conditionné} près de $d$ si $K_{\text{rel}}(d,\eta)$ est petit.
\end{definition}

Un résultat pratique pour étudier le conditionnement d'un problème est le suivant.
\begin{corollary}       \label{CorConditionnementNormeNabla}
	Soit $x=x(d)$ un problème stable. Supposons $\eD$ de dimension finie, supposons que $U$ est ouvert dans $\eD$. Supposons encore $x\colon U\to \eR$ différentiable en $d_0$. Alors quand $\eta$ est petit, on a
	\begin{equation}
		K_{\text{abs}}^{\eta}(d_0)\sim \| \nabla x(d_0) \|.
	\end{equation}
\end{corollary}

\begin{lemma}   \label{LemITCxqyS}
	 Tout  problème de la forme $x=x(d)$ avec $d\in\eR$ et $x \in C^1(\eR)$ est stable.
\end{lemma}

\begin{proof}
	Il faut démontrer qu'une fonction $C^1$ sur $\eR$ vérifie automatiquement la condition \ref{ItemProbStableB} de la définition de la stabilité. Pour cela, remarquons qu'une fonction $C^1$ possède une dérivée continue, et donc bornée sur tout compact\footnote{Un compact est un ensemble fermé et borné, typiquement un intervalle du type $[a,b]$.}

	Prenons $\eta>0$ et $d_0\in\eR$ et puis un $d$ tel que $| d-d_0 |<\eta$. Par le théorème des bornes atteintes, la fonction $x'$ est bornée sur l'intervalle $[d_0-\eta,d_0+\eta]$. Appelons $K$ un majorant de $x'$ sur cet intervalle. La fonction
	\begin{equation}
		f(d)=x(d_0)+K| d-d_0 |
	\end{equation}
	majore $x(d)$, et donc on a
	\begin{equation}
		\big| x(d)-x(d_0) \big|\leq K| d-d_0 |.
	\end{equation}

	Attention : vérifier si ce raisonnement est correct avec $d_0>d$, et adapter au besoin.
\end{proof}

\begin{example} \label{ExRZrOeoi}
	Un exemple de problème stable de la forme  $x=x(d)$ avec $d\in\eR$ et $x \in C^0(\eR)\setminus C^1(\eR)$.

	La fonction
	\begin{equation}
		x(d)=\begin{cases}
			0   &   \text{si $x\geq 0$}\\
			x   &   \text{si $x>0$}
		\end{cases}
	\end{equation}
	est continue, mais pas $C^1$ (non dérivable en $x=0$). La dérivée est partout bornée par $1$, et donc le problème est stable.

	Un autre exemple très classique serait de prendre $x(d)=| d |$. Dans ce cas, on peut prendre n'importe que $\eta$ et $K=1$. Le calcul est que
	\begin{subequations}
		\begin{align}
			| x(d)-x(d_0) |&<K| d-d_0 |\\
			\big| | d |-| d_0 | \big|&<| d-d_0 |.
		\end{align}
	\end{subequations}
	Cette dernière inéquation est correcte, comme on peut le voir en mettant au carré les deux membres.

\end{example}

\begin{example} \label{PIluknK}
	Un exemple de problème instable de la forme $x=x(d)$ avec $d\in\eR$ et $x \in C^0(\eR)$.

	Un exemple assez classique de fonction dont la dérivée n'est pas bornée sans pour autant que la fonction aie un comportement immoral\footnote{Penser à $x\mapsto x\sin(1/x)$.} est $x\mapsto\sqrt{x}$. Afin d'avoir une fonction définie sur $\eR$ tout entier, nous regardons la fonction
	\begin{equation}
		x(d)=\sqrt{|d|}.
	\end{equation}
	Si nous considérons maintenant $d_0=0$ et n'importe quel $\eta$, nous avons
	\begin{equation}
		\frac{ | x(d)-x(d_0) | }{ | d-d_0 | }=\frac{ \sqrt{d} }{ d }=\frac{1}{ \sqrt{d} }.
	\end{equation}
	Il n'est pas possible de trouver un $K$ qui majore ce rapport. Le problème est donc mal conditionné.

	Attention : dans ce calcul nous avons supposé $d>0$. Pensez à adapter au cas $d<0$.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Comment choisir et penser le $K$?}
%---------------------------------------------------------------------------------------------------------------------------

La formule \eqref{EqDefAABSOLU} contient une formule qui ressemble étrangement à la dérivée. La stabilité d'un problème est très liée à la dérivée de $F$. La stabilité et la dérivée ne sont pas les mêmes choses, mais il n'est pas mauvais de penser au $K$ de la stabilité comme la dérivée. Ou plus précisément : le supremum de la dérivée.

Un fil conducteur du lemme \ref{LemITCxqyS} et des exemples \ref{ExRZrOeoi}, \ref{PIluknK} est que l'on a un $K$ qui fonctionne lorsque la dérivée est bornée sur l'intervalle $\mathopen] d_0-\eta , d_0+\eta \mathclose[$. Dans le cas où ce supremum existe, le prendre en guise de $K$ fonctionne souvent.

Il faut cependant parfois faire acte d'imagination. La fonction $x\mapsto| x |$ n'est pas dérivable en $0$. Il n'empêche que $K=1$ fait fonctionner la définition de la stabilité. Remarquez que $K=1$ est le supremum de la dérivée là où elle existe.

À partir du moment où c'est clair que le $K$ est le supremum de la dérivée, on comprend pourquoi c'est le gradient qui arrive dans le corollaire \ref{CorConditionnementNormeNabla}. En effet, le gradient indique la direction de plus grande pente. C'est donc bien dans cette direction qu'il faut chercher la ``plus grande dérivée''.

\begin{proposition}
	Pour le problème stable $x=x(d)$ avec $x\in C^1(\eR^n,\eR)$, on a
	\begin{equation}
		K_{abs}(d)\sim|dx_d|_{\mbox{op}}
	\end{equation}
	où \( dx_d\) désigne la différentielle de $x$ en $d$.
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Algorithmes, stabilité, convergence et conditionnement}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode de Newton pour trouver une racine d'une fonction}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecMethodeNewton}

Nous parlons de méthode de Newton en dimension $n$ au théorème \ref{ThoHGpGwXk}.

La méthode de Newton consiste a exprimer la solution $x$ de $f(x)=0$ avec $f\in C^1(\eR)$ comme limite d'une suite $\{x_n\}_{n\in\eN}$ définie par récurrence par la formule
\begin{equation}
	x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}.
\end{equation}
où $x_0$ est arbitraire.

Si on veut exprimer cela en terme d'algorithme, nous disons que l'algorithme de Newton est donné par la suite de problèmes
\begin{equation}        \label{EqFPourNewtonUn}
	F_n(x_{n+1},x_n,f)=x_{n+1}-x_n+\frac{ f(x_n) }{ f'(x_n) }.
\end{equation}
La donnée du problème est la fonction $f$, et rien que elle.

Plus précisément, une fois que la fonction $f$ est donnée, il existe une infinité de problèmes : pour chaque $a\in \eR$ nous avons le problème
\begin{equation}
	G_a(x_n,f)=x-a+\frac{ f(a) }{ f'(a) }.
\end{equation}
La méthode de Newton consiste à sélectionner une partie de ces problèmes de la façon suivante :
\begin{subequations}
	\begin{numcases}{}
		F_0 = G_{x_0}\\
		F_n = G_{x_n}.
	\end{numcases}
\end{subequations}
Le problème $F_0$ fournit un nombre $x_1$ qui nous permet de sélectionner le problème $G_{x_1}$ qui va fournir le nombre $x_2$, etc.

Au moment de calculer le conditionnement de $F_n$, nous ne devons pas voir $x_{n-1}$ comme fonction de $x_0$ et de la donnée $f$. Il ne faut donc pas dériver à travers les $x_n$.

L'algorithme de Newton a les caractéristiques suivantes :
\begin{enumerate}

	\item
		Pour résoudre le problème numéro $n$, il faut avoir résolu le problème numéro $n-1$.
	\item
		Aucune des solutions $x_n$ aux problèmes intermédiaires n'est une solution au problème de départ (à moins d'un coup de chance).
	\item
		Étant donné que la donnée du problème $F_n$ est la fonction $f$ de départ, nous avons $d_m=d_n=d$ pour tout $m$ et $n$.

\end{enumerate}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Résoudre un système linéaire}
%---------------------------------------------------------------------------------------------------------------------------

Pour résoudre un système linéaire d'équations, nous échelonnons la matrice du système. Soit à résoudre le système $Ax=b$ où
\begin{equation}
	\begin{aligned}[]
		A&=\begin{pmatrix}
			2   &   4   &   -6  \\
			1   &   5   &   3   \\
			1   &   3   &   2
		\end{pmatrix}, &\text{et}&&b=\begin{pmatrix}
			-4  \\
			10  \\
			5
		\end{pmatrix}.
	\end{aligned}
\end{equation}
En termes de problèmes, on écrit $F\big( x,(A,b) \big)=Ax-b$. La donnée de ce problème est le couple $(A,b)$.

En ce qui concerne l'algorithme, on pose comme premier problème
\begin{equation}
	F_1\big(x_1,(A_1,b_1)\big)=A_1x_1-b_1=0
\end{equation}
avec $A_1=A$ et $b_1=b$.

Ensuite, on commence à échelonner et le second problème est
\begin{equation}
	F_2\big(x_2,(A_2,b_2)\big)=A_2x_2-b_2=0
\end{equation}
avec
\begin{equation}
	\begin{aligned}[]
		A&=\begin{pmatrix}
			2   &   4   &   -6  \\
			0   &   3   &   6   \\
			0   &   1   &   5
		\end{pmatrix}, &\text{et}&&b=\begin{pmatrix}
			-4  \\
			12  \\
			13
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Le troisième problème sera
\begin{equation}
	F_3\big(x_3,(A_3,b_3)\big)=A_3x_3-b_3=0
\end{equation}
avec
\begin{equation}
	\begin{aligned}[]
		A&=\begin{pmatrix}
			2   &   4   &   -6  \\
			0   &   3   &   6   \\
			0   &   0   &   3
		\end{pmatrix}, &\text{et}&&b=\begin{pmatrix}
			-4  \\
			12  \\
			3
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Ce problème est facile à résoudre ``à la main''. Nous nous arrêtons donc ici avec l'algorithme, et nous trouvons le $x_3$ qui résous le problème $F_3$.

L'algorithme de résolution de systèmes linéaires d'équations a les propriétés suivantes, à mettre en contraste avec celles de Newton :
\begin{enumerate}

	\item
		Pour résoudre le problème numéro $n$, il n'a pas fallu résoudre le problème numéro $n-1$.
	\item
		Toutes les solutions $x_n$ des problèmes intermédiaires sont solutions du problème de départ. Nous avons $F_n(x,d_n)=0$ pour tout $n$ (ici, $d_n=(A_n,b_n)$).
	\item
		D'un problème à l'autre, les données changent énormément : la matrice échelonnée peut être très différente de la matrice de départ.

\end{enumerate}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définitions}
%---------------------------------------------------------------------------------------------------------------------------

	Nous allons maintenant formaliser en donnant quelques définitions pour nommer les propriétés que nous avons vues. D'abord, un algorithme est une suite de problèmes. Un \defe{algorithme}{algorithme} pour résoudre un problème $F(x,d)=0$ est une suite de problèmes $\{F_n(x_n,d_n)=0\}_{n\in\eN}$.

\begin{definition}
	Un tel algorithme est dit  \defe{fortement consistant}{algorithme!fortement consistant} si pour toutes données admissibles $d_n$, on a
	\begin{equation}
		F_n(x,d_n)=0\quad\forall \;n,
	\end{equation}
	où $x$ est la solution de $F(x,d)=0$.
\end{definition}
L'algorithme des matrices est fortement consistant, mais pas l'algorithme de Newton.

\begin{definition}
	Un algorithme est \defe{consistant}{algorithme!consistant} si $\lim_{n\to\infty}F_n(x,d_n)=0$.
\end{definition}
Dans le cas de l'algorithme de Newton, c'est plutôt une telle consistance qu'on attend.

L'algorithme est dit \defe{stable}{algorithme!stable} si pour tout $n$ le problème correspondant est stable.  Dans ce cas, on note $K^{\mbox{num}}$ le  \defe{conditionnement relatif asymptotique}{conditionnement!relatif asymptotique} défini par
\begin{equation}
	K^{\mbox{num}}=\limsup_nK_n
\end{equation}
où $K_n$ est le conditionnement relatif du problème $F_n(x_n,d_n)=0$.

\begin{definition}      \label{DefAlgoConverge}
	Un algorithme est dit \defe{convergent}{algorithme!convergent} (en $d$) si pour tout $\epsilon>0$, il existe $N=N(\epsilon)$ et $\delta=\delta(N,\epsilon)$ tels que pour $n\geq0$ et $|d-d_n|<\delta$, on ait $|x(d)-x_n(d_n)|<\epsilon$.
\end{definition}

\begin{remark}      \label{RemConvAlgoNewton}
Dans le cas de l'algorithme de Newton, nous avons vu que la donnée $d_n$ du problème $F_n$ était en fait la même que la donnée initiale $d$, donc nous avons $d_n=d$, et par conséquent nous avons toujours $| d-d_n |<\delta$. Dans ce cas, la définition de la convergence revient à demander que la suite numérique des $x_n$ converge vers la solution $x$.
\end{remark}

\begin{remark}
Dans le cas des matrices par contre, les données sont très différentes les unes des autres, nous avons donc en général que $| d-d_n |>\delta$. Mais en revanche nous savons que tous les problèmes intermédiaires $F_n$ acceptent une solution unique\footnote{Nous n'envisageons que le cas où le déterminant est non nul.} $x_n(d_n)=x(d)$. Par conséquent, $| x_n(d_n)-x(d) |$ est toujours plus petit que $\epsilon$. L'algorithme des matrice est donc toujours un algorithme convergent.
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Représentations numériques, erreurs}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
	Soit $x$ un réel. On définit sa \defe{représentation en virgule fixe}{représentation!virgule fixe} par
	\begin{equation}
		x=\{[x_nx_{n-1}...x_0,x_{-1}...x_{-m}], b, s\}
	\end{equation}
	avec  $b\in\eN, b\geq2$, $s\in\{0,1\}$ et $x_j\in\eN,x_j<b$ suivant la formule
	\begin{equation}
		x=(-1)^{s}\sum_{j=-m}^nx_j.b^j.
	\end{equation}
	On définit sa \defe{représentation en \href{https://docs.python.org/tutorial/floatingpoint.html}{virgule flottante} normalisée}{Représentation!virgule flottante normalisée} par
	\begin{equation}
		\{[a_1...a_t],b,e,s\}
	\end{equation}
	où $e\in\eZ,e\in[L,U]$ et $a_j\in\eN;0\leq a_j<b; a_1\geq1$ suivant la formule
	\begin{equation}        \label{EqRepreFlotNOrm}
		x=(-1)^sb^e\sum_{j=1}^ta_jb^{-j}.
	\end{equation}
\end{definition}

\begin{definition}
	L'\defe{erreur relative}{erreur relative} commise en remplaçant un nombre réel $x$ par une valeur approchée $\hat{x}$ est définie par
	\begin{equation}
		\epsilon_x:=\left|\frac{x-\hat{x}}{x}\right|.
	\end{equation}
\end{definition}

L'erreur relative n'est pas influencée par l'ordre de grandeur de \( x\). En effet, l'ordre de grandeur de \( \hat x\) est certainement la même que celle de \( x\), dans la majorité des cas sans problèmes. Du coup si \( x'=200x\) alors \( \hat{x'}\simeq 200\hat{x}\) et le \( 200\) se simplifie.

Le nombre de chiffres significatifs correct dans l'approximation est donné par \( -\log_{10}(\epsilon_x)\). La partie entière de ce nombre est le nombre de chiffres tout à fait exacts et la partie décimale donne une idée sur le fait que le chiffre suivant est plus ou moins bien.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Problèmes pour écrire des nombres}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{remark}
	Si nous voulons donner \( x\in \eR\) à un ordinateur, nous sommes soumis à deux erreurs :
	\begin{enumerate}
		\item
			D'abord, vu que nous ne pouvons pas taper sur le clavier toutes les décimales de \( x\), nous faisons une \defe{erreur de troncature}{erreur!troncature}.
		\item
			L'ordinateur devant convertir cela en base deux, il commet une seconde erreur, dite \defe{erreur d'assignation}{erreur!assignation}.
	\end{enumerate}
\end{remark}

Supposons que nous voulions écrire le nombre (écrit ici en base \( 10\))
\begin{equation}
	0.4567894251
\end{equation}
de façon plus facile à lire, on peut demander de ne laisser que \( t\) chiffres significatifs. Disons \( t=3\).

\begin{description}
	\item[Technique de troncature] On garde \( 3\) chiffres significatifs : \( 0.456\). Facile.
	\item[Technique d'arrondi] Vu que le premier qu'on supprime est un \( 7\), le dernier qu'on garde est majoré de \( 1\) : on écrit \( 0.457\).


\end{description}

Si le premier chiffre rejeté est un \( 5\), il faut augmenter de \( 1\) de dernier chiffre gardé parce qu'il y a presque certainement encore un chiffre non nul derrière.

\begin{remark}
	Les ordinateurs travaillent tous en mode d'arrondi.
\end{remark}

Si on doit entrer le nombre \( 0.38358546\) dans un ordinateur qui ne garde que \( 3\) chiffres significatifs, il faut entrer \( 0.384\) (erreur classique dans les exercices).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelque bonnes règles}
%---------------------------------------------------------------------------------------------------------------------------

\begin{enumerate}
	\item
		Si on a plusieurs nombres à additionner ou soustraire, il vaut mieux commencer par sommer ou soustraire ceux dont on sait qu'ils ont le même ordre de grandeur. Il n'y a donc pas tout à fait «associativité» des erreurs.
	\item
		Les opérations délicates sont l'addition et la soustraction. La multiplication et la division sont sans dangers, à part l'erreur de dépassement du maximum. Dans une multiplication, on perd au pire quelque chiffres significatifs, mais certainement les derniers, pas les premiers.
\end{enumerate}

Soient deux nombres dont la différence d'ordre de grandeurs est plus grande que la précision de la machine. Alors de facto un des deux disparaît.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Erreur de ``cancellation''}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Lorsque deux nombres sont de même ordre de grandeur, avec plusieurs nombres significatifs identiques.

\begin{example}
	Soit à résoudre l'équation \( ax^2+bx+c=0\) avec \( a,b,c\neq 0\) et \( b^2-4ac>0\). Solution :
	\begin{equation}
		x_{1,2}=\frac{ -b\mp\sqrt{b^2-4ac} }{ 2a }.
	\end{equation}
	Note : on écrit \( \mp\) au lieu de \( \mp\) pour les classer dans l'ordre croissant, mais ce n'est pas crucial.

	Supposons que \( | 4ac |\ll b^2\) avec tout de même pas tellement petit qu'on se perd dans la précision. Bref, on suppose que seules quelque dernières décimales de \( b^2-4ac\) sont différentes de zéro.

	On a :
	\begin{subequations}
		\begin{align}
			\sqrt{b^2-4ac}&=\sqrt{\tilde b}= | \tilde b | \\
			x_1&=\frac{ -b-\sqrt{b^2-4ac} }{ 2a }\\
			x_2&=\frac{ -b+\sqrt{b^2-4ac} }{ 2a }
		\end{align}
	\end{subequations}
	Si \( b>0\), nous avons une erreur de ``cancellation'' dans \( x_2\) parce qu'on fait la différence entre deux nombres presque égaux. Donc \( x_2\) mal calculé. Par contre \( x_1\) est bien calculé.

	Si par contre \( b<0\), c'est le contraire.


	Avec \( a=10^{-3}\), \( b=0.8\), \( c=-1.2\times 10^{-5}\). À la main nous obtenons : \( x_1=-800\), \( x_2=1.5\times 10^{-5}\), et un ordinateur se tromperait \ldots


\lstinputlisting{sageSnip001.sage}

	Donc Sage ne tombe pas dans le piège.
\end{example}

Comment résoudre ce problème ? Ou, autre façon de poser la question : comment Sage a fait pour résoudre le problème ?

Utilisons les relations coefficients-racines :
\begin{subequations}
	\begin{align}
		x_1+x_2&=-b/a\\
		x_1x_2&=c/a
	\end{align}
\end{subequations}
La première lie les deux racines par des opérations de addition et soustractions, et donc n'est pas intéressantes. La seconde est bien. Si nous connaissons \( x_1\), nous calculons
\begin{equation}
	x_2=\frac{ c }{ ax_1 }.
\end{equation}

Quitte à redéfinir \( x_1\) et \( x_2\), la solution bien calculée est :
\begin{equation}
	x_1=\frac{ -b-\signe(b)\sqrt{b^2-4ac} }{ 2a }.
\end{equation}

\begin{example}
	Nous considérons :
	\begin{equation}
		f(x)=cos(x+\delta)-\cos(x).
	\end{equation}
	Cela a une erreur de ``cancellation'' lorsque \( | \delta |\ll | x |\). On élimine l'erreur de ``cancellation'' par
	\begin{equation}
		f(x)=-2\sin(\delta/2)\sin\left( x+\frac{ \delta }{ 2 } \right).
	\end{equation}

	\begin{probleme}
		Pourquoi la condition pour avoir l'erreur est \( \delta\ll x\) et non simplement \( \delta\ll 1\) ?
	\end{probleme}

\end{example}

\begin{example}
	Pour
	\begin{equation}
		f(x)=\sqrt{x+\delta}-\sqrt{x}.
	\end{equation}
	On fait la coup du binôme conjugué :
	\begin{equation}
		f(x)=\frac{ \delta }{ \sqrt{x+\delta}+\sqrt{x} }.
	\end{equation}
	Plus d'erreur de ``cancellation'', vu qu'au dénominateur nous avons une somme de deux positifs.
\end{example}

Les erreurs de ``cancellation'' ne se résolvent pas en augmentant la précision des nombres donnés.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Conditionnement et stabilité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Ne pas confondre :
\begin{description}
	\item[Le conditionnement] provient du problème lui-même.
	\item[La stabilité] provient de l'algorithme de résolution.
\end{description}

\begin{example}[Un problème mal conditionné]
	Le système
	\begin{subequations}
		\begin{numcases}{}
			2.1x +  3.5y = 8 \\
			4.19x + 7.0y = 15
		\end{numcases}
	\end{subequations}
	Solution : \( x=100\), \( y=  -57.714285\cdots \) (périodique)

	Perturbons : nous remplaçons \( 4.19\) par \( 4.192\). L'erreur relative est : \( 4.77\times 10^{-4}\).

	Solution : \( \bar x=125\), \( \bar y=-72.714285\cdots\), avec donc erreur relative de \( 0.26\). Autrement dit : l'erreur relative sur la solution est grande même avec une petite erreur relative sur la donnée.

	C'est un problème mal conditionné.

	Le fait est que c'est une intersection de deux droites presque parallèles. Donc effectivement une petite perturbation d'une des deux droites donne une grande perturbation du point d'intersection.

	Le fait est qu'un ordinateur effectue \emph{toujours} une perturbation, au moins de l'ordre \( 10^{-16}\) pour ne fut-ce que représenter les nombres. C'est à dire une perturbation sur les six nombres. Il n'y a donc pas d'espoir d'obtenir un algorithme donnant une bonne réponse.
\end{example}

Solution, très approximativement : le problème est fondamentalement le fait que la matrice du système est de petit déterminant. Donc on peut tout multiplier par une matrice à gros déterminant, résoudre puis appliquer la matrice inverse.

\begin{example}[Problème bien conditionné avec algorithme instable]
	Soit à calculer
	\begin{equation}
		I_n=\frac{1}{ e }\int_0^1x^ne^xdx
	\end{equation}
	avec \( n\geq 0\). Par partie, nous obtenons :
	\begin{equation}
		I_n=1-nI_{n-1}.
	\end{equation}
	D'autre part, \( I_0=\frac{ e-1 }{ e }\), \( I_1=\frac{1}{ e }\). Puis par récurrence, c'est tout en main.

	Du côté de l'ordinateur, nous lui donnons forcément une approximation de \( I_1\), parce que nous lui donnons une approximation de \( e\). Soit l'erreur \( \epsilon_1\) sur \( I_1\).

	Sans démonstration :
	\begin{lemma}
		Nous avons \( \lim_{n\to \infty} I_n=0\).
	\end{lemma}
	Mais numériquement, il n'est pas possible de rester longtemps sous \( \epsilon_1\) parce que nous n'espérons pas avoir une erreur plus petite que ça. Donc à partir du moment où \( I_n<\epsilon_1\), les valeurs sont toutes complètement fausses. Cela est le mieux que l'on puisse espérer. Mais la réalité est pire.

	En réalité, en lançant le calcul sur un ordinateur, les valeurs sont même croissantes avec \( n\) à partir d'un certain moment.

	On peut étudier l'erreur et montrer que l'erreur est donnée par :
	\begin{equation}
		\epsilon_n=(-1)^{n-1}n!\epsilon_1.
	\end{equation}
	Mais comme la factorielle est tellement forte que c'est sans espoir d'aller loin en essayant très fort de donner une petite erreur sur \( \epsilon_1\).

\end{example}

Il existe heureusement un algorithme stable pour cette intégrale. La formule est :
\begin{equation}
	I_{n-1}=\frac{1}{ n }(1-I_n).
\end{equation}
Si nous savons un \( I_N\) avec \( N\) grand, cette formule donne les \( I_i\) avec \( i=N,N-1,\ldots, 2\). Posons donc \( I_N=a\in \eR\) n'importe comment. Donc \( \epsilon_N\) est grand. Mais il se trouve que l'erreur sur \( \epsilon_1\) est donnée par
\begin{equation}
	\epsilon_1=\frac{ (-1)^{N-1} }{ N! }\epsilon_N.
\end{equation}
Donc même en prenant vraiment n'importe quoi pour \( I_N\), nous obtenons de bonnes approximations pour \( I_i\) avec les petits \( i\). Même avec \( I_{20}=1000\) (qui est complètement faux), nous trouvons énormément de chiffres significatifs corrects pour \( I_1\).

\begin{exercice}
	Soient les expressions (algébriquement égales) :
	\begin{enumerate}
		\item
			\(A= x(x+1)\)
		\item
			\(B= x^2+x\)
	\end{enumerate}
	Nous savons que
	\begin{equation}
		x=\fl(x)=10^{-30}
	\end{equation}
	et
	\begin{equation}
		1=\fl(1)
	\end{equation}
	parce que pour \( 1\) et \( 10^{-30}\), il n'y a pas d'erreurs d'assignation.

	En précision simple, \( 10^{-30}+1=1\) parce qu'en précision simple, il n'y a que \( 7\) ou \( 8\) chiffres significatifs\footnote{Erreur de « relation normale».}.

	Nous avons $A=10^{-30}$, mais \( x^2\) donne un \info{underflow} parce que \( 10^{-60}\) ne peut pas être représenté en précision simple. En pratique, beaucoup de logiciels en font \( 0\). Dans ce cas, en réalité \( B\) donne effectivement \( 10^{-30}\) après avoir fait \( x^2+x=0+x=10^{-30}\).
\end{exercice}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Équations non linéaire}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Certains équations non linéaires sont résoluble explicitement, par exemples les polynômes de degré jusqu'à \( 4\) ou des choses comme
\begin{equation}
	\sin^2(x)+3\sin(x)+5=0.
\end{equation}
Mais ces exemples sont très rares.

Nous allons étudier des équations du type \( f(x)=0\), dans \( \eR\).

\begin{enumerate}
	\item
Un problème écrit sous la forme \( x=g(x)\) peut utiliser des théorèmes de points fixes.
\item
	Un problème sous la forme \( f(x)=0\) peut utiliser des méthodes de bisection, Newton ou autres.
\end{enumerate}
Il y a évidemment beaucoup de façons de transformer un problème pour passer d'une forme à l'autre.

\begin{example}
	Soit \( f(x)=x^2-a=0\) avec \( a>0\). Nous pouvons l'écrire
	\begin{equation}
		x^2+x-a=x
	\end{equation}
	qui donne une forme \( g(x)=x\) pour \( g(x)=x^2+x-a\).

	Ou encore \( x=\frac{ a }{ x }\) et donc \( g(x)=a/x\) (si par ailleurs on sait que \( x\neq 0\)). Notons que \( x\neq 0\) n'est pas une hypothèse très forte parce qu'on la vérifie directement sur \( a\).
\end{example}

\begin{example}
	Soit l'équation à résoudre
	\begin{equation}
		f(x)=x^2-2-\ln(x)=0
	\end{equation}
	Les solutions de cette équations peuvent être vues comme les intersections avec l'axe \( X\) du graphe \( y=x^2-2-\ln(x)\). Tracer peut donc aider. Par ailleurs, il faut noter que
	\begin{equation}
		\lim_{x\to \pm\infty} f(x)=\infty,
	\end{equation}
	donc les solutions sont certainement contenues dans un compact de \( \eR\).

	À part tracer nous pouvons écrire
	\begin{equation}
		x^2-2=\ln(x).
	\end{equation}
	Et là, ce sont deux fonctions dont nous pouvons tracer le graphe pour trouver graphiquement les points d'intersection. Une étude de fonction montre vite qu'il y a exactement deux solutions, qu'elles sont strictement positives. Pour trouver des bornes, il faut calculer par exemple pour \( x=2\) les valeurs de \( \ln(x)\) et \( x^2-2\) pour voir si le graphe de \( x^2-2\) est déjà plus haut.
\end{example}

La majorité des méthodes numériques de résolution d'équation du type \( f(x)=0\) ou \( x=g(x)\) seront sous la forme de suites. Avec questions à la clefs :
\begin{enumerate}
	\item
		Quel point de départ choisir ?
	\item
		Convergence ?
	\item
		Est-ce que la limite est bien une solution ?
	\item
		Vu que la limite est unique, comment faire si l'équation a plusieurs solutions ? (souvent c'est le choix du point initial qui va jouer sur ce point)
\end{enumerate}

\begin{normaltext}
	Si la fonction est très plate, il est possible d'avoir
	\begin{equation}
		| f(\tilde \alpha) |\leq \epsilon
	\end{equation}
	sans que \( \tilde \alpha\) ne soit une bonne approximation.

	Lorsqu'on fait tourner une méthode itérative résolvant \( f(x)=0\), il n'est pas suffisant de s'arrêter lorsque
	\begin{equation}
		f(x_n)\leq \epsilon_1.
	\end{equation}
	Il faut aussi s'assurer que, si \( \bar x\) est la solution exacte, \( | x_n-\bar x |\leq \epsilon_2\). Ici \( \epsilon_1\) et \( \epsilon_2\) sont deux «précisions» que nous nous fixons au départ.

	Évidemment, vérifier la condition \( | x_n-\bar x |\leq \epsilon_2\), il faudrait savoir \( \bar x\). Et savoir \( \bar x\) c'est justement le problème. Nous sommes donc amenés à faire des estimation de \( | x_n-\bar x |\).
\end{normaltext}

Soit \( p\) l'ordre de convergence de la suite \( (x_n)\) vers \( \bar x\). Si \( p>1\) et \( | x_{n+1}-x_n |\leq \epsilon_2\) alors \( | \bar x-x_n |\leq \epsilon_2\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode de bisection}
%---------------------------------------------------------------------------------------------------------------------------

Il y a ce théorème des valeurs intermédiaires.
\begin{theorem}
	Soit \( f\) continue sur \( \mathopen[ a , b \mathclose]\) telle que \( f(a)f(b)<0\). Alors il existe au moins une solution à l'équation \( f(x)=0\) sur l'intervalle \( \mathopen] a , b \mathclose[\).
\end{theorem}

Pour démarrer une bisection, il est toujours bon de prendre l'intervalle \( \mathopen[ a , b \mathclose]\) de façon à ne contenir qu'une seule solution.

Soit donc un premier intervalle \( \mathopen[ a_0 , b_O \mathclose]\) tel que \( f(a_0)f(b_0)<0\) et ne contenant qu'une seule solution. À chaque itération nous considérons la moitié de l'intervalle précédent, mais la moitié contenant la solution.

Le test d'arrêt de la méthode de bisection se base uniquement sur la taille de l'intervalle qui reste. En effet si nous avons
\begin{equation}
	| b_n-a_n |\leq \epsilon
\end{equation}
nous avons certainement
\begin{equation}
	| \bar x-x_n |\leq \frac{ \epsilon }{2}
\end{equation}
où \( x_n\) est le point du milieu de \( \mathopen[ a_n , b_n \mathclose]\).

