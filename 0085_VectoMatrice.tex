% This is part of Mes notes de mathématique
% Copyright (c) 2011-2014
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Décomposition de Bruhat}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Décomposition de Bruhat]\index{Bruhat (décomposition)}\index{décomposition!Bruhat}    \label{ThoizlYJO}
    Soit \( \eK\) un corps; un élément \( M\in\GL(n,\eR)\) s'écrit sous la forme
    \begin{equation}
        M=T_1P_{\sigma}T_2
    \end{equation}
    où \( T_1\) et \( T_2\) sont des matrices triangulaires supérieures inversibles et où \( P_{\sigma}\) est une matrice de permutation \( \sigma\in S_n\). De plus il y a unicité de \( \sigma\).
\end{theorem}
\index{groupe!permutation}
\index{groupe!linéaire}
\index{matrice}

\begin{proof}
    Afin de rendre les choses plus visuelles, nous nous permettons de donner des exemples au fur et à mesure de la preuve. Nous prenons l'exemple de la matrice
    \begin{equation}
        \begin{pmatrix}
            1    &   3    &   4    \\
            2    &   5    &   6    \\
            0    &   7    &   8
        \end{pmatrix}.
    \end{equation}
    \begin{subproof}
    \item[Existence]
        Soit \( M\in \GL(n,\eR)\); vu qu'elle est inversible, on a un indice \( i_1\) maximum tel que \( M_{i_1,1}\neq 0\). Nous changeons toutes les lignes jusque là, c'est à dire que nous faisons, pour \( 1\leq i< i_1\),
        \begin{equation}        \label{EqGHUbwR}
            L_i\to L_i-\frac{ M_{i1} }{ M_{i_11} }L_{i_1}.
        \end{equation}

        Nous avons donc obtenu une matrice dont la première colonne est nulle sauf la case numéro \( i_1\). L'opération \eqref{EqGHUbwR} revient à considérer la multiplication par la matrice de transvection
        \begin{equation}
            T_1^{(i)}=T_{ii_1}\left( -\frac{ M_{i1} }{ M_{i_11} } \right)
        \end{equation}
        pour tout \( i<i_1\). Pour rappel nous ne changeons que les lignes \emph{au-)dessus} de la \( i_1\). Du coup les matrices \( T^{(i)}_1\) sont triangulaires supérieures. Nous avons donc la nouvelle matrice \( M_1=\left( \prod_{i<i_1}T_1^{(i)} \right)M\) pour laquelle toute la première colonne est nulle sauf un élément.

        Dans le cas de l'exemple, le «pivot» sera la ligne \( (2,5,6)\) et la matrice se transforme à l'aide de la matrice \( T_1=T_{12}(-1/2)\) :
        \begin{equation}    \label{EqyjXIYf}
            \begin{pmatrix}
                1    &   -1/2    &   0    \\
                0    &   1    &   0    \\
                0    &   0    &   1
            \end{pmatrix}
            \begin{pmatrix}
                1    &   3    &   4    \\
                2    &   5    &   6    \\
                0    &   7    &   8
            \end{pmatrix}=
            \begin{pmatrix}
                0    &   1/2    &   1    \\
                2    &   5    &   6    \\
                0    &   7    &   8
            \end{pmatrix}.
        \end{equation}

    
    Maintenant nous faisons de même avec les colonnes (en renommant \( M\) la matrice obtenue à l'étape précédente) :
    \begin{equation}
        C_j\to C_j-\frac{ M_{i_1j} }{ M_{i_11} }C_1,
    \end{equation}
    qui revient à multiplier à droite par les matrices \( T_{1j}(\frac{ M_{i_1i} }{ M_{i_11} })\) avec \( j>1\). Encore une fois ce sont des matrices triangulaires supérieures.

    Dans l'exemple, pour traiter la seconde colonne, nous multiplions \eqref{EqyjXIYf} à droite par la matrice \( T_{12}(-5/2)\) :
    \begin{equation}
            \begin{pmatrix}
                0    &   1/2    &   1    \\
                2    &   5    &   6    \\
                0    &   7    &   8
            \end{pmatrix}
            \begin{pmatrix}
                1    &   -5/2    &   0    \\
                0    &   1    &   0    \\
                0    &   0    &   1
            \end{pmatrix}=
            \begin{pmatrix}
                0    &   1/2    &   1    \\
                2    &   0    &   6    \\
                0    &   7    &   8
            \end{pmatrix}.
    \end{equation}
    Appliquer encore la matrice \( T_{13}(-6/2)\) apporte la matrice
    \begin{equation}
        \begin{pmatrix}
            0    &   1/2    &   1    \\
            2    &   0    &   0    \\
            0    &   7    &   8
        \end{pmatrix}.
    \end{equation}
    Enfin nous multiplions la matrice obtenue par \( \frac{1}{ M_{i_11} }\mtu\) pour normaliser à \( 1\) l'élément «pivot» que nous avions choisit. Dans notre exemple nous multiplions par \( 1/2\) pour trouver
    \begin{equation}        \label{Eqduglwu}
        \begin{pmatrix}
            0    &   1/4    &   1/2    \\
            1    &   0    &   0    \\
            0    &   7/2    &   4
        \end{pmatrix}.
    \end{equation}

    La matrice obtenue jusqu'ici possède une ligne et une colonne de zéros avec un \( 1\) à leur intersection, et elle est de la forme
    \begin{equation}
        M'=T_1MT_2
    \end{equation}
    où \( T_1\) et \( T_2\) sont triangulaires supérieures et inversibles, produits de matrices de transvection (et d'une matrice scalaire pour la normalisation).

    Il reste à recommencer l'opération avec la seconde colonne (qui n'est pas toute nulle parce que le déterminant est encore non nul) puis la suivante etc. Dans notre exemple de l'équation \eqref{Eqduglwu}, nous éliminerions le \( 1/4\) et le \( 4\) en utilisant le \( 7/2\).

    Encore une fois tout cela se fait à l'aide de matrice supérieures parce qu'à chaque étape, les colonnes précédent le pivot sont déjà nulles (saut un \( 1\)) et ne doivent donc pas être touchées.

    À la fin de ce processus, ce qui reste est une matrice \( TMT'\) qui ne contient plus que un seul \( 1\) sur chaque ligne et chaque colonne, c'est à dire une matrice de permutation : \( P_{\sigma}=TMT'\) et donc
    \begin{equation}
        M=T^{-1}_{\sigma}(T')^{-1}.
    \end{equation}

        \item[Unicité]

            Soient \( \sigma,\sigma\in S_n'\) tels que \( T_1P_{\sigma}T_2=S_1P_{\tau}S_2\) avec \( T_i\) et \( S_i\) triangulaires supérieures et inversibles. En posant \( T=T_2S_2^{-1}\) et \( S=T_1^{-1}S_1\), nous avons
            \begin{equation}
                P_{\sigma}T=SP_{\tau}
            \end{equation}
            où \( S\) et \( T\) sont des matrices triangulaires supérieures et inversibles. Par les calculs de la preuve du lemme \ref{LemyrAXQs},
            \begin{subequations}
                \begin{numcases}{}
                    (P_{\sigma}T)_{kl}=T_{\sigma^{-1}(k)l}\\
                    (SP_{\tau})_{kl}=S_{k\tau(l)},
                \end{numcases}
            \end{subequations}
            et donc
            \begin{equation}    \label{EqKlmgOT}
                T_{\sigma^{-1}(k)l}=S_{k\tau(l)}.
            \end{equation}
            En écrivant cette équation avec \( k=\sigma(i)\) (nous rappelons que \( \sigma\) est bijective),
            \begin{equation}
                T_{il}=S_{\sigma(i)\tau(l)}.
            \end{equation}
            Nous savons que les termes diagonaux de \( T\) sont non nuls parce que \( T\) est triangulaire supérieure et inversible (donc pas de colonnes entières nulles). Nous avons donc, en prenant \( i=l=k\),
            \begin{equation}
                0\neq T_{kk}=S_{\sigma(k)\tau(k)}.
            \end{equation}
            La matrice étant triangulaire supérieure, cela implique 
            \begin{equation}    \label{EqEmiBTX}
                \sigma(k)\leq\tau(k).
            \end{equation}
            De la même manière en écrivant \eqref{EqKlmgOT} avec \( l=\tau^{-1}(i)\),
            \begin{equation}
                S_{ki}=T_{\sigma^{-1}(k)\tau^{-1}(i)}
            \end{equation}
            et donc
            \begin{equation}
                \sigma^{-1}(k)\leq \tau^{-1}(k).
            \end{equation}
            En écrivant cela avec \( k=\sigma(j)\), nous avons \( j\leq \tau^{-1}\sigma(j)\) et en appliquant enfin \( \tau\),
            \begin{equation}
                \tau(j)\leq \sigma(j).
            \end{equation}
            En comparant avec \eqref{EqEmiBTX}, nous avons \( \sigma=\tau\).
    \end{subproof}
\end{proof}

%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces de polynômes}		\label{SecEspacePolynomes}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
Dans cette section nous abandonnons pour quelques minutes l'espace $\eR^m$ et considérons plus attentivement l'espace des fonctions polynômiales $\mathcal{P}_{\eR}$ et de ses sous-espaces $\mathcal{P}_{\eR}^k$, pour $k$ dans $\eN_0$. 

Attention : les polynômes en soi sont définis par la définition \ref{DefRGOooGIVzkx}.

Pour chaque $k>0$ donné nous définissons
\begin{equation}
\mathcal{P}_\eR^k=\{p:\eR\to \eR\,|\, p : x\mapsto a_0+a_1 x +a_2 x^2 + \ldots+a_k x^k, \, a_i\in\eR,\,\forall i=0,\ldots,k\}.
\end{equation}   
Il est facile de se convaincre que la somme de deux polynômes de degré inférieur ou égal à $k$ est encore un polynôme de degré inférieur ou égal à $k$. En outre il est clair que la multiplication par un scalaire ne peut pas augmenter le degré d'un polynôme. L'ensemble $\mathcal{P}_\eR^k$ est donc un espace vectoriel muni des opérations héritées de $\mathcal{P}_{\eR}$. 

La base canonique de l'espace $\mathcal{P}_\eR^k$ est donné par les monômes $\mathcal{B}=\{x\mapsto x^j \,|\, j=0, \ldots, k\}$. Le fait que cela soit une base est vraiment facile à démontrer et est un exercice très utile si vous ne l'avez pas encore vu dans un cours précédent. 

Nous allons maintenant étudier trois application linéaires de $\mathcal{P}_\eR^k$ vers des autres espaces vectoriels
\begin{description}
  \item[L'isomorphisme canonique  $\phi:\mathcal{P}_\eR^k \to\eR^{k+1}$] Nous définissons $\phi$ par les relations suivantes
\[
\phi(x^j)=e_{j+1}, \qquad \forall j\in\{0,\dots, k\}. 
\]
Cela veut dire que pour tout $p$ dans $\mathcal{P}_\eR^k$, avec $p(x)=a_0+a_1 x +a_2 x^2 + \ldots+a_k x^K$, l'image de $p$ par $\phi$ est 
\[
\phi(p)=\phi\left(\sum_{j=0}^k a_j x^j\right)=\sum_{j=0}^k a_j e_{j+1}.
\]
\begin{example} Soit $k=5$ on a 
  \begin{equation}
    \phi(-8-7x-4x^2+4x^3+2x^5)=
  \begin{pmatrix}
    -8\\
    -7\\
    -4\\
    4\\
    0\\
    2
  \end{pmatrix}.
  \end{equation}
\end{example}

Cette application est clairement bijective et respecte les opérations d'espace vectoriel, donc elle est un isomorphisme d'espaces vectoriels. L'existence d'un isomorphisme entre $\mathcal{P}_\eR^k$  et $\eR^{k+1}$ est un cas particulier du théorème qui dit que  pour chaque $m$ dans $\eN_0$ fixée, tous les espaces vectoriels sur $\eR$ de dimension $m$ sont isomorphes à $\eR^m$. Vous connaissez peut être déjà ce théorème depuis votre cours d'algèbre linéaire.  
    \item[La dérivation $d: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k-1}$] L'application de dérivation $d$ fait exactement ce qu'on s'attend d'elle 
\[
d(x^0)=d(1)=0, \qquad d(x^j)=j x^{j-1}, \quad \forall j\in\{1,\dots, k\}. 
\]
Cette application n'est pas injective, parce que l'image de $p$ ne dépend pas de la valeur de $a_0$, donc si deux polynômes sont les mêmes à une constante près ils auront la même image par $d$.

\begin{example} Soit $k=3$ on a 
  \begin{equation}
    d(-8-12x+4x^3)= -12 (1) + 4 (3x^2) = -12+12 x^2.
    \end{equation}

    Noter que $d(-30-12x+4x^3)=d(-8-12x+4x^3)$. Cela confirme, comme mentionné plus haut que la dérivée n'est pas injective.
\end{example}
      \item[L'intégration $I: \mathcal{P}_\eR^k \to \mathcal{P}_\eR^{k+1}$] Nous pouvons définir une application que est <<à une constante prés>> l'application inverse de la dérivation
        \begin{equation}
          I(p)= \int_0^x p(t) \,dt.
        \end{equation}
Il faut comprendre que dans l'intégral la variable $t$ est simplement la variable d'intégration. La <<vraie>> variable de la fonction image de $p$ sera $x$ !
 
Comme d'habitude nous écrivons explicitement l'action de $I$ sur les éléments de la base canonique
\begin{equation}
    I(x^j)=\int_0^x t^k \,dt= \frac{x^{j+1}}{j+1}.
\end{equation}

\begin{example} 
   Soit $k=4$ on a 
  \begin{equation}
    I(6+2x+x^2+x^4)= 6x+x^2+\frac{x^3}{3}+\frac{x^5}{5}.
    \end{equation}
\end{example}

Remarquez que, étant donné que dans la définition de $I$ nous avons décidé d'intégrer entre zéro et $x$, tous les polynômes dans $\mathcal{P}_\eR^{k+1}$ qui sont l'image par $I$ d'un polynôme de $\mathcal{P}_\eR^{k}$ ont $a_0=0$. Cela veut dire que nous pouvons générer toute l'image de $I$ en utilisant un sous-ensemble de la base canonique de $\mathcal{P}_\eR^{k+1}$,  en particulier $\mathcal{B}_1=\{x\mapsto x^j \,|\, j=1, \ldots, k\}\subset \mathcal{B}$ nous suffira. Cela n'est guère surprenant, parce que l'image par une application linéaire d'un espace vectoriel de dimension finie ne peut pas être un espace de dimension supérieure. 
\end{description}

Les applications de dérivation et intégration correspondent évidemment à des application linéaires de $\mathcal{P}_\eR$ dans lui-même. 

L'espace de tous les polynômes étant de dimension infinie, il peut servir de contre exemple assez simple. Dans la sous-section \ref{SubSecPOlynomesCE}, nous verrons que toutes les normes ne sont pas équivalentes sur l'espace des polynômes.



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes symétriques, alternés ou semi-symétriques}
%---------------------------------------------------------------------------------------------------------------------------
\cite{fJhCTE}.

Soit \( \eK\) un corps de caractéristique différente\footnote{Le truc de la caractéristique deux est que \( a=-a\) n'implique pas \( a=0\).} de \(2\). Le groupe \( S_n\) agit sur l'anneau \( \eK[T_1,\ldots, T_n]\) par
\begin{equation}
    (\sigma\cdot f)(T_1,\ldots, T_n)=f\big( T_{\sigma(1)},\ldots, T_{\sigma(n)} \big).
\end{equation}
On peut vérifier que c'est un action.

\begin{definition}
    Un polynôme \( Q\) en \( n\) indéterminées est 
    \begin{enumerate}
        \item
            \defe{symétrique}{polynôme!symétrique}\index{symétrique!polynôme} si \( Q=\sigma\cdot Q\) pour tout \( \sigma\in S_n\);
        \item
            \defe{alterné}{polynôme!alterné}\index{alterné!polynôme} si \( \sigma\cdot Q=\epsilon(\sigma)Q\) pour tout \( \sigma\in S_n\);
        \item
            \defe{semi-symétrique}{semi-symétrique!polynôme}\index{polynôme!semi-symétrique} si \( \sigma\cdot Q=Q\) pour tout \( \sigma\in A_n\)
    \end{enumerate}
\end{definition}
Le polynôme \( T_1+T_2\) est symétrique; le polynôme \( T_1+T_2^2\) ne l'est pas. 

\begin{example}
    Le déterminant de Vandermonde (proposition \ref{PropnuUvtj}) est alterné, semi-symétrique et non symétrique. Le fait qu'il soit alterné est le fait qu'il soit un déterminant. Étant donné qu'il est alterné, il est semi-symétrique parce que sur \( A_n\), nous avons \( \epsilon=1\). Étant donné qu'il est alterné, il change de signe sous l'action des éléments impairs de \( S_n\) et n'est donc pas symétrique.
\end{example}

\begin{proposition}\index{action de groupe} \label{PropUDqXax}
    Un polynôme semi-symétrique \( f\in \eK[T_1,\ldots, T_n]\) se décompose de façon unique en
    \begin{equation}
        f=P+VQ
    \end{equation}
    où \( P\) et \( Q\) sont deux polynômes symétriques.
\end{proposition}
\index{groupe!permutation}
\index{polynôme!symétrique}

\begin{proof}

    Nous commençons par prouver l'unicité en montrant que si \( f=PVQ\) avec \( P\) et \( Q\) symétrique, alors \( P\) et \( Q\) sont donnés par des formules explicites en termes de \( f\).


    Si \( \sigma_1\) et \( \sigma_2\) sont deux permutations impaires de \( \{ 1,\ldots, n \}\), alors \( \sigma_1\cdot f=\sigma_2\cdot f\) parce que l'élément \( \sigma_2^{-1}\sigma_1\) est pair (proposition \ref{ProphIuJrC}), de telle sorte que \( \sigma_2^{-1}\sigma_1\cdot f=f\). Nous posons donc \( g=\tau\cdot f\) où \( \tau\) est une permutation impaire quelconque -- par exemple une transposition.

    Vu que \( V\) est alternée et que \( \tau\) est une transposition nous avons
    \begin{equation}
        g=\tau\cdot f=P-VQ.
    \end{equation}
    Donc \( f+g=2P\) et \( f-g=2VQ\). Cela donne \( P\) et \( Q\) en terme de \( f\) et \( g\), et donc l'unicité.

    Attention : cela ne donne pas un moyen de prouver l'existence parce que rien ne prouve pour l'instant que \( f-g\) peut effectivement être écrit sous la forme \( VQ\), c'est à dire que \( f-g\) soit divisible par \( V\). C'est cela que nous allons nous atteler à démontrer maintenant.

    Nous commençons par prouver que \( f+g\) est symétrique et \( f-g\) alterné. Si \( \sigma\) est une transposition,
    \begin{equation}
        \sigma\cdot(f+g)=\sigma\cdot f+\sigma\tau\cdot f=g+f
    \end{equation}
    parce que \( \sigma\tau\) est pair. De la même façon,
    \begin{equation}
        \sigma\cdot(f-g)=g-f=\epsilon(\sigma)(f-g).
    \end{equation}
    Dans les deux cas nous concluons en utilisant le fait que toute permutation est un produit de transpositions (proposition \ref{PropPWIJbu}) et que \( \epsilon\) est un homomorphisme.

    Soient maintenant deux entiers \( h<k\) dans \( \{ 1,\ldots, n \}\) et l'anneau
    \begin{equation}
        \big( \eK[T_1,\ldots, \hat T_k,\ldots, T_n] \big)[T_k].
    \end{equation}
    Cet anneau contient le polynôme \( T_k-T_h\) où \( T_k\) est la variable et \( T_h\) est un coefficient. Nous faisons la division euclidienne de \( f-g\) par  \( T_k-T_h\) parce que nous avons dans l'idée de faire arriver le déterminant de Vandermonde et donc le produit de toutes les différences \( T_k-T_h\) :
    \begin{equation}    \label{EqSHdgrG}
        f-g=(T_k-T_h)q+r
    \end{equation}
    où \( \deg_{T_k}r<1\), c'est à dire que \( r\) ne dépends pas de \( T_k\). Nous revoyons maintenant l'égalité \eqref{EqSHdgrG} dans \( \eK[T_1,\ldots, T_n]\) et nous y appliquons la transposition \( \tau_{kh}\). Nous savons que \( \tau_{kh}(f-g)=-(f-g)\) et \( \tau_{kh}(T_k-T_h)=-(T_k-T_h)\), et donc
    \begin{equation}    \label{EqVOhjKB}
        -(f-g)=-(T_k-T_h)\tau_{kh}\cdot   q+\tau_{kh}\cdot r
    \end{equation}
    où \(\tau_{kh}\cdot r\) ne dépend pas de \( T_h\). Nous appliquons à \eqref{EqVOhjKB} l'application
    \begin{equation}
        \begin{aligned}
            t\alpha\colon \eK[T_1,\ldots, T_n]&\to \eK[T_1,\ldots, \hat T_k,\ldots, T_n] \\
            \alpha(PT_1,\ldots, \hat T_k,\ldots, T_n)&=P(T_1,\ldots, T_h,\ldots, T_n). 
        \end{aligned}
    \end{equation}
    Cette application vérifie \( \alpha\big( \tau_{kh}\cdot r \big)=\alpha(r)\) et nous avons
    \begin{equation}
        -\alpha(f-g)=\alpha(r).
    \end{equation}
    Puis en appliquant \( \alpha\) à la relation \( f-g=(T_k-T_h)q+r\), nous trouvons
    \begin{equation}
        \alpha(f-g)=\alpha(r),
    \end{equation}
    et par conséquent \( \alpha(r)=0\). Ici nous utilisons l'hypothèse de caractéristique différente de deux. Dire que \( \alpha(r)=0\), c'est dire que \( r\) est divisible par \( T_k-T_h\), mais \( r\) étant de degré zéro en \( T_k\), nous avons \( r=0\). Par conséquent \( T_k-T_h\) divise \( f-g\) pour tout \( h<k\), et nous pouvons définir un polynôme \( Q\) par
    \begin{equation}    \label{EqrnbgdA}
        f-g=2Q\prod_{h<k}\prod_{k\leq n}(T_k-T_h)=2Q(T_1,\ldots, T_n)V(T_1,\ldots, T_n),
    \end{equation}
    où nous avons utilisé la formule du déterminant de Vandermonde de la proposition \ref{PropnuUvtj}.

    Étant donné que \( f+g\) est un polynôme symétrique, nous allons aussi poser \( f+g=2P\) avec \( P\) symétrique.

    Montrons à présent que \( Q\) est un polynôme symétrique. Soit \( \sigma\in S_n\); vu que nous savons déjà que \( f-g\) est alternée, nous avons
    \begin{equation}    \label{EqpSPEyq}
        \sigma\cdot (f-g)=\epsilon(\sigma)(f-g)=\epsilon(\sigma)2QV,
    \end{equation}
    Mais en appliquant \( \sigma\) à l'équation \eqref{EqrnbgdA},
    \begin{subequations}
        \begin{align}
            \sigma\cdot (f-g)&=2(\sigma\cdot V)(T_1,\ldots, ,T_n)(\sigma\cdot Q)(T_1,\ldots,T_n)\\
            &=2\epsilon(\sigma)V(T_1,\ldots, T_n)(\sigma\cdot Q)(T_1,\ldots, T_n).
        \end{align}
    \end{subequations}
    En égalisant avec \eqref{EqpSPEyq} et en se souvenant que l'anneau \( \eK[T_1,\ldots, T_n]\) était intègre (théorème \ref{ThoBUEDrJ}), nous simplifions par \( 2\epsilon(\sigma)V\) pour obtenir
    \begin{equation}
        Q=\sigma\cdot Q,
    \end{equation}
    c'est à dire que \( Q\) est symétrique.

    Au final nous avons \( f+q=2P\) et \( f-g=2VQ\) avec \( P\) et \( Q\) symétriques. En faisant la somme,
    \begin{equation}
        f=P+VQ.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme symétrique élémentaire}
%---------------------------------------------------------------------------------------------------------------------------

Le \( k\)ième \defe{polynôme symétrique élémentaire}{élémentaire!polynôme symétrique}\index{polynôme!symétrique!élémentaire} à \( n\) inconnues est le polynôme est
\begin{equation}
    \sigma_k(T_1,\ldots, T_n)=\sum_{s\in F_k}\prod_{i=1}^kT_{s(i)}
\end{equation}
où \( F_k\) est l'ensemble des fonctions strictement croissantes \( \{ 1,2,\ldots, k \}\to\{ 1,2,\ldots, n \}\). Une autre façon de décrire ces polynômes élémentaires est
\begin{equation}
    \sigma_k=\sum_{1\leq i_1<\ldots<i_k\leq n}X_{i_1}\ldots X_{i_k}.
\end{equation}
Par exemple
\begin{subequations}
    \begin{align}
        \sigma_1(T_1,\ldots, T_n)&=T_1+T_2+\ldots +T_n\\
        \sigma_2(T_1,\ldots, T_n)&=T_1T_2+\ldots +T_1T_n+T_2T_3+\ldots +T_2T_n+\ldots +T_{n-1}T_n\\
        \sigma_n(T_1,\ldots, T_n)&=T_1\ldots T_n.
    \end{align}
\end{subequations}
En particulier, \( \sigma_2(x,y,z)=xy+yz+xz\).

\begin{theorem}[\cite{PoloPolSym}]  \label{TholReBiw}
    Si \( Q\) est un polynôme symétrique en \( T_1,\ldots, T_n\), alors il existe un et un seul polynôme \( P\) en \( n\) indéterminées tel que
    \begin{equation}
        Q(T_1,\ldots, T_n)=P\big( \sigma_1(T_1,\ldots, T_n),\ldots, \sigma_n(T_1,\ldots, T_n) \big).
    \end{equation}
\end{theorem}
%TODO : la preuve de ce théorème

\begin{example}
    Nous voulons décomposer \( P(x,y,z)=x^3+y^3+z^3\) en polynômes symétriques élémentaires, c'est à dire en
    \begin{subequations}
        \begin{numcases}{}
            \sigma_1=x+y+z\\
            \sigma_2=xy+xz+yz\\
            \sigma_3=xyz.
        \end{numcases}
    \end{subequations}
    Étant donné que \( P\) est de degré \( 3\), les seules combinaisons des \( \sigma_i\) qui peuvent intervenir sont \( \sigma_1^3\), \( \sigma_1\sigma_2\) et \( \sigma_3\). Étant donné que dans \( P\) le coefficient de \( x^3\) est un, il est obligatoire d'avoir un coefficient \( 1\) devant \( \sigma_1^3\). Nous le calculons :
    \begin{verbatim}
----------------------------------------------------------------------
| Sage Version 4.8, Release Date: 2012-01-20                         |
| Type notebook() for the GUI, and license() for information.        |
----------------------------------------------------------------------
sage: var('x,y,z')
(x, y, z)
sage: P=x**3+y**3+z**3  
sage: S1=x+y+z    
sage: S2=x*y+x*z+y*z
sage: S3=x*y*z
sage: (S1**3).expand()
x^3 + 3*x^2*y + 3*x^2*z + 3*x*y^2 + 6*x*y*z + 3*x*z^2 + y^3 + 3*y^2*z + 3*y*z^2 + z^3
sage: (S1**3-P).expand()
3*x^2*y + 3*x^2*z + 3*x*y^2 + 6*x*y*z + 3*x*z^2 + 3*y^2*z + 3*y*z^2
x^3 + 3*x^2*y + 3*x^2*z + 3*x*y^2 + 6*x*y*z + 3*x*z^2 + y^3 + 3*y^2*z + 3*y*z^2 + z^3
    \end{verbatim}
    Dans la différence \( \sigma_1^3-P\) nous voyons que le terme en \( xyz\) est \( 6xyz\); par conséquent nous savons que le coefficient de \( \sigma_3\) sera \( -6\). Il nous reste :
    \begin{verbatim}
sage: (S1**3+6*S3-P).expand()
3*x^2*y + 3*x^2*z + 3*x*y^2 + 12*x*y*z + 3*x*z^2 + 3*y^2*z + 3*y*z^2    
    \end{verbatim}
    que nous identifions facilement avec \( 3\sigma_1\sigma_2\). Nous avons donc
    \begin{equation}
        P=\sigma_1^3-3\sigma_1\sigma_2+3\sigma_3.
    \end{equation}
\end{example}


\begin{lemma}[\cite{fJhCTE}]    \label{LemSoXCQH}
    Soit \( \eK\) une extension de degré \( \delta\) de \( \eQ\) et \( P\in \eK[T_1,\ldots, T_m]\). Alors il existe \( \bar P\in \eQ[T_1,\ldots, T_m]\) tel que
    \begin{enumerate}
        \item
            $\deg\bar P=\delta\deg(P)$
        \item
            pour tout \( (z_1,\ldots, z_m)\in \eC^m\) tel que \( P(z_1,\ldots, z_m)=0\), on a \( \bar P(z_1,\ldots, z_m)=0\).
    \end{enumerate}
\end{lemma}
\index{polynôme!symétrique}
\index{polynôme!racines}
\index{extension!de corps}
\index{corps!extension}

\begin{proof}
    En vertu de la proposition \ref{PropUmxJVw} et de l'exemple \ref{ExvQTyBl}, \( \eK\) est une extension séparable de \( \eQ\), et donc vérifie le théorème de l'élément primitif (\ref{ThoORxgBC}). Il existe \( \theta\in \eK\) tel que \( \eK=\eQ(\theta)\). Soit \( P_{\theta}\in\eQ[X]\) le polynôme minimal de \( \theta\). L'extension \( \eK\) étant de degré \( \delta\), et \( \theta\) étant un générateur, une base de \( \eK\) comme espace vectoriel sur \( \eQ\) est 
    \begin{equation}
        \{ 1,\theta,\ldots, \theta^{\delta-1} \}.
    \end{equation}
    Mais par ailleurs la proposition \ref{PropURZooVtwNXE}\ref{ItemJCMooDgEHajiv} nous indique qu'une base de \( \eQ(\theta)\) sur \( \eQ\) est donnée par
    \begin{equation}
        \{ 1,\theta,\ldots, \theta^{n-1} \}
    \end{equation}
    où \( n\) est le degré de \( P_{\theta}\). Donc \( P_{\theta}\) est de degré \( \delta\). Nous nommons \( \theta_1,\ldots, \theta_{\delta}\) les racines de \( P_{\theta}\) dans un corps de décomposition. Ici nous notons \( \theta=\theta_1\) et nous ne prétendons pas que \( \theta_k\in \eK\). Notons que ces \( \theta_i\) sont toutes des racines simples de \( P_{\theta}\), sinon nous aurions un facteur irréductible \( (X-\theta_k)^2\), et \( P_{\theta}\) ne serait pas irréductible sur \( \eQ\).

    Soit \( \sigma_k\) le morphisme canonique
    \begin{equation}
        \begin{aligned}
            \sigma_k\colon \eQ(\theta)&\to \eQ(\theta_k) \\
            \sum_i q_i\theta^i&\mapsto \sum_iq_i\theta_k^i 
        \end{aligned}
    \end{equation}
    Nous avons \( \sigma_1\colon \eK\to \eK\) qui est l'identité.

    Notons \( N\) le degré du polynôme \( P\in \eK[T_1,\ldots, T_m]\) dont il est question dans l'énoncé. Nous le décomposons alors en
    \begin{equation}
        P=\sum_{l=0}^N\sum_{i=1}^mc_{il}T_i^l
    \end{equation}
    avec \( c_{il}\in \eK\). Nous voyons \( c_{i,.}\) comme un élément de \( \eK^m\) et donc nous écrivons\footnote{Il me semble qu'il manque la somme sur \( i\) dans \cite{fJhCTE}.}
    \begin{equation}
        P=\sum_{l=0}^N\sum_{i=1}^m c_l(\theta)_iT_i^l
    \end{equation}
    où \( c_l\in \eQ[X]^m\). Nous pouvons choisir \( \deg(c_l)<\delta\) parce que les puissances plus grandes de \( \theta\) ne génèrent rien de nouveau.

    Nous posons aussi
    \begin{equation}
        P^{\sigma_k}=\sum_{l,i} c_l(\theta_k)_iT_i^l\in \eQ(\theta_k)[T_1,\ldots, T_m],
    \end{equation}
    et \( \bar P=PP^{\sigma_2}\ldots P^{\sigma_k}\). Le coefficient de \( T_i^l\) dans \( \bar P\) est
    \begin{equation}
        \bar c_l(\theta_1,\ldots, \theta_{\delta})_i=\sum_{l_1+\ldots +l_{\delta}=l}c_{l_1}(\theta_1)_i\ldots c_{l_{\delta}}(\theta_{\delta})_i.
    \end{equation}
    Ce dernier est un polynôme en les \( \theta_k\) à coefficients dans \( \eQ\). Qui plus est, c'est un polynôme symétrique. En effet un terme contenant \( \theta_k^a\theta_l^b\) provenant de \( c_{l_i}(\theta_k)c_{l_j}(\theta_l)\) a un terme correspondant \( \theta_k^b\theta_l^a\) provenant de \( c_{l_j}(\theta_k)c_{l_i}(\theta_l)\).

    C'est donc le moment d'utiliser le théorème \ref{TholReBiw} à propos des polynômes symétriques élémentaires qui nous dit que les coefficients de \( \bar P\) sont en réalité des polynômes en ceux de \( P_{\theta}\) qui sont dans \( \eQ\). Donc \( \bar P\in \eQ[T_1,\ldots, T_m]\). Par ailleurs nous avons que
    \begin{equation}
        \deg(\bar P)=\delta \deg(P)
    \end{equation}
    parce que \( \bar P\) est le produit de \( \delta\) «copies»  de \( P\). De plus \( P=P^{\sigma_1}\) divise \( \bar P \) donc on a bien que si \( P(z)=0\) alors \( \bar P(z)=0\). Le polynôme \( \bar P\) est celui que nous cherchions. 
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Relations coefficients racines}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Relations coeffitients-racines] \label{ThoOQRgjpl}
    Soit le polynôme \( P=a_nX^n+\ldots +a_1X+a_0\) et \( r_i\) ses \( n\) racines. Alors nous avons pour chaque \( 1\leq k\leq n\) la relation
    \begin{equation}
        \sigma_k(r_1,\ldots, r_n)=(-1)^k\frac{ a_{n-k} }{ a_n }.
    \end{equation}
\end{theorem}
\index{relations!coefficient-racines}
\index{polynôme!symétrique!élémentaire}

%TODO : citer Wikipédia pour l'exemple suivant.
%TODO : ici aussi il faudra faire référence au théorème sur le fait qu'un polynôme ait toutes ses racines dans \eC.

\begin{example} \label{ExHIfHhBr}
    Soit le polynôme
    \begin{equation}
        P(x)=x^3+2x^2+3x+4
    \end{equation}
    et ses racines que nous nommons \( a,b,c\). Nous voudrions calculer \( a^2+b^2+c^2\). D'abord nous décomposons \( Q(a,b,c)=a^2+b^2+c^2\) en polynômes symétriques élémentaires : \( Q(a,b,c)=\sigma_1(a,b,c)^2-2\sigma_2(a,b,c)\).

    Mais les relations coefficients-racines (théorème \ref{ThoOQRgjpl}) nous donnent \( \sigma_1(a,b,c)=-2\) et \( \sigma_2(a,b,c)=3\), donc
    \begin{equation}
        a^2+b^2+c^2=(-2)^2-2\cdot 3=-2.
    \end{equation}

    Nous pouvons en avoir une vérification directe en calculant explicitement les racines (ce qui est possible pour le degré \( 3\)) :
    \lstinputlisting{VAYVmNRpolynomeSym.py}

    Notez qu'il faut un peu chipoter pour isoler les solutions depuis la réponse de la fonction \info{solve}.

\end{example}

En suivant le même cheminement que dans l'exemple, si \( P\) est un polynôme de degré \( n\) et si \( r_i\) sont ses racines, il est facile de calculer \( Q(r_1,\ldots, r_n)\) pour n'importe quel polynôme symétrique \( Q\)

\begin{proposition}[Annulation de fonctions polynômiales\cite{WARooZoFOBn}] \label{PropTETooGuBYQf}
    Soit \( \eK\) un corps et \( P\) un polynôme à \( n\) indéterminées. Nous supposons que \(P\) s'annule sur un ensemble de la forme \( A_1\times\cdots\times A_n\) avec \( \Card(A_j)>\deg_{X_j}(P)\) pour tout \( j\). Alors \( P=0\).

    De plus si \( P=0\) alors tous ses coefficients sont nuls\footnote{L'intérêt de cela est qu'un polynôme de \( \eZ[X_1,\ldots, X_n]\) peut s'évaluer sur un élément de n'importe quel corps; il restera le polynôme nul.}.
\end{proposition}

\begin{proof}
    Nous prouvons le résultat par récurrence sur le nombre \( n\) d'indéterminées. Si \( n=1\), cela est le théorème \ref{ThoLXTooNaUAKR}. Nous classons les monômes du polynôme \( P\) par ordre de puissance de \( X_n\) et nous le factorisons :
    \begin{equation}
        P=\sum_{i=1}^mP_iX_n^i
    \end{equation}
    avec \( P_i\in \eK[X_1,\ldots, X_{n-1}]\). Soit \( (a_1,\ldots, a_{n-1})\in A_1\times \ldots \times A_{n-1}\) et posons
    \begin{equation}
        Q(T)=P(a_1,\ldots, a_{n-1},T)= \sum_{i=1}^mP_i(a_1,\ldots, a_{n-1})T^i.
    \end{equation}
    Le polynôme \( Q\) s'annule sur \( A_n\) avec \( \deg(Q)=\deg_{X_n}(P)<\Card(A_n)\) et le théorème \ref{ThoLXTooNaUAKR} nous donne \( Q=0\). Or les coefficients des différentes puissances de \( T\) dans \( Q(T) \) sont les \( P_i(a_1,\ldots, a_{n-1})\); ils sont donc nuls.

    Nous avons montré que le polynôme \( P_i\) s'annule pour tout élément de \( A_1\times \ldots \times A_{n-1}\), mais nous avons
    \begin{equation}
        \deg_{X_j}(P_i)\leq \deg_{X_j}P<\Card(A_j),
    \end{equation}
    donc l'hypothèse de récurrence donne \( P_i=0\). Par suite, \( P=0\) également.
\end{proof}

\begin{example}\label{ExGRHooBNpjSP}
    Un polynôme à plusieurs variables peut s'annuler en une infinité de points sans être nul. Par exemple le polynôme \( X^2+Y^2-1\in\eR[X,Y]\) s'annule sur tout un cercle de \( \eR^2\) mais n'est pas nul, loin s'en faut.
\end{example}
