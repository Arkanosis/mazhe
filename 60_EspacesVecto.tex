% This is part of Mes notes de mathématique
% Copyright (c) 2011-2015
%   Laurent Claessens, Carlotta Donadello
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Polynôme d'endomorphismes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit \( A\) un anneau commutatif et \( \eK\), un corps commutatif. L'injection canonique \( A\to A[X]\) se prolonge en une injection
\begin{equation}
   \eM(A)\to\eM\big( A[X] \big).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes d'endomorphismes}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( u\in\End(E)\) où \( E\) est un \( \eK\)-espace vectoriel. Nous considérons l'application
\begin{equation}    \label{EqOVKooeMJuv}
    \begin{aligned}
        \varphi_u\colon \eK[X]&\to \End(E) \\
        P&\mapsto P(u). 
    \end{aligned}
\end{equation}
L'image de \( \varphi_u\) est un sous-espace vectoriel. En effet si \( A=\varphi_u(P)\) et \( B=\varphi_u(Q)\), alors \( A+B=\varphi_u(P+Q)\) et \( \lambda A=(\lambda P)(u)\). En particulier c'est un espace fermé.

Soit \( u\) un endomorphisme d'un \( \eK\)-espace vectoriel \( E\) et \( P\), un polynôme. Nous disons que \( P\) est un polynôme \defe{annulateur}{polynôme!annulateur} de \( u\) si \( P(u)=0\) en tant que endomorphisme de \( E\).

\begin{lemma}       \label{LemQWvhYb}
    Si \( P\) et \( Q\) sont des polynômes dans \( \eK[X]\) et si \( u\) est un endomorphisme d'un \( \eK\)-espace vectoriel \( E\), nous avons
    \begin{equation}
        (PQ)(u)=P(u)\circ Q(u).
    \end{equation}
\end{lemma}

\begin{proof}
    Si \( P=\sum_i a_iX^i\) et \( Q=\sum_j b_jX^j\), alors le coefficient de \( X^k\) dans \( PQ\) est
    \begin{equation}        \label{EqCoefGPyVcv}
        \sum_la_lb_{k-l}.
    \end{equation}
    Par conséquent \( (PQ)(u)\) contient \( \sum_la_lb_{k-l}u^k\). Par ailleurs \( P(u)\circ Q(u)\) est donné par
    \begin{equation}
        \sum_ia_iu^i\left( \sum_jb_ju^j \right)(x)=\sum_{ij}a_ib_ju^{i+j}(x).
    \end{equation}
    Le coefficient du terme en \( u^k\) est bien le même que celui donné par \eqref{EqCoefGPyVcv}.
\end{proof}

\begin{theorem}[Décomposition des noyaux ou lemme des noyaux]       \label{ThoDecompNoyayzzMWod}
    Soit \( u\) un endomorphisme du \( \eK\)-espace vectoriel \( E\). Soit \( P\in\eK[X]\) un polynôme tel que \( P(u)=0\). Nous supposons que \( P\) s'écrive comme le produit \( P=P_1\ldots P_n\) de polynômes deux à deux étrangers\footnote{Définition \ref{DefDSFooZVbNAX}.}. Alors
    \begin{equation}
        E=\ker P_1(u)\oplus\ldots\oplus\ker P_n(u).
    \end{equation}
    De plus les projecteurs associés à cette décomposition sont des polynômes en \( u\).
\end{theorem}
\index{lemme!des noyaux}
Ce résultat est utilisé pour prouver que toute représentation est décomposable en représentations irréductibles, proposition \ref{PropHeyoAN} ainsi que pour le théorème \ref{ThoDigLEQEXR} qui dit que si le polynôme minimal d'un endomorphisme est scindé à racine simple alors il est diagonalisable.

\begin{proof}
    Nous posons 
    \begin{equation}
        Q_i=\prod_{j\neq i}P_i.
    \end{equation}
    Par le lemme \ref{LemuALZHn} ces polynômes sont étrangers entre eux et le théorème de Bézout (théorème \ref{ThoBezoutOuGmLB}) donne l'existence de polynômes \( R_i\) tels que
    \begin{equation}
        R_1Q_1+\ldots+R_nQ_n=1.
    \end{equation}
    Si nous appliquons cette égalité à \( u\) et ensuite à \( x\in E\) nous trouvons
    \begin{equation}        \label{EqqVcpUy}
        \sum_{i=1}^n(R_iQ_i)(u)(x)=x,
    \end{equation}
    et en particulier si nous posons \( E_i=\Image\big(P_iQ_i(u)\big)\) nous avons
    \begin{equation}
        E=\sum_{i=1}^nE_i.
    \end{equation}
    Cette dernière somme n'est éventuellement pas une somme directe. Si \( i\neq j\), alors \( Q_iQ_j\) est multiple de \( P\) et nous avons, en utilisant le lemme \ref{LemQWvhYb}, 
    \begin{equation}
        (R_iQ_i)(u)\circ (R_jQ_j)(u)=\big( R_iQ_iR_jQ_j \big)(u)=S_{ij}(u)\circ P(u)=0
    \end{equation}
    où \( S_{ij}\) est un polynôme. 

    Nous pouvons voir \( E\) comme un \( \eK\)-module et appliquer le théorème \ref{ThoProjModpAlsUR}. Les opérateurs \( R_iQ_i(u)\) ont l'identité comme somme et sont orthogonaux, et nous avons donc la décomposition en somme directe :
    \begin{equation}
        E=\bigoplus_{i=1}^nR_iQ_i(u)E.
    \end{equation}

    Afin de terminer la preuve, nous devons montrer que \( R_iQ_i(u)E=\ker P_i(u)\). D'abord nous avons
    \begin{equation}
        P_iR_iQ_i(u)=(R_iP)(u)=R_i(u)\circ P(u)=0,
    \end{equation}
    par conséquent \( \Image(R_iQ_i(u))\subset \ker P_i(u)\). Pour obtenir l'inclusion inverse, nous reprenons l'équation \eqref{EqqVcpUy} avec \( x\in\ker P_i(u)\). Elle se réduit à
    \begin{equation}
        (R_iQ_i)(u)x=x.
    \end{equation}
    Par conséquent \( x\in\Image\big( R_iQ_i(u) \big)\).
\end{proof}

\begin{corollary}   \label{CorKiSCkC}
    Soit \( E\), un \( \eK\)-espace vectoriel de dimension finie et \( f\), un endomorphisme semi-simple dont la décomposition du polynôme minimal \( \mu_f\) en facteurs irréductibles sur \( \eK[X]\) est \( \mu_f=M_1^{\alpha_1}\cdots M_r^{\alpha_r}\). Si \( F\) est un sous-espace stable par \( f\), alors
    \begin{equation}
        F=\bigoplus_{i=1}^r\ker M_i^{\alpha_i}(f)\cap F
    \end{equation}
\end{corollary}

\begin{proof}
    Nous posons \( E_i=\ker M_i^{\alpha_i}(f)\) et \( F_i=E_i\cap F\). Les polynômes \( M_i^{\alpha_i}\) sont deux à deux étrangers et \( \mu_f(f)=0\), donc le lemme des noyaux (\ref{ThoDecompNoyayzzMWod}) s'applique et
    \begin{equation}
        E=E_1\oplus\ldots\oplus E_r.
    \end{equation}
    Nous pouvons décomposer \( x\in F\) en termes de cette somme :
    \begin{equation}     \label{EqbBbrdi}
        x=x_1+\ldots +x_r
    \end{equation}
    avec \( x_i\in E_i\). Toujours selon le lemme des noyaux, les projections sur les espaces \( E_i\) sont des polynômes en \( f\). Par conséquent \( F\) est stable sous toutes ces projections \( \pr_i\colon E\to E_i\), et en appliquant \( \pr_i\) à \eqref{EqbBbrdi}, \( \pr_i(x)=x_i\). Vu que \( x\in F\), le membre de gauche est encore dans \( F\) et \( x_i\in E_i\cap F\). Nous avons donc
    \begin{equation}
        F\subset\bigoplus_{i=1}^rF_i.
    \end{equation}
    L'inclusion inverse est immédiate parce que \( F_i\subset F\) pour chaque \( i\).
\end{proof}

\begin{lemma}   \label{LemVISooHxMdbr}
    Si \( x\) est un vecteur propre de valeur propre \( \lambda\) pour l'endomorphisme \( u\) et si \( P\) est un polynôme, alors \( x\) est vecteur propre de \( u\) pour la valeur propre \( P(\lambda)\).
\end{lemma}

\begin{proof}
    C'est un simple calcul de \( P(u)x\) en ayant noté \( P(X)=\sum_{k=0}^nc_kX^n\) :
    \begin{equation}
        P(u)x=\sum_{k=0}^nc_ku^k(x)=\sum_{k=0}^nc_k\lambda^ku=P(\lambda)x.
    \end{equation}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Calcul effectif de l'exponentielle d'une matrice}
%---------------------------------------------------------------------------------------------------------------------------

Nous reprenons l'exemple de \cite{MneimneReduct}. Soit \( A\) une matrice dont le polynôme minimum s'écrit
\begin{equation}
    P(X)=(X-1)^2(X-2).
\end{equation}
Par le théorème \ref{ThoDecompNoyayzzMWod} de décomposition des noyaux nous avons
\index{théorème!décomposition des noyaux!et exponentielle de matrice}
\begin{equation}
    E=\ker(A-1)^2\oplus\ker(A-2).
\end{equation}
En suivant les notations de ce théorème nous avons \( P_1(X)=(X-1)^2\), \( P_2(X)=X-2\) et
\begin{subequations}
    \begin{align}
        Q_1(X)&=X-2\\
        Q_2(X)&=(X-1)^2.
    \end{align}
\end{subequations}
Les polynômes \( R_i\) dont l'existence est assurée par le théorème de Bézout sont
\begin{equation}
    \begin{aligned}[]
        R_1(X)&=-X\\
        R_2(X)&=1.
    \end{aligned}
\end{equation}
Nous avons
\begin{equation}
    R_1Q_1+R_2Q_2=1.
\end{equation}
Le projecteur \( p_i\) sur \( \ker P_i\) est \( R_iQ_i\) :
\begin{equation}
    \begin{aligned}[]
        p_1&=-A(A-2)=\pr_{\ker(u-1)^2}\\
        p_2&=(A-1)^2=\pr_{\ker(u-2)}.
    \end{aligned}
\end{equation}
Passons maintenant au calcul de l'exponentielle. Nous avons évidemment
\begin{equation}
    e^A=e^Ap_1+e^Ap_2.
\end{equation}
Étant donné que \( p_1\) est le projecteur sur le noyau de \( (A-1)^2\), nous avons
\begin{equation}
    e^Ap_1=ee^{A-1}p_1=ep_1+e(u-1)1=ep_1=-Ae(A-2).
\end{equation}
En effet \( e^{A-1}p_1=\sum_{k=0}^{\infty}(A-1)^k\circ p_1\). De la même façon nous avons
\begin{equation}
    e^Ap_2=e^2e^{A-2}p_2=e^2p_2=e^2(A-1)^2.
\end{equation}
Au final,
\begin{equation}
    e^A=-Ae(A-2)+e^2(A-1)^2.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme minimal et minimal ponctuel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemmaDef}        \label{DefooOHUXooNkPWaB}
    Soit un endomorphisme \( f\colon E\to E\) d'un \( \eK\)-espace vectoriel de dimension finie. Il existe un unique polynôme annulateur normalisé de degré minimum.

    Il est nommé le \defe{polynôme minimal}{polynôme!minimal} de \( f\) et il est noté \( \mu_f\) ou simplement \( \mu\) lorsque la dépendance en \( f\) est claire.
\end{lemmaDef}

\begin{proof}
    Pour l'unicité, soient \( P\) et \( Q\) deux polynômes annulateur de \( f\) de même degré \( N\) et ayant tous deux \( 1\) comme coefficient de \( x^N\). Alors \( P-Q\) est de degré \( N-1\) tout en étant encore annulateur.

    Pour l'existence, les endomorphismes \( \id\), \( f\), \( f^2\), \ldots ne peuvent pas être tous linéairement indépendants parce que la dimension de \( \End(E)\) est finie. Il existe donc un nombre \( N\) et des coefficients \( a_k\) tels que \( \sum_{k=0}^Na_kf^k=0\). Le polynôme \( P(X)=\sum_{k=0}^Na_kX^k\) est donc annulateur de \( f\).

    Une autre façon de le dire est que l'application linéaire \( \varphi\colon \eK[X]\to \End(E)\) donnée par \( \varphi(P)=P(f)\) est un endomorphisme d'un espace vectoriel de dimension infinie vers un espace vectoriel de dimension finie. Il ne peut donc pas être injectif et possède donc un noyau non réduit à zéro.
\end{proof}

\begin{remark}
    La preuve donnée ci-dessus montre que \( \deg(\mu)\leq \dim(E)^2\). Comme conséquence du théorème de Caley-Hamilton \ref{ThoCalYWLbJQ} nous verrons qu'en réalité le degré du polynôme minimal est majoré par la dimension de l'espace.
\end{remark}

\begin{example}[Pas en dimension infinie]
    L'endomorphisme de dérivation
\end{example}


Dans la suite, l'endomorphisme \( f\) du \( \eK\)-espace vectoriel \( E\) de dimension \( n\) est fixé. Pour \( x\in E\) nous notons
\begin{equation}            \label{EqooOAYDooEpZELo}
    E_x=\{ P(f)x\tq P\in \eK[X] \}.
\end{equation}
Nous considérons le morphisme d'algèbres
\begin{equation}
    \begin{aligned}
        \varphi\colon \eK[X]&\to \End(E) \\
        P&\mapsto P(f) 
    \end{aligned}
\end{equation}
et si \( x\in E\) est donné nous considérons le morphisme de \( \eK\)-espaces vectoriels
\begin{equation}
    \begin{aligned}
        \varphi_x\colon \eK[X]&\to E \\
        P&\mapsto P(f)x. 
    \end{aligned}
\end{equation}
Les noyaux de ces applications sont des idéaux, entre autres par le lemme \ref{LemQWvhYb}. Ils ont donc un unique générateur unitaire (chacun) par le théorème \ref{ThoCCHkoU}. En termes de vocabulaire, l'ensemble
\begin{equation}
    \ker(\phi)=\{  Q\in\eK[X]\tq Q(f)=0  \}
\end{equation}
est l'\defe{idéal annulateur}{polynôme!annulateur} de \( f\) et un polynôme \( Q\) tel que \( Q(f)=0\) est une polynôme annulateur de \( f\).

\begin{definition}
    Le générateur unitaire de \( \ker(\varphi_x)\) est le \defe{polynôme minimal ponctuel}{polynôme!minimal!ponctuel} de \( f\) en \( x\). Il sera noté \( \mu_{f,x}\) ou \( \mu_x\) lorsque la dépendance en \( f\) est claire dans le contexte.
\end{definition}
Nous notons \( \mu\) le générateur unitaire du noyau de \( \varphi\) et \( \mu_x\) celui de \( \varphi_x\). Vu que \( \mu\in\ker(\varphi_x)\) pour tout \( x\) nous avons\( \mu_x\divides \mu\) pour tout \( x\).

\begin{example}[Pas en dimension infinie]       \label{ExooDTUJooIMqSKn}
    En dimension infinie, il n'y a pas toujours de polynôme annulateur. Si \( E\) est un espace vectoriel de dimension infine ayant une base dénombrable \( \{ e_i \}_{i\in \eN}\) alors l'opérateur donné par \( f(e_i)=e_{i+1}\) n'a pas de polynôme annulateur. Même pas ponctuel en quel que point que ce soir.

    De même l'opérateur donné par \( g(e_1)=0\) et \( g(e_i)=e_{i-1}\) si \( i\neq 1\) n'a pas de polynôme annulateur, mais il a un polynôme annulateur ponctuel évident en \( x=e_1\). L'exemple \ref{ExooLRHCooMYLQTU} donnera un habillage à peine subtil à cet exemple.
\end{example}

\begin{proposition}     \label{PropAnnncEcCxj}
    Si \( P\) est un polynôme tel que \( P(f)=0\), alors le polynôme minimal \( \mu_f\) divise \( P\). Autrement dit, le polynôme minimal engendre l'idéal des polynômes annulateurs.
\end{proposition}

\begin{proof}
    L'ensemble \( \ker(\varphi)=\{ Q\in \eK[X]\tq Q(u)=0 \} \) est un idéal par le lemme \ref{LemQWvhYb}. Le polynôme minimal de \( u\) est un élément de degré plus bas dans \( I\) et par conséquent \( I=(\mu_u)\) par le théorème \ref{ThoCCHkoU}. Nous concluons que \( \mu_u\) divise tous les éléments de \( I\).
\end{proof}

\begin{lemma}[\cite{ooRJDSooXpVtMD}]\label{LemSYsJJj}
    Soit \( f\colon E\to E\) un endomorphisme de l'espace vectoriel \( E\). Il existe un élément \( x\in E\) tel que \( \mu_{f,x}=\mu_f\).
\end{lemma}

\begin{proof}
    Soit une décomposition en irréductibles du polynôme minimal \( \mu=P_1^{\alpha_1}\ldots P_r^{\alpha_r}\). Nous notons \( E_i=\ker\big( P_i^{\alpha_i}(f) \big)\). Les polynômes \( P_i\) sont étrangers deux à deux (un diviseur commun aurait a fortiori été un diviseur et aurait contredit l'irréductibilité). Le lemme des noyaux \ref{ThoDecompNoyayzzMWod} nous donne la somme directe
    \begin{equation}
        E=\bigoplus_{i=1}^r\ker\big( P_i^{\alpha_i}(f) \big).
    \end{equation}
    Si \( x_i\in E_i\) alors \( \mu_{x_i}\) est une puissance de \( P_i\). En effet \( \mu_{x_i}\divides \mu\) et est donc un produit des puissances des \( P_j\). Or si \( (QP_j)(f)x_i=0\) alors \( (P_jQ)(f)x_i=0\), ce qui donne \( Q(f)x_i\in E_j\cap E_i=\{ 0 \}\). Donc \( \mu_{x_i}\) n'est pas de la forme \( QP_j\) pour \( j\neq i\). Nous en déduisons que \( \mu_{x_i}\) est une puissance de \( P_i\) dès que \( x_i\in E_i\). Nous choisissons \( x_i\in E_i\) tel que \( \mu_{x_i}=P_i^{\alpha_i}\).

    Nous posons enfin \( a=x_1+\ldots +x_r\); par définition du polynôme annulateur \( \mu_a\), nous avons
    \begin{equation}        \label{EqooVIGGooSfuvwB}
        0=\mu_a(f)a=\mu_a(f)x_1+\ldots +\mu_a(f)x_r.
    \end{equation}
    Mais \( m_a(f)x_j\in E_i\), et la somme des \( E_j\) est directe, donc l'annulation de la somme \eqref{EqooVIGGooSfuvwB} implique l'annulation de chacun des termes : \( \mu_a(f)x_i=0\) pour tout \( i\). Cela prouve que \( \mu_{x_i}\divides \mu_a\). Mais comme les \( \mu_{x_i}\) sont premiers deux à deux (parce que ce sont les \( P_i^{\alpha_i}\)), nous avons que le produit divise encore \( \mu_a\) :
    \begin{equation}
        \prod_{i=1}^r\mu_{x_i}\divides \mu_a,
    \end{equation}
    c'est à dire \( \mu\divides \mu_a\). Comme nous avons aussi \( \mu_a\divides \mu\), nous déduisons \( \mu_a=\mu\).
\end{proof}

\begin{definition}  \label{DEFooBOHVooSOopJN}
    Un endomorphisme d'un espace vectoriel est \defe{semi-simple}{semi-simple!endomorphisme} si tout sous-espace stable par \( u\) possède un supplémentaire stable.
\end{definition}

\begin{lemma}   \label{LemrFINYT}
    Si le polynôme minimal d'un endomorphisme est irréductible, alors il est semi-simple\footnote{Définition \ref{DEFooBOHVooSOopJN}.}.
\end{lemma}

\begin{proof}
    Soit \( f\), un endomorphisme dont le polynôme minimal est irréductible et \( F\), un sous-espace stable par \( f\). Nous devons en trouver un supplémentaire stable. Si \( F=E\), il n'y a pas de problèmes. Sinon nous considérons \( u_1\in E\setminus F\) et
    \begin{equation}
        E_{u_1}=\{ P(f)u_1\tq P\in \eK[X] \},
    \end{equation}
    qui est un espace stable par \( f\). 

    Montrons que \( E_{u_1}\cap F=\{ 0 \}\). Pour cela nous regardons l'idéal
    \begin{equation}
        I_{u_1}=\{ P\in \eK[X]\tq P(f)u_1=0 \}.
    \end{equation}
    Cela est un idéal non réduit à \( \{ 0 \}\) parce que le polynôme minimal de \( f\) par exemple est dans \( I_{u_1}\). Soit \( P_{u_1}\) un générateur unitaire de \( I_{u_1}\). Étant donné que \( \mu_f\in I_{u_1}\), nous avons que \( P_{u_1}\) divise \( \mu_f\) et donc \( P_{u_1}=\mu_f\) parce que \( \mu_f\) est irréductible par hypothèse.

    Soit \( y\in E_{u_1}\cap F\). Par définition il existe \( P\in\eK[X]\) tel que \( y=P(f)u_1\) et si \( y\neq 0\), ce la signifie que \( P\notin I_{u_1}\), c'est à dire que \( P_{u_1} \) ne divise pas \( P\). Étant donné que \( P_{u_1}\) est irréductible cela implique que \( P_{u_1}\) et \( P\) sont premiers entre eux (ils n'ont pas d'autre \( \pgcd\) que \( 1\)).

    Nous utilisons maintenant Bézout (théorème \ref{ThoBezoutOuGmLB}) qui nous donne \( A,B\in \eK[X]\) tels que 
    \begin{equation}
        AP+BP_{u_1}=1.
    \end{equation}
    Nous appliquons cette égalité à \( f\) et puis à \( u_1\):
    \begin{equation}
        u_1=A(f)\circ \underbrace{P(f)u_1}_{=y}+B(f)\circ \underbrace{P_{u_1}(u_1)}_{=0}=A(f)y.
    \end{equation}
    Mais \( y\in F\), donc \( A(f)y\in F\). Nous aurions donc \( u_1\in F\), ce qui est impossible par choix. Nous avons maintenant que l'espace \( E_{u_1}\oplus F\) est stable sous \( f\). Si cet espace est \( E\) alors nous arrêtons. Sinon nous reprenons le raisonnement avec \( E_{u_1}\oplus F\) en guise de \( F\) et en prenant \( u_2\in E\setminus(E_{u_1}\oplus F)\). Étant donné que \( E\) est de dimension finie, ce procédé s'arrête à un certain moment et nous aurons
    \begin{equation}
        E=F\oplus E_{u_1}\oplus\ldots\oplus E_{u_k}
    \end{equation}
    où chacun des \( E_{u_i}\) sont stables.
\end{proof}

\begin{theorem} \label{ThoFgsxCE}
    Un endomorphisme est semi-simple si et seulement si son polynôme minimal est produit de polynômes irréductibles distincts deux à deux.
\end{theorem}
\index{anneau!principal}

\begin{proof}

    Supposons que \( f\) soit semi-simple et que son polynôme minimal soit donné par \( \mu_f=M_1^{\alpha_1}\ldots M_r^{\alpha_r}\) où les \( M_i\) sont des polynômes irréductibles deux à deux distincts. Nous devons montrer que \( \alpha_i=1\) pour tout \( i\). Soit \( i\) tel que \( \alpha_i\geq 1\) et \( N\in \eK[X]\) tel que \( \mu_f=M^2N\) où l'on a noté \( M=M_i\). Nous étudions l'espace
    \begin{equation}
        F=\ker M(f)
    \end{equation}
    qui est stable par \( f\), et qui possède donc un supplémentaire \( S\) également stable par \( f\). Nous allons montrer que \( MN\) est un polynôme annulateur de \( f\).

    D'abord nous prenons \( x\in S\). Étant donné que \( F\) est le noyau de \( M(f)\),
    \begin{equation}
        M(f)\big( MN(f)x \big)=\mu_f(f)x=0,
    \end{equation}
    ce qui signifie que \( MN(f)x\in F\). Mais vu que \( S\) est stable par \( f\) nous avons aussi que \( MN(f)x\in S\). Finalement \( MN(f)x\in F\cap S=\{ 0 \}\). Autrement dit, \( MN(f)\) s'annule sur \( S\).

    Prenons maintenant \( y\in F\). Nous avons
    \begin{equation}
        MN(f)=N(f)\big( M(f)y \big)=0
    \end{equation}
    parce que \( y\in F=\ker M(f)\).

    Nous avons prouvé que \( MN(f)\) s'annule partout et donc que \( MN(f)\) est un polynôme annulateur de \( f\), ce qui contredit la minimalité de \( \mu_f=M^2N\).

    Nous passons au sens inverse. Soit \( m_f=M_1\ldots M_r\) une décomposition du polynôme minimal de l'endomorphisme \( f\) en irréductibles distincts deux à deux. Soit \( F\) un sous-espace vectoriel stable par \( f\). Nous notons
    \begin{equation}
        E_i=\ker(M_i(f))
    \end{equation}
    et \( f_i=f|_{E_i}\). Par le lemme \ref{CorKiSCkC} nous avons
    \begin{equation}
        F=\bigoplus_{i=1}^r(F\cap E_i).
    \end{equation}
    Les espaces \( E_i\) sont stables par \( f\) et étant donné que \( M_i\) est irréductible, il est le polynôme minimal de \( f_i\). En effet, \( M_i\) est annulateur de \( f_i\), ce qui montre que le minimal de \( f_i\) divise \( M_i\). Mais \( M_i\) étant irréductible, \( M_i\) est le polynôme minimal. Étant donné que \( \mu_{f_i}=M_i\), l'endomorphisme \( f_i\) est semi-simple par le lemme \ref{LemrFINYT}.

    L'espace \( F\cap E_i\) étant stable par l'endomorphisme semi-simple \( f_i\), il possède un supplémentaire stable que nous notons \( S_i\)~:
    \begin{equation}
        E_i=S_i\oplus(F\cap E_i).
    \end{equation}
    Étant donné que sur chaque \( S_i\) nous avons \( f|_{S_i}=f_i\), l'espace \( S=S_1\oplus\ldots\oplus S_r\) est stable par \( f\). Du coup nous avons
    \begin{subequations}
        \begin{align}
            E&=E_1\oplus\ldots\oplus E_r\\
            &=\big( S_1\oplus(F\cap E_1) \big)\oplus\ldots\oplus\big( S_r\oplus(F\cap E_r) \big)\\
            &=\big( \bigoplus_{i=1}^rS_i \big)\oplus\big( \bigoplus_{i=1}^rF\cap E_i \big)\\
            &=S\oplus F,
        \end{align}
    \end{subequations}
    ce qui montre que \( F\) a bien un supplémentaire stable par \( f\) et donc que \( f\) est semi-simple.
\end{proof}

\begin{example}[L'espace engendré par \( \mtu\), \( A\), \( A^2\),\ldots]
    Soit \( A\) une matrice, et 
    \begin{equation}
        V=\Span\{A^k\tq k\in \eN \}.
    \end{equation}
    Nous montrons que \( \dim(V)\) est le degré du polynôme minimal de \( A\).

    D'abord l'idéal annulateur de \( A\) est engendré par le polynôme minimal\footnote{Proposition \ref{PropAnnncEcCxj}.} que nous notons
        $\mu=\sum_{k=0}^pa_kX^k$.
    La partie \( \{ \mtu,\ldots, A^{p-1} \}\) est libre parce qu'une combinaison linéaire nulle de cela serait un polynôme annulateur en \( A\) de degré plus petit que \( p\). Donc \( \dim(V)\geq p\).

    La partie \( \{ \mtu,A,\ldots, A^p \}\) est liée à cause du polynôme minimal. Isoler \( A^p\) dans \( \mu(A)=0\) donne un polynôme \( f\) de degré \( p-1\) tel que \( A^p=f(A)\).

    Nous allons montrer à présent que la famille \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice (alors \( \dim(V)\leq p\)). Soit un entier \( q\geq p\)et de division euclidienne\footnote{Théorème \ref{ThoDivisEuclide}.} \( np+r=q\) avec \( r<p\). Nous avons \( A^q=A^{np}A^r\). D'une part
    \begin{equation}
        A^{np}=(A^p)^n=f(A)^n
    \end{equation}
    est de degré \( n(p-1)\). Par conséquent
    \begin{equation}
        A^q=f(A)^nA^r
    \end{equation}
    qui est de degré \( n(p-1)+r=q-n\). Autrement dit il existe un polynôme \( g_1\) de degré \( q-n\) tel que \( A^q=g_1(A)\). Si \( q-n>p-1\) alors nous pouvons recommencer et obtenir un polynôme \( g_2\) de degré strictement inférieur à celui de \( g_1\) tel que \( A^q=g_2(A)\). Au bout du compte, il existe un polynôme \( g\) de degré au maximum \( p-1\) tel que \( A^q=g(A)\). Cela prouve que la partie \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice de \( V\).

    La dimension de \( V\) est donc \( p\), le degré du polynôme minimal.
\end{example}

\begin{proposition}     \label{PropooCFZDooROVlaA}
    Soit \( f\) un endomorphisme d'un espace vectoriel de dimension finie. Nous avons l'isomorphisme d'espace vectoriel
    \begin{equation}
        \eK[f]\simeq\frac{ \eK[X] }{ (\mu_f) }
    \end{equation}
\end{proposition}

\begin{proof}
    Notons avant de commencer que \( (\mu)\) est l'idéal engendré par \( \mu\). Les classes dont il est question dans le quotient \( \eK[X]/(\mu)\) sont 
    \begin{equation}
        \bar P=\{ P+S\mu \}_{S\in \eK[X]}.
    \end{equation}
    Nous allons montrer que l'application suivante fournit l'isomorphisme : 
    \begin{equation}
        \begin{aligned}
            \psi\colon \frac{ \eK[X] }{ (\mu) }&\to \eK[f] \\
            \bar P&\mapsto P(f). 
        \end{aligned}
    \end{equation}
    \begin{subproof}
        \item[\( \psi\) est bien définie]
            Si \( Q\in \bar P\) alors \( Q=P+S\mu\) pour un certain \( S\in \eK[X]\). Du coup nous avons
            \begin{equation}
                \psi(\bar Q)=P(f)+(S\mu)(f).
            \end{equation}
            Mais \( \mu(f)=0\) donc le deuxième terme est nul. Donc \( \psi(\bar P)\) est bien définit.
        \item[Injectif]
            Si \( \psi(\bar P)=0\) nous avons \( P(f)=0\), ce qui signifie que \( P=S\mu\) pour un polynôme \( S\). Par conséquent \( P\in (\mu)\) et donc \( \bar P=0\).
        \item[Surjectif]
            Soit \( P\in \eK[X]\). L'élément \( P(f) \) de \( \eK[f]\) est dans l'image de \( \psi\) parce que c'est \( \psi(\bar P)\).
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Polynôme caractéristique}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefOWQooXbybYD}
    Soit un anneau commutatif \( A\). Si \( u\in\eM_n(A)\), nous définissons le \defe{polynôme caractéristique de \( u\)}{polynôme!caractéristique}\index{caractéristique!polynôme} :
    \begin{equation}    \label{Eqkxbdfu}
        \chi_u(X)=\det(X\mtu_n-u).
    \end{equation} 
    Nous définissons de même le polynôme caractéristique d'un 'endomorphisme \( u\colon E\to E\).
\end{definition}

\begin{lemma}       \label{LemooWCZMooZqyaHd}
    Le polynôme caractéristique \( \chi_u\) est unitaire et a pour degré la dimension de l'espace vectoriel \( E\)..
\end{lemma}

\begin{theorem}     \label{ThoNhbrUL}
    Soit \( E\) un \(\eK\)-espace vectoriel de dimension finie \( n\) et un endomorphisme \( u\in\End(E)\). Alors
    \begin{enumerate}
        \item
            Le polynôme caractéristique divise \( (\mu_u)^n\) dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes facteurs irréductibles dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes racines dans \(\eK[X]\).
        \item
            Le polynôme caractéristique est scindé si et seulement si le polynôme minimal est scindé.
    \end{enumerate}
\end{theorem}

Si \( \lambda\in\eK\) est une racine de \( \chi_u\), l'ordre de l'annulation est la \defe{multiplicité algébrique}{multiplicité!algébrique d'une valeur propre} de la valeur propre \( \lambda\) de \( u\). À ne pas confondre avec la \defe{multiplicité géométrique}{multiplicité!géométrique} qui sera la dimension de l'espace propre.

\begin{theorem} \label{ThoWDGooQUGSTL}
    Soit \( u\in\End(E)\) et \( \lambda\in\eK\). Les conditions suivantes sont équivalentes
    \begin{enumerate}
        \item\label{ItemeXHXhHi}
            \( \lambda\in\Spec(u)\)
        \item\label{ItemeXHXhHii}
            \( \chi_u(\lambda)=0\)
        \item\label{ItemeXHXhHiii}
            \( \mu_u(\lambda)=0\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    \ref{ItemeXHXhHi} \( \Leftrightarrow\) \ref{ItemeXHXhHii}. Dire que \( \lambda\) est dans le spectre de \( u\) signifie que l'opérateur \( u-\lambda\mtu\) n'est pas inversible, ce qui est équivalent à dire que \( \det(u-\lambda\mtu)\) est nul par la proposition \ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy} ou encore que \( \lambda\) est une racine du polynôme caractéristique de \( u\). 

    \ref{ItemeXHXhHii} \( \Leftrightarrow\) \ref{ItemeXHXhHiii}. Cela est une application directe du théorème \ref{ThoNhbrUL} qui précise que le polynôme caractéristique a les mêmes racines dans \(\eK\) que le polynôme minimal.
\end{proof}


\begin{proposition}[\cite{RombaldiO}]\label{PropNrZGhT}
    Soit \( f\), un endomorphisme de \( E\) et \( x\in E\). Alors
    \begin{enumerate}
        \item
            L'espace \( E_{f,x}\) est stable par \( f\).
        \item\label{ItemfzKOCo}
            L'espace \( E_{f,x}\) est de dimension
            \begin{equation}
                p_{f,x}=\dim E_{f,x}=\deg(\mu_{f,x})
            \end{equation}
            où \( \mu_{f,x}\) est le générateur unitaire de \( I_{f,x}\).
        \item   \label{ItemKHNExH}
            Le polynôme caractéristique de \( f|_{E_{f,x}}\) est \( \mu_{f,x}\).
        \item   \label{ItemHMviZw}
            Nous avons
            \begin{equation}
                \chi_{f|_{E_{f,x}}}(f)x=\mu_{f,x}(f)x=0.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le fait que \( E_{f,x}\) soit stable par \( f\) est classique. Le point \ref{ItemHMviZw} est un une application du point \ref{ItemKHNExH}. Les deux gros morceaux sont donc les points \ref{ItemfzKOCo} et \ref{ItemKHNExH}.

    Étant donné que \( \mu_{f,x}\) est de degré minimal dans \( I_{f,x}\), l'ensemble
    \begin{equation}
        B=\{ f^k(x)\tq 0\leq k\leq p_{f,x}-1 \}
    \end{equation}
    est libre. En effet une combinaison nulle des vecteurs de \( B\) donnerait un polynôme en \( f\) de degré inférieur à \( p_{f,x}\) annulant \( x\). Nous écrivons
    \begin{equation}
        \mu_{f,x}(X)=X^{p_{f,x}}-\sum_{i=0}^{p_{f,x}-1}a_iX^k. 
    \end{equation}
    Étant donné que \( \mu_{f,x}(f)x=0\) et que la somme du membre de droite est dans \( \Span(B)\), nous avons \( f^{p_{f,x}}(x)\in\Span(B)\). Nous prouvons par récurrence que \( f^{p_{f,x}+k}(x)\in\Span(B)\). En effet en appliquant \( f^k\) à l'égalité
    \begin{equation}
        0=f^{p_{f,x}}(x)-\sum_{i=0}^{p_{f,x}-1}a_if^i(x)
    \end{equation}
    nous trouvons
    \begin{equation}
        f^{p_{f,x}+k}(x)=\sum_{i=0}^{p_{f,x}-1}a_if^{i+k}(x),
    \end{equation}
    alors que par hypothèse de récurrence le membre de droite est dans \( \Span(B)\). L'ensemble \( B\) est alors générateur de \( E_{f,x}\) et donc une base d'icelui. Nous avons donc bien \( \dim(E_{f,x})=p_{f,x}\).

    Nous montrons maintenant que \( \mu_{f,x}\) est annulateur de \( f\) au point \( x\). Nous savons que
    \begin{equation}
        \mu_{f,x}(f)x=0.
    \end{equation}
    En y appliquant \( f^k\) et en profitant de la commutativité des polynômes sur les endomorphismes (proposition \ref{LemQWvhYb}), nous avons
    \begin{equation}
        0=f^k\big( \mu_{f,x}(f)x \big)=\mu_{f,x}(f)f^k(x),
    \end{equation}
    de telle sorte que \( \mu_{f,x}(f)\) est nul sur \( B\) et donc est nul sur \( E_{f,x}\). Autrement dit,
    \begin{equation}
        \mu_{f,x}\big( f|_{E_{f,x}} \big)=0.
    \end{equation}
    Montrons que \( \mu_{f,x}\) est même minimal pour \( f|_{E_{f,x}}\). Sot \( Q\), un polynôme non nul de degré \( p_{f,x}-1\) annulant \( f|_{E_{f,x}}\). En particulier \( Q(f)x=0\), alors qu'une telle relation signifierait que \( B\) est un système lié, alors que nous avons montré que c'était un système libre. Nous concluons que \( \mu_{f,x}\) est le polynôme minimal de \( f|_{E_{f,x}}\).
\end{proof}

Cette histoire de densité permet de donner une démonstration alternative du théorème de Cayley-Hamilton.
\begin{theorem}[Cayley-Hamlilton]   \label{ThoCalYWLbJQ}
    Le polynôme caractéristique est un polynôme annulateur.
\end{theorem}
\index{théorème!Cayley-Hamilton}

Une démonstration plus simple via la densité des diagonalisables est donnée en théorème \ref{ThoHZTooWDjTYI}.
\begin{proof}
    Nous devons prouver que \( \chi_f(f)x=0\) pour tout \( x\in E\). Pour cela nous nous fixons un \( x\in E\), nous considérons l'espace \( E_{f,x}\) et \( \chi_{f,x}\), le polynôme caractéristique de \( f|_{E_{f,x}}\). Étant donné que \( E_{f,x}\) est stable par \( f\), le polynôme caractéristique de \( f|_{E_{j,x}}\) divise \( \chi_f\), c'est à dire qu'il existe un polynôme \( Q_x\) tel que
    \begin{equation}
        \chi_f=Q_x\chi_{f,x},
    \end{equation}
    et donc aussi
    \begin{equation}
        \chi_f(f)x=Q_x(f)\big( \chi_{f,x}(f)x \big)=0
    \end{equation}
    parce que la proposition \ref{PropNrZGhT} nous indique que \( \chi_{f,x}\) est un polynôme annulateur de \( f|_{E_{f,x}}\).
\end{proof}

\begin{corollary}
    Le degré du polynôme minimal est majoré par la dimension de l'espace.
\end{corollary}

\begin{proof}
    Le polynôme minimal engendre l'idéal des polynôme annulateurs (proposition \ref{PropAnnncEcCxj}), et divise donc le polynôme caractéristique. Or le degré du polynôme caractéristique est la dimension de l'espace par le lemme \ref{LemooWCZMooZqyaHd}.
\end{proof}

\begin{example}[Calcul de l'inverse d'un endomorphisme]
    Le polynôme de Cayley-Hamilton donne un moyen de calculer l'inverse d'un endomorphisme inversible pourvu que l'on sache son polynôme caractéristique. En effet, supposons que
    \begin{equation}
        \chi_f(X)=\sum_{k=0}^na_kX^k.
    \end{equation}
    Nous aurons alors
    \begin{equation}
        0=\chi_f(f)=\sum_{k=0}^na_kf^k.
    \end{equation}
    Nous appliquons \( f^{-1}\) à cette dernière égalité en sachant que \( f^{-1}(0)=0\) :
    \begin{equation}
        0=a_0f^{-1}+\sum_{k=1}^na_kf^{k-1},
    \end{equation}
    et donc
    \begin{equation}
        u^{-1}=-\frac{1}{ \det(f) }\sum_{k=1}^na_kf^{k-1}
    \end{equation}
    où nous avons utilisé le fait que \( a_0=\chi_f(0)=\det(f)\).
\end{example}

\begin{proposition}\label{PropooBYZCooBmYLSc}
    Si \( (X-z)^l\) (\( l\geq 1\)) est la plus grande puissance de \( (X-z)\) dans le polynôme caractéristique d'un endomorphisme \( u\) alors 
    \begin{equation}
        1\leq \dim(E_e)\leq l.
    \end{equation}
    C'est à dire que nous avons au moins un vecteur propre pour chaque racine du polynôme caractéristique.
\end{proposition}

\begin{proof}
    Si $(X-z)$ divise \( \chi_u\) alors en posant \( \chi_u=(X-z)P(X)\) nous avons
    \begin{equation}
        \det(u-X\mtu)=(X-z)P(X),
    \end{equation}
    ce qui, évalué en \( X=z\), donne \( \det(u-z\mtu)=0\). L'annulation du déterminant étant équivalente à l'existence d'un noyau non trivial, nous avons \( v\neq 0\) dans \( E\) tel que \( (u-z\mtu)v=0\). Cela donne \( u(v)=zv\) et donc que \( v\) est vecteur propre de \( u\) pour la valeur propre \( z\). Donc aussi \( \dim(E_z)\geq 1\).

    Si \( \dim(E_z)=k\) alors le théorème de la base incomplète \ref{ThonmnWKs} nous permet d'écrire une base de \( E\) dont les \( k\) premiers vecteurs forment une base de \( E_z\). Dans cette base, la matrice de \( u\) est de la forme
    \begin{equation}
        \begin{pmatrix}
             z   &       &       &   *    \\
                &   \ddots    &       &   \vdots    \\
                &       &   z    &   *    \\ 
                &       &       &   *     
         \end{pmatrix}
    \end{equation}
    où les étoiles représentent des blocs a priori non nuls. En tout cas il est vu sous cette forme que \( (X-z\mtu)^k\) divise \( \chi_u\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Valeur propre et vecteur propre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}      \label{DefooMMKZooVcskCc}
    Soit un \( \eK\)-espace vectoriel \( E\) et un endomorphisme \( A\colon V\to V\). Un \defe{vecteur propre}{vecteur!propre} de \( A\) est un vecteur \( v \neq 0\) tel que \( Av=\lambda v\) pour un certain \( \lambda\in \eK\). Dans ce cas, \( \lambda\) est la \defe{valeur propre}{valeur!propre} de \( v\).

    L'\defe{espace propre}{espace!propre} de \( A\) pour la valeur \( \lambda\)\footnote{Nous laissons au lecteur le soin de vérifier que c'est bien un sous-espace vectoriel de \( E\).} est l'ensemble des vecteurs propres de \( A\) pour la valeur propre \( \lambda\) et zéro.
\end{definition}
L'ensemble de valeurs propres de l'endomorphisme \( u\) est son \defe{spectre}{spectre!d'un endomorphisme} et est noté \( \Spec(u)\).

\begin{remark}
    Le nombre zéro peut être une valeur propre; c'est le vecteur zéro qui ne peut pas être vecteur propre. La matrice nulle est une matrice diagonalisable.
\end{remark}

\begin{lemma}       \label{LemjcztYH}
    Soit \( u\) un endomorphisme et \( E_{\lambda}(u)\)\nomenclature[A]{\( E_{\lambda}(u)\)}{Espace propre de \( u\)} ses espaces propres. La somme des \( V_{\lambda}\) est directe.
\end{lemma}

\begin{proof}
    Soit \( v_i\in V_{\lambda_i}\) un choix de vecteurs propres de \( u\). Si la somme n'est pas directe, nous pouvons considérer une combinaison linéaire des \( v_i\) qui soit nulle :
    \begin{equation}
        v_1+\ldots+v_p=0.
    \end{equation}
    Appliquons \( (A-\lambda_1\mtu)\) à cette égalité :
    \begin{equation}
        (\lambda_2-\lambda_1)v_1+\ldots+(\lambda_p-\lambda_1)v_p=0.
    \end{equation}
    En appliquant encore successivement les opérateurs \( (A-\lambda_i\mtu)\) nous réduisons le nombre de termes jusqu'à obtenir \( v_p=0\).
\end{proof}

\begin{example} \label{ExICOJcFp}
    Sur \( \eR^2\), nous considérons la matrice \( A=\begin{pmatrix}
        1    &   0    \\ 
        1    &   1    
    \end{pmatrix}\) qui a pour polynôme caractéristique le polynôme \( \chi_A=(X-1)^2\). Le nombre \( \lambda=1\) est une racine double de ce polynôme, et pourtant il n'y a qu'une seule dimension d'espace propre :
    \begin{equation}
        \begin{pmatrix}
            1    &   0    \\ 
            1    &   1    
        \end{pmatrix}\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}=\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}
    \end{equation}
    entraine \( x=0\).

    Ici la multiplicité algébrique est différente de la multiplicité géométrique.
\end{example}

\begin{proposition}[\cite{RombaldiO}]   \label{PropTVKbxU}
    Soit \( E\), un espace vectoriel sur un corps infini et \( (F_k)_{k=1,\ldots, r}\), des sous-espaces vectoriels propres\footnote{Définition \ref{DefooMMKZooVcskCc}.} de \( E\) tels que \( \bigcup_{i=1}^rF_i=E\). Alors \( E=F_k\) pour un certain \( k\).

    Autrement dit, l'union finie de sous-espaces propres ne peut être égal à l'espace complet.
\end{proposition}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Diagonalisation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Quelques liens internes concernant la diagonalisation et des décompositions de matrices.
\begin{enumerate}
    \item
        Définition d'un endomorphisme diagonalisable : \ref{DefCNJqsmo}.
    \item
        Conditions équivalentes au fait d'être diagonalisable en termes de polynôme minimal, y compris la décomposition en espaces propres : théorème \ref{ThoDigLEQEXR}.
    \item
        Diagonalisation simultanée \ref{PropGqhAMei}, pseudo-diagonalisation simultanée \ref{CorNHKnLVA}.
    \item
        Diagonalisation d'exponentielle \ref{PropCOMNooIErskN} utilisant Dunford.
    \item
        Décomposition polaire théorème \ref{ThoLHebUAU}. \( M=SQ\), \( S\) est symétrique, réelle, définie positive, \( Q\) est orthogonale.
    \item
        Décomposition de Dunford \ref{ThoRURcpW}. \( u=s+n\) où \( s\) est diagonalisable et \( n\) est nilpotent, \( [s,n]=0\).
    \item 
        Réduction de Jordon (bloc-diagonale) \ref{ThoGGMYooPzMVpe}.
    \item 
        L'algorithme des facteurs invariants \ref{PropPDfCqee} donne \( U=PDQ\) avec \( P\) et \( Q\) inversibles, \( D\) diagonale, sans hypothèse sur \( U\). De plus les éléments de \( D\) forment une chaîne d'éléments qui se divisent l'un l'autre.
\end{enumerate}

Ici encore \( \eK\) est un corps commutatif.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices semblables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[matrices semblables] \label{DefCQNFooSDhDpB}
    Sur l'ensemble \( \eM_n(\eK)\) des matrices \( n\times n\) à coefficients dans \(\eK\) nous introduisons la relation d'équivalence \( A\sim B\) si et seulement si il existe une matrice \( P\in\GL(n,\eK)\) telle que \( B=P^{-1}AP\). Deux matrices équivalentes en ce sens sont dites \defe{semblables}{semblables!matrices}.
\end{definition}

Le polynôme caractéristique est un invariant sous les similitudes. En effet si \( P\) est une matrice inversible,
\begin{subequations}
    \begin{align}
        \chi_{PAP^{-1}}&=\det(PAP^{-1}-\lambda X)\\
        &=\det\big( P^{-1}(PAP^{-1}-\lambda X)P^{-1} \big)\\
        &=\det(A-\lambda X).
    \end{align}
\end{subequations}

La permutation de lignes ou de colonnes ne sont pas de similitudes, comme le montrent les exemples suivants :
\begin{equation}
    \begin{aligned}[]
        A&=\begin{pmatrix}
            1    &   2    \\ 
            3    &   4    
        \end{pmatrix}&
        B&=\begin{pmatrix}
            2    &   1    \\ 
            4    &   3    
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Nous avons \( \chi_A=x^2-5x-2\) tandis que \( \chi_B=x^2-5x+2\) alors que le polynôme caractéristique est un invariant de similitude.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes diagonalisables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefCNJqsmo}
    Une matrice est \defe{diagonalisable}{diagonalisable} si elle est semblable à une matrice diagonale.
\end{definition}

\begin{lemma}
    Une matrice triangulaire supérieure avec des \( 1\) sur la diagonale n'est diagonalisable que si elle est diagonale (c'est à dire si elle est la matrice unité).
\end{lemma}

\begin{proof}
    Si \( A\) est une matrice triangulaire supérieure de taille \( n\) telle que \( A_{ii}=1\), alors \( \det(A-\lambda\mtu)=(1-\lambda)^n\), ce qui signifie que \( \Spec(A)=\{ 1 \}\). Pour la diagonaliser, il faudrait une matrice \( P\in\GL(n,\eK)\) telle que \( \mtu=P^{-1}AP\), ce qui est uniquement possible si \( A=\mtu\).
\end{proof}

\begin{lemma}       \label{LemgnaEOk}
    Soit \( F\) un sous-espace stable par \( u\). Soit une décomposition du polynôme minimal
    \begin{equation}
        \mu_u=P_1^{n_1}\ldots P_r^{n_r}
    \end{equation}
    où les \( P_i\) sont des polynômes irréductibles unitaires distincts. Si nous posons \( E_i=\ker P_i^{n_i}\), alors
    \begin{equation}
        F=(F\cap E_1)\oplus\ldots \oplus(F\cap E_r).
    \end{equation}
\end{lemma}

\begin{theorem}     \label{ThoDigLEQEXR}
    Soit \( E\), un espace vectoriel de dimension \( n\) sur le corps commutatif \( \eK\) et \( u\in\End(E)\). Les propriétés suivantes sont équivalentes.
    \begin{enumerate}
        \item\label{ItemThoDigLEQEXRiv}
            L'endomorphisme \( u\) est diagonalisable.
        \item       \label{ItemThoDigLEQEXRi}
            Il existe un polynôme \( P\in\eK[X]\) non constant, scindé sur \(\eK\) dont toutes les racines sont simples tel que \( P(u)=0\).
        \item\label{ItemThoDigLEQEXRii}
            Le polynôme minimal \( \mu_u\) est scindé sur \(\eK\) et toutes ses racines sont simples\footnote{Le polynôme \emph{caractéristique}, lui, n'a pas spécialement ses racines simples; il peut encore être de la forme
            \begin{equation}
                \chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i},
        \end{equation}
        mais alors \( \dim(E_{\lambda_i})=\alpha_i\). }.
        \item\label{ItemThoDigLEQEXRiii}
            Tout sous-espace de \( E\) possède un supplémentaire stable par \( u\).
        \item       \label{ITEMooZNJFooEiqDYp}
            Dans une base adaptée, la matrice de \( u\) est diagonale et les éléments diagonaux sont ses valeurs propres.
    \end{enumerate}
\end{theorem}
\index{diagonalisable!et polynôme minimum scindé}

\begin{proof}
    Plein d'implications à prouver.
    \begin{subproof}
    \item[\ref{ItemThoDigLEQEXRi} implique \ref{ItemThoDigLEQEXRii}] Étant donné que \( P(u)=0\), il est dans l'idéal des polynôme annulateurs de \( u\), et le polynôme minimal \( \mu_u\) le divise parce que l'idéal des polynôme annulateurs est généré par \( \mu_u\) par le théorème \ref{ThoCCHkoU}.

    \item[\ref{ItemThoDigLEQEXRii} implique \ref{ItemThoDigLEQEXRiv}] Étant donné que le polynôme minimal est scindé à racines simples, il s'écrit sous forme de produits de monômes tous distincts, c'est à dire
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots(X-\lambda_r)
    \end{equation}
    où les \( \lambda_i\) sont des éléments distincts de \( \eK\). Étant donné que \( \mu_u(u)=0\), le théorème de décomposition des noyaux (théorème \ref{ThoDecompNoyayzzMWod}) nous enseigne que
    \begin{equation}
        E=\ker(u-\lambda_1)\oplus\ldots\oplus\ker(u-\lambda_r).
    \end{equation}
    Mais \( \ker(u-\lambda_i)\) est l'espace propre \( E_{\lambda_i}(u)\). Donc \( u\) est diagonalisable.

\item[\ref{ItemThoDigLEQEXRiv} implique \ref{ItemThoDigLEQEXRiii}] Soit \( \{ e_1,\ldots, e_n \}\) une base qui diagonalise \( u\), soit \( F\) un sous-espace de \( E\) un \( \{ f_1,\ldots, f_r \}\) une base de \( F\). Par le théorème \ref{ThoBaseIncompjblieG} (qui généralise le théorème de la base incomplète), nous pouvons compléter la base de \( F\) par des éléments de la base \( \{ e_i \}\). Le complément ainsi construit est invariant par \( u\).

\item[\ref{ItemThoDigLEQEXRiii} implique \ref{ItemThoDigLEQEXRiv}] En dimension un, tout endomorphisme est diagonalisable, nous supposons donc que \( \dim E=n\geq 2\). Nous procédons par récurrence sur le nombre de vecteurs propres connus de \( u\). Supposons avoir déjà trouvé \( p\) vecteurs propres \( e_1,\ldots, e_p\) de \( u\). Considérons \( H\), un hyperplan qui contient les vecteurs \( e_1,\ldots, e_p\). Soit \( F\) un supplémentaire de \( H\) stable par \( u\); par construction \( \dim F=1\) et si \( e_{p+1}\in F\), il doit être vecteur propre de \( u\).

\item[\ref{ItemThoDigLEQEXRiv} implique \ref{ItemThoDigLEQEXRi}] Nous supposons maintenant que \( u\) est diagonalisable. Soient \( \lambda_1,\ldots, \lambda_r\) les valeurs propres deux à deux distinctes, et considérons le polynôme
    \begin{equation}
        P(x)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Alors \( P(u)=0\). En effet si \( e_i\) est un vecteur propre pour la valeur propre \( \lambda_i\), 
    \begin{equation}
        P(u)e_i=\prod_{j\neq i}(u-\lambda_j)\circ(u-\lambda_i)e_i=0
    \end{equation}
    par le lemme \ref{LemQWvhYb}. Par conséquent \( P(u)\) s'annule sur une base.

\item[\ref{ITEMooZNJFooEiqDYp} implique \ref{ItemThoDigLEQEXRi}]
    Si la matrice \( A\) est diagonale alors le polynôme \( P=\prod_{i=1}^n(A-A_{ii}\mtu)\) est annulateur de \( A\).
        \item[\ref{ItemThoDigLEQEXRii} implique \ref{ITEMooZNJFooEiqDYp}]
            le polynôme minimal de \( u\) s'écrit 
            \begin{equation}
                \mu=(X-\lambda_1)\ldots(X-\lambda_r),
            \end{equation}
            et les espaces $E_i$ du lemme \ref{LemgnaEOk} sont les espaces propres \( E_i=\ker(u-\lambda_i)\). Nous avons donc une somme directe
            \begin{equation}
                E=E_1\oplus\ldots\oplus E_r.
            \end{equation}
            Dans chacun des espaces propres, $u$ a une matrice diagonale avec la valeur propre correspondante sur la diagonale. Une base de \( E\) constituée d'une base de chacun des espaces propres est donc une base comme nous en cherchons.
    \end{subproof}
\end{proof}

\begin{corollary}       \label{CorQeVqsS}
    Si \( u\) est diagonalisable et si \( F\) est une sous-espace stable par \( u\), alors
    \begin{equation}
        F=\bigoplus_{\lambda}E_{\lambda}(u)\cap F
    \end{equation}
    où \( E_{\lambda}(u)\) est l'espace propre de \( u\) pour la valeur propre \( \lambda\). En particulier la restriction de \( u\) à \( F\), \( u|_F\) est diagonalisable.
\end{corollary}

\begin{proof}
    Par le théorème \ref{ThoDigLEQEXR}, le polynôme \( \mu_u\) est scindé et ne possède que des racines simples. Notons le
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Les espaces \( E_i\) du lemme \ref{LemgnaEOk} sont maintenant les espaces propres.

    En ce qui concerne la diagonalisabilité de \( u|_F\), notons que nous avons une base de \( F\) composée de vecteurs dans les espaces \( E_{\lambda}(u)\). Cette base de \( F\) est une base de vecteurs propres de \( u\).
\end{proof}

\begin{lemma}
    Soit \( E\) un \( \eK\)-espace vectoriel et \( u\in\End(E)\). Si \( \Card\big( \Spec(u) \big)=\dim(E)\) alors \( u\) est diagonalisable.
\end{lemma}

\begin{proof}
    Soient \( \lambda_1,\ldots, \lambda_n\) les valeurs propres distinctes de \( u\). Nous savons que les espaces propres correspondants sont en somme directe (lemme \ref{LemjcztYH}). Par conséquent \( \Span\{ E_{\lambda_i}(u) \}\) est de dimension \( n\) est \( u\) est diagonalisable.
\end{proof}

Voici un résultat de diagonalisation simultanée. Nous donnerons un résultat de trigonalisation simultanée dans le lemme \ref{LemSLGPooIghEPI}.
\begin{proposition}[Diagonalisation simultanée]     \label{PropGqhAMei}
    Soit \( (u_i)_{i\in I}\) une famille d'endomorphismes qui commutent deux à deux.
    \begin{enumerate}
        \item       \label{ItemGqhAMei}
            Si \( i,j\in I\) alors tout sous-espace propre de \( u_i\) est stable par \( u_j\). Autrement dit \( u_j\big(E_{\lambda}(u)\big)\subset E_{\lambda}(u)\).
        \item
            Si les \( u_i\) sont diagonalisables, alors ils le sont simultanément.
    \end{enumerate}
\end{proposition}
\index{diagonalisation!simultanée}

\begin{proof}
    Supposons que \( u_i\) et \( u_j\) commutent et soit \( x\) un vecteur propre de \( u_i\) : \( u_ix=\lambda x\). Nous montrons que \( u_jx\in E_{\lambda}(u)\). Nous avons
    \begin{equation}
        u_i\big( u_j(x) \big)=u_j\big( u_i(x) \big)=\lambda u_j(x).
    \end{equation}
    Par conséquent \( u_j(x)\) est vecteur propre de \( u_i\) de valeur propre \( \lambda\).

    Montrons maintenant l'affirmation à propos des endomorphismes simultanément diagonalisables. Si \( \dim E=1\), le résultat est évident. Nous supposons également qu'aucun des \( u_i\) n'est multiple de l'identité. Nous effectuons une récurrence sur la dimension.

    Soit \( u_0\) un des \( u_i\) et considérons ses valeurs propres deux à deux distinctes \( \lambda_1,\ldots, \lambda_r\). Pour chaque \( k\) nous avons
    \begin{equation}
        E_{\lambda_k}(u_0)\neq E,
    \end{equation}
    sinon \( u_0\) serait un multiple de l'identité. Par contre le fait que \( u_0\) soit diagonalisable permet de décomposer \( E\) en espaces propres de \( u_0\) :
    \begin{equation}
        E=\bigoplus_{k}E_{\lambda_k}(u_0).
    \end{equation}
    Ce que nous allons faire est de simultanément diagonaliser les \( (u_i)_{i\in I}\) sur chacun des \( E_{\lambda_k}\) séparément. Par le point \ref{ItemGqhAMei}, nous avons \( u_i\colon E_{\lambda_k}(u_0)\to E_{\lambda_k}(u_0)\), et nous pouvons considérer la famille d'opérateurs
    \begin{equation}
        \left( u_i|_{E_{\lambda_k}(u_0)} \right)_{i\in I}.
    \end{equation}
    Ce sont tous des opérateurs qui commutent et qui agissent sur un espace de dimension plus petite. Par hypothèse de récurrence nous avons une base de \( E_{\lambda_k}(u_0)\) qui diagonalise tous les \( u_i\).
\end{proof}

\begin{example}     \label{ExewINgYo}
    Soit un espace vectoriel sur un corps \( \eK\). Un opérateur \defe{involutif}{involution} est un opérateur différent de l'identité dont le carré est l'identité. Typiquement une symétrie orthogonale dans \( \eR^3\). Le polynôme caractéristique d'une involution est \( X^2-1=(X+1)(X-1)\).
    
    Tant que \( 1\neq -1\), \( X^1-1\) est donc scindé à racines simples et les involutions sont diagonalisables (\ref{ThoDigLEQEXR}). Cependant si le corps est de caractéristique \( 2\), alors \( X^2-1=(X+1)^2\) et l'involution n'est plus diagonalisable.

    Par exemple si le corps est de caractéristique \( 2\), nous avons
    \begin{subequations}
        \begin{align}
            A&=\begin{pmatrix}
                1    &   1    \\ 
                0    &   1    
            \end{pmatrix}\\
            A^1&=\begin{pmatrix}
                1    &   2    \\ 
                0    &   1    
            \end{pmatrix}=\begin{pmatrix}
                1    &   0    \\ 
                0    &   1    
            \end{pmatrix}.
        \end{align}
    \end{subequations}
    Ce \( A\) est donc une involution mais n'est pas diagonalisable.
\end{example}



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons maintenant le cas de l'espace \( E=\eC^n\) comme espace vectoriel de dimension \( n\) sur \( \eC\). Il est muni d'une forme sesquilinéaire
\begin{equation}    \label{EqFormSesqQrjyPH}
    \langle x, y\rangle =\sum_{k=1}^nx_k\bar y_k
\end{equation}
pour tout \( x,y\in\eC^n\).

\begin{lemma}
    Pour un opérateur hermitien,
    \begin{enumerate}
        \item
            le spectre est réel,
        \item
            deux vecteurs propres à des valeurs propres distinctes sont orthogonales\footnote{Pour la forme \eqref{EqFormSesqQrjyPH}.}.
    \end{enumerate}
\end{lemma}
\index{spectre!matrice hermitienne}

\begin{proof}
    Soit \( v\) un vecteur de valeur propre \( \lambda\). Nous avons d'une part 
    \begin{equation}
        \langle Av, A\rangle =\lambda\langle v, v\rangle =\lambda\| v \|^2,
    \end{equation}
    et d'autre part, en utilisant le fait que \( A\) est hermitien,
    \begin{equation}
        \langle Av, v\rangle =\langle v, A^*v\rangle =\langle v, Av\rangle =\bar\lambda\| v \|^2,
    \end{equation}
    par conséquent \( \lambda=\bar\lambda\) parce que \( v\neq 0\).

    Soient \( \lambda_i\) et \( v_i\) (\( i=1,2\)) deux valeurs propres de \( A\) avec leurs vecteurs propres correspondants. Alors d'une part
    \begin{equation}
        \langle Av_1, v_2\rangle =\lambda_1\langle v_1, v_2\rangle ,
    \end{equation}
    et d'autre part
    \begin{equation}
        \langle Av_1, v_2\rangle =\langle v_1, Av_2\rangle =\lambda_2\langle v_1, v_2\rangle .
    \end{equation}
    Nous avons utilisé le fait que \( \lambda_2\) était réel. Par conséquent, soit \( \lambda_1=\lambda_2\), soit \( \langle v_1, v_2\rangle =0\).
\end{proof}

Le lemme suivant est utile en soi et dit que toute matrice complexe est trigonalisable. Une démonstration alternative passant par le polynôme caractéristique sera présentée dans la remarque \ref{RemXFZTooXkGzQg} utilisant la proposition \ref{PropKNVFooQflQsJ}.
\begin{lemma}[Lemme de Schur complexe\cite{NormHKNPKRqV}]  \label{LemSchurComplHAftTq}
    Si \( A\in\eM(n,\eC)\), il existe une matrice unitaire \( U\) telle que \( UAU^{-1}\) soit triangulaire supérieure.
\end{lemma}
\index{lemme!Schur complexe}
%TODO : Le lemme de Schur est souvent énoncé en disant que si p est une représentation irréductible, alors les seuls endomorphismes de V commutant avec tous les p(g) sont les multiples de l'idenditié. Quel est le lien avec ceci ?

\begin{proof}
    Étant donné que \( \eC\) est algébriquement clos, nous pouvons toujours considérer un vecteur propre \( v_1\) de \( A\), de valeur propre \( \lambda_1\). Nous pouvons utiliser un procédé de Gram-Schmidt pour construire une base orthonormée \( \{ v,u_2,\ldots, u_n \}\) de \( \eR^n\), et la matrice (unitaire)
    \begin{equation}
        Q=\begin{pmatrix}
             \uparrow   &   \uparrow    &       &   \uparrow    \\
             v   &   u_2    &   \cdots    &   u_n    \\ 
             \downarrow   &   \downarrow    &       &   \downarrow
         \end{pmatrix}.
    \end{equation}
    Nous avons \( Q^{-1}AQe_1=Q^{-1} Av=\lambda Q^{-1} v=\lambda e_1\), par conséquent la matrice \( Q^{-1} AQ\) est de la forme
    \begin{equation}
        Q^{-1}AQ=\begin{pmatrix}
            \lambda_1    &   *    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( *\) représente une ligne quelconque et \( A_1\) est une matrice de \( \eM(n-1,\eC)\). Nous pouvons donc répéter le processus sur \( A_1\) et obtenir une matrice triangulaire supérieure (nous utilisons le fait qu'un produit de matrices orthogonales est une matrice orthogonale).  
\end{proof}
En particulier les matrices hermitiennes, anti-hermitiennes et unitaires sont trigonalisables par une matrice unitaire, qui peut être choisie de déterminant \( 1\).

\begin{corollary}   \label{CorUNZooAZULXT}
    Le polynôme caractéristique\footnote{Définition \ref{DefOWQooXbybYD}.} sur \( \eC\) d'une matrice s'écrit sous la forme
    \begin{equation}
        \chi_A(X)=\prod_{i=1}^r(X-\lambda_i)^{m_i}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres distinctes de \( A\) et \( m_i\) sont les multiplicités correspondantes.
\end{corollary}
\index{polynôme!caractéristique}

\begin{proof}
    Le lemme \ref{LemSchurComplHAftTq} nous donne l'existence d'une base de trigonalisation; dans cette base les valeurs propres de \( A\) sont sur la diagonale et nous avons 
    \begin{equation}
        \chi_A(X)=\det(A-X\mtu)=\det\begin{pmatrix}
            X-\lambda_1    &   *    &   *    \\
            0    &   \ddots    &   *    \\
            0    &   0    &   X-\lambda_r
        \end{pmatrix},
    \end{equation}
    qui vaut bien le produit annoncé.
\end{proof}

\begin{theorem}[Théorème spectral pour les matrices normales\cite{LecLinAlgAllen,OMzxpxE,HOQzXCw}]\index{théorème!spectral!matrices normales}  \index{diagonalisation!cas complexe}  \label{ThogammwA}
    Soit \( A\in\eM(n,\eC)\) une matrice de valeurs propres \( \lambda_1,\ldots, \lambda_n\) (non spécialement distinctes). Alors les conditions suivantes sont équivalentes :
    \begin{enumerate}
        \item   \label{ItemJZhFPSi}
            \( A\) est normale,
        \item   \label{ItemJZhFPSii}
            \( A\) se diagonalise par une matrice unitaire,
        \item
            \( \sum_{i,j=1}^n| A_{ij} |^2=\sum_{j=1}^n| \lambda_j |^2\),
        \item
            il existe une base orthonormale de vecteurs propres de \( A\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous allons nous contenter de prouver \ref{ItemJZhFPSi}\( \Leftrightarrow\)\ref{ItemJZhFPSii}.
    %TODO : le reste.

    Soit \( Q\) la matrice unitaire donnée par la décomposition de Schur (lemme \ref{LemSchurComplHAftTq}) : \( A=QTQ^{-1}\). Étant donné que \( A\) est normale nous avons
    \begin{equation}
        QTT^*Q^{-1}=QT^*TQ^{-1},
    \end{equation}
    ce qui montre que \( T\) est également normale. Or une matrice triangulaire supérieure normale est diagonale. En effet nous avons \( T_{ij}=0\) lorsque \( i>j\) et
    \begin{equation}
        (TT^*)_{ii}=(T^*T)_{ii}=\sum_{k=1}^n| T_{ki} |^2=\sum_{k=1}^n| T_{ik} |^2.
    \end{equation}
    Écrivons cela pour \( i=1\) en tenant compte de \( | T_{k1} |^2=0\) pour \( k=2,\ldots, n\),
    \begin{equation}
        | T_{11} |^2=| T_{11} |^2+| T_{12} |^2+\ldots+| T_{1n} |^2,
    \end{equation}
    ce qui implique que \( T_{11}\) est le seul non nul parmi les \( T_{1k}\). En continuant de la sorte avec \( i=2,\ldots, n\) nous trouvons que \( T\) est diagonale.

    Dans l'autre sens, si \( A\) se diagonalise par une matrice unitaire, \( UAU^*=D\), nous avons
    \begin{equation}
        DD^*=UAA^*U^*
    \end{equation}
    et 
    \begin{equation}
        D^*D=UA^*AU^*,
    \end{equation}
    qui ce prouve que \( A\) est normale.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Lemme de Schur réel]  \label{LemSchureRelnrqfiy}
    Soit \( A\in\eM(n,\eR)\). Il existe une matrice orthogonale \( Q\) telle que \( Q^{-1}AQ\) soit de la forme
    \begin{equation}        \label{EqMtrTSqRTA}
        QAQ^{-1}=\begin{pmatrix}
            \lambda_1    &   *    &   *    &   *    &   *\\  
            0    &   \ddots    &   \ddots    &   \ddots    &   \vdots\\  
            0    &   0    &   \lambda_r    &   *    &   *\\  
            0    &   0    &   0    &   \begin{pmatrix}
                a_1    &   b_1    \\ 
                c_1    &   d_1    
            \end{pmatrix}&   *\\  
            0    &   0    &  0     &   0    &   \begin{pmatrix}
                a_s    &   b_s    \\ 
                c_s    &   d_s    
            \end{pmatrix}
        \end{pmatrix}.
    \end{equation}
    Le déterminant de \( A\) est le produit des déterminants des blocs diagonaux et les valeurs propres de \( A\) sont les \( \lambda_1,\ldots, \lambda_r\) et celles de ces blocs.
\end{lemma}
\index{lemme!Schur réel}

\begin{proof}
    Si la matrice \( A\) a des valeurs propres réelles, nous procédons comme dans le cas complexe. Cela nous fournit le partie véritablement triangulaire avec les valeurs propres \( \lambda_1,\ldots, \lambda_r\) sur la diagonale. Supposons donc que \( A\) n'a pas de valeurs propres réelles. Soit donc \( \alpha+i\beta \) une valeur propre (\( \beta\neq 0\)) et \( u+iv\) un vecteur propre correspondant où \( u\) et \( v\) sont des vecteurs réels. Nous avons
    \begin{equation}
        Au+iAv=A(u+iv)=(\alpha+i\beta)(u+iv)=\alpha u-\beta v+i(\alpha v+\beta v),
    \end{equation}
    et en égalisant les parties réelles et imaginaires,
    \begin{subequations}
        \begin{align}
            Au&=\alpha u-\beta v\\
            Av&=\alpha v+\beta u.
        \end{align}
    \end{subequations}
    Sur ces relations nous voyons que ni \( u\) ni \( v\) ne sont nuls. De plus \( u\) et \( v\) sont linéairement indépendants (sur \( \eR\)), en effet si \( v=\lambda u\) nous aurions \( Au=\alpha u-\beta\lambda u=(\alpha-\beta\lambda)u\), ce qui serait une valeur propre réelle alors que nous avions supposé avoir déjà épuisé toutes les valeurs propres réelles.

    Étant donné que \( u\) et \( v\) sont deux vecteurs réels non nuls et linéairement indépendants, nous pouvons trouver une base orthonormée \( \{ q_1,q_2 \}\) de \( \Span\{ u,v \}\). Nous pouvons étendre ces deux vecteurs en une base orthonormée \( \{ q_1,q_2,q_3,\ldots, q_n \}\) de \( \eR^n\). Nous considérons à présent la matrice orthogonale dont les colonnes sont formées de ces vecteurs : \( Q=[q_1\,q_2\,\ldots q_n]\).

    L'espace \( \Span\{ e_1,e_2 \}\) est stable par \( Q^{-1} AQ\), en effet nous avons
    \begin{equation}
        Q^{-1} AQe_1=Q^{-1} Aq_1=Q^{-1}(aq_1+bq_2)=ae_1+be_2.
    \end{equation}
    La matrice \( Q^{-1}AQ\) est donc de la forme
    \begin{equation}
        Q^{-1} AQ=\begin{pmatrix}
            \begin{pmatrix}
                \cdot    &   \cdot    \\ 
                \cdot    &   \cdot    
            \end{pmatrix}&   C_1    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( C_1\) est une matrice réelle \( 2\times (n-1)\) quelconque et \( A_1\) est une matrice réelle \( (n-2)\times (n-2)\). Nous pouvons appliquer une récurrence sur la dimension pour poursuivre.

    Notons que si \( A\) n'a pas de valeurs propres réelles, elle est automatiquement d'ordre pair parce que les valeurs propres complexes viennent par couple complexes conjuguées.

    En ce qui concerne les valeurs propres, il est facile de voir en regardant \eqref{EqMtrTSqRTA} que les valeurs propres sont celles des blocs diagonaux. Étant donné que \( QAQ^{-1}\) et \( A\) ont même polynôme caractéristique, ce sont les valeurs propres de \( A\).
\end{proof}

\begin{theorem}[Théorème spectral, matrice symétrique\cite{KXjFWKA}] \label{ThoeTMXla}
    Une matrice symétrique réelle,
    \begin{enumerate}
        \item
            a un spectre contenu dans \( \eR\)
        \item
            est diagonalisable par une matrice orthogonale.
    \end{enumerate}
    Si \( M\) est une matrice symétrique réelle alors \( \eR^n\) possède une base orthonormée de vecteurs propres de \( M\).
\end{theorem}
\index{diagonalisation!cas réel}
\index{rang!diagonalisation}
\index{endomorphisme!diagonalisation}
\index{spectre!matrice symétrique réelle}
\index{théorème!spectral!matrice symétrique}

\begin{proof}
    Soit \( A\) une matrice réelle symétrique. Si \( \lambda\) est une valeur propre complexe pour le vecteur propre complexe \( v\), alors d'une part \( \langle Av, v\rangle =\lambda\langle v, v\rangle \) et d'autre part \( \langle Av, v\rangle =\langle v, Av\rangle =\bar\lambda\langle v, v\rangle \). Par conséquent \( \lambda=\bar\lambda\).
    
    Le lemme de Schur réel \ref{LemSchureRelnrqfiy} donne une matrice orthogonale qui trigonalise \( A\). Les valeurs propres étant toutes réelles, la matrice \( QAQ^{-1}\) est même triangulaire (il n'y a pas de blocs dans la forme \eqref{EqMtrTSqRTA}). Prouvons que \( QAQ^{-1}\) est symétrique :
    \begin{equation}
        (QAQ^{-1})^t=(Q^{-1})^tA^tQ^t=QA^tQ^{-1}=QAQ^{-1}
    \end{equation}
    où nous avons utilisé le fait que \( Q\) était orthogonale (\( Q^{-1}=Q^t\)) et que \( A\) était symétrique (\( A^t=A\)). Une matrice triangulaire supérieure symétrique est obligatoirement une matrice diagonale.

    En ce qui concerne la base de vecteurs propres, soit \( \{ e_i \}_{i=1,\ldots, n}\) la base canonique de \( \eR^n\) et \( Q\) une matrice orthogonale e telle que \( A=Q^tDQ\) avec \( D\) diagonale. Nous posons \( f_i=Q^te_i\) et en tenant compte du fait que \( Q^t=Q^{-1}\) nous avons \( Af_i=Q^tDQQ^te_i=Q^t\lambda_i e_i=\lambda_if_i\). Donc les \( f_i\) sont des vecteurs propres de \( A\). De plus ils sont orthonormés parce que
    \begin{equation}
        \langle f_i, f_j\rangle =\langle Q^te_i, Q^te_j\rangle =\langle e_i, Q^tQe_j\rangle =\langle e_i, e_j\rangle =\delta_{ij}.
    \end{equation}
\end{proof}
Le théorème spectral pour les opérateurs auto-adjoints sera traité plus bas parce qu'il a besoin de choses sur les formes bilinéaires, théorème \ref{ThoRSBahHH}.
% et les choses sur la dégénérescences utilisent le théorème spectral, cas réel. Donc l'enchaînement est très loumapotiste.

\begin{remark}  \label{RemGKDZfxu}
    Une matrice symétrique est diagonalisable par une matrice orthogonale. Nous pouvons en réalité nous arranger pour diagonaliser par une matrice de \( \SO(n)\). Plus généralement si \( A\) est une matrice diagonalisable par une matrice \( P\in\GL^+(n,\eR)\) alors elle est diagonalisable par une matrice de \( \GL^-(n,\eR)\) en changeant le signe de la première ligne de \( P\). Et inversement.

    En effet, si nous avons \( P^tDP=A\), alors en notant \( *\) les quantités qui ne dépendent pas de \( a\), \( b\) ou~\( c\),
    \begin{equation}
        \begin{aligned}[]
        \begin{pmatrix}
            a    &   *    &   *    \\
            b    &   *    &   *    \\
            c    &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1    &       &       \\
                &   \lambda_2    &       \\
                &       &   \lambda_3
            \end{pmatrix}
            \begin{pmatrix}
                a    &   b    &   c    \\
                *    &   *    &   *    \\
                *    &   *    &   *
            \end{pmatrix}&=
        \begin{pmatrix}
            a    &   *    &   *    \\
            b    &   *    &   *    \\
            c    &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1a    &   \lambda_1b    &   \lambda_1c    \\
            *    &   *    &   *    \\
            *    &   *    &   *
        \end{pmatrix}\\
        &=\begin{pmatrix}
            \lambda_1 a^2+*   &   \lambda_1ab+*    &   \lambda_1ac  +*  \\
            \ldots    &   \ldots    &   \ldots    \\
            \ldots    &   \ldots    &   \ldots
        \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Nous voyons donc que si nous changeons les signes de \( a\), \( b\) et \( c\) en même temps, le résultat ne change pas.
\end{remark}

\begin{definition}[Matrice définie positive]    \label{DefAWAooCMPuVM}
    Une matrice symétrique est \defe{définie positive}{matrice!définie positive} si toutes ses valeurs propres sont strictement positives. Elle est \defe{semi-définie positive}{semi-définie positive} si ses valeurs propres sont positives ou nulles.
\end{definition}
Afin d'éviter l'une ou l'autre confusion, nous disons souvent \emph{strictement} définie positive pour positive.

Nous notons \( S^+(n,\eR)\)\nomenclature[A]{\( S^+(n,\eR)\)}{matrices symétriques semi-définies positives} l'ensemble des matrices réelles \( n\times n\) semi-définies positives. L'ensemble \( S^{++}(n,\eR)\)\nomenclature[A]{\( S^{++}(n,\eR)\)}{matrices symétriques strictement définies positives} est l'ensemble des matrices symétriques strictement définies positives.

\begin{remark}
    Nous ne définissons pas la notion de matrice définie positive pour une matrice non symétrique.
\end{remark}

\begin{lemma}   \label{LemWZFSooYvksjw}
    La matrice symétrique \( M\) est définie positive si et seulement si \( \langle x, Mx\rangle >0\) pour tout \( x\in \eR^n\). 
\end{lemma}

\begin{proof}
    Soit \( \{ e_i \}_{i=1,\ldots, n}\) une base orthonormée de vecteurs propres de \( M\) dont l'existence est assurée par le théorème spectral \ref{ThoeTMXla}. Nous nommons \( x_i\) les coordonnées de \( x\) dans cette base. Alors,
    \begin{equation}
        \langle x,Mx \rangle =\sum_{i,j}x_i\langle e_i, x_jMe_j\rangle =\sum_{i,j}x_ix_j\langle e_i, \lambda_je_j\rangle =\sum_{ij}x_ix_j\lambda_j\delta_{ij}=\sum_i\lambda_ix_i^2
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( M\). Cela est strictement positif pour tout \( x\) si et seulement si tous les \( \lambda_i\) sont positifs.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Pseudo-réduction simultanée}
%---------------------------------------------------------------------------------------------------------------------------

\begin{corollary}[Pseudo-réduction simultanée\cite{JMYQgLO}]  \label{CorNHKnLVA}
    Soient \( A,B\in \gS(n,\eR)\) avec \( A\) définie positive\footnote{Définition \ref{DefAWAooCMPuVM}.}. Alors il existe \( Q\in \GL(n,\eR)\) tell que \( Q^tBQ\) soit diagonale et \( Q^tAQ=\mtu\).
\end{corollary}

\begin{proof}
    Nous allons noter \( x\cdot y\) le produit scalaire usuel de \( \eR^n\) et \( \{ e_i \}_{i=1,\ldots, n}\) sa base canonique.

    Vu que \( A\) est définie positive, nous avons que l'expression\footnote{On peut aussi l'écrire de façon plus matricielle sous la forme \( \langle x, y\rangle =x^tAy\).} \( \langle x, y\rangle =x\cdot Ay\) est un produit scalaire sur \( \eR^n\). Autrement dit, \( E\) muni de cette forme bilinéaire symétrique est un espace euclidien, ce qui fait dire à la proposition \ref{PropUMtEqkb} qu'il existe une base de \( \eR^n\) orthonormée \( \{ f_i \}_{i=1,\ldots, n}\) pour ce produit scalaire, c'est à dire qu'il existe une matrice \( P\in \GL(n,\eR)\) telle que \( P^tAP=\mtu\). Ici, \( P\) est la matrice de changement de base de la base canonique à notre base orthonormée, c'est à dire la matrice qui fait \( Pe_i=f_i\) pour tout \( i\). Voyons cela avec un peu de détails.

    Pour savoir ce que valent les éléments de la matrice \( P^tAP\), nous nous souvenons que \( P^tAPe_j\) est un vecteur dont les coordonnées sont les éléments de la \( j\)\ieme colonne de \( P^tAP\). Nous avons donc \( (P^tAP)_{ij}=e_i\cdot P^tAPe_i\). Calculons :
    \begin{equation}
            (P^tAP)_{ij}=e_i\cdot P^tAPe_i
            =Pe_i\cdot APe_j
        =f_i\cdot Af_j
        =\langle f_i, f_j\rangle 
        =\delta_{ij}
    \end{equation}
    où nous avons utilisé le fait que \( A\) était auto-adjointe pour la passer de l'autre côté du produit scalaire (usuel). Au final nous avons effectivement \( P^tAP=\mtu\).

    La matrice \( P^tBP\) est une matrice symétrique, donc le théorème spectral \ref{ThoeTMXla} nous donne une matrice \( R\in \gO(n,\eR)\) telle que \( R^tP^tBPR\) soit diagonale. En posant maintenant \( Q=PR\) nous avons la matrice cherchée.
\end{proof}
Note : nous avons prouvé la pseudo-réduction simultanée comme corollaire du théorème de diagonalisation des matrices symétriques \ref{ThoeTMXla}. Il aurait aussi pu être vu comme un corollaire du théorème spectral \ref{ThoRSBahHH} sur les opérateurs auto-adjoints via son corollaire \ref{CorSMHpoVK}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dilatations et transvections}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[\cite{PAXrsMn}]     \label{ThoooAZKDooNDcznv}
    Soit une application linéaire \( u\colon E\to E\) dont les points fixes forment un hyperplan noté \( H\) d'équation \( H=\ker(f)\) avec \( f\in E^*\).
    \begin{enumerate}
        \item     
            Les affirmations suivantes sont équivalentes :
            \begin{enumerate}
                \item  \label{ITEMooZHYRooFGKaifi}
                    \( \det(u)\neq 1\)
                \item       \label{ooXKLWooTfUMzV}
                    L'application \( u\) est diagonalisable et a une valeur propre qui vaut \( \det(u)\neq 1\).
                \item       \label{ooMZPTooCLylbh}  
                    \( \Image(u-\id)\nsubseteq H\).
                \item   \label{ITEMooZHYRooFGKaifiv}
                    Il existe une base de \( E\) dans laquelle la matrice de \( u\) est \( \diag(1,\ldots, 1,\lambda)\) avec \( \lambda\neq 1\).
            \end{enumerate}
        \item
            Les affirmation suivantes sont équivalentes :
            \let\oldthenumii\theenumi
            \renewcommand{\theenumii}{\roman{enumii}}
            \begin{enumerate}
                \item       \label{ITEMooRTIEooOoWCFsa}
                    Il existe \( a\in H\) tel que pour tout \( x\in E\), \( u(x)=x+f(x)a\).
                \item       \label{ITEMooRTIEooOoWCFsb}
                    Dans une base adaptée, la matrice de \( u\) est donnée par
                    \begin{equation}
                        \begin{pmatrix}
                             1   &       &       &       \\
                                &   \ddots    &       &       \\
                                &       &   1    &   1    \\ 
                                &       &       &   1     
                         \end{pmatrix}.
                    \end{equation}
            \end{enumerate}
            \let\theenumii\oldtheenumii
        \item
            Les conditions \ref{ITEMooZHYRooFGKaifi}-\ref{ITEMooZHYRooFGKaifiv} sont respectées si et seulement si les conditions \ref{ITEMooRTIEooOoWCFsa}-\ref{ITEMooRTIEooOoWCFsb} ne sont pas respectées (elles sont les négations l'une de l'autre.).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous allons prouver plein d'implications \ldots
    \begin{subproof}
    \item[\ref{ITEMooZHYRooFGKaifi} implique \ref{ooXKLWooTfUMzV}]
        Le théorème de la base incomplete (voir remarque \ref{REMooYGJEooEcZQKa}) permet de considérer une base \( \{ e_1,\ldots, e_n \}\) de \( E\) telle que \( \{ e_1,\ldots, e_{n-1} \} \) soit une base de \( H\). Dans cette base, la matrice de \( u\) est de la forme suivante (les cases non remplies sont nulles et les étoiles correspondent à des valeurs inconnues mais pas spécialement nulles) :
        \begin{equation}        \label{EqooPQOEooGUyIwa}
        \begin{pmatrix}
             1   &       &       &   *    \\
                &   \ddots    &       &   \vdots    \\
                &       &   1    &   *    \\ 
                &       &       &   \lambda     
         \end{pmatrix}
        \end{equation}
        Le fait que le déterminant de \( u\) ne soit pas \( 1\) implique que \( \lambda\neq 1\). Par conséquent le polynôme caractéristique
        \begin{equation}
            \chi_u(X)=(1-X)^{n-1}(\lambda-X)
        \end{equation}
        possède une racine \( \lambda\neq 1\), et donc \( u\) possède un vecteur propre \( v\) pour cette valeur\footnote{Proposition \ref{PropooBYZCooBmYLSc}.}. Le vecteur \( v\) est linéairement indépendant de \( \{ e_1,\ldots, e_{n-1} \}\) (parce que vecteur propre de valeur propre différente). Par conséquent l'ensemble \( \{ e_1,\ldots, e_{n-1},v \}\) est une base par le théorème \ref{ThoMGQZooIgrXjy}\ref{ItemHIVAooPnTlsBi}. Cela est une base de vecteurs propres et donc une base de diagonalisation\footnote{Nous pourrions en dire à peine plus et prouver le point \ref{ITEMooZHYRooFGKaifiv}, mais cela ne servirait à rien parce que nous voulons prouver les équivalences et qu'il faudra quand même prouver que \ref{ooMZPTooCLylbh} implique \ref{ITEMooZHYRooFGKaifiv}.}.
    \item[\ref{ooXKLWooTfUMzV} implique \ref{ooMZPTooCLylbh}]
        Nous nommons maintenant \( \{ e_1,\ldots, e_{n} \}\) la base de diagonalisation. Nous avons \( u(e_n)=\lambda e_n\) avec \( \det(u)=\lambda\neq 1\). Nous avons
        \begin{equation}
            (u-\id)(e_n)=(\lambda-1)e_n\notin H,
        \end{equation}
        ce qui prouver que l'image de \( e_n\) par \( u-\id\) n'est pas dans \( H\).
    \item[\ref{ooMZPTooCLylbh} implique \ref{ITEMooZHYRooFGKaifiv}]
        Reprenons une base \( \{ e_1,\ldots, ,e_n \}\) donnant la matrice \eqref{EqooPQOEooGUyIwa}. Il existe \( x\in E\) tel que \( u(x)-x\) n'est pas dans \( H\), c'est à dire tel que \( u\big( u(x)-x \big)\neq u(x)-x\). Nous en déduisons que
        \begin{equation}
            u^2(x)-2u(x)+x\neq 0
        \end{equation}
        ou encore que 
        \begin{equation}
            (X-1)^2(u)x\neq 0.
        \end{equation}
        C'est à dire que \( (X-1)^2\) n'est pas un polynôme annulateur de \( u\). Or ce serait le cas si \( X-1\) était le polynôme minimal (proposition \ref{PropAnnncEcCxj}). Le polynôme caractéristique étant \( (X-1)^{n-1}(X-\lambda)\) (et étant annulateur\footnote{Théorème de Cayley-Hamilton \ref{ThoCalYWLbJQ}.}), le polynôme minimal est de la forme 
        \begin{equation}
            \mu_u(X)=\begin{cases}
                (X-1)(X-\lambda)    &   \text{si \( \lambda\neq 1\)}\\
                X-1    &    \text{si \( \lambda=1\)}.
            \end{cases}
        \end{equation}
        Dans notre cas nous venons de voir que ce n'est pas \( X-1\) et donc c'est \( (X-1)(X-\lambda)\) avec \( \lambda\neq 1\).

        Nous devons trouver une base de diagonalisation \ldots Supposons
        \begin{equation}
            u(e_n)=\sum_{k=1}^{n-1}a_ke_k+\lambda e_n,
        \end{equation}
        dans lequel nous venons de prouver que \( \lambda\neq 1\), et cherchons
        \begin{equation}
            e'_n=\sum\_{j=1}^np_je_j
        \end{equation}
        de telle sorte à avoir \( u(e'_n)=\lambda e_n\). Nous avons
        \begin{subequations}
            \begin{align}
                u(e'_n)&=\sum_{j=1}^{n-1}p_ju(e_j)+p_nu(e_n)\\
                &=\sum_{j=1}^{n-1}(p_j+p_na_j)e_j+p_n\lambda e_n.
            \end{align}
        \end{subequations}
        En égalisant à \( \lambda\sum_{j=1}^np_je_j\), il vient
        \begin{equation}
            p_j+p_na_j=\lambda p_j
        \end{equation}
        pour tout \( j=1,\ldots, n-1\) et la condition triviale \( p_n\lambda=\lambda p_n\) pour \( j=n\). Nous en déduisons que le choix
        \begin{equation}
            p_j=\frac{ p_na_j }{ \lambda-1 }
        \end{equation}
        fonctionne (parce que \( \lambda\neq 1\) comme nous l'avons démontré plus haut). En bref, il suffit de poser
        \begin{equation}
            e'_n=\sum_{j=1}^{n-1}\frac{ p_na_j }{ \lambda-1 }e_j+p_ne_n
        \end{equation}
        avec \( p_n\) au choix pour avoir une base \( \{ e_1,\ldots, e_{n-1},e'_n \}\) de diagonalisation de \( u\) avec \( \lambda\neq 1\) comme dernière valeur propre.
    \item[\ref{ITEMooZHYRooFGKaifiv} implique \ref{ITEMooZHYRooFGKaifi}] Évident \ldots encore qu'il faut invoquer l'invariance du déterminant par changement de base.
    \end{subproof}
    Nous avons terminé la première série d'équivalences. Nous continuons avec la seconde.
       \begin{subproof}
        \item[\ref{ITEMooRTIEooOoWCFsa} implique \ref{ITEMooRTIEooOoWCFsb}]
            Nous prenons \( e_{n-1}=a\) et nous complétons en une base de \( H\). Pour \( e_n\) il suffit de prendre n'importe quel vecteur \( v\) tel que \( f(v)\neq 0\) (qui existe parce que \( f=0\) est seulement un hyperplan), et de le normaliser.

            Dans cette base, la matrice de \( u\) a la forme désirée parce que \( u(e_n)=e_n+f(e_n)a=e_n+e_{n-1}\) du fait que \( e_{n-1}=a\) et \( f(e_n)=1\).
        \item[\ref{ITEMooRTIEooOoWCFsb} implique \ref{ITEMooRTIEooOoWCFsa}]
            Soit \( \{ e_1,\ldots, e_n \}\) cette base. En prenant \( a=e_{n-1}\) et en posant \( x=\sum_kx_ke_k\) nous avons
            \begin{equation}
                u(x)=\sum_{k=1}^{n-1}x_ke_k+x_n(e_{n-1}+e_n)=x+x_ne_{n-1}=x_na.
            \end{equation}
            Mais vu que \( f(x)=\sum_if_ix_i\), et que \( f(e_i)=0\) pour tout \( i=1,\ldots, n-1\) nous avons \( f(x)=f_nx_n\). Il n'y a cependant pas de raisons d'avoir \( f_n=1\). Cependant en définissant
            \begin{equation}
                e'_i=\frac{1}{ f_n }e_i
            \end{equation}
            nous avons bien \( u(e'_n)=\frac{1}{ f_n }(e_{n-1}+e_n)=e'_{n-1}+e'_n\). Donc dans cette base nous avons encore la matrice de \( u\) de la forme
            \begin{equation}
                \begin{pmatrix}
                     1   &       &       &       \\
                        &   \ddots    &       &       \\
                        &       &   1    &   1    \\ 
                        &       &       &   1     
                 \end{pmatrix},
            \end{equation}
            mais cette fois avec \( f(e'_n)=1\).
    \end{subproof}
    Nous avons terminé avec la seconde série d'équivalences. Il nous reste à prouver que la première est équivalente à la négation de la seconde.
    \begin{subproof}
        \item[non \ref{ooMZPTooCLylbh} implique \ref{ITEMooRTIEooOoWCFsa}]
            Considérons \( x_0\in E\) tel que \( f(x_0)=1\) et posons \( a=u(x_0)-x_0\in\Image(u-\id)\). Par la négation de \ref{ooMZPTooCLylbh} nous avons \( a\in H\). De plus \( x_0\notin H\) (sinon \( f(x_0)=0\)) donc \( u(x_0)\neq x_0\) et \( a\neq 0\).

            Nous montrons que ce choix de \( a\) fonctionne : \( u(x)=x+f(x)a\) pour tout \( x\in E\). Nous faisons cela séparément pour \( x\in H\) et pour \( x=x_0\). 

            Si \( h\in H\) alors \( u(h)=h\) et \( f(h)=0\) donc \( h+f(h)a=h=u(h)\). Si \( x=x_0\) alors \( u(x_0)=a+x_0\) (cela est la définition de \( a\)) et\( x_0+f(x_0)a=x_0+a\).
        \item[\ref{ITEMooRTIEooOoWCFsb} implique non \ref{ITEMooZHYRooFGKaifi}]
           Dans une base adaptée nous avons 
           \begin{equation}
               \begin{pmatrix}
                    1    &       &       &       \\
                        &   \ddots    &       &       \\
                        &       &   1    &   1    \\ 
                        &       &       &   1     
                \end{pmatrix},
           \end{equation}
           et donc \( \det(u)=1\), ce qui contredit \ref{ITEMooZHYRooFGKaifi}.
    \end{subproof}
\end{proof}



%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Sous espaces caractéristiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

% TODO : lire le blog de Pierre Bernard; en particulier celle-ci : http://allken-bernard.org/pierre/weblog/?p=2299

Lorsqu'un opérateur n'est pas diagonalisable, les valeurs propres jouent quand même un rôle important.

\begin{definition}  \label{DefFBNIooCGbIix}
    Soit \( E\) un \( \eK\)-espace vectoriel  \( f\in\End(E)\). Pour \( \lambda\in \eK\) nous définissons
    \begin{equation}
        F_{\lambda}(f)=\{ v\in E\tq (f-\lambda\mtu)^nv=0, n\in\eN \}
    \end{equation}
    et nous appelons ça un \defe{sous-espace caractéristique}{sous-espace!caractéristique} de \( f\).
\end{definition}
L'espace \( F_{\lambda}(f)\) est l'ensemble de nilpotence de l'opérateur \( f-\lambda\mtu\) et

\begin{lemma}   \label{LemBLPooHMAoyJ}
    L'ensemble \( F_{\lambda}(f)\) est non vide si et seulement si \( \lambda\) est une valeur propre de \( f\). L'espace \( F_{\lambda}(f)\) est invariant sous \( f\).
\end{lemma}

\begin{proof}
    Si \( F_{\lambda}(f)\) est non vide, nous considérons \( v\in F_{\lambda}(f)\) et \( n\) le plus petit entier non nul tel que \( (f-\lambda)^nv=0\). Alors \( (f-\lambda)^{n-1}v\) est un vecteur propre de \( f\) pour la valeur propre \( \lambda\). Inversement si \( v\) est une valeur propre de \( f\) pour la valeur propre \( \lambda\), alors \( v\in F_{\lambda}(f)\).

    En ce qui concerne l'invariance, remarquons que \( f\) commute avec \( f-\lambda\mtu\). Si \( x\in F_{\lambda}(f)\) il existe \( n\) tel que \( (f-\lambda\mtu)^nx=0\). Nous avons aussi
    \begin{equation}
        (f-\lambda\mtu)^nf(x)=f\big( (f-\lambda\mtu)^nx \big)=0,
    \end{equation}
    par conséquent \( f(x)\in F_{\lambda}(f)\).
\end{proof}

\begin{remark}  \label{RemBOGooCLMwyb}
    Toute matrice sur \( \eC\) n'est pas diagonalisable : nous en avons déjà donné une exemple simple en \ref{ExBRXUooIlUnSx}. Nous en voyons maintenant un moins simple. Considérons en effet l'endomorphisme \( f\) donné par la matrice
    \begin{equation}
        \begin{pmatrix}
            a&    \alpha    &   \beta    \\
            0    &   a    &   \gamma    \\
            0    &   0    &   b
        \end{pmatrix}
    \end{equation}
    où \( a\neq b\), \( \alpha\neq 0\), \( \beta\) et \( \gamma\) sont des nombres complexes quelconques.
    Son polynôme caractéristique est 
    \begin{equation}
        \chi_f(\lambda)=(a-\lambda)^2(b-\lambda),
    \end{equation}
    et les valeurs propres sont donc \( a\) et \( b\). Nous trouvons les vecteurs propres pour la valeur \( a\) en résolvant
    \begin{equation}
        \begin{pmatrix}
            a    &   \alpha    &   \beta    \\
            0    &   a    &   \gamma    \\
            0    &   0    &   b
        \end{pmatrix}\begin{pmatrix}
            x    \\ 
            y    \\ 
            z    
        \end{pmatrix}=\begin{pmatrix}
            ax    \\ 
            ay    \\ 
            az    
        \end{pmatrix}.
    \end{equation}
    L'espace propre \( E_a(f)\) est réduit à une seule dimension générée par \( (1,0,0)\). De la même façon l'espace propre correspondant à la valeur propre \( b\) est donné par 
    \begin{equation}
        \begin{pmatrix}
            \frac{1}{ b-a }\left( \beta+\frac{ \alpha\gamma }{ b-a } \right)    \\ 
            \frac{ \gamma }{ b-a }    \\ 
            1    
        \end{pmatrix}.
    \end{equation}
    Il n'y a donc pas trois vecteurs propres linéairement indépendants, et l'opérateur \( f\) n'est pas diagonalisable.

    Par contre nous pouvons voir que
    \begin{equation}
        (f-\alpha\mtu)^2\begin{pmatrix}
             0   \\ 
            1    \\ 
            0    
        \end{pmatrix}=
        \begin{pmatrix}
            a    &   \alpha    &   \beta    \\
            0    &   a    &   \gamma    \\
            0    &   0    &   b
        \end{pmatrix}
        \begin{pmatrix}
            \alpha    \\ 
            0    \\ 
            0    
        \end{pmatrix}-\begin{pmatrix}
            a\alpha    \\ 
            0    \\ 
            0    
        \end{pmatrix}=\begin{pmatrix}
            0    \\ 
            0    \\ 
            0    
        \end{pmatrix},
    \end{equation}
    de telle sorte que le vecteur \( (0,1,0)\) soit également dans l'espace caractéristique \( F_a(f)\).

    Dans cet exemple, la multiplicité algébrique de la racine \( a\) du polynôme caractéristique vaut \( 2\) tandis que sa multiplicité géométrique vaut seulement \( 1\).
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorèmes de décomposition}
%---------------------------------------------------------------------------------------------------------------------------

%TODO : Je crois qu'on peut remplacer l'hypothèse de corps algébriquement clos par le polynôme caractéristique scindé.
\begin{theorem}[Théorème spectral, décomposition primaire]\index{théorème!spectral}     \label{ThoSpectraluRMLok}
    Soit \( E\) espace vectoriel de dimension finie sur le corps algébriquement clos \( \eK\) et \( f\in\End(E)\). Alors
    \begin{equation}    \label{EqCTFHooBSGhYK}
        E=F_{\lambda_1}(f)\oplus\ldots\oplus F_{\lambda_k}(f)
    \end{equation}
    où la somme est sur les valeurs propres distinctes de \( f\).

    Les projecteurs sur les espaces caractéristique forment un système complet et orthogonal.
\end{theorem}
\index{décomposition!primaire}
\index{décomposition!spectrale}
\index{décomposition!sous-espaces caractéristiques}

\begin{proof}
    Soit \( P\) le polynôme caractéristique de \( f\) et une décomposition
    \begin{equation}
        P=(f-\lambda_1)^{\alpha_1}\ldots(f-\lambda_r)^{\alpha_r}
    \end{equation}
    en facteurs irréductibles. La le théorème de noyaux (\ref{ThoDecompNoyayzzMWod}) nous avons
    \begin{equation}        \label{EqDeFVSaYv}
        E=\ker(f-\lambda_1)^{\alpha_1}\oplus\ldots\oplus\ker(f-\lambda_r)^{\alpha_r}.
    \end{equation}
    Les projecteurs sont des polynômes en \( f\) et forment un système orthogonal. Il nous reste à prouver que \( \ker(f-\lambda_i)^{\alpha_i}=F_{\lambda_i}(f)\). L'inclusion
    \begin{equation}    \label{EqzmNxPi}
        \ker(f-\lambda_i)^{\alpha_i}\subset F_{\lambda_i}(f)
    \end{equation}
    est évidente. Nous devons montrer l'inclusion inverse. Prouvons que la somme des \( F_{\lambda_i}(f)\) est directe. Si \( v\in F_{\lambda_i}(f)\cap F_{\lambda_j}(f)\), alors il existe \( v_1=(f-\lambda_i)^nv\neq 0\) avec \( (f-\lambda_i)v_1=0\). Étant donné que \( (f-\lambda_i)\) commute avec \( (f-\lambda_j)\), ce \( v_1\) est encore dans \( F_{\lambda_j}(f)\) et par conséquent il existe \( w=(f-\lambda_j)^mv_1\) non nul tel que 
    \begin{subequations}
        \begin{numcases}{}
            (f-\lambda_i)w=0\\
            (f-\lambda_j)w=0.
        \end{numcases}
    \end{subequations}
    Ce \( w\) serait donc un vecteur propre simultané pour les valeurs propres \( \lambda_i\) et \( \lambda_j\), ce qui est impossible parce que les espaces propres sont linéairement indépendants. Les espaces \( F_{\lambda_i}\) sont donc en somme directe et
    \begin{equation}
        \sum_i\dim F_{\lambda_i}(f)\leq \dim E.
    \end{equation}
    En tenant compte de l'inclusion \eqref{EqzmNxPi} nous avons même
    \begin{equation}
        \dim E=\sum_i\dim\ker(f-\lambda_i)^{\alpha_i}\leq\sum_i F_{\lambda_i}(f)\leq \dim E.
    \end{equation}
    Par conséquent nous avons \( \dim\ker(f-\lambda_i)^{\alpha_i}=\dim F_{\lambda_i}(f)\) et l'égalité des deux espaces.
\end{proof}


\begin{probleme}
    Dans le cas où le corps n'est pas algébriquement clos, il paraît qu'il faut remplacer «diagonalisable» par «semi-simple».
\end{probleme}
%TODO : peut-être qu'il y a la réponse dans http://www.math.jussieu.fr/~romagny/agreg/dvt/endom_semi_simples.pdf

Si l'espace vectoriel est sur un corps algébriquement clos, alors les endomorphismes semi-simples\footnote{Définition \ref{DEFooBOHVooSOopJN}.} sont les endomorphismes diagonaux.


%TODO : Je crois qu'on peut remplacer l'hypothèse de corps algébriquement clos par le polynôme caractéristique scindé.
\begin{theorem}[Décomposition de Dunford] \label{ThoRURcpW}
    Soit \( E\) un espace vectoriel sur le corps algébriquement clos \( \eK\) et \( u\in\End(E)\) un endomorphisme de \( E\). 
    
    \begin{enumerate}
        \item
            
            L'endomorphisme \( u\) se décompose de façon unique sous la forme
            \begin{equation}
                u=s+n
            \end{equation}
            où \( s\) est diagonalisable, \( n\) est nilpotent et \( [s,n]=0\).
        \item
            Les endomorphismes \( s\) et \( n\) sont des polynômes en \( u\) et commutent avec \( u\).
        \item   \label{ItemThoRURcpWiii}
            Les parties \( s\) et \( n\) sont données par
            \begin{subequations}
                \begin{align}
                    s&=\sum_i\lambda_ip_i\\
                    n&=\sum_i(s-\lambda_i\mtu)p_i
                \end{align}
            \end{subequations}
            où les sommes sont sur les valeurs propres distinctes\footnote{C'est à dire sur les sous-espaces caractéristiques.} de \( f\) et où \( p_i\colon E\to F_{\lambda_i}(u)\) est la projection de \( E\) sur \( F_{\lambda_i}(u)\).
    \end{enumerate}
\end{theorem}
\index{décomposition!Dunford}
\index{Dunford!décomposition}
\index{réduction!d'endomorphisme}
\index{endomorphisme!sous-espace stable}
\index{polynôme!d'endomorphisme!décomposition de Dunford}
\index{endomorphisme!diagonalisable!Dunford}
\index{endomorphisme!nilpotent!Dunford}
%TODO : comprendre comment on calcule des exponentielles de matrices avec Dunford.

\begin{proof}
    Le théorème spectral \ref{ThoSpectraluRMLok} nous indique que
    \begin{equation}
        E=\bigoplus_iF_{\lambda_i}(f).
    \end{equation}
    Nous considérons l'endomorphisme \( s\) de \( E\) qui consiste à dilater d'un facteur \( \lambda\) l'espace caractéristique \( F_{\lambda}(f)\) :
    \begin{equation}
        s=\sum_i\lambda_ip_i
    \end{equation}
    où \( p_i\colon E\to F_{\lambda_i}(u)\) est la projection de \( E\) sur \( F_{\lambda_i}(u)\).

    Nous allons prouver que \( [s,f]=0\) et \( n=f-s\) est nilpotent. Cela impliquera que \( [s,n]=0\).

    Si \( x\in F_{\lambda}(f)\), alors nous avons \( sf(x)=\lambda f(x)\) parce que \( f(x)\in F_{\lambda}(f)\) tandis que \( fs(x)=f(\lambda x)=\lambda f(x)\). Par conséquent \( f\) commute avec \( s\).

    Pour montrer que \( f-s\) est nilpotent, nous en considérons la restriction
    \begin{equation}
        f-s\colon F_{\lambda}(f)\to F_{\lambda}(f).
    \end{equation}
    Cet opérateur est égal à \( f-\lambda\mtu\) et est par conséquent nilpotent.

    Prouvons à présent l'unicité. Soit \( u=s'+n'\) une autre décomposition qui satisfait aux conditions : \( s'\) est diagonalisable, \( n'\) est nilpotent et \( [n',s']=0\). Commençons par prouver que \( s'\) et \( n'\) commutent avec \( u\). En multipliant \( u=s'+n'\) par \( s'\) nous avons
    \begin{equation}
        s'u=s'^2+s'n'=s'^2+n's'=(s'+n')s'=us',
    \end{equation}
    par conséquent \( [u,s']=0\). Nous faisons la même chose avec \( n'\) pour trouver \( [u,n']=0\). Notons que pour obtenir ce résultat nous avons utilisé le fait que \( n'\) et \( s'\) commutent, mais pas leur propriétés de nilpotence et de diagonalisibilité.
    
    
    Si \( s'+n'=s+n\) est une autre décomposition, \( s'\) et \( n'\) commutent avec \( u\), et par conséquent avec tous les polynômes en \( u\). Ils commutent en particulier avec \( n\) et \( s\). Les endomorphismes \( s\) et \( s'\) sont alors deux endomorphismes diagonalisables qui commutent. Par la proposition \ref{PropGqhAMei}, ils sont simultanément diagonalisables. Dans la base de simultanée diagonalisation, la matrice de l'opérateur \( s'-s=n-n'\) est donc diagonale. Mais \( n-n'\) est également nilpotent, en effet si \( A\) et \( B\) sont deux opérateurs nilpotents,
    \begin{equation}
        (A+B)^n=\sum_{k=0}^n\binom{k}{n}A^kB^{n-k}.
    \end{equation}
    Si \( n\) est assez grand, au moins un parmi \( A^k\) ou \( B^{n-k}\) est nul.

    Maintenant que \( n-n'\) est diagonal et nilpotent, il est nul et \( n=n'\). Nous avons alors immédiatement aussi \( s=s'\).

\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Diverses conséquences}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}
    Soit une matrice \( A\in \eM(n,\eC)\). On a que la suite \( (A^kx)\) tends vers zéro pour tout \( x\) si et seulement si \( \rho(A)<1\) où \( \rho(A)\)\index{rayon!spectral} est le rayon spectral de $A$
\end{theorem}
\index{décomposition!Dunford!exponentielle de matrice}

\begin{proof}
    Dans le sens direct, il suffit de prendre comme \( x\), un vecteur propre de \( A\). Dans ce cas nous avons \( A^kx=\lambda^kx\). Mais \( \lambda^kx\) ne tend vers zéro que si \( \lambda<1\). Donc toute les valeurs propres de \( A\) doivent être plus petite que \( 1\) et \( \rho(A)<1\).

    Pour l'autre sens nous utilisons la décomposition de Dunford (théorème \ref{ThoRURcpW}) : il existe une matrice inversible \( P\) telle que
    \begin{equation}
        A=P^{-1}(D+N)P
    \end{equation}
    où \( D\) est diagonale, \( N\) est nilpotente et \( [D,N]=0\). Étant donné que \( D+N\) est triangulaire, son polynôme caractéristique que
    \begin{equation}
        \chi_{D+N}(\lambda)=\prod_i D_{ii}-\lambda.
    \end{equation}
    Par similitude, c'est le même polynôme caractéristique que celui de \( A\) et nous savons alors que la diagonale de \( D\) contient les valeurs propres de \( A\).

    Par ailleurs nous avons
    \begin{subequations}
        \begin{align}
            A^k&=P^{-1}(D+N)^kP\\
            &=P^{-1}\sum_{j=0}^k{j\choose k}D^{j-k}N^jP\\
            &=P^{-1}\sum_{j=0}^{n-1}{j\choose k}D^{j-k}N^jP
        \end{align}
    \end{subequations}
    où nous avons utilité le fait que \( D\) et \( N\) commutent ainsi que \( N^{n-1}=0\) parce que \( N\) est nilpotente. Nous utilisons la norme matricielle usuelle, pour laquelle \( \| D \|=\rho(D)=\rho(A)\). Nous avons alors
    \begin{equation}
        \| (D+N)^k \|\leq \sum_{j=0}^k{j\choose k}\rho(D)^{k-j}\| N \|^j.
    \end{equation}
    Du coup si \( \rho(D)<1\) alors \( \| (D+N)^k \|\to 0\) (et c'est même un si et seulement si).
\end{proof}

Une application de la décomposition de Jordan est l'existence d'un logarithme pour les matrices. La proposition suivant va d'une certaine manière donner un logarithme pour les matrices inversibles complexes. Dans le cas des matrices réelles \( m\) telles que \( \| m-\mtu \|<1\), nous donnerons au lemme \ref{LemQZIQxaB} une formule pour le logarithme sous forme d'une série; ce logarithme sera réel.
\begin{proposition} \label{PropKKdmnkD}
    Toute matrice inversible complexe est une exponentielle.
\end{proposition}
\index{exponentielle!de matrice}
\index{décomposition!Jordan!et exponentielle de matrice}

\begin{proof}
    Soit \( A\in \GL(n,\eC)\); nous allons donner une matrice \( B\in \eM(n,\eC)\) telle que \( A=\exp(B)\). D'abord remarquons qu'il suffit de prouver le résultat pour une matrice par classe de similitude. En effet si \( A=\exp(B)\) et si \( M\) est inversible alors 
    \begin{subequations}    \label{EqqACuGK}
        \begin{align}
            \exp(MBM^{-1})&=\sum_k\frac{1}{ k! }(MBM^{-1})^k\\
            &=\sum_k\frac{1}{ k! }MB^kM^{-1}\\
            &=M\exp(B)M^{-1}.
        \end{align}
    \end{subequations}
    Donc \( MAM^{-1}=\exp(MBM^{-1})\). Nous pouvons donc nous contenter de trouver un logarithme pour les blocs de Jordan. Nous supposons donc que \( A=(\mtu+N)\) avec \( N^m=0\). En nous inspirant de \eqref{EqweEZnV}, nous posons
    \begin{equation}
        D(t)=tN-\frac{ t^2 }{ 2 }N^2+\ldots +(-1)^m\frac{ t^{m-1} }{ m-1 }N^{m-1}
    \end{equation}
    et nous allons prouver que \(  e^{D(1)}=\mtu+N\). Notons que \( N\) étant nilpotente, cette somme ainsi que toutes celles qui viennent sont finies. Il n'y a donc pas de problèmes de convergences dans cette preuve (si ce n'est les passages des équations \eqref{EqqACuGK}).

    Nous posons \( S(t)= e^{D(t)}\) (la somme est finie), et nous avons
    \begin{equation}
        S'(t)=D'(t) e^{D(t)}
    \end{equation}
    Afin d'obtenir une expression qui donne \( S'\) en termes de \( S\), nous multiplions par \( (\mtu+tN)\) en remarquant que \( (\mtu+tN)D'(t)=N\) nous avons
    \begin{equation}
        (\mtu+tN)S'(t)=NS(t).
    \end{equation}
    En dérivant à nouveau,
    \begin{equation}    \label{EqKjccqP}
        (\mtu+tN)S''(t)=0.
    \end{equation}
    La matrice \( (\mtu+tN)\) est inversible parce que son noyau est réduit à \( \{ 0 \}\). En effet si \( (\mtu+tN)x=0\), alors \( Nx=-\frac{1}{ t }x\), ce qui est impossible parce que \( N\) est nilpotente. Ce que dit l'équation \eqref{EqKjccqP} est alors que \( S''(t)=0\). Si nous développons \( S(t)\) en puissances de \( t\) nous nous arrêtons au terme d'ordre \( 1\) et nous avons
    \begin{equation}
        S(t)=S(0)+tS'(0)=\mtu+tD'(0)=1+tN.
    \end{equation}
    En \( t=1\) nous trouvons \( S(1)=\mtu+N\). La matrice \( D(1)\) donnée est donc bien un logarithme de $\mtu+N$.
\end{proof}

\begin{proposition}[\cite{fJhCTE}]
    Si \( A\in \eM(n,\eC)\) est telle que \( \rho(A)<1\), alors \( A^n\to 0\).
\end{proposition}

\begin{proof}
    Nous nous plaçons dans une base des espaces caractéristiques de \( A\), c'est à dire que nous supposons que la matrice \( A\) a la forme
    \begin{equation}        \label{EqWMvkgLo}
        A=\begin{pmatrix}
            \lambda_1\mtu+N_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_s\mtu+N_s
        \end{pmatrix}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( A\) et les \( N_i\) sont nilpotentes. En effet nous savons que l'espace caractéristique \( F_{\lambda_i}\) est l'espace de nilpolence de \( A-\lambda_i\mtu\). Si nous notons \( A_i\) la restriction de \( A\) à cet espace, la matrice \( N_i=A_i-\lambda_i\mtu\) est nilpotente. Du coup \( A_i=\lambda_I\mtu+N_i\) et nous avons bien la décomposition \eqref{EqWMvkgLo}.

    Nous avons donc \( A^n\to 0\) si et seulement si \( (N_i+\lambda_i\mtu)^n\to 0\) pour tout \( i\). Soit donc \( N\) nilpotente et \( \lambda<1\) (parce que nous savons que toutes les valeurs propres de \( A\) sont inférieures à un). Nous avons
    \begin{equation}
            (\lambda\mtu+N)^n=\sum_{k=0}^n\binom{ n }{ k }\lambda^{n-k}N^{k}
            =\sum_{k=0}^{r-1}\binom{ n }{ k }\lambda^{n-k}N^{k}.
    \end{equation}
    Nous voyons que le nombre de termes dans la somme ne dépend pas de \( n\). De plus pour chacun de termes, la puissance de \( N\) ne dépend pas non plus de \( n\). Le terme
    \begin{equation}
        \binom{ n }{ k }\lambda^{n-k}\leq P(n)\lambda^{n-k}
    \end{equation}
    où \( P\) est un polynôme tend vers zéro lorsque \( n\) devient grand parce que c'est une cas polynôme fois exponentielle.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Diagonalisabilité d'exponentielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{fJhCTE}]      \label{PropCOMNooIErskN}
    Si \( A\in \eM(n,\eR)\) a un polynôme caractéristique scindé, alors \( A\) est diagonalisable si et seulement si \( e^A\) est diagonalisable.
\end{proposition}
\index{décomposition!Dunford!application}
\index{exponentielle!de matrice}
\index{diagonalisable!exponentielle}

\begin{proof}
    Si \( A\) est diagonalisable, alors il existe une matrice inversible \( M\) telle que \( D=M^{-1}AM\) soit diagonale (c'est la définition \ref{DefCNJqsmo}). Dans ce cas nous avons aussi \( (M^{-1}AM)^k=M^{-1}A^kM\) et donc \( M^{-1}e^AM=e^{M^{-1}AM}=e^D\) qui est diagonale.

    La partie difficile est donc le contraire. 
    
    \begin{subproof}
        \item[Qui est diagonalisable et comment ?]
            Nous supposons que \( e^A\) est diagonalisable et nous écrivons la décomposition de Dunford (théorème \ref{ThoRURcpW}) :
            \begin{equation}
                A=S+N
            \end{equation}
            où \( S\) est diagonalisable, \( N\) est nilpotente, \( [S,N]=0\). Nous avons besoin de prouver que \( N=0\).
    
            Les matrices \( A\) est \( S\) commutent; en passant au développement nous en déduisons que \( A\) et \( e^S\) commutent, puis encore en passant au développement que \( e^A\) et \( e^S\) commutent. Vu que \( S\) est diagonalisable, \( e^S\) l'est et par hypothèse \( e^A\) est également diagonalisable. Donc \( e^A\) et \( e^{-S}\) sont simultanément diagonalisables par la proposition \ref{PropGqhAMei}.

            Étant donné que \( A\) et \( S\) commutent, nous avons \( e^N=e^{A-S}=e^Ae^{-S}\), et nous en déduisons que \( e^N\) est diagonalisable vu que les deux facteurs \( e^A\) et \( e^{-S}\) sont simultanément diagonalisables.

        \item[Unipotence]

            Si \( r\) est le degré de nilpotence de \( N\), nous avons
            \begin{equation}    \label{EqQHjvLZQ}
                e^N-\mtu=N+\frac{ N^2 }{2}+\ldots +\frac{ N^{r-1} }{ (r-1)! }.
            \end{equation}
            Donc
            \begin{equation}
                (e^N-\mtu)^k=\left( N+\frac{ N^2 }{2}+\ldots +\frac{ N^{r-1} }{ (r-1)! } \right)^k
            \end{equation}
            où le membre de droite est un polynôme en \( N\) dont le terme de plus bas degré est de degré \( k\). Donc \( (e^N-\mtu)\) est nilpotente et \( e^N\) est unipotente.

            Si \( M\) est la matrice qui diagonalise \( e^N\), alors la matrice diagonale \( M^{-1}e^NM\) est tout autant unipotente que \( e^N\) elle-même. En effet,
            \begin{subequations}
                \begin{align}
                    (M^{-1}e^NM-\mtu)^r&=\sum_{k=0}^r\binom{ r }{ k }(-1)^{r-k}M^{-1}(e^N)^kM\\
                    &=M^{-1}\left( \sum_{k=0}^r\binom{ r }{ k }(-1)^{r-k}(e^N)^k \right)M\\
                    &=M^{-1}(e^N-\mtu)^rM\\
                    &=0.
                \end{align}
            \end{subequations}

            La matrice \( M^{-1}e^NM\) est donc une matrice diagonale et unipotente; donc \( M^{-1}e^NM=\mtu\), ce qui donne immédiatement que \( e^N=\mtu\).

        \item[Polynômes annulateurs]

            En reprenant le développement \eqref{EqQHjvLZQ} sachant que \( e^N=\mtu\), nous savons que
            \begin{equation}
                N+\frac{ N^2 }{2}+\ldots +\frac{ N^{r-1} }{ (r-1)! }=0.
            \end{equation}
            Dit en termes pompeux (mais non moins porteurs de sens), le polynôme
            \begin{equation}
                Q(X)=X+\frac{ X^2 }{2}+\ldots +\frac{ X^{r-1} }{ (r-1)! }
            \end{equation}
            est un polynôme annulateur de \( N\).
            
            La proposition \ref{PropAnnncEcCxj} stipule que le polynôme minimal d'un endomorphisme divise tous les polynômes annulateurs. Dans notre cas, \( X^r\) est un polynôme annulateur et donc le polynôme minimal de \( N\) est de la forme \( X^k\). Donc il est \( X^r\) lui-même.
            
            Nous avons donc \( X^r\divides Q\). Mais \( Q\) est un polynôme contenant le monôme \( X\) donc \( X^r\) ne peut diviser \( Q\) que si \( r=1\). Nous en concluons que \( X\) est un polynôme annulateur de \( N\). C'est à dire que \( N=0\).

        \item[Conclusion]

            Vu que Dunford\footnote{Théorème \ref{ThoRURcpW}.} dit que \( A=S+N\) et que nous venons de prouver que \( N=0\), nous concluons que \( A=S\) avec \( S\) diagonalisable.

    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Valeurs singulières}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Soit \( M\) une matrice \( m\times n\) sur \( \eK\) (\( \eK\) est \( \eR\) ou \( \eC\)). Un nombre réel \( \sigma\) est une \defe{valeur singulière}{valeur!singulière} de \( M\) si il existent des vecteurs unitaires \( u\in \eK^m\), \( v\in \eK^n\) tels que
    \begin{subequations}
        \begin{align}
            Mv&=\sigma u\\
            M^*u&=\sigma v.
        \end{align}
    \end{subequations}
\end{definition}

\begin{theorem}[Décomposition en valeurs singulières]
    Soit \( M\in \eM(m\times n,\eK)\) où \( \eK=\eR,\eC\). Alors \( M\) se décompose en
    \begin{equation}
        M=ADB
    \end{equation}
    où
    il existe deux matrices unitaires \( A\in \gU(m\times m)\), \( B\in \gU(n\times n)\) et une matrice (pseudo)diagonale \( D\in \eM(m\times n)\) tels que
    \begin{enumerate}
        \item 
            \( A\in\gU(m\times m)\), \( B\in\gU(n\times n)\) sont deux matrices unitaires;,
        \item
            \( D\) est (pseudo)diagonale,
        \item
            les éléments diagonaux de \( D\) sont les valeurs singulières de \( M\),
        \item
            le nombre d'éléments non nuls sur la diagonale de \( D\) est le rang de \( M\).
    \end{enumerate}
\end{theorem}

\begin{corollary}
    Soit \( M\in \eM(n,\eC)\). Il existe un isomorphisme \( f\colon \eC^n\to \eC^n\) tel que \( fM\) soit autoadjoint.
\end{corollary}

\begin{proof}
    Si \( M=ADB\) est la décomposition de \( M\) en valeurs singulières, alors nous pouvons prendre \( f=\overline{ B }^tA^{-1}\) qui est une matrice inversible. Pour la vérification que ce \( f\) répond bien à la question, ne pas oublier que \( D\) est réelle, même si \( M\) ne l'est pas.
\end{proof}
