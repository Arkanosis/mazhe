% This is part of Mes notes de mathématique
% Copyright (c) 2011-2013
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Parties libres, génératrices, bases et dimension}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
\begin{definition}
	Un sous-ensemble $B=\{v_1,\ldots,v_q\}$ de $\eR^m$ est une \defe{base}{base} de $\eR^m$ s'il satisfait les conditions suivantes
\begin{itemize}
	\item $B$ est \defe{libre}{libre}, c'est à dire
\[
\sum_{i=1}^{q}a_i v_i=0_{m} \quad\Leftrightarrow\quad a_i=0, \forall i=1,\ldots,q.
\]
\item $B$ est \defe{générateur}{générateur}, c'est à dire que pour tout $x$ dans $\eR^m$ il existe un ensemble de coefficients $\{a_i\in\eR, i=1,\ldots,n\}$ tel que
\[\sum_{i=1}^{q}a_i v_i=x.\]
\end{itemize}
\end{definition}
Il existe une infinité de bases de $\eR^m$. On peut démontrer que le cardinal de toute base de $\eR^m$ est $m$, c'est à dire que toute base de $\eR^m$ possède exactement $m$ éléments.

La base de $\eR^m$ qu'on dit \defe{canonique}{canonique!base}\index{base!canonique de $\eR^m$} (c.à.d. celle qu'on utilise tout le temps) est $\mathcal{B}=\{e_1,\ldots, e_m\}$, où le vecteur $e_j$ est 
\begin{equation}\nonumber
  e_j=
\begin{array}{cc}
  \begin{pmatrix}
    0\\\vdots\\0\\1\\ 0\\\vdots\\0
  \end{pmatrix} & 
  \begin{matrix}
    \quad\\\quad\\\leftarrow\textrm{j-ème} \quad\\\quad\\\quad\\
  \end{matrix}
\end{array}.
\end{equation}
La composante numéro $j$ de $e_i$ est $1$ si $i=j$ et $0$ si $i\neq j$. Cela s'écrit $(e_i)_j=\delta_{ij}$ où $\delta$ est le \defe{symbole de Kronecker}{Kronecker} défini par
\begin{equation}
	\delta_{ij}=\begin{cases}
		1	&	\text{si $i=j$}\\
		0	&	 \text{si $i\neq j$.}
	\end{cases}
\end{equation}
Les éléments de la base canonique de $\eR^m$ peuvent donc être écrits $e_i=\sum_{k=1}^m\delta_{ik}e_k$.


\begin{definition}
    Si \( E\) est un espace vectoriel, une partie finie \( (u_i)_{1\leq i\leq n}\) de \( E\) est \defe{libre}{libre!partie} si l'égalité
    \begin{equation}
        a_1 u_1+\ldots +a_nu_n=0
    \end{equation}
    implique \( a_i=0\) pour tout \( i\).

    Une partie infinie est libre si toute ses parties finies le sont.
\end{definition}
La définition de liberté dans le cas des parties infinies a son importance lorsqu'on parle d'espaces vectoriels de dimension infinies (en dimension finie, aucune partie infinie n'est libre) parce que cela fera une différence entre une base algébrique et une base hilbertienne par exemple.

Un espace vectoriel est \defe{de type fini}{type!fini!espace vectoriel} si il contient une partie génératrice finie. Nous verrons dans les résultats qui suivent que cette définition est en réalité inutile parce qu'une espace vectoriel sera de type fini si et seulement si il est de dimension finie.

\begin{lemma}       \label{LemytHnlD}
    Si \( E\) a une famille génératrice de cardinal \( n\), alors toute famille de \( n+1\) éléments est liée.
\end{lemma}

\begin{proof}
    Nous procédons par récurrence sur \( n\) -- qui n'est pas exactement la dimension d'un espace vectoriel fixé. Pour \( n=1\), nous avons \( E=\langle e\rangle\) et donc si \( v_1,v_2\in E\) nous avons \( v_1=\lambda_1 e\), \( v_2=\lambda_2e\) et donc \( \lambda_2v_1-\lambda_1v1=0\). Cela prouve le résultat dans le cas de la dimension \( 1\).

    Supposons maintenant que le résultat soit vrai pour \( k<n\), c'est à dire que pour tout espace vectoriel contenant une partie génératrice de cardinal \( k<n\), les parties de \( k+1\) éléments sont liées. Soit maintenant un espace vectoriel muni d'une partie génératrice \( G=\{ e_1,\ldots, e_n \}\) de \( n\) éléments, et montrons que toute partie \( V=\{ v_1,\ldots, v_{n+1} \}\) contenant \( n+1\) éléments est liée. Dans nos notations nous supposons que les \( e_i\) sont des vecteurs distincts et les \( v_i\) également. Nous les supposons également tous non nuls. Étant donné que \( \{ e_i \}\) est génératrice nous pouvons définir les nombres \( \lambda_{ij}\) par
    \begin{equation}
        v_i=\sum_{k=1}^n\lambda_{ij}e_j
    \end{equation}
    Vu que
    \begin{equation}
        v_{n+1}=\sum_{k=1}^n\lambda_{n+1,k}e_k\neq 0,
    \end{equation}
    quitte à changer la numérotation des \( e_i\) nous pouvons supposer que \( \lambda_{n+1,n}\neq 0\). Nous considérons les vecteurs
    \begin{equation}
        w_i=\lambda_{n+1,n}v_i-\lambda_{i,n}v_{n+1}.
    \end{equation}
    En calculant un peu,
    \begin{subequations}
        \begin{align}
            w_i&=\lambda_{n+1,n}\sum_k\lambda_{i,k}e_k-\lambda_{i,n}\sum_k\lambda_{n+1,k}e_k\\
            &=\sum_{k=1}^{n-1}\big( \lambda_{n+1,n}\lambda_{i,k}-\lambda_{i,n}\lambda_{n+1,} \big)e_k
        \end{align}
    \end{subequations}
    parce que les termes en \( e_n\) se sont simplifiés. Donc la famille \( \{ w_1,\ldots, w_n \}\) est une famille de \( n\) vecteurs dans l'espace vectoriel \( \Span\{ e_1,\ldots, e_{n-1} \}\); elle est donc liée par l'hypothèse de récurrence. Il existe donc des nombres \( \alpha_1,\ldots, \alpha_n\in \eK\) non tous nuls tels que
    \begin{equation}        \label{EqOQGGoU}
        0=\sum_{i=1}^n\alpha_iw_i=\sum_{i=1}^n\alpha_i\lambda_{n+1,n}v_i-\left( \sum_{i=1}^n\alpha_i\lambda_{i,n} \right)v_{n+1}.
    \end{equation}
    Vu que \( \lambda_{n+1,n}\neq 0\) et que parmi les \( \alpha_i\) au moins un est non nul, nous avons au moins un des produits \( \alpha_i\lambda_{n+1,n}\) qui est non nul. Par conséquent \eqref{EqOQGGoU} est une combinaison linéaire nulle non triviale des vecteurs de \( \{ v_1,\ldots, v_{n+1} \}\). Cette partie est donc liée.
\end{proof}

\begin{lemma}   \label{LemkUfzHl}
    Soit \( L\) une partie libre et \( G\) une partie génératrice. Soit \( B\) une partie maximale parmi les parties libres \( L'\) telles que \( L\subset L'\subset G\). Alors \( B\) est une base.
\end{lemma}
Qu'entend-t-on par «maximale» ? La partie \( B\) doit être libre, contenir \( L\), être contenue dans \( G\) et de plus avoir la propriété que \( \forall x\in G\setminus B\), la partie \( B\cup\{ x \}\) est liée.

\begin{proof}
    D'abord si \( G\) est une base, alors toutes les parties de \( G\) sont libres et le maximum est \( B=G\). Dans ce cas le résultat est évident. Nous supposons donc que \( G\) est liée.

    La partie \( B=\{ b_1,\ldots, b_l \}\) est libre parce qu'on l'a prise parmi les libres. Montrons que \( B\) est génératrice. Soit \( x\in G\setminus B\); par hypothèse de maximalité, \( B\cup\{ x \}\) est liée, c'est à dire qu'il existe des nombres \( \lambda_i\), \( \lambda_x\) non tous nuls tels que
    \begin{equation}    \label{EqxfkevM}
        \sum_{i=1}^l\lambda_ib_i+\lambda_xx=0.
    \end{equation}
    Si \( \lambda_x=0\) alors un de \( \lambda_i\) doit être non nul et l'équation \eqref{EqxfkevM} devient une combinaison linéaire nulle non triviale des \( b_i\), ce qui est impossible parce que \( B\) est libre. Donc \( \lambda_x\neq 0\) et
    \begin{equation}
        x=\frac{1}{ \lambda_x }\sum_{i=1}^l\lambda_ib_i.
    \end{equation}
    Donc tous les éléments de \( G\setminus B\) sont des combinaisons linéaires des éléments de \( B\), et par conséquent, \( G\) étant génératrice, tous les éléments de \( E\) sont combinaisons linéaires d'éléments de \( B\). 
\end{proof}

\begin{theorem} \label{ThonmnWKs}
    Soit \( E\) un espace vectoriel de type fini sur le corps \( \eK\).
    \begin{enumerate}
        \item   \label{ItemBazxTZ}
            Si \( L\) est une partie libre et si \( G\) est une partie génératrice contenant \( L\), alors il existe une base \( B\) telle que \( L\subset B\subset G\).
        \item
            Toutes les bases sont finies et ont même cardinal.
    \end{enumerate}
\end{theorem}
\index{espace!vectoriel!dimension}
\index{rang}
Notons que puisque \( E\) lui-même est générateur, le point \ref{ItemBazxTZ} implique que toute partie libre peut être étendue en une base.

\begin{proof}
    Vu que \( E\) est de type fini, il admet une partie génératrice \( G\) de cardinal fini \( n\). Donc une partie libre est de cardinal au plus \( n\) par le lemme \ref{LemytHnlD}. Soit \( L\), une partie libre contenue dans \( G\) (ça existe : par exemple \( L=\emptyset\)). La partie \( B\) maximalement libre contenue dans \( G\) et contenant \( L\) est une base par le lemme \ref{LemkUfzHl}.

    En ce qui concerne la seconde partie du théorème, soient \( B\) et \( B'\), deux bases. En particulier \( B\) est génératrice et \( B'\) est libre, donc le lemme \ref{LemytHnlD} indique que \( \Card(B')\leq \Card(B)\). Par symétrie on a l'inégalité inverse. Donc \( \Card(B)=\Card(B')\).
\end{proof}

Le théorème suivant est essentiellement une reformulation du théorème \ref{ThonmnWKs}.
\begin{theorem} \label{ThoBaseIncompjblieG}
    Soit \( E\) un espace vectoriel de dimension finie et \( \{ e_i \}_{i\in I}\) une partie génératrice de \( E\).

    \begin{enumerate}
        \item
            Il existe \( J\subset I\) tel que \( \{ e_i \}_{i\in J}\) est une base. Autrement dit : de toute partie génératrice nous pouvons extraire une base.
        \item
            Soit \( \{ f_1,\ldots, f_l \}\) une partie libre. Alors nous pouvons la compléter en utilisant des éléments \( e_i\). C'est à dire qu'il existe \( J\subset I\) tel que \( \{ f_k \}\cup\{ e_i \}_{i\in J}\) soit une base.
    \end{enumerate}
\end{theorem}

Soit \( F\) un sous-espace vectoriel de l'espace vectoriel \( E\). La \defe{codimension}{codimension} de \( F\) dans \( E\) est
\begin{equation}
    \codim_E(F)=\dim(E/F).
\end{equation}

Le théorème suivant est valable également en dimension infinie; ce sera une des rares incursions en dimension infinie de ce chapitre.
\begin{theorem}[Théorème du rang]\index{théorème!du rang}       \label{ThoGkkffA}
       Soient \( E\) et \( F\) deux espaces vectoriels (de dimensions finies ou non) et soit \( f\colon E\to F\) une application linéaire. Alors le rang de \( f\) est égal à la codimension du noyau, c'est à dire
       \begin{equation}
           \rang(f)+\dim\ker f=\dim E.
       \end{equation}

       Dans le cas de dimension infinie afin d'éviter les problèmes d'arithmétique avec l'infini nous énonçons le théorème en disant que si \( (u_s)_{s\in S}\) est une base de \( \ker f\) et si \( \big( f(v_t) \big)_{t\in T}\) est une base de \( \Image(f)\) alors  \( (u_s)_{s\in s}\cup (v_t)_{t\in T}\) est une base de \( E\).
\end{theorem}

\begin{proof}
    Nous devons montrer que 
    \begin{equation}
          (u_s)_{s\in S}\cup (v_t)_{t\in T}
    \end{equation}
    est libre et générateur.

    Soit \( x\in E\). Nous définissons les nombres \( x_t\) par la décomposition de \( f(x)\) dans la base \( \big( f(v_t) \big)\) :
    \begin{equation}
        f(x)=\sum_{t\in T}x_tf(v_t).
    \end{equation}
    Ensuite le vecteur \( x=\sum_tx_tv_t\) est dans le noyau de \( f\), par conséquent nous le décomposons dans la base \( (u_s)\) :
    \begin{equation}
        x-\sum_tx_tv_t=\sum_s\in S x_su_s.
    \end{equation}
    Par conséquent
    \begin{equation}
        x=\sum_sx_su_s+\sum_tx_tv_t.
    \end{equation}
    
    En ce qui concerne la liberté nous écrivons
    \begin{equation}
        \sum_tx_tv_t+\sum_sx_su_s=0.
    \end{equation}
    En appliquant \( f\) nous trouvons que 
    \begin{equation}
        \sum_tx_tf(v_t)=0
    \end{equation}
    et donc que les \( x_t\) doivent être nuls. Nous restons avec \( \sum_sx_su_s=0\) qui à son tour implique que \( x_s=0\).
\end{proof}
Un exemple d'utilisation de ce théorème en dimension infinie sera donné dans le cadre du théorème de Fréchet-Riesz, théorème \ref{ThoQgTovL}.

\begin{proposition}[\cite{RombaldiO}]   \label{PropTVKbxU}
    Soit \( E\), un espace vectoriel sur un corps infini et \( (F_k)_{k=1,\ldots, r}\), des sous-espaces vectoriels propres de \( E\) tels que \( \bigcup_{i=1}^rF_i=E\). Alors \( E=F_k\) pour un certain \( k\).

    Autrement dit, l'union finie de sous-espaces propres ne peut être égal à l'espace complet.
\end{proposition}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Dualité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}  \label{DefJPGSHpn}
    Si \( E\) est un espace vectoriel, le \defe{dual}{dual} de \( E\) est l'ensemble des formes linéaires sur \( E\). Le \defe{dual topologique}{dual!topologique} est l'ensemble des formes linéaires continues.

    En dimension infinies, ces deux notions ne coïncident pas.
\end{definition}
%TODO : trouver un exemple où ça ne coïncide pas.
% Si je me souviens bien, pour les opérateurs linéaires, borné est équivalent à continu. Il faudra chercher de ce côté.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Orthogonal}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E\), un espace vectoriel, et \( F\) une sous-espace de \( E\). L'\defe{orthogonal}{orthogonal!sous-espace} de \( F\) est la partie \( F^{\perp}\subset E^*\) donnée par
\begin{equation}    \label{Eqiiyple}
    F^{\perp}=\{ \alpha\in E^*\tq \forall x\in F,\alpha(x)=0 \}.
\end{equation}
Cette définition d'orthogonal via le dual n'est pas du pur snobisme. En effet, la définition «usuelle» qui ne parle pas de dual,
\begin{equation}
    F^{\perp}=\{ y\in E\tq \forall x\in F,y\cdot x=0 \},
\end{equation}
demande la donnée d'un produit scalaire. Évidemment dans le cas de \( \eR^n\) munie du produit scalaire usuel et de l'identification usuelle entre \( \eR^n\) et \( (\eR^n)^*\) via une base, les deux notions d'orthogonal coïncident.

Si \( B\subset E^*\), on note \( B^o\)\nomenclature[G]{\( B^o\)}{orthogonal dans le dual} son orthogonal :
\begin{equation}
    B^o=\{ x\in E\tq \omega(x)=0\,\forall \omega\in B \}.
\end{equation}
Notons qu'on le note \( B^o\) et non \( B^{\perp}\) parce qu'on veut un peu s'abstraire du fait que \( (E^*)^*=E\). Du coup on impose que \( B\) soit dans un dual et on prend une notation précise pour dire qu'on remonte au pré-dual et non qu'on va au dual du dual.

La définition \eqref{Eqiiyple} est intrinsèque : elle ne dépend que de la structure d'espace vectoriel.

\begin{proposition} \label{PropXrTDIi}
    Soit \( E\) un espace vectoriel, et \( F\) un sous-espace vectoriel. Alors nous avons
    \begin{equation}
        \dim F+\dim F^{\perp}=\dim E.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \{ e_1,\ldots, e_p \}\) une base de \( F\) que nous complétons en une base \( \{ e_1,\ldots, e_n \}\) de \( E\) par le théorème \ref{ThonmnWKs}. Soit \( \{ e_1^*,\ldots, e^*_n \}\) la base duale. Alors nous prouvons que \( \{ e^*_{p+1},\ldots, e_n^* \}\) est une base de \( F^{\perp}\). 
    
    Déjà c'est une partie libre en tant que partie d'une base.

    Ensuite ce sont des éléments de \( F^{perp}\) parce que si \( i\leq p\) et si \( k\geq 1\) nous avons \( e^*_{p+k}(e_i^*)=0\); donc oui, \( e^*_{p+k}\in F^{\perp}\).

    Enfin \( F^{\perp}\subset\Span\{ e_{p+1}^*,e_n^* \}\) parce que si \( \omega=\sum_{k=1}^n\omega_ke_k^*\), alors \( \omega(e_i)=\omega_i\), mais nous savons que si \( \omega\in F^{\perp}\), alors \( \omega(e_i)=0\) pour \( i\leq p\). Donc \( \omega=\sum_{i=p+1}^n\omega_ke^*_k\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Transposée}
%---------------------------------------------------------------------------------------------------------------------------

Si \( f\colon E\to F\) est une application linéaire entre deux espaces vectoriels, la \defe{transposée}{transposée} est l'application \( f^t\colon F^*\to E^*\) donnée par
\begin{equation}
    f^t(\omega)(x)=\omega\big( f(x) \big).
\end{equation}
pour tout \( \omega\in F^*\) et \( x\in E\).

\begin{lemma}
    Soit \( E\) muni de la base \( \{ e_i \}\) et \( F\) muni de la base \( \{ g_i \}\) et une application \( f\colon E\to F\). Si \( A\) est la matrice de \( f\) dans ces bases, alors \( A^t\) est la matrice de \( f^t\) dans les bases \( \{ e^*_i \}\) et \( \{ g^*_i \}\) de \( E^*\) et \( F^*\).
\end{lemma}

\begin{proof}
    Nous allons montrer que les formes \( f^t(g^*_i)\) et \( \sum_k(A^t)_{ik}g^*_k\) sont égales en les appliquant à un vecteur.

    Par définition de la matrice d'une application linéaire dans une base,
    \begin{equation}
        f^t(g_i^*)=\sum_j(f^t)_{ij}e^*j,
    \end{equation}
    et
    \begin{equation}
        f(e_k)=\sum_lA_{klg_l}.
    \end{equation}
    Du coup, si \( x=\sum_kx_ke_k\), nous avons
    \begin{equation}    \label{EqCzwftH}
        f^t(g_i^*)x=\sum_{kl}x_kg_i^*A_{kl}g_l=\sum_{kl}x_kA_{kl}\delta_{il}=\sum_k x_kA_{ki}=\sum_k(A^t)_{ik}x_k.
    \end{equation}
    D'autre part, 
    \begin{equation}    \label{EqWlQlrR}
        \sum_k(A^t)_{ik}g_k^*x=\sum_{kl}(A^t)_{ik}g^*_kx_le_l=\sum_k(A^t)_{ik}x_k.
    \end{equation}
    Le fait que \eqref{EqCzwftH} et \eqref{EqWlQlrR} donnent le même résultat prouve le lemme.
\end{proof}
En corollaire, les rangs de \( f\) et de \( f^t\) sont égaux parce que le rang est donné par la plus grande matrice carré de déterminant non nul. Nous prouvons cependant ce résultat de façon plus intrinsèque.

\begin{lemma}[\href{http://gilles.dubois10.free.fr/algebre_lineaire/dualite.html}{Gilles Dubois}]   \label{LemSEpTcW}
    Si \( f\colon E\to F\) est une application linéaire, alors
    \begin{equation}
        \rang(f)=\rang(f^t).
    \end{equation}
\end{lemma}

\begin{proof}
    Nous posons \( \dim\ker(f)=p\) et donc \( \rang(f)=n-p\). Soit \( \{ e_1,\ldots, e_p \}\) une base de \( \ker(f)\) que l'on complète en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Nous considérons maintenant les vecteurs
    \begin{equation}
        g_i=f(e_{p+i})
    \end{equation}
    pour \( i=1,\ldots, n-p\). C'est à dire que les \( g_i\) sont les images des vecteurs qui ne sont pas dans le noyau de \( f\). Prouvons qu'ils forment une famille libre. Si
    \begin{equation}
        \sum_{k=1}^{n-p}a_kf(e_{p+k})=0,
    \end{equation}
    alors \( f\big( \sum_ka_ke_{p+k} \big)=0\), ce qui signifierait que \( \sum_ka_ke_{p+k}\) se trouve dans le noyau de \( f\), ce qui est impossible par construction de la base \( \{ e_i \}_{i=1,\ldots, n}\). Étant donné que les vecteurs \( g_1,\ldots, g_{n-p}\) sont libres, nous les complétons en une base
    \begin{equation}
        \{ \underbrace{g_1,\ldots, g_{n-p}}_{\text{images}},\underbrace{g_{n-p+1},\ldots, g_r}_{\text{complétion}} \}
    \end{equation}
    de \( F\).

    Nous prouvons maintenant que \( \rang(f^t)\geq n-p\) en montrant que les formes \( \{ g_i^* \}_{i=1,\ldots, n-p}\) est libre (et donc l'espace image de \( f^f\) est au moins de dimension \( n-p\)). Pour cela nous prouvons que \( f^t(g_i^*)=e^*_{i+p}\). En effet
    \begin{equation}
        f^t(g^*_i)e_k=g_i^*(fe_k),
    \end{equation}
    Si \( k=1,\ldots, p\), alors \( fe_k=0\) et donc \( g_i^*(fe_k)=0\); si \( k=p+l\) alors
    \begin{equation}
        f^t(g_i^*)e_k=g_i^*(fe_{k+l})=g^*_i(g_l)=\delta_{i,l}=\delta_{i,k-p}=\delta_{k,i+p}.
    \end{equation}
    Donc \( f^t(g_i^*)=e^*_{i+p}\). Cela prouve que les formes \( f^t(g_i^*)\) sont libres et donc que
    \begin{equation}
        \rang(f^t)\geq n-p=\rang(f).
    \end{equation}
    En appliquant le même raisonnement à \( f^t\) au lieu de \( f\), nous trouvons
    \begin{equation}
        \rang\big( (f^t)^t \big)\geq \rang(f^t)
    \end{equation}
    et donc, vu que \( (f^t)^t=f\), nous obtenons \( \rang(f)=\rang(f^t)\).
    
\end{proof}

\begin{proposition}[\cite{DualMarcSAge}]
    Si \( f\) est une application linéaire entre les espaces vectoriels \( E\) et \( F\), alors nous avons
    \begin{equation}
        \Image(f^t)=\ker(f)^{\perp}.
    \end{equation}
\end{proposition}

\begin{proof}
    Soient donc l'application \( f\colon E\to F\) et sa transposée \( f^t\colon F^*\to E^*\). Nous commençons par prouver que \( \Image(f^{t})\subset(\ker f)^{\perp}\). Pour cela nous prenons \( \omega\in \Image(f^t)\), c'est à dire \( \omega=\alpha\circ f\) pour un certain élément \( \alpha\in F^*\). Si \( z\in\ker(f)\), alors \( \omega(z)=(\alpha\circ f)(z)=0\), c'est à dire que \( \omega\in (\ker f)^{\perp}\).

    Pour prouver qu'il y a égalité, nous n'allons pas démontrer l'inclusion inverse, mais plutôt prouver que les dimensions sont égales. Après, on sait que si \( A\subset B\) et si \( \dim A=\dim B\), alors \( A=B\). Nous avons
    \begin{subequations}
        \begin{align}
            \dim\big( \Image(f^t) \big)&=\rang(f^t)\\
            &=\rang(f)  &\text{lemme \ref{LemSEpTcW}}\\
            &=\dim(E)-\dim\ker(f)   &\text{théorème \ref{ThoGkkffA}}\\
            &=\dim\big( (\ker f)^{\perp} \big)  &\text{proposition \ref{PropXrTDIi}}.
        \end{align}
    \end{subequations}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes de Lagrange}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E=\eR_n[X]\) l'ensemble des polynômes à coefficients réels de degré au plus \( n\). Soient les \( n+1\) réels distincts \( a_0,\ldots, a_n\). Nous considérons les formes linéaires associées \( f_i\in E^*\),
\begin{equation}
    f_i(P)=P(a_i).
\end{equation}
\begin{lemma}
    Ces formes forment une base de \( E^*\).
\end{lemma}

\begin{proof}
    Nous prouvons que l'orthogonal est réduit au nul :
    \begin{equation}
        \Span\{ f_0,\ldots, f_n \}^{\perp}=\{ 0 \}
    \end{equation}
    pour que la proposition \ref{PropXrTDIi} conclue. Si \( P\in\Span\{ f_i \}^{\perp}\), alors \( f_i(P)=0\) pour tout \( i\), ce qui fait que \( P(a_i)=0\) pour tout \( i=0,\ldots, n\). Un polynôme de degré au plus \( n\) qui s'annule en \( n+1\) points est automatiquement le polynôme nul.
\end{proof}

Les \defe{polynômes de Lagrange}{Lagrange!polynôme}\index{polynôme!Lagrange} sont les polynômes de la base (pré)duale de la base \( \{ f_i \}\).

\begin{proposition}
    Les polynômes de Lagrange sont donnés par
    \begin{equation}
        P_i=\prod_{k\neq i}\frac{ X-a_k }{ a_i-a_k }.
    \end{equation}
\end{proposition}

\begin{proof}
    Il suffit de vérifier que \( f_j(P_i)=\delta_{ij}\). Nous avons
    \begin{equation}
        f_j(P_i)=P_i(a_j)=\prod_{k\neq i}\frac{ a_j-a_k }{ a_i-a_k }.
    \end{equation}
    Si \( j\neq i\) alors un des termes est nul. Si au contraire \( i=j\), tous les termes valent \( 1\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dual de \texorpdfstring{$ \eM(n,\eK)$}{M(n,K)}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Deux matrices \( A\) et \( B\) sont \defe{équivalentes}{matrice!équivalence} dans \( \eM(n,\eK)\) si il existe \( P,Q\in\GL(n,\eK)\) telles que \( A=PBQ^{-1}\). Deux matrices sont \defe{semblables}{matrices!similitude} si il existe une matrice \( P\in \GL(n,\eK)\) telle que \( A=PBP^{-1}\).
\end{definition}

\begin{lemma}   \label{LemZMxxnfM}
    Une matrice de rang \( r\) dans \( \eM(n,\eK)\) est équivalente à la matrice par blocs
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r    &   0    \\ 
            0    &   0    
        \end{pmatrix}.
    \end{equation}
\end{lemma}
\index{rang!classe d'équivalence}

\begin{proof}
    Nous devons prouver que pour toute matrice \( A\in\eM(n,\eK)\) de rang \( r\), il existe \( P,Q\in\GL(n,\eK)\) telles que \(QAP=J_r\). Soit \( \{ e_i \}\) la base canonique de \( \eK^n\), puis \( \{ f_i \}\) une base telle que \( Af_i=0\) dès que \( i>r\).

    Nous considérons la matrice inversible \( P\) telle que \( Pe_i=f_i\). Elle vérifie
    \begin{equation}
        APe_i=Af_i=\begin{cases}
            0    &   \text{si \( i>r\)}\\
            \neq 0    &    \text{sinon}.
        \end{cases}
    \end{equation}
    La matrice \( AP\) se présente donc sous la forme
    \begin{equation}
        AP=\begin{pmatrix}
            M    &   0    \\ 
            *    &   0    
        \end{pmatrix}
    \end{equation}
    où \( M\) est une matrice \( r\times r\). Nous considérons maintenant une base \( \{ g_i \}_{i=1,\ldots, n}\) dont les \( r\) premiers éléments sont les \( r\) premières colonnes de \( AP\) et une matrice inversible \( Q\) telle que \( Qg_i=e_i\). Alors
    \begin{equation}
        QAPe_i=\begin{cases}
            e_i    &   \text{si \( i<r\)}\\
            0    &    \text{sinon}.
        \end{cases}.
    \end{equation}
    Cela signifie que \( QAP\) est la matrice \( J_r\).
\end{proof}

\begin{proposition}[\cite{KXjFWKA}]     \label{PropHOjJpCa}
    soit \( \eK\), un corps. Les formes linéaires sur \( \eM(n,\eK)\) sont les applications de la forme
    \begin{equation}
        \begin{aligned}
            f_A\colon \eM_n(\eK)&\to \eK \\
            M&\mapsto \tr(AM). 
        \end{aligned}
    \end{equation}
\end{proposition}
\index{trace!dual de \( \eM(n,\eK)\)}
\index{dual!de \( \eM(n,\eK)\)}

Ce lemme signifie que toutes les matrices de même rang sont équivalentes.

\begin{proof}
    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            f\colon \eM(n,\eK)&\to \eM(n,\eK)' \\
            A&\mapsto f_A 
        \end{aligned}
    \end{equation}
    et nous voulons prouver que c'est une bijection. Étant donné que nous sommes en dimension finie, nous avons égalité des dimensions de \( \eM_n(\eK)\) et \( \eM_n(\eK)'\), et il suffit de prouver que \( f\) est injective. Soit donc \( A\) telle que \( f_A=0\). Nous l'appliquons à la matrice \( (E_{ij})_{kl}=\delta_{ik}\delta_{jl}\) :
    \begin{equation}
            0=f_A(E_{ij})
            =\sum_{k}(AE_{ij})_{kk}
            =\sum_{kl}A_{kl}\delta_{il}\delta_{jk}
            =A_{ij}.
    \end{equation}
    Donc \( A=0\).
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]
    Soit \( \eK\) un corps et \( \phi\in\eM(n,\eK)^*\) telle que pour tout \( X,Y\in \eM(n,\eK)\) on ait
    \begin{equation}
        \phi(XY)=\phi(YX).
    \end{equation}
    Alors il existe \( \lambda\in \eK\) tel que \( \phi=\lambda\Tr\).
\end{corollary}
\index{trace!unicité pour la propriété de trace}

\begin{proof}
    La proposition \ref{PropHOjJpCa} nous donne une matrice \( A\in \eM(n,\eK)\) telle que \( \phi=f_A\). L'hypothèse nous dit que \( f_A(XY)=f_A(YX)\), c'est à dire
    \begin{equation}
        \Tr(AXY)=\Tr(AYX)
    \end{equation}
    pour toute matrices \( X,Y\in \eM(n,\eK)\). L'invariance cyclique de la trace nous dit que \( \Tr(AXY)=\Tr(XAY)\), ce qui signifie que
    \begin{equation}
        \Tr\big( (AX-XA)Y \big)=0
    \end{equation}
    ou encore que \( f_{AX-XA}=0\) pour tout \( X\). La fonction \( f\) étant injective nous en déduisons que la matrice \( A\) doit satisfaire
    \begin{equation}
        AX=XA
    \end{equation}
    pour tout \( X\in\eM\). En particulier en prenant la fameuse matrice \( E_{ij}\) et en calculant un peu,
    \begin{equation}
        A_{li}\delta_{jm}=\delta_{il}A_{jm}
    \end{equation}
    pour tout \( i,j,l,m\). Cela implique que \( A_{ll}=A_{mm}\) pour tout \( l\) et \( m\) et que \( A_{jm}=0\) dès que \( j\neq m\). Il existe donc \( \lambda\in \eK\) tel que \( A=\lambda\mtu\). En fin de compte,
    \begin{equation}
        \phi(X)=f_{\lambda\mtu}(X)=\lambda\Tr(X).
    \end{equation}
\end{proof}

\begin{corollary}[\cite{KXjFWKA}]
    Soit \( \eK\) un corps. Tout hyperplan de \( \eM(n,\eK)\) coupe \( \GL(n,\eK)\).
\end{corollary}
\index{groupe!linéaire!hyperplan}

\begin{proof}
    Soit \( \mH\) un hyperplan de \( \eM\). Il existe une forme linéaire \( \phi\) sur \( \eM(n,\eK)\) telle que \( \mH=\ker(\phi)\). Encore une fois la proposition \ref{PropHOjJpCa} nous donne \( A\in \eM\) telle que \( \phi=f_A\); nous notons \( r\) le rang de \( A\). Par le lemme \ref{LemZMxxnfM} nous avons \( A=PJ_rQ\) avec \( P,Q\in \GL(n,\eK)\) et
    \begin{equation}
        J_r=\begin{pmatrix}
            \mtu_r    &   0    \\ 
            0    &   0    
        \end{pmatrix}.
    \end{equation}
    Pour tout \( X\in \eM\) nous avons
    \begin{equation}
        \phi(X)=\Tr(AX)=\Tr(PJ_rQX)=\Tr(J_rQXP).
    \end{equation}
    Ce que nous cherchons est \( X\in \GL(n,\eK)\) telle que \( \phi(X)=0\). Nous commençons par trouver \( Y\in\GL(n,\eK)\) telle que \( \Tr(J_rY)=0\). Celle-là est facile : c'est
    \begin{equation}
        Y=\begin{pmatrix}
            0    &   1    \\ 
            \mtu_{n-1}    &   0    
        \end{pmatrix}.
    \end{equation}
    Les éléments diagonaux de \( J_rY\) sont tous nuls. Par conséquent en posant \( X=Q^{-1}YP^{-1}\) nous avons notre matrice inversible dans le noyau de \( \phi\).
\end{proof}
\index{hyperplan!de \( \eM(n,\eK)\)}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Déterminants}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecGYzHWs}


%  Lire http://www.les-mathematiques.net/phorum/read.php?2,302266

\begin{definition}\index{déterminant!forme linéaire alternée}
    Soit \( E\), un \( \eK\)-espace vectoriel. Une forme linéaire \defe{alternée}{forme linéaire!alternée}\index{alternée!forme linéaire} sur \( E\) est une application linéaire \( f\colon E\to \eK\) telle que \( f(v_1,\ldots, v_k)=0\) dès que \( v_i=v_j\) pour certains \( i\neq j\).
\end{definition}

\begin{lemma}   \label{LemHiHNey}
    Une forme linéaire alternée est antisymétrique. Si \( \eK\) est de caractéristique différente de \( 2\), alors une forme antisymétrique est alternée.
\end{lemma}

\begin{proof}
    Soit \( f\) une forme alternée; quitte à fixer toutes les autres variables, nous pouvons travailler avec une \( 2\)-forme et simplement montrer que \( f(x,y)=-f(y,x)\). Pour ce faire nous écrivons
    \begin{equation}
        0=f(x+y,x+y)=f(x,x)+f(x,y)+f(y,x)+f(y,y)=f(x,y)+f(y,x).
    \end{equation}
    
    Pour la réciproque, si \( f\) est antisymétrique, alors \( f(x,x)=-f(x,x)\). Cela montre que \( f(x,x)=0\) lorsque \( \eK\) est de caractéristique différente de deux.
\end{proof}

\begin{proposition}[\cite{GQolaof}] \label{ProprbjihK}
    Soit \( E\), un \( \eK\)-espace vectoriel de dimension \( n\), où la caractéristique de \( \eK\) n'est pas deux. L'espace des \( n\)-formes multilinéaires alternées sur \( E\) est de \( \eK\)-dimension \( 1\).
\end{proposition}
\index{groupe!permutation}
\index{groupe!et géométrie}
\index{espace!vectoriel!dimension}
\index{rang}
\index{déterminant}


\begin{proof}
    Soit \( \{ e_i \}\), une base de \( E\) et \( f\colon E\to \eK\) une \( n\)-forme linéaire alternée, puis \( (v_1,\ldots, v_n)\) des vecteurs de \( E\). Nous pouvons les écrire dans la base
    \begin{equation}
        v_j=\sum_{i=1}^n\alpha_{ij}e_i
    \end{equation}
    et alors exprimer \( f\) par
    \begin{subequations}
        \begin{align}
            f(v_1,\ldots, v_n)&=f\big( \sum_{i_1=1}^n\alpha_{1i_1}e_{i_1},\ldots, \sum_{i_n=1}^n\alpha_{ni_n}e_{i_n} \big)\\
            &=\sum_{i,j}\alpha_{1i_1}\ldots \alpha_{ni_n}f(e_{i_1},\ldots, e_{i_n}).
        \end{align}
    \end{subequations}
    Étant donné que \( f\) est alternée, les seuls termes de la somme sont ceux dont les \( i_k\) sont tous différents, c'est à dire ceux où \( \{ i_1,\ldots, i_n \}=\{ 1,\ldots, n \}\). Il y a donc un terme par élément du groupe des permutations \( S_n\) et
    \begin{equation}
        f(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\alpha_{\sigma(1)1}\ldots \alpha_{\sigma(n)n}f(e_{\sigma(1)},\ldots, e_{\sigma(n)}).
    \end{equation}
    En utilisant encore une fois le fait que la forme \( f\) soit alternée, \( f=f(e_1,\ldots, e_n)\Pi\) où
    \begin{equation}
        \Pi(v_1,\ldots, v_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\ldots \alpha_{\sigma(n)n}.
    \end{equation}
    Pour rappel, la donnée des \( v_i\) est dans les nombres \( \alpha_{ij}\).
    
    L'espace des \( n\)-formes alternées est donc \emph{au plus} de dimension \( 1\). Pour montrer qu'il est exactement de dimension \( 1\), il faut et suffit de prouver que \( \Pi\) est alternée. Par le lemme \ref{LemHiHNey}, il suffit de prouver que cette forme est antisymétrique\footnote{C'est ici que joue l'hypothèse sur la caractéristique de \( \eK\).}. 

    Soient donc \( v_1,\ldots, v_n\) tels que \( v_i=v_j\). En posant \( \tau=(1i)\) et \( \tau'=(2j)\) et en sommant sur \( \sigma\tau\tau'\) au lieu de \( \sigma\), nous pouvons supposer que \( i=1\) et \( j=2\). Montrons que \( \Pi(v,v,v_3,\ldots, v_n)=0\) en tenant compte que \( \alpha_{i1}=\alpha_{i2}\) :
    \begin{subequations}
        \begin{align}
            \Pi(v,v,v_3,\ldots, v_n)&=\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\alpha_{\sigma(2)2}\alpha_{\sigma(3)3}\ldots \alpha_{\sigma(n)n}\\
            &=\sum_{\sigma\in S_n}\epsilon(\sigma\tau)\alpha_{\sigma\tau(1)1}\alpha_{\sigma\tau(2)2}\alpha_{\sigma\tau(3)3}\ldots \alpha_{\sigma\tau(n)n}&\text{où \( \tau=(12)\)}\\
            &=-\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\alpha_{\sigma(2)2}\alpha_{\sigma(3)3}\ldots \alpha_{\sigma(n)n} \\
            &=-\Pi(v,v,v_3,\ldots, v_n).
        \end{align}
    \end{subequations}
\end{proof}

\begin{proposition}[\cite{LoFdlw}]
    Soit \( n\geq 3\) et \( \eK\) un corps de caractéristique différente de \( 2\). Alors
    \begin{enumerate}
        \item
            le groupe dérivé de \( D(\GL(n,\eK))\) est \(\SL(n,\eK)\);  \index{groupe!dérivé!de \( \GL(n,\eK)\)}
        \item
            le groupe dérivé de \( \SL(n,\eK)\) est \( \SL(n,\eK)\).\index{groupe!dérivé!de \( \SL(n,\eK)\)}
    \end{enumerate}
\end{proposition}
La preuve utilise le fait que les transvections engendrent \( \SL(n,\eK)\) et que les transvections avec les dilatations engendrent \( \GL(n,\eK)\).
%TODO : faire une preuve de cela. C'est dans l'écrit d'algèbre de 2013.

\begin{proposition}\label{PropYGBEECo}
    Le groupe \( \GL^+(n,\eR)\) des matrices inversibles de déterminant \( 1\) est connexe.
            % La proposition \ref{PropIFabDZz} en parle aussi.
\end{proposition}
\index{connexité!le groupe \( \GL^+(n,\eR)\)}
%TODO : une preuve.

\begin{lemma}   \label{LemcDOTzM}
    Soit \( \eK\) un corps fini autre que \( \eF_2\)\footnote{Je ne comprends pas très bien à quel moment joue cette hypothèse.}, soit un groupe abélien \( M\) et un morphisme \( \varphi\colon \GL(n,\eK)\to M\). Alors il existe un unique morphisme \( \delta\colon \eK^*\to M\) tel que \( \varphi=\delta\circ\det\).
\end{lemma}

\begin{proof}
    D'abord le groupe dérivé de \( \GL(n,\eK)\) est \( \SL(n,\eK)\) parce que les éléments de \( D\big( \GL(n,\eK) \big)\) sont de la forme \( ghg^{-1}h^{-1}\) dont le déterminant est \( 1\).
    
    De plus le groupe \( \SL(n,\eK)\) est normal dans \( \GL(n,\eK)\). Par conséquent \( \GL(n,\eK)/\SL(n,\eK)\) est un groupe et nous pouvons définir l'application relevée
    \begin{equation}
        \tilde \varphi\colon \frac{ \GL(n,\eK) }{ \SL(n,\eK) }\to M
    \end{equation}
    vérifiant \( \varphi=\tilde \varphi\circ\pi\) où \( \pi\) est la projection. 

    Nous pouvons faire la même chose avec l'application
    \begin{equation}
        \det\colon \GL(n,\eK)\to \eK^*
    \end{equation}
    qui est un morphisme de groupes dont le noyau est \( \SL(n,\eK)\). Cela nous donne une application
    \begin{equation}
        \tilde \det\colon \frac{ \GL(n,\eK) }{ \SL(n,\eK) }\to \eK^*
    \end{equation}
    telle que \( \det=\tilde \det\circ\pi\). Cette application \( \tilde \det\) est un isomorphisme. En effet elle est surjective parce que le déterminant l'est et elle est injective parce que son noyau est précisément ce par quoi on prend le quotient. Par conséquent \( \tilde \det \) possède un inverse et nous pouvons écrire
    \begin{equation}
        \varphi=\tilde \varphi\circ\tilde \det^{-1}\circ\tilde \det\circ\pi.
    \end{equation}
    État donné que \( \tilde \det\circ\pi=\det\), nous avons alors \( \varphi=\delta\circ\det\) avec \( \delta=\tilde \varphi\circ\tilde \det^{-1}\). Ceci conclu la partie existence de la preuve.

    En ce qui concerne l'unicité, nous considérons \( \delta'\colon \eK^*\to M\) telle que \( \varphi=\delta'\circ\det\). Pour tout \( u\in \GL(n,\eK)\) nous avons \( \delta'(\det(u))=\varphi(u)=\delta(\det(u))\). L'application \( \det\) étant surjective depuis \( \GL(n,\eK)\) vers \( \eK^*\), nous avons \( \delta'=\delta\).
\end{proof}

\begin{theorem}
    Soit \( p\geq 3\) un nombre premier et \( V\), un \( \eF_p\)-espace vectoriel de dimension finie \( n\). Pour tout \( u\in\GL(V)\) nous avons
    \begin{equation}
        \epsilon(u)=\left(\frac{\det(u)}{p}\right).
    \end{equation}
\end{theorem}
Ici \( \epsilon\) est la signature de \( u \) vue comme une permutation des éléments de \( \eF_p\).

\begin{proof}
    Commençons par prouver que
    \begin{equation}
        \epsilon\colon \GL(V)\to \{ -1,1 \}.
    \end{equation}
    est un morphisme. Si nous notons \( \bar u\in S(V)\) l'élément du groupe symétrique correspondant à la matrice \( u\in \GL(V)\), alors nous avons \( \overline{ uv }=\bar u\circ\bar v\), et la signature étant un homomorphisme (proposition \ref{ProphIuJrC}), 
    \begin{equation}
        \epsilon(uv)=\epsilon(\bar u\circ\bar v)=\epsilon(\bar u)\epsilon(\bar v).
    \end{equation}
    Par ailleurs \( \{ -1,1 \}\) est abélien, donc le lemme \ref{LemcDOTzM} s'applique et nous pouvons considérer un morphisme \( \delta\colon \eF_p^*\to \{ -1,1 \}\) tel que \( \epsilon=\delta\circ\det\).

    Nous allons utiliser le lemme \ref{Lemoabzrn} pour montrer que \( \delta\) est le symbole de Legendre. Pour cela il nous faudrait trouver un \( x\in \eF_p^*\) tel que \( \delta(x)=-1\). Étant donné que \( \det\) est surjective, nous cherchons ce \( x\) sous la forme \( x=\det(u)\). Par conséquent nous aurions
    \begin{equation}
        \delta(x)=(\delta\circ\det)(u)=\epsilon(u),
    \end{equation}
    et notre problème revient à trouver une matrice \( u\in\GL(V)\) dont la permutation associée soit de signature \( -1\).

    Soit \( n=\dim V\); en conséquence de la proposition \ref{PropHfrNCB}\ref{ItemiEFRTg}, l'espace \( \eE_q=\eF_{p^n}\) est un \( \eF_p\)-espace vectoriel de dimension \( n\) et est donc isomorphe en tant qu'espace vectoriel à \( V\). Étant donné que \( \eF_q\) est un corps fini, nous savons que \( \eF_q^*\) est un groupe cyclique à \( q-1\) éléments. Soit \( y\), un générateur de \( \eF_q^*\) et l'application
    \begin{equation}
        \begin{aligned}
            \beta\colon \eF_q&\to \eF_q \\
            x&\mapsto yx. 
        \end{aligned}
    \end{equation}
    Cela est manifestement \( \eF_p\)-linéaire (ici \( y\) et \( x\) sont des classes de polynômes et \( \eF_p\) est le corps des coefficients). L'application \( \beta\) fixe zéro et à part zéro, agit comme le cycle
    \begin{equation}
        (1,y,y^2,\ldots, y^{q-2}).
    \end{equation}
    Nous savons qu'un cycle de longueur \( n\) est de signature \( (-1)^{n+1}\). Ici le cycle est de longueur \( q-1\) qui est pair (parce que \( p\geq 3\)) et par conséquent, l'application \( \beta\) est de signature \( -1\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Vandermonde}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{fJhCTE}]  \label{PropnuUvtj}
    Le \defe{déterminant de Vandermonde}{déterminant!Vandermonde}\index{Vandermonde (déterminant)} est le polynôme en \( n\) variables donné par
    \begin{equation}
        V(T_1,\ldots, T_n)=\det\begin{pmatrix}
             1   &   1    &   \ldots    &   1    \\
             T_1   &   T_2    &   \ldots    &   T_n    \\
             \vdots   &   \ddots    &   \ddots    &   \vdots    \\ 
             T_1^{n-1}   &   T_2^{n-1}    &   \ldots    &   T_n^{n-1}     
         \end{pmatrix}=\prod_{1\leq i<j\leq n}(T_j-T_i).
    \end{equation}
    Notez que l'inégalité du milieu est stricte (sinon d'ailleurs l'expression serait nulle).
\end{proposition}
Le déterminant de Vandermonde est entre autres utilisé pour prouver que \( \tr(u^p)=0\) pour tout \( p\) si et seulement si \( u\) est nilpotente (lemme \ref{LemzgNOjY}).

\begin{proof}
    Nous considérons le polynôme
    \begin{equation}
        f(X)=V(T_1,\ldots, T_{n-1},X)\in \big( \eK[T_1,\ldots, T_{n-1}] \big)[X].
    \end{equation}
    C'est un polynôme de degré au plus \( n-1\) en \( X\) et il s'annule aux points \( T_1,\ldots, T_{n-1}\). Par conséquent il existe \( \alpha\in \eK[T_1,\ldots, T_{n-1}]\) tel que
    \begin{equation}    \label{EqeVxRwO}
        f=\alpha(X-T_{n-1})\ldots(X-T_1).
    \end{equation}
    Nous trouvons \( \alpha\) en écrivant \( f(0)\). D'une part la formule \eqref{EqeVxRwO} nous donne
    \begin{equation}    \label{EqblwWMj}
        f(0)=\alpha(-1)^{n-1}T_1\ldots T_{n-1}.
    \end{equation}
    D'autre par la définition donne
    \begin{subequations}
        \begin{align}
            f(0)&=\det\begin{pmatrix}
                 1   &   \cdots    &   1    &   1    \\
                 T_1      &       &   T_{n-1}    &   0    \\
                 \vdots   &       &   \vdots    &   \vdots    \\ 
                 T_1^{n-1}   &   \cdots    &   T_{n-1}^{n-1}    &   0     
             \end{pmatrix}\\
             &=(-1)^{n-1}\det\begin{pmatrix}
                 T_1   &   \ldots    &   T_{n-1}    \\
                 \vdots   &   \ddots    &   \vdots    \\
                 T_1^{n-1}   &   \ldots    &   T_{n-1}^{n-1}
             \end{pmatrix}\\
             &=(-1)^{n-1}T_1\ldots T_{n-1}\det\begin{pmatrix}
                 1   &   \cdots    &   1    \\
                 \vdots   &   \ddots    &   \vdots    \\
                 T_1^{n-1}   &   \cdots    &   T_{n-1}^{n-1}
             \end{pmatrix}\\
             &=(-1)^{n-1}T_1\ldots T_{n-1}V(T_1,\ldots, T_{n-1})
        \end{align}
    \end{subequations}
    En égalisant avec \eqref{EqblwWMj}, nous trouvons \( \alpha=V(T_1,\ldots, T_{n-1})\), et donc
    \begin{equation}
        f=V(T_1,\ldots, T_{n-1})\prod_{j\leq n-1}(X-T_j)
    \end{equation}
    Enfin, une récurrence montre que
    \begin{subequations}
        \begin{align}
            V(T_1,\ldots, T_n)&=f(T_n)\\
            &=V(T_1,\ldots, T_{n-1})\prod_{j\leq n-1}(T_n-T_j)\\
            &=\prod_{k\leq n}\prod_{j\leq k-1}(T_k-T_j)\\
            &=\prod_{1\leq j<k\leq n}(T_i-T_j).
        \end{align}
    \end{subequations}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Gram}
%---------------------------------------------------------------------------------------------------------------------------

Si \( x_1,\ldots, x_r\) sont des vecteurs d'un espace vectoriel, alors le \defe{déterminant de Gram}{déterminant!Gram}\index{Gram (déterminant)} est le déterminant
\begin{equation}
    G(x_1,\ldots, x_r)=\det\big( \langle x_i, x_j\rangle  \big).
\end{equation}
Notons que la matrice est une matrice symétrique.

\begin{proposition}\label{PropMsZhIK}
    Si \( F\) est un sous-espace vectoriel de base \( \{ x_1,\ldots, x_n \}\) et si \( x\) est un vecteur, alors le déterminant de Gram est un moyen de calculer la distance entre \( x\) et \( F\) par 
    \begin{equation}
        d(x,F)^2=\frac{ G(x,x_1,\ldots, x_n)}{ G(x_1,\ldots, x_n) }.
    \end{equation}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Déterminant de Cauchy}
%---------------------------------------------------------------------------------------------------------------------------

Soient des nombres \( a_i\) et \( b_i\) (\( i=1,\ldots, n\)) tels que \( a_i+b_j\neq 0\) pour tout couple \( (i,j)\). Le \defe{déterminant de Cauchy}{déterminant!de Cauchy}\index{Cauchy!déterminant} est 
\begin{equation}
    D_n=\det\left( \frac{1}{ a_i+b_j } \right).
\end{equation}

\begin{proposition}[\cite{RollandRobertjyYDzY}] \label{ProptoDYKA}
    Le déterminant de Cauchy est donné par la formule
    \begin{equation}
        D_n=\frac{ \prod_{i<j}(a_j-a_i)\prod_{i<j}(b_j-b_i) }{ \prod_{ij}(a_i+b_j) }.
    \end{equation}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice de Sylvester}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecSQBJfr}

La définition est pompée de \wikipedia{fr}{Matrice_de_Sylvester}{wikipédia}. Soient \( P\) et \( Q\) deux polynômes non nuls, de degrés respectifs \( m\) et \( n\) :
\begin{subequations}
    \begin{align}
        P(x)=p_0+p_1x+\ldots +p_nx^n\\
        Q(x)=q_0+q_1x+\ldots +q_mx^m.
    \end{align}
\end{subequations}
La \defe{matrice de Sylvester}{matrice!de Sylvester}\index{Sylvester (matrice)} associée à \( P\) et \( Q\) est la matrice carrée \( m+n\times m+n\) définie ainsi :
\begin{enumerate}
    \item
la première ligne est formée des coefficients de \( P\), suivis de 0 :
\begin{equation}
\begin{pmatrix} p_n & p_{n-1} & \cdots & p_1 & p_0 & 0 & \cdots & 0 \end{pmatrix} ;
\end{equation}
\item la seconde ligne s'obtient à partir de la première par permutation circulaire vers la droite ;
\item les $(m-2)$ lignes suivantes s'obtiennent en répétant la même opération ;
\item la ligne $(m+1)$ est formée des coefficients de \( Q\), suivis de 0 :
    \begin{equation}
    \begin{pmatrix} q_m & q_{m-1} & \cdots & q_1 & q_0 & 0 & \cdots & 0 \end{pmatrix} ;
    \end{equation}
    \item les $(m-1)$ lignes suivantes sont formées par des permutations circulaires.
\end{enumerate}

Ainsi dans le cas $n=4$ et $m=3$, la matrice obtenue est
\begin{equation}    \label{EqPEgtle}
S_{p,q}=\begin{pmatrix} 
p_4 & p_3 & p_2 & p_1 & p_0 & 0 & 0 \\
0 & p_4 & p_3 & p_2 & p_1 & p_0 & 0 \\
0 & 0 & p_4 & p_3 & p_2 & p_1 & p_0 \\
q_3 & q_2 & q_1 & q_0 & 0 & 0 & 0 \\
0 & q_3 & q_2 & q_1 & q_0 & 0 & 0 \\
0 & 0 & q_3 & q_2 & q_1 & q_0 & 0 \\
0 & 0 & 0 & q_3 & q_2 & q_1 & q_0 \\
\end{pmatrix}.
\end{equation}
Le déterminant de la matrice de Sylvester associée à \( P\) et \( Q\) est appelé le \defe{résultant}{résultant} de \( P\) et \( Q\) et noté \( \res(P,Q)\)\nomenclature[A]{\( \res(P,Q)\)}{résultat des polynômes \( P\) et \( Q\)}.

Attention : si \( P\) est de degré \( n\) et \( Q\) de degré \( m\), il y a \( m\) lignes pour \( P\) et \( n\) pour \( Q\) dans le déterminant du résultant (et non le contraire).

\begin{lemma}[\cite{QQuRUzA}]       \label{LemBFrhgnA}
    Si \( P\) et \( Q\) sont deux polynômes de degrés \( n\) et \( m\) à coefficients dans l'anneau \( \eA\), alors pour tout \( \lambda\in \eA\),
    \begin{subequations}
        \begin{align}
            \res(\lambda P,Q)&=\lambda^m\res(P,Q)\\
            \res(P,\lambda Q)&=\lambda^n\res(P,Q).
        \end{align}
    \end{subequations}
\end{lemma}

\begin{proof}
    Cela est simplement un comptage de nombre de lignes. Il y a \( m\) lignes contenant les coefficients de \( P\); donc prendre \( \lambda P\) revient à multiplier \( m\) lignes dans un déterminant et donc le multiplier par \( \lambda^m\).
\end{proof}

L'équation de Bézout \eqref{EqkbbzAi}\index{théorème!Bézout!utilisation} peut être traitée avec une matrice de Sylvester. Soient \( P\) et \( Q\), deux polynômes donnés et à résoudre l'équation 
\begin{equation}    \label{EqSsyXOo}
    xP+yQ=0
\end{equation}
par rapport aux polynômes inconnus \( x\) et \( y\) dont les degrés sont \( \deg(x)<\deg(Q)\) et \( \deg(y)<\deg(P)\). Si nous notons \( \tilde x\) et \( \tilde y\) la liste des coefficients de \( x\) et \( y\) (dans l'ordre décroissant de degré), nous pouvons récrire l'équation \eqref{EqSsyXOo} sous la forme
\begin{equation}
    S_{PQ}^t\begin{pmatrix}
        \tilde x    \\ 
        \tilde y    
    \end{pmatrix}=0.
\end{equation}
Pour s'en convaincre, écrivons pour les polynômes de l'exemple \eqref{EqPEgtle} :
\begin{equation}
    \begin{pmatrix}
        p_4    &   0    &   0    &   q_3    &   0    &   0    &   0\\ 
        p_3    &   p_4    &   0    &   q_2    &   q_3    &   0    &   0\\ 
        p_2    &   p_3    &   p_4    &   q_1    &   q_2    &   q_3    &   0\\ 
        p_1    &   p_2    &   p_3    &   q_0    &   q_1    &   q_2    &   q_3\\ 
        p_0    &   p_1    &   p_2    &   0    &   q_0    &   q_1    &   q_2\\ 
        0    &   p_0    &   p_1    &   0    &   0    &   q_0    &   q_1\\ 
        0    &   0    &   p_0    &   0    &   0    &   0    &   q_0\\    
    \end{pmatrix}\begin{pmatrix}
        x_2    \\ 
        x_1  \\ 
        x_0  \\    
        y_3   \\ 
        y_2    \\ 
        y_1    \\ 
        y_0    
    \end{pmatrix}=
    \begin{pmatrix}
        x_2p_4+y_2q_3    \\ 
        p_3x_2+p_4x_1+q_2y_3+q_3y_2  \\ 
          \\    
           \\ 
        \vdots    \\ 
            \\ 
            
    \end{pmatrix}
\end{equation}
Nous voyons que sur la ligne numéro \( k\) (en partant du bas et en numérotant de à partir de zéro) nous avons les produits \( p_ix_j\) et \( q_iy_j\) avec \( i+j=k\). La colonne de droite représente donc bien les coefficients du polynôme \( xP+yQ\).


\begin{proposition} \label{PropAPxzcUl}
    Le résultant de deux polynômes est non nul si et seulement si les deux polynômes sont premiers entre eux.
\end{proposition}
\index{déterminant!résultant}

Un polynôme \( P\) a une racine double en \( a\) si et seulement si \( P\) et \( P'\) ont \( a\) comme racine commune, ce qui revient à dire que \( P\) et \( P'\) ne sont pas premiers entre eux. 

Une application importante de ces résultats sera le théorème de Rothstein-Trager \ref{ThoXJFatfu} sur l'intégration de fractions rationnelles.

\begin{example}
    Si nous prenons \( P=aX^2+bX+c\) et \( P'=2aX+b\) alors la taille de la matrice de Sylvester sera \( 2+1=3\) et
    \begin{equation}
        S_{P,P'}=\begin{pmatrix}
              a  &   b    &   c    \\
            2a    &   b    &   0    \\
            0    &   2a    &   b
        \end{pmatrix}.
    \end{equation}
    Le résultant est alors
    \begin{equation}
        \res(P,P')=-a(b^2-4ac).
    \end{equation}
    Donc un polynôme du second degré a une racine double si et seulement si \( b^2-4ac=0\). Cela est un résultat connu depuis longtemps mais qui fait toujours plaisir à revoir.
\end{example}

La matrice de Sylvester permet aussi de récrire l'équation de Bézout pour les polynômes; voir le théorème \ref{ThoBezoutOuGmLB} et la discussion qui s'ensuit.

Une proposition importante du résultant est qu'il peut s'exprimer à l'aide des racines des polynômes.
\begin{proposition} \label{PropNDBOGNx}
    Si
    \begin{subequations}
        \begin{align}
        P(X)&=a_p\prod_{i=1}^p(X-\alpha_i)\\
        Q(X)&=b_q\prod_{j=1}^q(X-\beta_i)
        \end{align}
    \end{subequations}
    alors nous avons les expressions suivantes pour le résultant :
    \begin{equation}        \label{EqCFUumjx}
        \res(P,Q)=a_p^qb_q^p\prod_{i=1}^p\prod_{j=1}^q(\beta_j-\alpha_i)=b_q^p\prod_{j=1}^qP(\beta_j)=(-1)^{pq}a_p^q\prod_{i=1}^pQ(\alpha_i).
    \end{equation}
\end{proposition}

\begin{proof}
    Si \( P\) et \( Q\) ne sont pas premiers entre eux, d'une part la proposition \ref{PropAPxzcUl} nous dit que \( \res(P,Q)=0\) et d'autre part, \( P\) et \( Q\) ont un facteur irréductible en commun, ce qui  signifie que nous devons avoir un des \( X-\alpha_i\) égal à un des \( X-\beta_j\). Autrement dit, nous avons \( \alpha_i=\beta_j\) pour un couple \( (i,j)\). Par conséquent tous les membres de l'équation \eqref{EqCFUumjx} sont nuls.

    Nous supposons donc que \( P\) et \( Q\) sont premiers entre eux. Nous commençons par supposer que les polynômes \( P\) et \( Q\) sont unitaires, c'est à dire que \( a_p=b_q=1\). Nous considérons alors l'anneau
    \begin{equation}
        \eA=\eZ[\alpha_1,\ldots, \alpha_p,\beta_1,\ldots, \beta_q].
    \end{equation}
    Dans cet anneau, l'élément \( \beta_j-\alpha_i\) est irréductible (tout comme \( X-Y\) est irréductible dans \( \eZ[X,Y]\)). Le résultant \( R=\res(P,Q)\) est un élément de \( \eA\) parce que tous leurs coefficients peuvent être exprimés à l'aide des \( \alpha_i\) et des \( \beta_j\). Dans \( \eA\), l'élément \( \beta_j-\alpha_i\) divise \( R\). En effet lorsque \( \beta_j=\alpha_i\), le déterminant définissant le résultant est nul, ce qui signifie que \( \beta_j-\alpha_i\) est un facteur irréductible de \( R\).

    Par conséquent il existe un polynôme \( T\in \eA\) tel que
    \begin{equation}
        R=\lambda(\alpha_1,\ldots, \beta_q)\prod_{i=1}^p\prod_{j=1}^r(\beta_j-\alpha_i).
    \end{equation}
    Comptons les degrés. Pour donner une idée de ce calcul de degré, voici comment se présente, au niveau des dimensions, le déterminant :
    \begin{equation}  \label{EqJCaATOH}
    \xymatrix{%
        \ar@{<->}[rrr]^{p+1}&&&& \ar@{<->}[r]^{q-1}  &\\
        a_p\ar@{.}[rrd] &a_{p-1}\ar@{.}[rr]  &  & a_0\ar@{.}[rrd] & 0\ar@{.}[r]&0&\ar@{<->}[d]^q \\
        0\ar@{.}[r]&0&a_p\ar@{.}[rr]&&a_1&a_0&\\
        \ar@{<->}[rrrrr]_{p+q}&&&&&&
       }
    \end{equation}
    si les \( a_i\) sont les coefficients de \( P\). Mais chacun des \( a_i\) est de degré \( 1\) en les \( \alpha_i\), donc le déterminant dans son ensemble est de degré \( q\) en les \( \alpha_i\), parce que \( R\) contient \( q\) lignes telles que \eqref{EqJCaATOH}. Le même raisonnement montre que \( R\) est de degré \( p\) en les \( \beta_j\). Par ailleurs le polynôme \( \prod_{i=1}^p\prod_{j=1}^r(\beta_j-\alpha_i)\) est de degré \( p\) en les \( \beta_j\) et \( q\) en les \( \alpha_i\). Nous en déduisons que \( T\) doit être un polynôme ne dépendant pas de \( \alpha_i\) ou de \( \beta_j\).

    Nous pouvons donc calculer la valeur de \( T\) en choisissant un cas particulier. Avec \( P(X)=X^p\) et \( Q(X)=X^q+1\), il est vite vu que \( R(P,Q)=1\) et donc que \( T=1\).

    Si les polynômes \( P\) et \( Q\) ne sont pas unitaires, le lemme \ref{LemBFrhgnA} nous permet de conclure. 

\end{proof}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Kronecker}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons \( K_n\) l'ensemble des polynômes de \( \eZ[X]\)
\begin{enumerate}
    \item
        unitaires de degré \( n\),
    \item
        dont les racines dans \( \eC\) sont de modules plus petits ou égaux à \( 1\),
    \item
        et qui ne sont pas divisés par \( X\).
\end{enumerate}
Un tel polynôme s'écrit sous la forme
\begin{equation}
    P=X^n+\sum_{k=0}^{n-1}a_kX^k.
\end{equation}

\begin{theorem}[Kronecker\cite{KXjFWKA}]    \label{ThoOWMNAVp}
    Les racines des éléments de \( K_n\) sont des racines de l'unité.
\end{theorem}
\index{théorème!Kronecker}
\index{polynôme!à plusieurs indéterminées}
\index{résultant!utilisation}
\index{polynôme!symétrique}

\begin{proof}
    Vu que \( \eC\) est algébriquement clos 
    %(théorème \ref{}) 
    nous pouvons considérer les racines \( \alpha_1,\ldots, \alpha_n\) de \( P\) dans \( \eC\). Nous les considérons avec leurs multiplicités.
%TODO : lorsqu'on aura démontré que \eC est algébriquement clos, il faudra le référentier ici.

    Soit \( R=X^n+\sum_{k=0}^{n-1}b_kX^k\) un élément de \( K_n\) dont nous notons \( \beta_1,\ldots, \beta_n\) les racines dans \( \eC\). Les relations coefficients-racines stipulent que
    \begin{equation}
        b_k=\sum_{1\leq i_1<\ldots <i_{n-k}\leq n}\prod_{j=1}^{n-k}\beta_{i_j}.
    \end{equation}
    En prenant le module et en se souvenant que \( | \beta_{l} |\leq 1\) pour tout \( l\), nous trouvons que
    \begin{equation}
        | b_k |\leq\binom{ n }{ n-k }.
    \end{equation}
    Mais comme \( b_k\in \eZ\), nous avons
    \begin{equation}
        b_k\in\big\{    -\binom{ n }{ n-k },-\binom{ n }{ n-k }+1,\ldots, 0,\cdots,\binom{ n }{ n-k }   \big\}
    \end{equation}
    qui est de cardinal \( \binom{ n }{ n-k }+1\). Nous avons donc
    \begin{equation}
        \Card(K_n)\leq\prod_{k=0}^{n-1}\big( 1+\binom{ n }{ n-k } \big)<\infty.
    \end{equation}
    La conclusion jusqu'ici est que \( K_n\) est un ensemble fini.

    Pour chaque \( k\in \eN^*\) nous considérons les polynômes
    \begin{subequations}
        \begin{align}
            P_k&=\prod_{i=1}^n(X-\alpha_i^k)\\
            Q_k&=X^k-Y\in \eZ[X,Y],
        \end{align}
    \end{subequations}
    et puis nous considérons le résultant \( R_k=\res_X(P,Q_k)\in \eZ[Y]\) :
    \begin{equation}
        R_k=\res_X(P,Q_k)=
        \begin{pmatrix}
            1&a_{n-1}&\cdots&a_0&0&\cdots&0&0&0\\  
            0&1&a_{n-1}&\cdots&a_0&0&\cdots&0&0\\  
            \vdots&\ddots&\ddots&\ddots&&\ddots&\\  
            0&\cdots&0&1&a_{n-1}&\cdots&a_0&0&0\\
            0&\cdots&0&0&1&a_{n-1}&\cdots&a_0&0\\
            0&\cdots&0&0&0&1&a_{n-1}&\cdots&a_0\\
        \\
                    1&0&\ldots&0&-Y&0&\ldots&0&0\\
                    0&1&0&\ldots&0&-Y&0&\ldots&0\\
                &&\ddots&&&\ddots&\ddots\\
                    0&\cdots&0&1&0&\cdots&0&-Y&0\\
                    0&0&\cdots&0&1&0&\cdots&0&-Y
        \end{pmatrix}
    \end{equation}
    Cela est un polynôme en \( Y\) dont le terme de plus haut degré est \( (-1)^nY^n\). Les petites formules de la proposition \ref{PropNDBOGNx} nous permettent d'exprimer \( R_k(Y)\) en termes des racines de \( P\) :
    \begin{equation}
        R_k(Y)=\prod_{i=1}^nQ_k(\alpha_i)=\prod_{i=1}^n(\alpha_i^k-Y)=(-1)^n\prod_{i=1}^n(Y-\alpha_i^k)=(-1)^nP_k(Y).
    \end{equation}
    Vu que \( P\in K_n\) nous savons que les \( \alpha_i\) ne sont pas tous nuls; donc \( P_k\in K_n\). Cependant nous avons vu que \( K_n\) est un ensemble fini; donc parmi les \( P_k\), il y a des doublons (et pas un peu)\quext{Ici dans \cite{KXjFWKA}, il déduit qu'on a un \( k\) tel que \( P_k=P_1=P\). Mois je vois pourquoi on a un \( k\) et un \( l\) tels que \( P_k=P_l\), mais pourquoi on peut en trouver un spécialement égal au premier ? Une réponse à cette question permettrait de solidement réduire la lourdeur de la suite de la preuve.}. Nous regardons même l'ensemble des \( P_{2^n}\) dans lequel nous pouvons en trouver deux les mêmes. Soit \( l>k\) tels que \( P_{2^k}=P_{2^l}\). Si \( \alpha\) est racine de \( P_{2^k}\), alors il est de la forme \( \alpha=\beta^{2^k}\) pour une certaine racines \( \beta\) de \( P\). Par conséquent
    \begin{equation}    \label{EqBEgJtzm}
        \alpha^{2^l/2^k}=\alpha^{2^{l-k}}
    \end{equation}
    est racine de \( P_{2^l}\). Notons que dans cette expression il n'y a pas de problèmes de définition d'exposant fractionnaire dans \( \eC\) parce que \( l>k\). Vu que \eqref{EqBEgJtzm} est racine de \( P_{2^l}\), il est aussi racine de \( P_{2^k}\). Donc
    \begin{equation}
        \big( \alpha^{2^{l-k}} \big)^{2^{l-k}}=\alpha^{2^{2(l-k)}}
    \end{equation}
    est racine de \( P_{2^l}\) et donc de \( P_{2^k}\). Au final nous savons que tous les nombres de la forme \( \alpha^{2^{n(l-k)}}\) sont racines de \( P_{2^k}\). Mais comme \( P_{2^k}\) a un nombre fini de racines, nous pouvons en trouver deux égales. Si nous avons
    \begin{equation}
        \alpha^{2^{n(l-k)}}=\alpha^{2^{m(l-k)}}
    \end{equation}
    pour certains entiers \( m>n\), alors
    \begin{equation}
        \alpha^{2^{n(l-k)}-2^{m(l-k)}}=1,
    \end{equation}
    ce qui prouver que \( \alpha\) est une racine de l'unité. Nous avons donc prouvé que toutes les racines de \( P_{2^k}\) sont des racines de l'unité et donc que les racines de \( P\) sont racines de l'unité.
    
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Intégration de fractions rationnelles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Mes sources pour parler d'intégration de fractions rationnelles : \cite{MKucxNb,CPheFRq,LTjwacY,KXjFWKA}.

\begin{theorem}[Rothstein-Trager\cite{MKucxNb}] \label{ThoXJFatfu}
    Soient \( P,Q\in \eQ[X]\) premiers entre eux avec \( \pgcd(P,Q)=1\) et \( \deg(P)<\deg(Q)\). Nous supposons que \( Q\) est unitaire et sans facteurs carrés. Supposons que nous puissions écrire, dans un extension \( \eK\) de \( \eQ\) la primitive de \( P/Q\) de la façon suivante :
    \begin{equation}        \label{EqCHVaDay}
        \int\frac{ P }{ Q }=\sum_{i=1}^n c_i\ln(P_i)
    \end{equation}
    où les \( c_i\) sont des constantes non nulles et deux à deux distinctes et où les \( P_i\) sont des polynômes unitaires non constants sans facteurs carrés et premiers deux à deux entre eux dans \( \eK[X]\).

    Alors les \( c_i\) sont les racines distinctes du polynôme
    \begin{equation}
        R(Y)=\res_X(P-YQ',Q)\in \eK[Y]
    \end{equation}
    et
    \begin{equation}
        P_i=\pgcd(P-c_iQ',Q).
    \end{equation}
\end{theorem}
\index{théorème!Rothstein-Trager}
\index{fraction!rationnelle!intégration}
\index{intégration!fraction rationnelle}
\index{déterminant!résultant}
\index{résultant!utilisation}


\begin{proof}
    Nous posons 
    \begin{equation}
        U_i=\prod_{j\neq i}P_j.
    \end{equation}
    \begin{subproof}
    \item[Question de division]
    Ensuite nous dérivons formellement l'équation \eqref{EqCHVaDay} et nous multiplions les deux côtés du résultat par \( \prod_{j=1}^nP_j\) :
    \begin{equation}        \label{EqGSJKyDw}
        P\prod_{j=1}^nP_j=Q\sum_{i=1}^nc_i\frac{ P'i }{ P_i }\prod_{j=1}^nP_j=Q\sum_{i=1}^nc_iP'_iU_i.
    \end{equation}
    Une première chose que nous en tirons est que \( Q\) divise le produit \( P\prod_{j=1}^nP_j\); mais \( P\) et \( Q\) étant premiers entre eux, 
    \begin{equation}
        Q\divides \prod_{j=1}^nP_j
    \end{equation}
    par le théorème de Gauss \ref{ThoLLgIsig}.

    Une seconde chose que nous tirons de \eqref{EqGSJKyDw} est que \( P_j\) divise \( Q\sum_{i=1}^nc_iP'_iU_i\). De cette somme, à cause du \( U_i\) qui est divisé par \( P_j\) pour tout \( i\) sauf \( i=j\), le polynôme \( P_j\) divise tous les termes sauf peut-être un. Donc il les divise tous et en particulier
    \begin{equation}
        P_j\divides Qc_jP'_JU_j
    \end{equation}
    En nous souvenant que les \( P_k\) sont premiers entre eux, \( P_j\) ne divise pas \( U_j\). De plus \( P_j\) étant sans facteurs carrés, les polynômes \( P_j\) et \( P'_j\) sont premiers entre eux. Il ne reste que \( Q\). Nous en déduisons que
    \begin{equation}
        P_j\divides Q
    \end{equation}
    pour tout \( 1\leq j\leq n\). Et vu que les \( P_i\) sont premiers entre eux, le fait que chacun divise \( Q\) implique que leur produit divise \( Q\), c'est à dire
    \begin{equation}
        \prod_{j=1}^nP_j\divides Q.
    \end{equation}
    Or nous avions déjà prouvé la division contraire. Du fait que les deux polynômes sont unitaires nous en déduisons qu'ils sont en réalité égaux :
    \begin{equation}        \label{EqJImORVe}
        Q=\prod_{j=1}^nP_j.
    \end{equation}
    Nous pouvons simplifier les deux membres de \eqref{EqGSJKyDw} par cela :
    \begin{equation}        \label{EqJMtGhGR}
        P=\sum_{i=1}^nc_iP'_iU_i.
    \end{equation}
    
\item[Encore un peu de division]

    En dérivant \eqref{EqJImORVe} nous trouvons
    \begin{equation}
        Q'=\sum_{j=1}^nP'_jU_j,
    \end{equation}
    et en écrivant \( P\) sous sa forme \eqref{EqJMtGhGR},
    \begin{equation}    \label{EqLZoYqxP}
        P-c_iQ'=\sum_{j=1}^nc_jP'_jU_j-\sum_{j=1}^nc_iP'_jU_j=\sum_{j=1}^n(c_j-c_i)P'_jU_j.
    \end{equation}
    Le terme \( i=j\) de la somme est nul; en ce qui concerne les autres termes, ils sont divisés par \( P_i\) parce que \( P_i\divides U_j\). Donc \( P_i\) divise tous les termes de la somme et nous avons
    \begin{equation}
        P_i\divides P-c_iQ'.
    \end{equation}
    
\item[Un pgcd pour continuer]

    Nous montrons à présent que \( P_i=\pgcd(P-c_iQ',Q)\). Pour cela nous utilisons la multiplicativité du PGCD lorsque les facteurs sont premiers entre eux :
    \begin{equation}
        \pgcd(P-c_iQ',Q)=\pgcd(P-c_iQ',\prod_{j=1}^nP_j)=\prod_{j=1}^n\pgcd(P-c_iQ',P_j).
    \end{equation}
    Nous remplaçons \( P-c_iQ'\) par son expression \eqref{EqLZoYqxP} et nous écrivons un des facteurs du produit :
    \begin{equation}
        \pgcd(P-c_iQ',P_j)=\pgcd(\sum_{k=1}^n(c_k-c_i)P'_kU_k,P_j)
    \end{equation}
    Le polynôme \( P_j\) divise tous les \( U_k\) sauf celui avec \( k=j\). Donc le lemme \ref{LemUELTuwK} nous permet de dire
    \begin{equation}
        \pgcd(P-c_iQ',P_j)=\pgcd\big( (c_j-c_i)P'_jU_j,P_j \big)=\begin{cases}
            1    &   \text{si \( i\neq j\)}\\
            P_j    &    \text{si \( i=j\).}
        \end{cases}
    \end{equation}
    La seconde ligne provient du fait que nous ayons déjà montré que \( P_j\divides P-c_jQ'\). En fin de compte,
    \begin{equation}
        \pgcd(P-c_iQ',Q)=P_i.
    \end{equation}
    
\item[Une histoire de résultant]

    Les nombres \( c_i\) sont tels que les polynômes \( P-c_iQ'\) et \( Q\) ne sont pas premiers entre eux. Vu que les \( P_i\) sont non nuls, la proposition \ref{PropAPxzcUl} nous dit que le résultant
    \begin{equation}
        \res_X(P-c_iQ',Q)=0.
    \end{equation}
    Donc les \( c_i\) sont des racines du polynôme (en \( Y\))
    \begin{equation}    \label{EqOOimwJj}
        R(Y)=\res_X(P-YQ',Q).
    \end{equation}
    Nous n'avons pas prouvé qu'ils étaient \emph{toutes} les racines\footnote{De plus, nous n'avons pas de garanties que ces racines soient dans \( \eQ\), et en fait il y a des cas dans lesquels les \( c_i\) n'y sont pas.}.

\item[Toutes les racines]

    Nous allons maintenant montrer que les \( c_i\) étaient toutes les racines imaginables du polynôme \eqref{EqOOimwJj} dans toutes les extensions de \( \eQ\). Soit donc \( c\) une racine de \( R\) dans une extension \( \hat \eK\) de \( \eK\) qui ne soit pas parmi les \( c_i\) de la formule \eqref{EqCHVaDay}. Étant donné que \( c\) est racine du résultat, les polynômes \( P-cQ'\) et \( Q\) ont un PGCD non trivial, c'est à dire non constant. Donc
    \begin{equation}
        \pgcd(P-cQ',Q)=s\in \hat\eK[X]
    \end{equation}
    est un polynôme non constant. Si \( T\) un facteur irréductible de \( S\), alors \( T\) divise \( P-cQ'\) et \( Q\), mais \( Q=\prod_{i=1}^nP_i\) avec les \( P_i\) premiers entre eux. Donc \( T\) ne peut diviser que l'un (et exactement un) d'entre eux\footnote{On ne peut pas diviser deux trucs qui sont premiers entre eux; c'est une question de cohérence, madame !}. Soit \( P_{i_0}\) celui qui est divisé par \( T\). La relation \eqref{EqLZoYqxP} dans ce contexte donne :
    \begin{equation}
        P-cQ'=\sum_{j=1}^n(c_j-c)P'_jU_j
    \end{equation}
    Le polynôme \( T\) divise tous les \( U_j\) avec \( j\neq i_0\), mais comme en plus il divise \( P-cQ'\), il divise aussi le dernier terme de la somme :
    \begin{equation}
        T\divides (c_{i_0}-c)P'_{i_0}U_{i_0}.
    \end{equation}
    Le polynôme \( T\) ne divisant pas \( U_{i_0}\) et \(  (c_{i_0}-c)  \) étant non nul, nous concluons que \( T\) divise \( P'_{i_0}\). Mais cela n'est pas possible parce que nous avons supposé que \( P_{i_0}\) était sans facteur carré, ce qui voulait entre autres dire que \( P_{i_0}\) et \( P'_{i_0}\) n'ont pas de facteurs communs.

    \end{subproof}
\end{proof}

Ce théorème suggère la méthode suivante pour trouver la primitive de la fraction rationnelle \( P/Q\) (si elle vérifie les hypothèses)
\begin{enumerate}
    \item
        Écrire le résultant \( R(y)=\res_X(P-yQ',Q)\) et en trouver les racines \( \{ c_i \}_{i=1,\ldots, n}\).
    \item
        Calculer les \( P_p=\pgcd(P-c_iQ',Q)\).
    \item
        Écrire la réponse :
        \begin{equation}
            \int\frac{ P }{ Q }=\sum_{i=1}^nc_i\ln(P_i).
        \end{equation}
\end{enumerate}

Notons que le polynôme \( R(Y)\) est de degré \( \deg(Q)\) (pour le voir, faire un peu de comptage de lignes et colonnes dans la matrice de Sylvester), donc il n'est a priori pas pire à factoriser que \( Q\) lui-même\footnote{C'est de la factorisation de \( Q\) qu'on a besoin pour utiliser la méthode de décomposition en fractions simples.}. Mais il se peut que nous ayons de la chance et que \( R\) soit plus facile que \( Q\).

À part qu'on a peut-être plus de chance avec \( R\) qu'avec \( Q\), l'avantage de la méthode est qu'elle permet d'éviter de passer par des extensions de \( \eQ\) non nécessaires\quext{J'imagine que pour un ordinateur, c'est plus facile d'éviter les extensions.}.

\begin{example}
    Cet exemple provient de \cite{MKucxNb}. Prenons la fraction rationnelle \( \frac{ x }{ x^2-3 }\). L'intégration via les fraction simples est :
\begin{equation}
    \int\frac{ x }{ x^2-3 }dx=\frac{ 1 }{2}\int\frac{ 1 }{ x-\sqrt{3} }+\frac{ 1 }{2}\int\frac{1}{ x+\sqrt{3} }=\frac{ 1 }{2}\ln(x-\sqrt{3})+\frac{ 1 }{2}\ln(x+\sqrt{3})=\frac{ 1 }{2}\ln(x^2-3).
\end{equation}
Nous voyons que dans la réponse, il n'y a pas de racines. Passer par l'extension \( \eQ[\sqrt{3}]\) est par conséquent peut-être un effort inutile. Voyons comment les choses se mettent avec la méthode Rothstein-Trager.

D'abord
\begin{subequations}
    \begin{align}
        R(Y)&=\res_X(X-2YX,X^2-3)\\
        &=\res_X\big( (1-2Y)X,X^2-3 \big)\\
        &=\det\begin{pmatrix}
            1-2Y    &   0    &   0    \\
            0    &   1-2Y    &   0    \\
            1    &   0    &   -3
        \end{pmatrix}\\
        &=(1-2Y)\big( -3(1-2Y) \big)\\
        &=-3(1-2Y)^2,
    \end{align}
\end{subequations}
dont les solutions sont faciles : il n'y a que la racine double \( y=\frac{ 1 }{2}\). La somme \eqref{EqCHVaDay} sera donc réduite à un seul terme avec \( c_1=\frac{ 1 }{2}\). Nous calculons \( P_1\) :
\begin{equation}
    P_1=\pgcd(X-\frac{ 1 }{2}2X,X^2-3)=\pgcd(0,X^2-3)=X^2-3,
\end{equation}
et par conséquent
\begin{equation}
    \int\frac{ X }{ X^2-3 }=\frac{ 1 }{2}\ln(X^2-3).
\end{equation}
À aucun moment nous ne sommes sortis de \( \eQ\).
\end{example}

Comme vu sur cet exemple, l'intérêt du théorème de Rothstein-Trager est de permettre, lorsqu'on a de la chance, d'en profiter, et non de nous en rendre compte à la fin en remarquant bêtement que la réponse pouvait s'écrire dans \( \eQ[X]\).

\begin{remark}
    Afin d'utiliser cette méthode, il faut s'assurer que \( Q\) soit sans facteurs carrés. Si nous devons intégrer un \( \frac{ P }{ Q }\) quelconque, nous devons commencer par écrire
    \begin{equation}
        Q=Q_1Q_2^2Q_3^3\ldots Q_r^r,
    \end{equation}
    et ensuite il y a moyen de ramener l'intégrale de \( P/Q\) à des intégrales de \( \frac{ P }{ Q_1\ldots Q_r }\). Cela ne demande pas de factoriser complètement \( Q\), mais seulement de trouver ses facteurs irréductibles \( Q_i\) dans \( \eQ[X]\).

    Dans l'exemple donné plus haut, \( Q=X^2-3\) a des facteurs irréductibles autres que \( Q\) lui-même dans \( \eR[X]\), mais nous n'en avons pas besoin.
\end{remark}

Voici une exemple provenant de \cite{LTjwacY} où nous évitons de passer par les complexes.

\begin{example}
    D'abord le calcul en décomposant complètement en fractions simples :
    \begin{equation}
        \int\frac{1}{ x^3+x }=\int\left( \frac{1}{ x }-\frac{ 1/2 }{ x-i }-\frac{ 1/2 }{ x+i } \right)=\ln(x)-\frac{ 1 }{2}\ln(x-i)-\frac{ 1 }{2}\ln(x+i)=\ln(x)-\frac{ 1 }{2}\ln(x^2+1).
    \end{equation}
    Ici encore nous passons par l'extension \( \eQ[i]\) alors que la réponse ne contient que des polynômes dans \( \eQ[X]\). En ce qui concerne la méthode de Rothstein-Trager, nous commençons par calculer le résultat (qui est tout de même un peu de calcul) :
    \begin{subequations}
        \begin{align}
        P(y)&=\res_X\big( -3yX^2-y+1,X^3-X \big)\\
        &=\det\begin{pmatrix}
            -3y    &   0    &   1-y    &   0    &   0\\  
            0    &   -3y    &   0    &   1-y    &   0\\  
            0    &   0    &   -3y    &   0    &   1-y\\  
            1    &   0    &   1    &   0    &   0\\  
            0    &   1    &   0    &   1    &   0    
        \end{pmatrix}\\
        &=-(y-1)^2(2y+1)^2
        \end{align}
    \end{subequations}
    
    \begin{verbatim}
----------------------------------------------------------------------
| Sage Version 5.7, Release Date: 2013-02-19                         |
| Type "notebook()" for the browser-based notebook interface.        |
| Type "help()" for help.                                            |
----------------------------------------------------------------------
sage: y=var('y')
sage: R=matrix(5,5,[-3*y,0,1-y,0,0,0,-3*y,0,1-y,0,0,0,-3*y,0,1-y,1,0,1,0,0,0,1,0,1,0])
sage: R.determinant().factor()                                                        
-(y - 1)*(2*y + 1)^2
    \end{verbatim}
    Les solutions sont \( c_1=1\) et \( c_2=-\frac{ 1 }{2}\). Nous pouvons alors calculer les \( P_i\) :
    \begin{equation}
        P_1=\pgcd(-3X^2,X^3+X)=X
    \end{equation}
    et
    \begin{equation}
        P_2=\pgcd(\frac{ 3 }{2}X^2+\frac{ 3 }{2},X^3+X)=X^2+1,
    \end{equation}
    et finalement
    \begin{equation}
        \int\frac{ 1 }{ X^3+X }=\ln(X)-\frac{ 1 }{2}\ln(X^2+1).
    \end{equation}
\end{example}

Notons qu'il n'y a pas de miracles : lorsque la réponse contient des racines, nous ne pouvons pas couper à passer par des extensions et factoriser un peu à la dure.

\begin{example} \label{ExYQODuyU}
    Cet exemple provient de \cite{LTjwacY}. Nous voulons calculer
    \begin{equation}
        \int\frac{ 1 }{ X^2+1 }.
    \end{equation}
    Nous posons donc \( P=1\) et \( Q=X^2+1\). Le résultant à calculer est
    \begin{subequations}
        \begin{align}
            P(y)=\res_X(-2yX+1,X^2+1)=\det\begin{pmatrix}
                -2y    &   1    &   0    \\
                0    &   -2y    &   1    \\
                1    &   0    &   1
            \end{pmatrix}=4y^2+1.
        \end{align}
    \end{subequations}
    Les racines de cela sont complexes et il n'y a donc pas d'échappatoires : \( c_1=\frac{ i }{2}\), \( c_2=-\frac{ i }{2}\). Ensuite, étant donné que \( X^2+1=(X+i)(X-i)=i(-iX+1)(X-1)\) nous avons
    \begin{equation}
        P_1=\pgcd(-iX+1,X^2+1)=X+i.
    \end{equation}
    Notons que de façon naturelle , nous aurions écrit \( P_1=1-iX\), mais par convention nous considérons le PGCD unitaire. Cela ne change rien à la réponse parce que changer \( P_i\) en \( kP_i\) ne fait que rajouter une constante \( \ln(k)\) à la primitive trouvée.

    De la même façon,
    \begin{equation}
        P_2=\pgcd(1+iX,X^2+1)=X-i.
    \end{equation}
    Au final nous écrivons
    \begin{equation}
        \int\frac{1}{ X^2+1 }=\frac{ i }{2}\ln(X+i)-\frac{ i }{2}\ln(X-i).
    \end{equation}
\end{example}

\begin{remark}
    Tout cela est si nous voulons absolument écrire la primitive avec des logarithmes de polynômes. Pour celui de l'exemple \ref{ExYQODuyU}, nous avons trouvé
    \begin{equation}    \label{EqBCjCCbs}
        \int\frac{1}{ X^2+1 }=\frac{ i }{2}\ln(X+i)-\frac{ i }{2}\ln(X-i).
    \end{equation}
    Mais
    \begin{verbatim}
    sage: f(x)=1/(x**2+1)                                                                                                                      
    sage: f.integrate(x)
    x |--> arctan(x)
    \end{verbatim}
    Si nous acceptons de passer aux fonctions trigonométriques (inverses), la primitive prend un tour très différent et bien réel. Ces deux visions de l'univers sont bien entendu\footnote{Si on croit que la mathématique est cohérente.} compatibles. En effet, affin de tomber juste, nous allons prendre la primitive
    \begin{equation}
        f(x)=\frac{ i }{2}\ln(ix-1)-\frac{ i }{2}\ln(ix+1)
    \end{equation}
    au lieu de \eqref{EqBCjCCbs}. Il s'agit seulement de multiplier l'intérieur des logarithmes, ce qui ne donne qu'une constante de différence. Ensuite nous passons à la forme trigonométrique des nombres complexes : \( ix-1=\sqrt{x^2+1} e^{i\arctan(-x)}\) et \( ix+1=\sqrt{x^2+1} e^{i\arctan(x)}\). Avec un peu de calcul,
    \begin{equation}
        f(x)=-\frac{ 1 }{2}\Big( \arctan(-x)-\arctan(x) \Big)=\arctan(x).
    \end{equation}
\end{remark}

%TODO : lorsque j'aurai fait la construction du logarithme et ses propriétés, il faudra en faire référence ici. 

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Applications linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définition}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
	Une application $T: \eR^m\to\eR^n$ est dite \defe{linéaire}{linéaire (application)} si 
\begin{itemize}
\item $T(x+y)=T(x)+T(y)$ pour tout $x$ et $y$ dans $\eR^m$,  
\item $T(\lambda x)=\lambda T(x)$ pour tout $\lambda$ dans $\eR^m$ et $\lambda$ dans $\eR$.
\end{itemize}
\end{definition}

Si $V$ et $W$ sont deux espaces vectoriels réels, nous définissons de la même manière une application linéaire de $V$ dans $W$ comme étant une application $T\colon V\to W$ telle que $T(v_1+v_2)=T(v_1)+T(v_2)$ et $T(\lambda v)=\lambda T(v)$ pour tout $v,v_1,v_2$ dans $V$ et pour tout réel $\lambda$.

L'ensemble des applications linéaires de $\eR^m$ vers $\eR^n$ est noté $\mathcal{L}(\eR^m, \eR^n)$, et plus généralement nous notons $\aL(V,W)$\nomenclature{$\aL(V,W)$}{Ensemble des applications linéaires de $V$ dans $W$} l'ensemble des applications linéaires de $V$ dans $W$. 

\begin{example}
Soit $m=n=1$. Pour tout $b$ dans $\eR$ la fonction $T_b(x)= bx$ est une application linéaire de $\eR$ dans $\eR$. En effet,
\begin{itemize}
\item  $T_b(x+y)= b(x+y)= bx + by = T_b(x)+T_b(y)$,
\item $T_b(ax)=b(ax)= abx = a T_b(x)$.
\end{itemize}
De la même façon on peut montrer que la fonction $T_{\lambda}$ définie par $T_{\lambda}(x)=bx$ est un application linéaire de $\eR^m$ dans $\eR^m$ pour tout $\lambda$ dans $\eR$ et $m$ dans $\eN$.
\end{example}

\begin{example}\label{ex_affine}
	Soit $m=n$. On fixe $\lambda$ dans $\eR$ et $v$ dans $\eR^m$. L'application $U_{\lambda}$ de $\eR^m$ dans $\eR^m$ définie par $U_{\lambda}(x)=\lambda x+v$ n'est pas une application linéaire, parce que 
\[
U_{\lambda}(ax)=\lambda(ax)+v\neq \lambda(bx+v)=a U_{\lambda}(x).
\]
\end{example}

\begin{example}\label{exampleT_A}
	Soit $A$ une matrice fixée de $\mathcal{M}_{n\times m}$\nomenclature{$\mathcal{M}_{n\times m}$}{l'ensemble des matrices $n\times m$}. La fonction $T_A\colon \eR^m\to \eR^n$ définie par $T_A(x)=Ax$ est une application linéaire. En effet, 
\begin{itemize}
\item  $T_A(x+y)= A(x+y)= Ax + Ay = T_A(x)+T_A(y)$,
\item $T_A(ax)=A(ax)= a(Ax) = a T_A(x)$.
\end{itemize}
\end{example}

On peut observer que, si on identifie $\mathcal{M}_{1\times 1}$ et $\eR$, on obtient le premier exemple comme cas particulier.

\begin{proposition}
 Toute application linéaire $T$ de $\eR^m$ dans $\eR^n$ s'écrit de manière unique par rapport aux bases canoniques de $\eR^m$ et $\eR^n$ sous la forme
\[
T(x)=Ax,
\]
avec $A$ dans $\mathcal{M}_{n\times m}$.
\end{proposition}

\begin{proof}
  Soit $x$ un vecteur dans $\eR^m$. On peut écrire $x$ sous la forme $ x=\sum_{i=1}^{m}x_i e_i$. Comme $T$ est une application linéaire on a
\[
T(x)=\sum_{i=1}^{m}x_iT(e_i).
\]
Les images de la base de $\eR^m$, $T(e_j), \, j=1,\ldots,m$, sont des éléments de $\eR^n$, donc on peut les écrire sous la forme de vecteurs
\[
T(e_i)=
\begin{pmatrix}
  a_{1i}\\
\vdots\\
a_{ni}
\end{pmatrix}.
\] 
On obtient alors
\[
T(x)=\sum_{i=1}^{m}x_iT(e_i)=\sum_{i=1}^{m}x_i\begin{pmatrix}
  a_{1i}\\
\vdots\\
a_{ni}
\end{pmatrix}=
\begin{pmatrix}
  a_{11} \ldots a_{1m}\\
\vdots \ddots \vdots\\
 a_{n1} \ldots a_{nm}\\
\end{pmatrix}
\begin{pmatrix}
  x_1\\
\vdots\\
x_m
\end{pmatrix}=Ax.
\]
\end{proof}

\begin{definition}
  Une application $S: \eR^m\to\eR^n$ est dite \defe{affine}{affine (application)} si elle est la somme d'une application linéaire et d'une application constante. Autrement dit, $S$ est affine s'il existe $T: \eR^m\to\eR^n$, linéaire, telle que $S(x)-T(x)$ soit un vecteur constant dans $\eR^n$. 
\end{definition}

\begin{example}
	Les exemples les plus courants d'applications affines sont les droites et les plans ne passant pas par l'origine.
	\begin{description}
		\item[Les droites] Une droite dans $\eR^2$ (ou $\eR^3$) qui ne passe pas par l'origine est le graphe d'une fonction de la forme $s(x)=ax+b$ (ou $s(t)=u x +v$, avec $u$ et $v$  dans $\eR^2$). On reconnait ici la fonction de l'exemple \ref{ex_affine}.
			
		\item[Les plans]
			De la même façon nous savons que tout plan qui ne passe pas par l'origine dans $\eR^3$ est le graphe d'une application affine, $P(x,y)= (a,b)^T\cdot(x,y)^T+(c,d)^T$.
	\end{description}
\end{example}
