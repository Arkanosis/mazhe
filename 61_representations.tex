% This is part of Mes notes de mathématique
% Copyright (c) 2011-2013,2015
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Action de groupe et connexité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Sources : \cite{MneimneLie} et \wikipedia{fr}{Matrice_normale}{wikipédia}.

\begin{theorem}     \label{ThojrLKZk}
    Soit \( G\) un groupe topologique localement compact et dénombrable à l'infini\footnote{Cela signifie qu'il est une réunion dénombrable de compacts} agissant continument et transitivement sur un espace topologique localement compact \( E\). Alors l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon G/G_x&\to E \\
            [g]&\mapsto g\cdot x 
        \end{aligned}
    \end{equation}
    est un homéomorphisme.
\end{theorem}

\begin{lemma}       \label{LemkLRAet}
    Si \( G\) et \( H\) sont des groupes topologiques tels que $G/H$ et \( H\) sont connexes\footnote{Définition \ref{DefIRKNooJJlmiD}.}, alors \( G\) est connexe.
\end{lemma}

\begin{proof}
    Soit \( f\colon G\to \{ 0,1 \}\) une fonction continue. Considérons l'application
    \begin{equation}
        \begin{aligned}
            \tilde f\colon G/H&\to \{ 0,1 \} \\
            [g]&\mapsto f(g). 
        \end{aligned}
    \end{equation}
    D'abord nous montrons qu'elle est bien définie. En effet si \( h\in H\) nous aurions \( \tilde f([gh])=f(gh)\), mais étant donné que \( H\) est connexe, l'ensemble \( gH\) est également connexe; la fonction continue \( f\) est donc constante sur \( gH\). Nous avons donc \( f(gh)=f(g)\).

    Étant donné que \( G/H\) est également connexe, la fonction \( \tilde f\) doit être constante. Si \( g_1\) et \( g_2\) sont deux éléments du groupe, nous avons \( f(g_1)=\tilde f([g_1])=\tilde f([g_2])=f(g_2)\). Nous en déduisons que \( f\) est constante et que \( G\) est connexe.
\end{proof}

\begin{theorem}
    Le groupe \( \SO(n)\) est connexe, le groupe \( \gO(n)\) a deux composantes connexes.
\end{theorem}

\begin{proof}
    La seconde assertion découle de la première parce que les matrices de déterminant \( 1\) et celles de déterminant \( -1\) ne peuvent pas être reliées par un chemin continu tandis que l'application
    \begin{equation}
        M\mapsto \begin{pmatrix}
            -1    &       &       \\
                &   1    &       \\
                &       &   1
        \end{pmatrix}M
    \end{equation}
    est un homéomorphisme entre les matrices de déterminant \( 1\) et celles de déterminants \( -1\). Montrons donc que \( G=\SO(n)\) est connexe par arcs pour \( n\geq 2\) en procédant par récurrence sur la dimension.
    
    Nous acceptons le résultat pour $G=\SO(2)$. Notons que nous en avons besoin pour prouver que la sphère \( S^{n-1}\) est connexe.
    
    Le groupe \( \SO(n)\) agit, par définition, de façon transitive sur la sphère \( S^{n-1}\). Soit \( a\in S^{n-1}\), nous avons
    \begin{subequations}
        \begin{align}
            G\cdot a&=S^{n-1}\\
            G_a&\simeq \SO(n-1)
        \end{align}
    \end{subequations}
    où \( G_a\) est le fixateur de \( a\) dans \( G\). Pour montrer le second point, nous considérons \( \{ e_i \}\), la base canonique de \( \eR^n\) et \( M\in G\) telle que \( Ma=e_1\). Le fixateur de \( e_1\) est évidemment isomorphe à \( \SO(n-1)\) parce qu'il est constitué des matrices de la forme
    \begin{equation}
        \begin{pmatrix}
             1   &   0    &   \ldots    &   0    \\
             0   &   a_{11}    &   \ldots    &   a_{1,n-1}    \\
             \vdots   &   \vdots    &   \ddots    &   \vdots    \\ 
             0   &   a_{n-1,1}    &   \ldots    &   a_{n-1,n-1}     
         \end{pmatrix}
    \end{equation}
    où \( (a_{ij})\in \SO(n-1)\). L'application 
    \begin{equation}
        \begin{aligned}
            \alpha\colon G_{e_1} &\to G_{a} \\
            A&\mapsto M^{-1}A M
        \end{aligned}
    \end{equation}
    est un isomorphisme entre \( G_a\) et \( \SO(n-1)\). Le théorème \ref{ThojrLKZk} nous montre alors que, en tant qu'espaces topologiques,
    \begin{equation}
        G/G_a=S^{n-1}.
    \end{equation}
    L'hypothèse de récurrence montre que \( G_a=\SO(n-1)\) est connexe tandis que nous savons que \( S^{n-1}\) est connexe. Le lemme \ref{LemkLRAet} conclut que \( G=\SO(n)\) est connexe.
\end{proof}

\begin{lemma}       \label{LemIbrsFT}
    Une bijection continue entre un espace compact et un espace séparé est un homéomorphisme.
\end{lemma}

\begin{proposition}
    Les groupes \( \gU(n)\) et \( \SU(n)\) sont connexes.
\end{proposition}

\begin{proof}
    Soit \( G(n)\) le groupe \( \SU(n)\) ou \( \gU(n)\). Ce groupe opère transitivement sur la sphère complexe
    \begin{equation}
        S_{\eC}^{n-1}=\{ z\in \eC^n\tq \langle z, z\rangle=\sum_k| z_k |^2 =1 \}.
    \end{equation}
    Cet ensemble est le même que \( S^{2n-1}\) parce que \( |z_k|=x_k^2+y_k^2\). Nous avons une bijection continue entre \( S^{n-1}\) et \( S^{n-1}_{\eC}\) et donc un homéomorphisme (lemme \ref{LemIbrsFT}). Soit \( a\in S^{n-1}_{\eC}\), nous avons
    \begin{subequations}
        \begin{align}
            G\cdot a&=S^{n-1}_{\eC}\\
            G_a&\simeq G(n-1).
        \end{align}
    \end{subequations}
    La seconde ligne est un isomorphisme de groupe et un homéomorphisme. Il est donné de la façon suivante. D'abord le fixateur de \( e_1\) dans \( G(n)\) est donné par les matrices de la forme
    \begin{equation}
        \begin{pmatrix}
             1   &   0    &   \ldots    &   0    \\
             0   &   a_{11}    &   \ldots    &   a_{1,n-1}    \\
             \vdots   &   \vdots    &   \ddots    &   \vdots    \\ 
             0   &   a_{n-1,1}    &   \ldots    &   a_{n-1,n-1}     
         \end{pmatrix}
    \end{equation}
    où \( (a_{ij})\in G(n-1)\). Par ailleurs si \( M\) est une matrice de \( G(n)\) telle que \( Ma=e_1\), nous avons l'homéomorphisme
  
    \begin{equation}
        \begin{aligned}
            \alpha\colon G_{e_1}&\to G_a \\
            A&\mapsto M^{-1} AM. 
        \end{aligned}
    \end{equation}
    Encore une fois, cela est un homéomorphisme par le lemme \ref{LemIbrsFT}. Par composition nous avons \( G_a\simeq G(n-1)\) et un homéomorphisme
    \begin{equation}
        G(n)/G_a=S^{n-1}_{\eC}.
    \end{equation}
    Le groupe \( G_a\) et l'ensemble \( S^{n-1}_{\eC}\) étant connexes, le groupe \( G(n)\) est connexe par le lemme \ref{LemkLRAet}.
\end{proof}

\begin{lemma}[\cite{PAXrsMn}]
    Si \( G\) est un sous-groupe connexe de \( \GL(n,\eC)\) alors son groupe dérivé\footnote{Définition \ref{DefVUFBooNQjEdn}.} l'est également.
\end{lemma}
\index{groupe dérivé!de \( \GL(n,\eC)\)}

\begin{proof}
    Soit \( S_m\) l'ensemble des produits de \( m\) commutateurs de \( G\) :
    \begin{equation}
        S_m=\{ g_1,\ldots, g_m\,\text{où les \( g_i\) sont des commutateurs} \}.
    \end{equation}
    La partie \( S_m\) est l'image de \( G\) par l'application continue
    \begin{equation}
        \begin{aligned}
            \underbrace{G\times \ldots\times G}_{\text{\( 2m\) facteurs}}&\to G \\
            (g_1,h_1,g_2,h_2,\ldots, g_m,h_m)&\mapsto [g_1,h_1]\ldots [g_m,h_m] 
        \end{aligned}
    \end{equation}
    En tant qu'image d'un connexe par une application continue, \( S_m\) est connexe par la proposition \ref{PropGWMVzqb}. Vu que les \( S_m\) ont l'identité en commun, le groupe dérivé
    \begin{equation}
        D(G)=\bigcup_{m=1}^{\infty}S_m
    \end{equation}
    est également connexe.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces de matrices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

L'ensemble des matrices est un espace vectoriel. Nous identifions $\eM(n,\eR)$ avec $ \eR^{n^2}$; plus précisément, nous identifions une matrice 
\begin{equation}
    A = (a_{i,j})_{1\leq i \leq n, 1 \leq j \leq n}
\end{equation}
avec le vecteur $x = (x_1, x_2, \dots, x_{n^2}) \in \eR^{n^2}$, où $ a_{i,j} = x_{(n-1)i + j}$. 

\begin{definition}  \label{DefWQNooKEeJzv}
    Un endomorphisme est \defe{normal}{normal!endomorphisme}\index{matrice!normale} si il commute avec son adjoint.
\end{definition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dilatations et transvections}
%---------------------------------------------------------------------------------------------------------------------------

Soit un corps commutatif \( \eK\) et \( n\geq 2\).

\begin{theoremDef}[\cite{PAXrsMn}]     \label{ThoooAZKDooNDcznv}
    Soit une application linéaire \( u\colon E\to E\) dont les points fixes forment un hyperplan noté \( H\) d'équation \( H=\ker(f)\) avec \( f\in E^*\).
    \begin{enumerate}
        \item     \label{ITEMooGTKRooQSPNoI}
            Les affirmations suivantes sont équivalentes :
            \begin{enumerate}
                \item  \label{ITEMooZHYRooFGKaifi}
                    \( \det(u)\neq 1\)
                \item       \label{ooXKLWooTfUMzV}
                    L'application \( u\) est diagonalisable et a une valeur propre qui vaut \( \det(u)\neq 1\).
                \item       \label{ooMZPTooCLylbh}  
                    \( \Image(u-\id)\nsubseteq H\).
                \item   \label{ITEMooZHYRooFGKaifiv}
                    Il existe une base de \( E\) dans laquelle la matrice de \( u\) est \( \diag(1,\ldots, 1,\lambda)\) avec \( \lambda\neq 1\).
            \end{enumerate}
        \item       \label{ITEMooMSJXooUsLCHx}
            Les affirmation suivantes sont équivalentes :
            \let\oldthenumii\theenumi
            \renewcommand{\theenumii}{\roman{enumii}}
            \begin{enumerate}
                \item       \label{ITEMooRTIEooOoWCFsa}
                    Il existe \( a\in H\) tel que pour tout \( x\in E\), \( u(x)=x+f(x)a\).
                \item       \label{ITEMooRTIEooOoWCFsb}
                    Dans une base adaptée, la matrice de \( u\) est donnée par
                    \begin{equation}        \label{EQooFXBDooTgZwMv}
                        \begin{pmatrix}
                             1   &       &       &       \\
                                &   \ddots    &       &       \\
                                &       &   1    &   1    \\ 
                                &       &       &   1     
                         \end{pmatrix}.
                    \end{equation}
            \end{enumerate}
            \let\theenumii\oldtheenumii
        \item
            Les conditions \ref{ITEMooZHYRooFGKaifi}-\ref{ITEMooZHYRooFGKaifiv} sont respectées si et seulement si les conditions \ref{ITEMooRTIEooOoWCFsa}-\ref{ITEMooRTIEooOoWCFsb} ne sont pas respectées (elles sont les négations l'une de l'autre.).
    \end{enumerate}
    Un endomorphisme qui est soit l'identité soit respecte les conditions \ref{ITEMooGTKRooQSPNoI} est une \defe{dilatation}{dilatation}. Un endomorphisme qui est soit l'identité soit qui vérifie les conditions \ref{ITEMooMSJXooUsLCHx} est une \defe{transvection}{transvection} (dans les deux cas il faut que les points fixes forment un hyperplan).
\end{theoremDef}

Notons que selon cette terminologie, l'application \( x\mapsto \lambda x\) n'est pas une dilation mais un produit de dilations.

\begin{proof}
    Nous allons prouver plein d'implications \ldots
    \begin{subproof}
    \item[\ref{ITEMooZHYRooFGKaifi} implique \ref{ooXKLWooTfUMzV}]
        Le théorème de la base incomplete (voir remarque \ref{REMooYGJEooEcZQKa}) permet de considérer une base \( \{ e_1,\ldots, e_n \}\) de \( E\) telle que \( \{ e_1,\ldots, e_{n-1} \} \) soit une base de \( H\). Dans cette base, la matrice de \( u\) est de la forme suivante (les cases non remplies sont nulles et les étoiles correspondent à des valeurs inconnues mais pas spécialement nulles) :
        \begin{equation}        \label{EqooPQOEooGUyIwa}
        \begin{pmatrix}
             1   &       &       &   *    \\
                &   \ddots    &       &   \vdots    \\
                &       &   1    &   *    \\ 
                &       &       &   \lambda     
         \end{pmatrix}
        \end{equation}
        Le fait que le déterminant de \( u\) ne soit pas \( 1\) implique que \( \lambda\neq 1\). Par conséquent le polynôme caractéristique
        \begin{equation}
            \chi_u(X)=(1-X)^{n-1}(\lambda-X)
        \end{equation}
        possède une racine \( \lambda\neq 1\), et donc \( u\) possède un vecteur propre \( v\) pour cette valeur\footnote{Proposition \ref{PropooBYZCooBmYLSc}.}. Le vecteur \( v\) est linéairement indépendant de \( \{ e_1,\ldots, e_{n-1} \}\) (parce que vecteur propre de valeur propre différente). Par conséquent l'ensemble \( \{ e_1,\ldots, e_{n-1},v \}\) est une base par le théorème \ref{ThoMGQZooIgrXjy}\ref{ItemHIVAooPnTlsBi}. Cela est une base de vecteurs propres et donc une base de diagonalisation\footnote{Nous pourrions en dire à peine plus et prouver le point \ref{ITEMooZHYRooFGKaifiv}, mais cela ne servirait à rien parce que nous voulons prouver les équivalences et qu'il faudra quand même prouver que \ref{ooMZPTooCLylbh} implique \ref{ITEMooZHYRooFGKaifiv}.}.
    \item[\ref{ooXKLWooTfUMzV} implique \ref{ooMZPTooCLylbh}]
        Nous nommons maintenant \( \{ e_1,\ldots, e_{n} \}\) la base de diagonalisation. Nous avons \( u(e_n)=\lambda e_n\) avec \( \det(u)=\lambda\neq 1\). Nous avons
        \begin{equation}
            (u-\id)(e_n)=(\lambda-1)e_n\notin H,
        \end{equation}
        ce qui prouver que l'image de \( e_n\) par \( u-\id\) n'est pas dans \( H\).
    \item[\ref{ooMZPTooCLylbh} implique \ref{ITEMooZHYRooFGKaifiv}]
        Reprenons une base \( \{ e_1,\ldots, ,e_n \}\) donnant la matrice \eqref{EqooPQOEooGUyIwa}. Il existe \( x\in E\) tel que \( u(x)-x\) n'est pas dans \( H\), c'est à dire tel que \( u\big( u(x)-x \big)\neq u(x)-x\). Nous en déduisons que
        \begin{equation}
            u^2(x)-2u(x)+x\neq 0
        \end{equation}
        ou encore que 
        \begin{equation}
            (X-1)^2(u)x\neq 0.
        \end{equation}
        C'est à dire que \( (X-1)^2\) n'est pas un polynôme annulateur de \( u\). Or ce serait le cas si \( X-1\) était le polynôme minimal (proposition \ref{PropAnnncEcCxj}). Le polynôme caractéristique étant \( (X-1)^{n-1}(X-\lambda)\) (et étant annulateur\footnote{Théorème de Cayley-Hamilton \ref{ThoCalYWLbJQ}.}), le polynôme minimal est de la forme 
        \begin{equation}
            \mu_u(X)=\begin{cases}
                (X-1)(X-\lambda)    &   \text{si \( \lambda\neq 1\)}\\
                X-1    &    \text{si \( \lambda=1\)}.
            \end{cases}
        \end{equation}
        Dans notre cas nous venons de voir que ce n'est pas \( X-1\) et donc c'est \( (X-1)(X-\lambda)\) avec \( \lambda\neq 1\).

        Nous devons trouver une base de diagonalisation \ldots Supposons
        \begin{equation}
            u(e_n)=\sum_{k=1}^{n-1}a_ke_k+\lambda e_n,
        \end{equation}
        dans lequel nous venons de prouver que \( \lambda\neq 1\), et cherchons
        \begin{equation}
            e'_n=\sum\_{j=1}^np_je_j
        \end{equation}
        de telle sorte à avoir \( u(e'_n)=\lambda e_n\). Nous avons
        \begin{subequations}
            \begin{align}
                u(e'_n)&=\sum_{j=1}^{n-1}p_ju(e_j)+p_nu(e_n)\\
                &=\sum_{j=1}^{n-1}(p_j+p_na_j)e_j+p_n\lambda e_n.
            \end{align}
        \end{subequations}
        En égalisant à \( \lambda\sum_{j=1}^np_je_j\), il vient
        \begin{equation}
            p_j+p_na_j=\lambda p_j
        \end{equation}
        pour tout \( j=1,\ldots, n-1\) et la condition triviale \( p_n\lambda=\lambda p_n\) pour \( j=n\). Nous en déduisons que le choix
        \begin{equation}
            p_j=\frac{ p_na_j }{ \lambda-1 }
        \end{equation}
        fonctionne (parce que \( \lambda\neq 1\) comme nous l'avons démontré plus haut). En bref, il suffit de poser
        \begin{equation}
            e'_n=\sum_{j=1}^{n-1}\frac{ p_na_j }{ \lambda-1 }e_j+p_ne_n
        \end{equation}
        avec \( p_n\) au choix pour avoir une base \( \{ e_1,\ldots, e_{n-1},e'_n \}\) de diagonalisation de \( u\) avec \( \lambda\neq 1\) comme dernière valeur propre.
    \item[\ref{ITEMooZHYRooFGKaifiv} implique \ref{ITEMooZHYRooFGKaifi}] Évident \ldots encore qu'il faut invoquer l'invariance du déterminant par changement de base.
    \end{subproof}
    Nous avons terminé la première série d'équivalences. Nous continuons avec la seconde.
       \begin{subproof}
        \item[\ref{ITEMooRTIEooOoWCFsa} implique \ref{ITEMooRTIEooOoWCFsb}]
            Nous prenons \( e_{n-1}=a\) et nous complétons en une base de \( H\). Pour \( e_n\) il suffit de prendre n'importe quel vecteur \( v\) tel que \( f(v)\neq 0\) (qui existe parce que \( f=0\) est seulement un hyperplan), et de le normaliser.

            Dans cette base, la matrice de \( u\) a la forme désirée parce que \( u(e_n)=e_n+f(e_n)a=e_n+e_{n-1}\) du fait que \( e_{n-1}=a\) et \( f(e_n)=1\).
        \item[\ref{ITEMooRTIEooOoWCFsb} implique \ref{ITEMooRTIEooOoWCFsa}]
            Soit \( \{ e_1,\ldots, e_n \}\) cette base. En prenant \( a=e_{n-1}\) et en posant \( x=\sum_kx_ke_k\) nous avons
            \begin{equation}
                u(x)=\sum_{k=1}^{n-1}x_ke_k+x_n(e_{n-1}+e_n)=x+x_ne_{n-1}=x_na.
            \end{equation}
            Mais vu que \( f(x)=\sum_if_ix_i\), et que \( f(e_i)=0\) pour tout \( i=1,\ldots, n-1\) nous avons \( f(x)=f_nx_n\). Il n'y a cependant pas de raisons d'avoir \( f_n=1\). Cependant en définissant
            \begin{equation}
                e'_i=\frac{1}{ f_n }e_i
            \end{equation}
            nous avons bien \( u(e'_n)=\frac{1}{ f_n }(e_{n-1}+e_n)=e'_{n-1}+e'_n\). Donc dans cette base nous avons encore la matrice de \( u\) de la forme
            \begin{equation}
                \begin{pmatrix}
                     1   &       &       &       \\
                        &   \ddots    &       &       \\
                        &       &   1    &   1    \\ 
                        &       &       &   1     
                 \end{pmatrix},
            \end{equation}
            mais cette fois avec \( f(e'_n)=1\).
    \end{subproof}
    Nous avons terminé avec la seconde série d'équivalences. Il nous reste à prouver que la première est équivalente à la négation de la seconde.
    \begin{subproof}
        \item[non \ref{ooMZPTooCLylbh} implique \ref{ITEMooRTIEooOoWCFsa}]
            Considérons \( x_0\in E\) tel que \( f(x_0)=1\) et posons \( a=u(x_0)-x_0\in\Image(u-\id)\). Par la négation de \ref{ooMZPTooCLylbh} nous avons \( a\in H\). De plus \( x_0\notin H\) (sinon \( f(x_0)=0\)) donc \( u(x_0)\neq x_0\) et \( a\neq 0\).

            Nous montrons que ce choix de \( a\) fonctionne : \( u(x)=x+f(x)a\) pour tout \( x\in E\). Nous faisons cela séparément pour \( x\in H\) et pour \( x=x_0\). 

            Si \( h\in H\) alors \( u(h)=h\) et \( f(h)=0\) donc \( h+f(h)a=h=u(h)\). Si \( x=x_0\) alors \( u(x_0)=a+x_0\) (cela est la définition de \( a\)) et\( x_0+f(x_0)a=x_0+a\).
        \item[\ref{ITEMooRTIEooOoWCFsb} implique non \ref{ITEMooZHYRooFGKaifi}]
           Dans une base adaptée nous avons 
           \begin{equation}
               \begin{pmatrix}
                    1    &       &       &       \\
                        &   \ddots    &       &       \\
                        &       &   1    &   1    \\ 
                        &       &       &   1     
                \end{pmatrix},
           \end{equation}
           et donc \( \det(u)=1\), ce qui contredit \ref{ITEMooZHYRooFGKaifi}.
    \end{subproof}
\end{proof}

\begin{remark}
    Nous notons \( E_{ij}\) la matrice qui possède uniquement \( 1\) en position \( (i,j)\). C'est à dire que \( \big( E_{ij} \big)_{kl}=\delta_{ik}\delta_{jl}\). Soit \( H\) l'hyperplan des points fixes de \( f\). Dans une base contenant une base de $H$, la matrice d'une transvection a pour forme type :
    \begin{equation}        \label{EqooZAKHooBjKlTd}
        T_{ij}(\lambda)=\mtu+\lambda E_{ij}
    \end{equation}
    avec \( i\neq j\) et \( \lambda\in \eK\), et une dilatation a pour forme type la matrice diagonale
    \begin{equation}
        D_i(\alpha)=\mtu+(\alpha-1)E_{ii}
    \end{equation}
    avec \( \alpha\in \eK^*\).

    Bien entendu, en choisissant une base quelconque, les matrices des dilatations et des translations peuvent avoir des formes différentes.
\end{remark}

\begin{lemma}       \label{LemooTQJXooGoIxsI}
    Quelque manipulations de lignes et de colonnes pour les matrices.
    \begin{enumerate}
        \item       \label{ITEMooRWANooPAVjkm}
            La multiplication à gauche par \( T_{ij}(\lambda)\) revient à effectuer le remplacement de ligne
            \begin{equation}
                L_i\to L_i+\lambda L_j.
            \end{equation}
        \item       \label{ITEMooHPSMooWBrSXP}
            La multiplication à droite par \( T_{ij}(\lambda)\) revient à effectuer le remplacement de colonne
            \begin{equation}
                C_j\to C_j+\lambda C_i.
            \end{equation}
        \item       \label{ITEMooXUGFooKcbrxs}
            La multiplication à gauche par \( T_{ij}(1)T_{ji}(-1)T_{ij}(1)\) revient à la substitution de lignes
            \begin{subequations}
                \begin{numcases}{}
                    L_i\to L_j\\
                    L_j\to -L_i.
                \end{numcases}
            \end{subequations}
    \end{enumerate}
\end{lemma}
Note qu'il n'est pas possible d'inverser deux lignes à l'aide de transvections sans changer un signe parce que les transvections sont de déterminant \( 1\) alors que l'inversion de lignes change le signe du déterminant.

\begin{proof}
    Point par point.
    \begin{subproof}
        \item[Pour \ref{ITEMooRWANooPAVjkm}]
            Nous devons prouver que
            \begin{equation}
                \big( T_{ij}(\lambda)A \big)_{kl}=\begin{cases}
                    A_{kl}    &   \text{si \( k\neq i\)}\\
                    A_{il}+\lambda A_{jl}    &    \text{si \( k=i\)}.
                \end{cases}
            \end{equation}
            Un peu de calcul matriciel avec utilisation modérée des indices donner :
            \begin{subequations}
                \begin{align}
                    \big( T_{ij}(\lambda)A \big)_{kl}&=\sum_s\big( T_{ij}(\lambda) \big)_{ks}A_{sl}\\
                    &=\sum_s\delta_{ks}A_{sl}+\lambda\delta_{ik}\delta_{js}A_{sl}\\
                    &=A_{kl}+\lambda\delta_{ik}A_{jl}.
                \end{align}
            \end{subequations}
        \item[Pour \ref{ITEMooHPSMooWBrSXP}] C'est la même chose.
        \item[Pour \ref{ITEMooXUGFooKcbrxs}] Si nous appliquons successivement ces trois matrices (de droite à gauche) nous effectuons les substitutions :
            \begin{equation}
                \begin{aligned}[]
                \begin{cases}
                    L'_i=L_i+L_j\\
                    L'_j=L_j
                \end{cases}
                \text{suivit de }
                \begin{cases}
                    L''_i=L'_i\\
                    L''_j=L'_j-L'_i
                \end{cases}
                \text{et de}
                \begin{cases}
                    L'''_i=L''_i+L''_j\\
                    L'''_j=L''_j.
                \end{cases}
                \end{aligned}
            \end{equation}
            En effectuant ces substitutions,
            \begin{equation}
                L'''_i=L''_i+L''_j=L'_i+(L'_j-L'_i)=L'_j=L_j
            \end{equation}
            et
            \begin{equation}
                L'''_j=L''_j=L'_j-L'_i=L_j-(L_i+L_j)=-L_i,
            \end{equation}
            ce qu'il fallait.
    \end{subproof}
\end{proof}

\begin{proposition}[\cite{ooUWTWooPKySTQ}]      \label{PropooFDNRooWFfUDd}
    Soit \( n\geq 2\) et un corps commutatif \( \eK\).
    \begin{enumerate}
        \item
            Si \( A\in \GL(n,\eK)\), il existe des transvections \( U_1,\ldots, U_r\), \( V_1,\ldots, V_s\) telles que
                \begin{equation}        \label{EQooKSQVooIpkdIE}
                    A=U_1\ldots U_r\,D_n\big( \det(A) \big)\,V_1\ldots V_s.
                \end{equation}
        \item       \label{ITEMooLRYXooSoKRiA}
            L'ensemble des transvections engendre le groupe spécial linéaire \( \SL(n,\eK)\).
        \item
            L'ensemble des transvections et des dilatations engendre le groupe linéaire \( \GL(n,\eK)\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous allons montrer que toutes les matrices de \( \SL(n,\eK)\) peuvent être écrites comme produits de matrices de la forme \eqref{EqooZAKHooBjKlTd}. Cela montrera qu'étant donné un endomorphisme \( f\) et une base \emph{pas spécialement liée à \( f\)}, il est possible décrire la matrice de \( f\) comme produit de transvections dont les hyperplans invariants sont «contenus» dans cette base. Cela suffit à prouver que les transvections engendrent \( \SL(n,\eK)\) grâce au lemme \ref{LemFUIZooBZTCiy}.

    Toutes les transvections ont un déterminant égal à \( 1\). Donc le groupe engendré par les transvections est inclus à \( \SL(2,\eK)\). Soit \( A\in\GL(n,\eK)\); nous allons utiliser le pivot de Gauss pour la diagonaliser. Étant donné que \( A\) est inversible, sa première colonne n'est pas nulle. Si \( A_{i1}\neq 0\) alors une multiplication à gauche par \( L_{1i}\big(   (A_{11}-1)/A_{i1}  \big)\) effectue la substitution
    \begin{equation}
        L_1\to L_1-\frac{ A_{11}-1 }{ A_{i1} }L_i
    \end{equation}
    qui met un \( 1\) en la position \( (1,1)\). Notons que si la première colonne est de la forme 
    \begin{equation}
        \begin{pmatrix}
            s    \\ 
            0    \\ 
            \vdots    \\ 
            0    
        \end{pmatrix}
    \end{equation}
    avec \( s\neq 0\) alors il faut plutôt faire les substitutions \( L_2\to L_2+L_1\) et ensuite \( L_1\to L_1-\frac{1}{ s }L_2\) pour obtenir le même résultat. En effectuant le pivot avec \( A_{11}\), une suite d'opérations sur les lignes et les colonnes donnent
    \begin{equation}
        M_1\ldots M_pAN_1\ldots N_q=\begin{pmatrix}
            1    &   0    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( A_1\in\GL(n-1,\eK)\) et \( \det(A_1)=\det(A)\). En continuant de la sorte nous arrivons sur une matrice diagonale\footnote{Attention : les opérations sur les lignes et le colonnes ne sont pas des opérations de similitude. Il n'est pas question de prétendre ici que toutes les matrices de \( \GL(n,\eK)\) sont diagonales, voir la définition \ref{DefBLELooTvlHoB}.}
    \begin{equation}
        M_1\ldots M_{p'}AN_1\ldots N_{q'}=
        \begin{pmatrix}
             1   &       &       &       \\
                &   \ddots    &       &       \\
                &       &   1    &       \\ 
                &       &       &   \alpha     
         \end{pmatrix}
    \end{equation}
    avec \( \alpha=\det(A)\). En d'autres termes nous avons prouvé qu'il existe des transvections \( U_1,\ldots, U_r\) et \( V_1,\ldots, V_s\) telles que
    \begin{equation}        \label{EQooZYYFooQGCgxU}
        A=U_1\ldots U_r\,D_n\big( \det(A) \big)\,V_1\ldots V_s.
    \end{equation}
    Cela prouve que les transvections et les translations engendrent \( \GL(n,\eK)\). Si \( A\in \SL(n,\eK)\) alors \( D_n\big( \det(A) \big)=1\) et l'équation \eqref{EQooZYYFooQGCgxU} est un produit de transvections.
\end{proof}

\begin{proposition}
    Le groupe \( \GL(n,\eR)\) est engendré par les endomorphismes inversibles diagonalisables.
\end{proposition}

\begin{proof}
    Par la proposition \ref{PropooFDNRooWFfUDd}, le groupe \( \GL(n,\eR)\) est engendré par les dilatations et les transvections. Il suffit donc de montrer qu'à leur tour, ces deux types d'endomorphismes sont engendrés par les endomorphismes inversibles et diagonalisables.

    Les dilatations sont diagonalisables et inversibles. Soit une transvection \( u\), et une base \( \{ e_i \}_{i=1,\ldots, n}\) dans laquelle \( u\) est de la forme \eqref{EQooFXBDooTgZwMv}. Nous considérons l'endomorphisme \( d\colon E\to E\) défini par \( d(e_k)=ke_k\). Cet endomorphisme est diagonalisable parce que son polynôme minimal, \( \mu_d=\prod_{k=1}^n(X-k)\), est scindé à racines simples (voir le théorème \ref{ThoDigLEQEXR}).

    Nous avons évidemment \( u= d^{-1}\circ(d\circ u) \) où \( d^{-1}\) est diagonalisable et inversible. Voyons que \( d\circ u\) est également diagonalisable en montrant que \( \mu_d\) est son polynôme minimal (qui est scindé à racines simples).

    Il suffit de montrer que \( \mu_d(d\circ u)(e_k)=0\) pour tout \( k\). Ainsi \( \mu_d\) sera un polynôme annulateur de \( d\circ u\) de degré \( n\), et donc minimal.
    \begin{subproof}
        \item[Si \( k\leq n-1\)]
            Alors \( u(e_k)=e_k\) et \( (d\circ u-n)e_k=(k-n)e_k\). En tout :
            \begin{equation}
                \mu_d(d\circ u)(e_k)=(d\circ u-1)(d\circ u-2)\ldots (d\circ u-n)e_k=(k-1)(k-2)\ldots (k-n)e_k=0
            \end{equation}
            parce que dans le produit des \( k-i\), il y en a forcément un de nul.
        \item[Si \( k=n\)]
            Dans un premier temps,
            \begin{equation}
                (d\circ u-n)e_n=d(e_n+e_{n-1})-ne_n=ne_n+(n-1)e_{n-1}-ne_n=(n-1)e_{n-1}.
            \end{equation}
            Ensuite 
            \begin{subequations}
                \begin{align}
                    \big( d\circ u-(n-1) \big)e_{n-1}&=d(e_{n-1})-(n-1)e_{n-1}\\
                    &=d(e_{n-1})-(n-1)e_{n-1}\\
                    &=(n-1)e_{n-1}-(n-1)e_{n-1}\\
                    &=0
                \end{align}
            \end{subequations}
    \end{subproof}
    Le polynôme \( \mu_d\) est donc un polynôme scindé à \( n\) racines simples annulateur de \( d\circ u\), qui est alors diagonalisable et inversible (parce que \( u\) et \( d\) le sont).

    Donc sous la forme \( u=d^{-1}(du)\), la transvection \( u\) est écrite comme produit de diagonalisables inversibles.
\end{proof}

\begin{proposition}[\cite{LoFdlw}]
    Soit \( n\geq 3\) et \( \eK\) un corps de caractéristique différente de \( 2\). Alors
    \begin{enumerate}
        \item
            le groupe dérivé de \( D(\GL(n,\eK))\) est \(\SL(n,\eK)\);  \index{groupe!dérivé!de \( \GL(n,\eK)\)}
        \item
            le groupe dérivé de \( \SL(n,\eK)\) est \( \SL(n,\eK)\).\index{groupe!dérivé!de \( \SL(n,\eK)\)}
    \end{enumerate}
\end{proposition}
La preuve utilise le fait que les transvections engendrent \( \SL(n,\eK)\) et que les transvections avec les dilatations engendrent \( \GL(n,\eK)\). Voir la proposition \ref{PropooFDNRooWFfUDd}.
%TODO : faire une preuve de cela. C'est dans l'écrit d'algèbre de 2013.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Connexité  de certains groupes}
%---------------------------------------------------------------------------------------------------------------------------


\begin{lemma}           \label{LEMooIPOVooZJyNoH}
    Le groupe \( \gO(n),\eR\) n'est pas connexe.
\end{lemma}

\begin{proof}
    La non connexité par arcs est facile parce que les éléments de déterminant \( 1\) ne peuvent pas être reliés aux éléments de déterminant \( -1\) par un chemin continu restant dans \( \gO(n)\) à cause du théorème des valeurs intermédiaires \ref{ThoValInter}.

    En ce qui concerne la connexité, il faut en dire un peu plus.

    Les éléments de \( \gO(n,\eR)\) ont des déterminants égaux à \( 1\) ou à \( -1\). Ces deux parties sont des ouverts (pour la topologie induite de \( \eM(n,\eR)\)). En effet soit \( A\in\SO(n,\eR)\) (la partie contenant les déterminants \( 1\); ce que l'on va dire tient pour l'autre partie). Alors, vu que le déterminant est une fonction continue sur \( \eM(n,\eR)\) il exist un voisinage \( \mO\) de \( A\) dans \( \\eM(n,\eR)\) dans lequel le déterminant reste entre \( \frac{ 1 }{2}\) et \( \frac{ 3 }{2}\) (c'est la définition de la continuité avec \( \epsilon=1/2\)). L'ensemble \( \mO\cap\gO(n,\eR)\) est par définition un ouvert de \( \gO(n,\eR)\) et ne contient que des éléments de déterminant \( 1\).

    La partie \( \gO(n,\eR)\) de \( \\eM(n,\eR)\) est donc non-connexe selon la définition \ref{DefIRKNooJJlmiD}.
\end{proof}

\begin{lemma}       \label{LEMooQMXHooZQozMK}
    Les groupes \( \gU(n)\) et \( \SU(n)\) sont connexes par arcs.
\end{lemma}

\begin{proof}
    Soit \( A\), une matrice unitaire et \( Q\) une matrice unitaire qui diagonalise \( A\). Étant donné que les valeurs propres arrivent par paires complexes conjuguées,
    \begin{equation}
        QAQ^{-1}=\begin{pmatrix}
            e^{i\theta_1}    &       &       &       &   \\  
            &    e^{-i\theta_1}    &       &       &   \\  
            &       &    \ddots    &       &   \\  
            &       &       &    e^{i\theta_r}    &   \\  
            &       &       &       &        e^{-i\theta_r}
        \end{pmatrix}.
    \end{equation}
    Le chemin \( U(t)\) obtenu en remplaçant \( \theta_i\) par \( t\theta_i\) avec \( t\in\mathopen[ 0 , 1 \mathclose]\) joint \( QAQ^{-1}\) à l'identité. Par conséquent \( Q^{-1}U(t)Q\) joint \( A\) à l'unité.
\end{proof}

\begin{theorem}
    Les matrices \wikipedia{fr}{Endomorphisme_normal}{normales}\footnote{Définition \ref{DefWQNooKEeJzv}.} forment un espace connexe par arc.
\end{theorem}

\begin{proof}
    Soit \( A\) une matrice normale, et \( U\) une matrice unitaire qui diagonalise \( A\). Nous considérons \( U(t)\), un chemin qui joint \( \mtu\) à \( U\) dans \( \gU(n)\). Pour chaque \( t\), la matrice
    \begin{equation}
        A(t)=U(t)^{-1} AU(t)
    \end{equation}
    est normale. Nous avons donc trouvé un chemin dans les matrices normales qui joint \( A\) à une matrice diagonale. Il est à présent facile de la joindre à l'identité.

    Toutes les matrices normales étant connexes à l'identité, l'ensemble des matrices normales est connexe.
\end{proof}

\begin{proposition}     \label{PROPooALQCooLZCKrH}
    Le groupe \( \SL(n,\eK)\) est connexe par arcs.
\end{proposition}

\begin{proof}
    Soit \( A\in \SL(n,\eK)\); par la proposition \ref{PropooFDNRooWFfUDd}\ref{ITEMooLRYXooSoKRiA} nous pouvons écrire
    \begin{equation}
        A=\prod_{c\in X}T_c(\lambda_c)
    \end{equation}
    où \( X\) est une partie de l'ensemble des couples \( (i,j)\) dans \( \{ 1,\ldots, n \}\). En posant
    \begin{equation}
        \begin{aligned}
            \varphi\colon \mathopen[ 0 , 1 \mathclose]&\to \SL(n,\eK) \\
            t&\mapsto \prod_{c\in X}T_c(t\lambda_c) 
        \end{aligned}
    \end{equation}
    nous avons une application continue de \( A\) vers \( \mtu\), dont pour tout \( t\) la matrice \( \varphi(t)\) est inversible de déterminant\( 1\).

    Donc tous les éléments de \( \SL(n,\eK)\) peuvent être reliés à \( \mtu\). Donc \( \SL(n,\eK)\) est connexe par arcs.
\end{proof}

\begin{proposition}[\cite{ooGKOIooXKUQKk}]\label{PROPooVJNIooMByUJQ}
    Le groupe \( \GL(n,\eC)\) est connexe par arcs.
\end{proposition}

\begin{proof}
    Soit \( A\in\GL(n,\eC)\) et sa décomposition \eqref{EQooKSQVooIpkdIE}. Comme fait précédemment, chacune des transvections peut être reliée à \( \mtu\) par un chemin continu dans \( \SL(n,\eC)\). En ce qui concerne le facteur de translation,  nous ne pouvons pas simplement prendre le chemin donné par \( t\mapsto D_n\big( t\det(A) \big)\) parce que le résultat n'est pas inversible en \( t=0\).

    Vu que \( C^*\) il existe une application continue \( \alpha\colon \mathopen[ 0 , 1 \mathclose]\to \eC^*\) telle que \( \alpha(0)=\det(A)\in \eC^*\) et \( \alpha(1)=1\). Il suffit alors de prendre \( D_n\big( \alpha(t) \big)\) et nous avons un chemin continu de \( A\) vers \( \mtu\) restant dans \( \GL(n,\eC)\).
\end{proof}

\begin{proposition} \label{PROPooBIYQooWLndSW}
    Le groupe \( \GL(n,\eR)\) a exactement deux composantes connexes par arcs.
\end{proposition}
\index{connexité!le groupe \( \GL^+(n,\eR)\)}

\begin{proof}
    Nous notons \( \GL^+(n,\eR)\) et \( \GL^-(n,\eR)\) les parties de \( \GL(n,\eR)\) formées des applications de déterminant \( \pm1\) respectivement. Vu le théorème des valeurs intermédiaires (théorème \ref{ThoValInter}), il n'existe pas d'applications continues dans \( \GL(n,\eR)\) reliant \( \GL^+(n,\eR)\) à \( \GL^-(n,\eR)\) tout en restant dans les applications de déterminant non nul\footnote{Si \( \varphi\colon \mathopen[ 0 , 1 \mathclose]\to \GL(n,\eR)\) est le chemin, la fonction à mettre dans le théorème des valeurs intermédiaires est la fonction \( f\colon \mathopen[ 0 , 1 \mathclose]\to \eR\) \(t\mapsto \det\big( \varphi(t) \big)\).}.

    Montrons que \( \GL^{\pm}(n,\eR)\) sont connexes par arcs. Si \( A\in\GL^+(n,\eR)\) alors grâce à la décomposition \eqref{EQooKSQVooIpkdIE}, il existe un chemin continu de \( A\) vers \( D_n\big( \det(A) \big)\). Vu que \( \eR^{\pm}\) sont connexes par arc, il est possible de relier \( D_n\big( \det(A) \big)\) à \( D_n(\pm 1)\) par un chemin continu.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Densité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropDigDensVxzPuo}
    Les matrices diagonalisables sont denses dans \( \eM(n,\eC)\).
\end{proposition}
\index{densité!matrices diagonalisables dans \( \eM(n,\eC)\)}

\begin{proof}
    D'après le lemme de Schur \ref{LemSchurComplHAftTq}, une matrice de \( \eM(n,\eC)\) est de la forme
    \begin{equation}
        A=Q\begin{pmatrix}
            \lambda_1    &   *    &   *    \\
              0  &   \ddots    &   *    \\
            0    &   0    &   \lambda_n
        \end{pmatrix}Q^{-1}.
    \end{equation}
    Les valeurs propres sont sur la diagonale. La matrice est diagonalisable si les éléments de la diagonales sont tous différents. Il suffit maintenant de considérer \( n\) suites \( (\epsilon^{(r)}_k)_{k\in\eN}\) convergentes vers zéro telles que pour chaque \( k\) les nombres \( \lambda_r+\epsilon^{(r)}_k\) soient tous différents. La suite de matrices
    \begin{equation}
        A_k=Q\begin{pmatrix}
            \lambda_1+\epsilon^{(1)}_k    &   *    &   *    \\
              0  &   \ddots    &   *    \\
              0    &   0    &   \lambda_n+\epsilon^{(n)}_k
        \end{pmatrix}Q^{-1}.
    \end{equation}
    est alors diagonalisable pour tout \( k\) et nous avons \( \lim_{k\to \infty} A_k=A\).
\end{proof}

\begin{proposition} \label{PropQGUPooVudelJ}
    Les matrices inversibles sont denses dans l'ensemble des matrices. C'est à dire que \( \GL(n,\eR)\) est dense dans \( \eM(n,\eR)\).
\end{proposition}
\index{densité!de \( \GL(n,\eR)\) dans \( \eM(n,\eR)\)}

\begin{proof}
    Soit \( A\in \eM(n,\eR)\); le lemme de Schur réel \ref{LemSchureRelnrqfiy} nous permet d'écrire
    \begin{equation}
        A=Q
        \begin{pmatrix}
            \lambda_1    &       &       &       &   \\  
                &   \ddots    &       &       &   \\  
                &       &  \lambda_r     &       &   \\  
                &  &     &   \begin{pmatrix}
                    a    &   b    \\ 
                    c    &   d    
                \end{pmatrix}&          \\  
                &       &       &       &   \ddots    
        \end{pmatrix}
        Q^{-1}
    \end{equation}
    avec \( Q\) orthogonale.

    Pour définir \( A_k\) nous remplaçons \( \lambda_i\) par \( \lambda_i+\epsilon^{(i)_k}\) de façon à avoir \( \epsilon^{(i)}_k\to 0\) et \( \lambda_i+\epsilon^{(i)}_k\neq 0\). En ce qui concerne les blocs, ceux dont le déterminant est non nul, nous n'y touchons pas, et ceux dont le déterminant est nul, nous remplaçons \( a\) par \( a+\epsilon_k\).

    Avec cela, \( QA_kA^{-1}\) est une suite dans \( \GL(n,\eR)\) qui converge vers \( A\).
\end{proof}

\begin{proposition}
    Si \( A\in\eM(n,\eC)\) alors
    \begin{equation}
        e^{\tr(A)}=\det( e^{A}).
    \end{equation}
\end{proposition}

\begin{proof}
    Ici, \( e^A\) est l'exponentielle soit d'endomorphisme soit de matrice définie par la proposition \ref{PropPEDSooAvSXmY}.

    Le résultat est un simple calcul pour les matrices diagonalisable. Si \( A\) n'est pas diagonalisable, nous considérons une suite de matrices diagonalisables \( A_k\) dont la limite est \( A\) (proposition \ref{PropDigDensVxzPuo}). La suite
    \begin{equation}
        a_k= e^{\tr(A_k)}
    \end{equation}
    converge vers \(  e^{\tr(A)}\) tandis que la suite 
    \begin{equation}
        b_k=\det( e^{A_k})
    \end{equation}
    converge vers \( \det( e^{A})\). Mais nous avons \( a_k=b_k\) pour tout \( k\); les limites sont donc égales.
\end{proof}

\begin{theorem}[Cayley-Hamilton\cite{QATooFIHVMw,MOSooRVRrHw}]  \label{ThoHZTooWDjTYI}
    Tout endomorphisme d'un espace vectoriel de dimension finie sur un corps commutatif quelconque annule son propre polynôme caractéristique
\end{theorem}
\index{théorème!Cayley-Hamilton}
% position EYRooJkxiFf

Une autre démonstration est donnée en le théorème \ref{ThoCalYWLbJQ}.
\begin{proof}
    La preuve est divisée en plusieurs étapes.
    \begin{subproof}
        \item[Endomorphisme diagonalisable]
            Soit \( u\) un endomorphisme sur un espace vectoriel \( V\) de dimension \( n\) sur un corps \( \eK\) et \( \chi_u\) sont polynôme caractéristique. Nous savons que si \( \lambda\) est une valeur propre de \( u\) alors \( \chi_u(\lambda)=0\) le théorème \ref{ThoWDGooQUGSTL}\ref{ItemeXHXhHii}. En combinant avec le lemme \ref{LemVISooHxMdbr}, si \( x\) est vecteur propre pour la valeur propre \( \lambda\) de \( u\) nous avons
            \begin{equation}
                \chi_u(u)x=\chi_u(\lambda)x=0.
            \end{equation}
            Donc tant que \( u\) possède une base de vecteurs propres nous avons \( \chi_u(u)=0\).

        \item[Le cas complexe]

            Nous nous restreignons à présent (et provisoirement) au cas \( \eK=\eC\), ce qui nous donne \( u\in \eM(n,\eC)\). Les matrices diagonalisables sont denses dans \( \eM(n,\eC)\) par la proposition \ref{PropDigDensVxzPuo}. Si \( A\in \eM(n,\eC)\) nous considérons une suite de matrices diagonalisables \( A_k\stackrel{\eM(n,\eC)}{\longrightarrow}A\). Pour chaque \( k\) nous avons par le point précédent 
            \begin{equation}
                \chi_{u_k}(u_k)=0.
            \end{equation}
            Chacune des composantes de \( \chi_{u_k}(u_k)\) est un polynôme en les composantes de \( u_k\), ce qui légitime le passage à la limite :
            \begin{equation}
                \chi_u(u)=0.
            \end{equation}
            Le théorème est établi pour toutes les matrices de \( \eM(n,\eC)\) et donc aussi pour tous les sous-corps de \( \eC\) comme \( \eR\) ou \( \eZ\).

        \item[La cas général]

            Par définition, \( \chi_u(X)=\det(u-X\mtu)\); les coefficients de \( X\) sont des polynômes à coefficients entiers en les composantes de \( u\). En substituant \( u\) à \( X\) nous obtenons une matrice dont chacune des entrées est un polynôme à coefficients entiers en les coefficients de \( u\). Pour chaque \( i\) et \( j\) entre \( 1\) et \( n\) il existe donc un polynôme \( P_{ij}\in \eZ(X_1,\ldots, X_{n^2})\) tel que
            \begin{equation}
                \chi_u(u)_{ij}=P(u_{11},\ldots, u_{nn}).
            \end{equation}
            Ces polynômes ne dépendent pas de \( u\) ni du corps sur lequel on travaille. Notre but est maintenant de prouver que \( P_{ij}=0\).

            Étant donné que le cas complexe (et a fortiori entier) est déjà prouvé nous savons que pour tout \( u\in \eM(n,\eZ)\) nous avons \( P(u_{11},\ldots, u_{nn})=0\). La proposition \ref{PropTETooGuBYQf} nous donne effectivement \( P=0\), en conséquence de quoi l'endomorphisme \( \chi_u(u)\) est nul.

    \end{subproof}
\end{proof}

\begin{example}
    Pour montrer que chaque composante \( \chi_u(u)\) est bien un polynôme à coefficients entiers en les coefficients de \( u\), voyons l'exemple \( 2\times 2\) : \( u=\begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}\). D'abord
    \begin{equation}
        \chi_u(X)=\det\begin{pmatrix}
            a-X    &   b    \\ 
            c    &   d-X    
        \end{pmatrix}=X^2-(a+d)X+ad-cb.
    \end{equation}
    Le coefficient de \( X^2\) est \( 1\), celui de \( X\) est \( -a-d\) et le terme indépendant est \( ad-cb\); tout trois sont des polynômes à coefficients entiers en \( a,b,c,d\). Après substitution de \( X\) par \( u\), 
    \begin{equation}
        \chi_u(u)_{ij}=(u^2)_{ij}-(a+d)u_{ij}+ad-cb.
    \end{equation}
    Cela est bien un polynôme à coefficients entiers en les entrées de la matrice \( u\).
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Racine carré d'une matrice hermitienne positive}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropVZvCWn}
    Si \( A\in \eM(n,\eC)\) est une matrice hermitienne positive, alors il existe une unique matrice hermitienne positive \( R\) telle que \( A=R^2\). De plus \( R\) est un polynôme (de \( \eR[X]\)) en \( A\).
\end{proposition}
\index{matrice!semblable}
\index{polynôme!d'endomorphisme}
\index{endomorphisme!diagonalisable}
\index{matrice!hermitienne!racine carré}
\index{racine!carré!de matrice hermitienne}

La matrice \( R\) ainsi définie est la \defe{racine carré de}{matrice!racine carré}\index{racine!carré de matrice!hermitienne positive} de \( A\), et est notée \( \sqrt{A}\)\nomenclature[A]{\( \sqrt{A}\)}{racine d'une matrice hermitienne positive}. Une des applications usuelles de cette proposition est la décomposition polaire.

\begin{proof}
    \begin{subproof}
    \item[Existence]
        Étant donné que \( A \) est hermitienne, elle est diagonalisable par une unitaire (proposition \ref{ThogammwA}), et ses valeurs propres sont réelles et positives (parce que \( A\) est positive). Soit donc \( P\) une matrice unitaire telle que
        \begin{equation}
            P^*AP=\begin{pmatrix}
                \alpha_1    &       &       \\
                    &   \ddots    &       \\
                    &       &   \alpha_n
            \end{pmatrix}
        \end{equation}
        avec \( \alpha_i>0\). Si on pose
        \begin{equation}
            R=P\begin{pmatrix}
                \sqrt{\alpha_1}    &       &       \\
                    &   \ddots    &       \\
                    &       &   \sqrt{\alpha_n}
            \end{pmatrix}P^*,
        \end{equation}
        alors \( R^2=A\) parce que \( P^*P=\mtu\).
    \item[Hermitienne positive]
        La matrice \( R\) est hermitienne parce que, avec un peu de notation raccourcie, \( R=P^*\sqrt{\alpha}P\) et \( R^*=P^*\sqrt{\alpha}P\). D'autre part, elle est positive parce que ses valeurs propres sont les \( \sqrt{\alpha_i}\) qui sont positives.
        
    \item[Polynôme]
        Nous montrons maintenant que la matrice \( R\) est un polynôme en \( A\). Pour cela nous considérons un polynôme \( Q\) tel que \( A(\alpha_i)=\sqrt{\alpha_i}\) pour tout \( i\). Soit \( \{ e_i \}\) une base de diagonalisation de \( A\) : \( Ae_i=\alpha_ie_i\). Alors c'est encore une base de diagonalisation de \( Q(A)\). En effet si \( Q=\sum_ka_kX^k\), alors
        \begin{equation}
            Q(A)e_i=(\sum_ka_kA^k)e_i=(\sum_ka_k\alpha_i^k)e_i=Q(\alpha_i)e_i=\sqrt{\alpha_i}e_i.
        \end{equation}
        Les valeurs propres de \( Q(A)\) sont donc \( \sqrt{\alpha_i}\). Nous savons maintenant que \( Q(A)\) a la même base de diagonalisation de \( A\) (et donc la même matrice unitaire \( P\) qui diagonalise), c'est à dire que
        \begin{equation}
            Q(A)=P^*\begin{pmatrix}
                \sqrt{\alpha_1}    &       &       \\
                    &   \ddots    &       \\
                    &       &   \sqrt{\alpha_n}
            \end{pmatrix}=R.
        \end{equation}
        Donc oui, \( R\) est un polynôme en \( A\).

        Notons que ce \( Q\) n'est pas du tout unique; il existe une infinité de polynômes qui envoient \( n\) nombres donnés sur \( n\) nombres donnés.

    \item[Unicité]
        Soit \( S\) une matrice hermitienne positive telle que \( R^2=S^2=A\). D'abord \( S\) commute avec \( A\) parce que
        \begin{equation}
            SA=S^3=S^2S=AS.
        \end{equation}
        Donc \( S\) commute aussi avec \( Q(A)=R\). Étant donné que \( S\) et \( R\) commutent et sont diagonalisables, ils sont simultanément diagonalisables par le corollaire \ref{CorQeVqsS}. Soient \( D_R=PRP^*\) et \( D_S=PSP^*\) les formes diagonales de \( R\) et \( S\) dans une base de simultanée diagonalisation. Les carrés des valeurs propres de \( R\) et \( S\) étant identiques (ce sont les valeurs propres de \( A\)) et les valeurs propres de \( R\) et \( S\) étant positives, nous déduisons que \( D_R=D_S\) et donc que \( R=P^*D_RP=P^*D_SP=S\).
    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Racine carré d'une matrice symétrique positive}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[\cite{JJdQPyK}]   \label{LemTLlTAAf}
    Le groupe orthogonal \( \gO(n,\eR) \) est compact.
\end{lemma}

\begin{proof}
    Nous avons \( O(n)=f^{-1}\big( \{ \mtu_n \} \big)\) où \( f\) est l'application continue \( A\mapsto A^tA\). En tant qu'image inverse d'un fermé par une application continue, le groupe \( O(n)\) est fermé.

    De plus il est borné parce que tous les coefficients d'une matrice orthogonale sont \( \leq 1\), donc \( \| A \|_{\infty}\) pour tout \( A\in O(n)\).
\end{proof}

\begin{proposition} \label{PropPEMDqVT}
    Une matrice symétrique semi (ou pas) définie positive admet une unique racine carré symétrique. Le spectre de la racine carré est la racine carré du spectre de la matrice de départ.
\end{proposition}

\begin{proof}
    Ceci est une phrase pour que les titres se mettent bien.
    \begin{subproof}
        \item[Existence]
            Soit \( T\) une matrice symétrique et \( Q\) une matrice orthogonale qui diagonalise\footnote{Théorème \ref{ThoeTMXla}.} \( T\) : \( QTQ^{-1}=D\) avec \( D=\diag(\lambda_i)\) et \( \lambda_i\geq 0\). En posant \( R=Q^{-1}\sqrt{D}Q\), il est vite vérifié que \( R^2=T\) et que \( R\) est symétrique. En ce qui concerne le spectre, \( R\) a pour valeurs propres les \( \sqrt{\lambda_i}\).
        \item[Unicité]

            Soit \( R\) une matrice symétrique de \( T\) : \( R^2=T\). Du coup \( R\) et \( T\) commutent : \( RT=R^3=TR\). Par conséquent les espaces propres de \( T\) sont stables sous \( R\). Soit \( E_{\lambda} \) l'un d'eux de dimension \( d\), et \( T_F\), \( R_F\) les restrictions de \( T\) et \( R\) à \( E_{\lambda}\). L'application \( T_F\) est une homothétie et \( R_F^2=T_F=\lambda\mtu\). Mais \( R_F\) est encore une matrice symétrique définie positive, donc nous pouvons considérer une base \( \{ e_1,\ldots, e_d \}\) de \( E_{\lambda}\) qui diagonalise \( R_F\) avec les valeurs propres \( \mu_i\); nous avons donc en même temps
            \begin{subequations}
                \begin{align}
                    R_f^2(e_i)&=\mu_i^2 e_i\\
                    T_F(e_i)&=\lambda e_i,
                \end{align}
            \end{subequations}
            de telle sorte que \( \mu_i^2=\lambda\). Mais les valeurs propres de \( R_F\) sont positives, sont \( \mu_i=\sqrt{\lambda}\) pour tout \( i\). En conclusion \( R_F\) est univoquement déterminé par la donnée de \( T\). Vu que cela est valable pour tous les espaces propres de \( T\) et que ces espaces propres engendrent tout \( E\), l'opérateur \( R\) est déterminé de façon univoque par \( T\).
    \end{subproof}
\end{proof}
Notons que nous n'avons démontré l'unicité qu'au sein des matrices symétriques.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Décomposition polaires : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

Nous nommons \( S^+(n,\eR)\) l'ensemble des matrices \( n\times n\) symétriques réelles définies positives et \( S^{++}(n,\eR)\) le sous-ensemble de \( S^+(n,\eR)\) des matrices strictement définies positives.
\nomenclature[B]{\( S^+(n,\eR)\)}{matrices symétriques définies positives}
\nomenclature[B]{\( S^{++}(n,\eR)\)}{matrices symétriques strictement définies positives}

\begin{lemma}   \label{LemMGUSooPqjguE}
    La partie \( S^+(n,\eR)\) est fermée dans \( \eM(n,\eR)\).
\end{lemma}

\begin{proof}
    En effet si \( S_k\) est une suite de matrices symétriques convergeant dans \( \eM(n,\eR)\) vers la matrice \( A\), les suites \( (S_k)_{ij}\) et \( (S_k)_{ji}\) des composantes \( ij\) et \( ji\) sont des suites égales, et donc leurs limites sont égales\footnote{Ici nous utilisons le critère de convergence composante par composante et le fait que nous ne sommes pas trop inquiétés par la norme que nous choisissons parce que toutes les normes sont équivalentes par le théorème \ref{ThoNormesEquiv}.}. Donc la limite est symétrique.

    En ce qui concerne le spectre, le théorème \ref{ThoeTMXla} nous permet de diagonaliser : \( S_k=Q_kD_kQ_k^{-1}\) où les \( D_k\) sont des matrices diagonales remplies de nombres positifs ou nuls. Vu que \( O(n)\) est compact\footnote{Lemme \ref{LemTLlTAAf}.}, nous avons une sous-suite \( Q_{\varphi(k)}\) convergente : \( Q_{\varphi(k)}\to Q\). Pour chaque \( k\), nous avons
    \begin{equation}
        S_{\varphi(k)}=Q_{\varphi(k)}D_{\varphi(k)}Q^{-1}_{\varphi(k)},
    \end{equation}
    dont la limite existe et vaut \( A\). Vu que pour tout \( k\), \( D_{\varphi(k)}=Q^{-1}_{\varphi(k)}S_{\varphi(k)}Q_{\varphi(k)}\) et que le produit matriciel est continu, la suite \( k\mapsto D_{\varphi(k)}\) est une suite convergente dans \( \eM(n,\eR)\). Nous notons \( D\) sa limite qui est encore une matrice diagonale contenant des nombres positifs ou nuls sur la diagonale.
    \begin{equation}
        A=\lim_{k\to \infty } S_{\varphi(k)}=QDQ^{-1},
    \end{equation}
    et donc le spectre de \( A\) est la limite de ceux des matrices \( D_{\varphi(k)}\). Chacun étant positif, la limite est positive. Donc \( A\in S^+(n,\eR)\).
\end{proof}

\begin{lemma}   \label{LemZKJWqIP}
    La fermeture de l'ensemble des matrice symétriques strictement définies positives est l'ensemble des matrices définies positives : \( \overline{ S^{++}(n,\eR) }=S^+(n,\eR)\).
\end{lemma}
\index{densité!de \( S^+(n,\eR)\) dans \( S^{++}(n,\eR)\)}

\begin{proof}
    Le lemme \ref{LemMGUSooPqjguE} nous a à peine dit que \( S^+(n,\eR)\) était fermé. Nous devons prouver que pour tout élément de \( S^+(n,\eR)\), il existe une suite \( (S_k)\) dans \( S^{++}(n,\eR)\) convergeant vers \( S\).

    Si \( S\in S^+(n,\eR)\) alors nous avons la diagonalisation
    \begin{equation}
        S=QDQ^{-1} =Q
        \begin{pmatrix}
            \lambda_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_n
        \end{pmatrix}
        Q^{-1}
    \end{equation}
    où \( \lambda_i\geq 0\) pour tout \( i\). Nous définissons
    \begin{equation}
        D_k=
        \begin{pmatrix}
            \lambda_1+\epsilon^{(1)}_k    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_n+\epsilon^{(n)}_k
        \end{pmatrix}
    \end{equation}
    où \( \epsilon^{i}_k\) est une suite convergent vers \( 0\) telle que \( \lambda_i+\epsilon^{(i)}_n>0\) pour tout \( n\). Typiquement si \( \lambda_i>0\) alors \( \epsilon^{(i)}_k=0\) et sinon \( \epsilon^{(i)}_k=1/k\).

    Pour tout \( k\) nous avons \( QD_kQ^{-1}\in S^{++}(n,\eR)\) et de plus \( QD_kQ^{-1}\to QDQ=S\).
\end{proof}

\begin{theorem}[Décomposition polaire de matrices symétriques définies positives\cite{JJdQPyK,AABkVai,WWBTooITOwEn}] \label{ThoLHebUAU}
   En ce qui concerne les matrices inversibles :
   \begin{equation}
       \begin{aligned}
           f\colon O(n,\eR)\times S^{++}(n,\eR)&\to \GL(n,\eR) \\
           (Q,S)&\mapsto SQ 
       \end{aligned}
   \end{equation}
   est un homéomorphisme\footnote{Cela est en réalité en difféomorphisme, voir la remarque \ref{RemBJCBooGLiRmG}.}.

   En ce qui concerne les matrices en général :
   \begin{equation}
       \begin{aligned}
           g\colon O(n,\eR)\times S^+(n,\eR)&\to \eM(n,\eR) \\
           (Q,S)&\mapsto SQ 
       \end{aligned}
   \end{equation}
   est une surjection mais pas une injection.

   De plus les mêmes conclusions tiennent si nous regardons \( (Q,S)\mapsto QS\) au lieu de \( SQ\).
\end{theorem}
\index{groupe!linéaire!décomposition polaire}
\index{endomorphisme!décomposition!polaire}
\index{décomposition!polaire}

%TODO : prouver le difféomorphisme.
%TODO : je crois qu'on doit pouvoir prouver que les éléments de la décomposition polaire sont des polynômes en M.

\begin{proof}
    Nous commençons par prouver les résultats concernant les matrices inversibles.
    \begin{subproof}
        \item[Existence et unicité]

            Si \( M=SQ\), alors \( MM^t=SQQ^tS^t=S^2\), donc \( S\) doit être une racine carré symétrique de la matrice définie positive \( MM^t\). La proposition \ref{PropPEMDqVT} nous dit que ça existe et que c'est unique. Donc \( S\) est univoquement déterminé par \( M\). Maintenant avoir \( Q=MS^{-1}\) est obligatoire (unicité) et fonctionne :
            \begin{equation}
                Q^tQ=(S^{-1})^tM^tMS^{-1}=S^{-1}S^2S^{-1}=\mtu,
            \end{equation}
            donc \( Q\) ainsi défini est orthogonale.

            Notons que ceci ne fonctionne pas lorsque \( M\) n'est pas inversible parce qu'alors \( S\) n'est pas inversible.
        
        \item[Homéomorphisme]

            Le fait que \( f\) soit continue n'est pas un problème : c'est un produit de matrice. Nous devons vérifier que \( f^{-1}\) est continue. Soit une suite convergente \( M_k\to M\) dans \( \GL(n,\eR)\). Si nous nommons \( (Q_k,S_k)\) la décomposition polaire de \( M_k\) et \( (Q,S)\) celle de \( M\), nous devons prouver que \( Q_k\to Q\) et \( S_k\to S\). En effet dans ce cas nous aurions
            \begin{equation}    \label{EqJIkoaJv}
                \lim_{k\to \infty} f^{-1}(M_k)=\lim_{k\to \infty} (Q_k,S_k)=(Q,S)=f^{-1}(M).
            \end{equation}
            
            Étant donné que \( O(n)\) est compact (lemme \ref{LemTLlTAAf}), la suite \( (Q_k)\) admet une sous-suite convergente (Bolzano-Weierstrass, théorème \ref{ThoBWFTXAZNH}) que nous nommons
            \begin{equation}
                Q_{\varphi(k)}\to F\in O(n).
            \end{equation}
            Vu que la suite \( (M_k)\) converge, sa sous-suite converge vers la même limite : \( M_{\varphi(k)}\to M\) et vu que pour tout \( k\) nous avons \( S_k=M_kQ_k^{-1}\),
            \begin{equation}
                S_{\varphi(k)}\to G=MF^{-1}.
            \end{equation}
            Vu que chacune des matrices \( S_{\varphi(k)}\) est symétrique définie positive, la limite est symétrique et semi-définie positive\footnote{Lemme \ref{LemZKJWqIP}}. Donc \( G\in S^+(n,\eR)\cap \GL(n,\eR)\) parce que de plus \( M\) et \( F\) étant inversibles, \( G\) est inversible. En ce qui concerne la sous-suite nous avons
            \begin{equation}
                M_{\varphi(k)}=S_{\varphi(k)}Q_{\varphi(k)}\to GF=M
            \end{equation}
            où \( F\in O(n)\) et \( G\in S^+(n,\eR)\). Par unicité de la décomposition polaire de \( M\) (partie déjà démontrée), nous avons \( G=S\) et \( F=Q\).

            Nous avons prouvé que toute sous-suite convergente de \( Q_k\) a \( Q\) pour limite. Donc la suite elle-même converge\footnote{Proposition \ref{PropHNylIAW}, pas difficile.} vers \( Q\). Donc \( Q_k\to Q\). Du coup vu que \( S_k=M_kQ_k^{-1}\) est un produit de suites convergentes, \( S_k\) converge également, vers \( S\) :  \( S_k\to S\).

            Au final l'application \( f^{-1}\) est bien continue parce que les égalités \eqref{EqJIkoaJv} ont bien lieu.
    \end{subproof}

    Nous passons maintenant à la preuve dans le cas des matrices en général.

    Soit \( A\in \eM(n,\eR)\); par densité (lemme \ref{PropQGUPooVudelJ}), il existe une suite \( (A_k)\) dans \( \GL(n,\eR)\) telle que \( A_k\to A\). Pour chacun des \( k\) nous appliquons la décomposition polaire déjà prouvée : \( A_k=Q_kS_k\). D'abord \( (Q_k)\) est une suite dans le compact\footnote{Lemme \ref{LemTLlTAAf}.} \( \gO(n,\eR)\) et accepte donc une sous-suite convergente. Quitte à redéfinir la suite de départ, nous supposons pour alléger les notations que \( Q_k\to Q\in \gO(n,\eR)\). Vu que \( Q_k\) est inversible, 
    \begin{equation}
        S_k=Q^{-1}_kA_k
    \end{equation}
    Le produit matriciel étant continu nous avons \( S_k\to S\) dans \( \eM(n,\eR)\). Mais \( S^+(n,\eR)\) étant fermé (lemme \ref{LemMGUSooPqjguE}) nous avons aussi \( S\in S^+(n,\eR)\).
\end{proof}

\begin{remark}  \label{RemBJCBooGLiRmG}
    Pour démontrer que \( f\) est différentiable, nous devons utiliser le théorème d'inversion locale \ref{ThoXWpzqCn}; cela est fait dans la proposition \ref{PropWCXAooDuFMjn}.
\end{remark}

\begin{corollary}       \label{CorAWYBooNCCQSf}
    Toute matrice peut être écrite sous la forme \( Q_1DQ_2\) où \( Q_1\) et \( Q_2\) sont orthogonale et \( D\) est diagonale.
\end{corollary}

\begin{proof}
    Si \( A\in\eM(n,\eR)\) alors la décomposition polaire \ref{ThoLHebUAU} nous donne \( A=SQ\) où \( S\) est symétrique définie positive et \( Q\) est orthogonale. La matrice \( S\) peut ensuite être diagonalisée par le théorème \ref{ThoeTMXla} : \( S=RDR^{-1}\) où \( D\) est diagonale et \( R\) est orthogonale. Avec ces deux décompositions en main, \( A=SQ=RDR^{-1}Q\). La matrice \( R^{-1}Q\) est orthogonale.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Enveloppe convexe}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Sur \( C\) est un ensemble convexe, un point \( x\in C\) est un \defe{point extrémal}{extrémal!point dans un convexe} si \( C\setminus\{ x \}\) est encore convexe.
\end{definition}

\begin{theorem}[\cite{KXjFWKA}] \label{ThoBALmoQw}
    Soit \( E\) un espace euclidien de dimension \( n\geq 1\) et \( \aL(E)\) l'espace des opérateurs linéaires sur \( E\) sur lequel nous considérons la norme subordonnée\footnote{Voir la définition donnée dans l'exemple \ref{ExemdefnormpMrt}.} à celle sur \( E\). L'ensemble des points extrémaux de la boule unité fermée de \( \aL(E)\) est le groupe orthogonal \( O(n,\eR)\).
\end{theorem}
\index{densité!points extrémaux dans \( \aL\)}

\begin{proof}
    Nous notons \( \mB\) la boule unité fermée de \( \aL(E)\). Montrons pour commencer que les éléments de \( O(n)\) sont extrémaux dans \( \mB\). D'abord si \( A\in O(E)\) alors \( \| A \|=1\) parce que \( \| Ax \|=\| x \|\). Supposons maintenant que \( A\) n'est pas extrémal, c'est à dire qu'il est le milieu d'un segment joignant deux points (distincts) de la boule unité de \( \aL(E)\). Soient donc \( T,U\in\mB\) tels que \( A=\frac{ 1 }{2}(T+U)\). Pour tout \( x\in E\) tel que \( \| x \|=1\) nous avons 
    \begin{equation}    \label{EqKTuAIIE}
        1=\| x \|=\| Ax \|=\frac{ 1 }{2}\| Tx+Ux \|\leq \frac{ 1 }{2}\big( \| Tx \|+\| Ux \| \big)\leq\frac{ 1 }{2}\big( \| T \|+| U | \big)\leq 1
    \end{equation}
    Toutes les inégalités sont en réalité des égalités. En particulier nous avons
    \begin{equation}
        \| Tx+Ux \|=\| Tx \|+\| Ux \|,
    \end{equation}
    mais alors nous sommes dans un cas d'égalité dans l'inégalité de Cauchy-Schwartz (théorème \ref{ThoAYfEHG}) et donc il existe \( \lambda\geq 0\) tel que \( Tx=\lambda Ux\). Mais de plus les \sout{inégalité} égalités \eqref{EqKTuAIIE} nous donnent
    \begin{equation}
        \frac{ 1 }{2}\big( \| Tx \|+\| Ux \| \big)=1
    \end{equation}
    alors que nous savons que \( \| Tx \|,\| Ux \|\leq 1\), donc \( \| Tx \|=\| Ux \|=1\). La seule possibilité est d'avoir \( \lambda=1\) et donc que \( U=T\) parce que nous avons \( Tx=Ux\) pour tout \( x\) de norme \( 1\). Au final \( A\) n'est pas le milieu d'un segment dans \( \mB\).

    Nous passons donc à l'inclusion inverse : nous prouvons que les points extrémaux de \( \mB\) sont dans \( O(E)\). Pour cela nous prenons \( U\in\mB\setminus O(E)\) et nous allons montrer que \( U\) n'est pas un point extrémal : nous allons l'écrire comme milieu d'un segment dans \( \mB\).

    Par la seconde partie du théorème de décomposition polaire \ref{ThoLHebUAU}, il existe \( Q\in O(n,\eR)\) et \( S\in S^+(n,\eR)\) tels que \( U=QS\). Nous diagonalisons \( S\) à l'aide de la matrice orthogonale \( P\) :
    \begin{equation}
        S=PDP^{-1}
    \end{equation}
    avec \( D=\diag(\lambda_i)\). En termes de normes, nous avons
    \begin{equation}
        \| U \|=\| S \|=\| S \|.
    \end{equation}
    En effet vu que \( Q\) est orthogonale, \( \| Ux \|=\| QSx \|=\| Sx \|\) pour tout \( x\), donc \( \| U \|=\| S \|\). De plus pour tout \( x\) nous avons
    \begin{equation}
        \| Sx \|=\| PDP^{-1} x \|=\| DP^{-1}x \|.
    \end{equation}
    Étant donné que \( P^{-1}\) est une bijection, le supremum des \( \| Sx \|\) sera le même que celui des \( \| Dx \|\) et donc \( \| S \|=\| D \|\). Étant donné que par définition \( \| U \|\leq 1\), nous avons aussi \( \| D \|\leq 1\) et donc \( 0\leq\lambda_i\leq 1\) (pour rappel, les valeurs propres de \( D\) sont positives ou nulles parce que \( S\) est ainsi). 

    Comme \( U\notin O(E)\), au moins une des valeurs propres n'est pas \( 1\), supposons que ce soit \( \lambda_1\). Alors nous avons \( \alpha,\beta\in\mathopen[ -1 , 1 \mathclose]\) avec \( -1\leq \alpha<\beta\leq 1\) et \( \lambda_1=\frac{ 1 }{2}(\alpha+\beta)\). Nous posons alors
    \begin{subequations}
        \begin{align}
            D_1=\diag(\alpha,\lambda_2,\ldots, \lambda_n)\\
            D_2=\diag(\beta,\lambda_2,\ldots, \lambda_n).
        \end{align}
    \end{subequations}
    Nous avons bien \( D_1\neq D2\) et \( D_1+ D_2=D\). Par conséquent
    \begin{equation}
        U=\frac{ 1 }{2}\big( QPD_1P^{-1}+QPD_2P^{-1} \big)
    \end{equation}
    avec \( QPD_1P^{-1}\neq QPD_2^{-1}\). La matrice \( U \) est donc le milieu d'un segment. Reste à montrer que ce segment est dans \( \mB\). Pour ce faire, prenons \( x\in E\) et calculons :
    \begin{equation}
        \| QPD_iP^{-1}x \|=\| D_iP^{-1}x \|\leq\| P^{-1}x \|=\| x \|
    \end{equation}
    parce que \( \| D_i \|\leq 1\) et \( P^{-1}\) est orthogonale. Au final la norme de \( QPD_iP\) est plus petite que \( 1\) et donc \( U\) est bien le milieu d'un segment dans \( \mB\), et donc non extrémal.
\end{proof}

\begin{theorem}[\cite{NHXUsTa}] \label{ThoVBzqUpy}
    L'enveloppe convexe de \( O(n)\) dans \( \eM_n(\eR)\) est la boule unité pour la norme induite de \( \| . \|_2\) sur \( \eR^n\).
\end{theorem}
\index{convexité!enveloppe de $O(n)$}
\index{groupe!linéaire!enveloppe convexe de $\Omega(n)$}

\begin{proof}
    Nous notons \( \mB\) la boule unité fermée de \( \eM(n,\eR)\) et \( \Conv\big( O(n,\eR) \big)\) l'enveloppe convexe de \( O(n,\eR)\). Vu que \( \mB\) est convexe nous avons \( \Conv\big( O(n) \big)\subset\mB\).


    Maintenant nous devons prouver l'inclusion inverse. Pour ce faire nous supposons avoir un élément \( A\in \mB\setminus\Conv\big( O(n) \big)\) et nous allons dériver une contradiction.
    
    Remarquons que \( O(n)\) est compact par le lemme \ref{LemTLlTAAf} et que par conséquent \( \Conv(O(n))\) est compacte par le corollaire \ref{CorOFrXzIf} et donc fermée. Nous considérons un produit scalaire \( (X,Y)\mapsto X\cdot Y\) sur \( \eM\). Vu que \( \Conv\big( O(n) \big)\) est un fermé convexe nous pouvons considérer la projection\footnote{Le théorème de projection : théorème \ref{ThoWKwosrH}.} sur \( \Conv(A)\) relativement au produit scalaire choisis.

    Nous notons \( P=\pr_{\Conv\big( O(n) \big)}(A)\). En vertu du théorème de projection, nous avons
    \begin{equation}    \label{EqYSisLTL}
        (A-P)\cdot (M-P)\leq 0
    \end{equation}
    pour tout \( M\in\Conv O(n)\). Notons \( B=A-P\) pour alléger les notations. L'équation \eqref{EqYSisLTL} s'écrit
    \begin{equation}    \label{EqQDLZqXQ}
        B\cdot M\leq B\cdot P.
    \end{equation}
    D'autre par vu que \( B \neq 0\) nous avons \( B\cdot B> 0\), c'est à dire \( B\cdot (A-P)>0\) et donc
    \begin{equation}
        B\cdot A>B\cdot P.
    \end{equation}
    En combinant avec \eqref{EqQDLZqXQ},
    \begin{equation}        \label{EqIQNlwql}
        B\cdot M\leq B\cdot P<B\cdot A.
    \end{equation}
    Nous utilisons maintenant la décomposition polaire, théorème \ref{ThoLHebUAU}, pour écrire \( B=QS\) avec \( Q\in O(n)\) et \( S\in S^+(n,\eR)\). Vu que l'inégalité \eqref{EqIQNlwql} tient pour tout \( M\in\Conv(O(n))\), elle tient en particulier pour \( Q\in O(n)\). Donc
    \begin{equation}
        B\cdot Q=B\cdot A.
    \end{equation}
    Nous nous particularisons à présent au produit scalaire \( (X,Y)\mapsto\tr(X^tY)\) de la proposition \ref{PropMAQoKAg}. D'abord
    \begin{equation}    \label{EaHVxWdau}
        B\cdot Q=\tr(B^tQ)=\tr(S^tQ^tQ)=\tr(S^t)=\tr(S),
    \end{equation}
    et ensuite l'inégalité \eqref{EaHVxWdau} devient
    \begin{equation}
        \tr(S)<B\cdot A=\tr(S^tQ^tA).
    \end{equation}
    Nous choisissons une basse \( \{ e_i \}\) diagonalisant \( S\) : \( Se_i=\lambda_ie_i\) vérifiant automatiquement \( \lambda_i\geq 0\) parce que \( S\) est semi-définie positive\footnote{Définition \ref{DefAWAooCMPuVM}.}. Alors
    \begin{subequations}
        \begin{align}
            \tr(S)&<\tr(S^tQ^tA)\\
            &=\sum_i\langle S^tQ^tAe_i, e_i\rangle \\
            &=\sum_i\langle Ae_i, QSe_i\rangle \\
            &\leq \sum_i \| Ae_i \| | \lambda_i | \underbrace{\| Qe_i \|}_{=1} \\
            &\leq \sum_i\lambda_i   & A\in\mB\Rightarrow\| Ae_i \|\leq 1\\
            &=\tr(S).
        \end{align}
    \end{subequations}
    Il faut noter que la première inégalité est stricte, et donc nous avons une contradiction.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Décomposition de Bruhat}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Décomposition de Bruhat]\index{Bruhat (décomposition)}\index{décomposition!Bruhat}    \label{ThoizlYJO}
    Soit \( \eK\) un corps; un élément \( M\in\GL(n,\eR)\) s'écrit sous la forme
    \begin{equation}
        M=T_1P_{\sigma}T_2
    \end{equation}
    où \( T_1\) et \( T_2\) sont des matrices triangulaires supérieures inversibles et où \( P_{\sigma}\) est une matrice de permutation \( \sigma\in S_n\). De plus il y a unicité de \( \sigma\).
\end{theorem}
\index{groupe!permutation}
\index{groupe!linéaire}
\index{matrice}

\begin{proof}
    Afin de rendre les choses plus visuelles, nous nous permettons de donner des exemples au fur et à mesure de la preuve. Nous prenons l'exemple de la matrice
    \begin{equation}
        \begin{pmatrix}
            1    &   3    &   4    \\
            2    &   5    &   6    \\
            0    &   7    &   8
        \end{pmatrix}.
    \end{equation}
    \begin{subproof}
    \item[Existence]
        Soit \( M\in \GL(n,\eR)\); vu qu'elle est inversible, on a un indice \( i_1\) maximum tel que \( M_{i_1,1}\neq 0\). Nous changeons toutes les lignes jusque là, c'est à dire que nous faisons, pour \( 1\leq i< i_1\),
        \begin{equation}        \label{EqGHUbwR}
            L_i\to L_i-\frac{ M_{i1} }{ M_{i_11} }L_{i_1}.
        \end{equation}
        Voir le lemme \ref{LemooTQJXooGoIxsI}\ref{ITEMooXUGFooKcbrxs}.

        Nous avons donc obtenu une matrice dont la première colonne est nulle sauf la case numéro \( i_1\). L'opération \eqref{EqGHUbwR} revient à considérer la multiplication par la matrice de transvection
        \begin{equation}
            T_1^{(i)}=T_{ii_1}\left( -\frac{ M_{i1} }{ M_{i_11} } \right)
        \end{equation}
        pour tout \( i<i_1\). Pour rappel nous ne changeons que les lignes \emph{au-dessus} de la \( i_1\). Du coup les matrices \( T^{(i)}_1\) sont triangulaires supérieures. Nous avons donc la nouvelle matrice \( M_1=\left( \prod_{i<i_1}T_1^{(i)} \right)M\) pour laquelle toute la première colonne est nulle sauf un élément.

        Dans le cas de l'exemple, le «pivot» sera la ligne \( (2,5,6)\) et la matrice se transforme à l'aide de la matrice \( T_1=T_{12}(-1/2)\) :
        \begin{equation}    \label{EqyjXIYf}
            \begin{pmatrix}
                1    &   -1/2    &   0    \\
                0    &   1    &   0    \\
                0    &   0    &   1
            \end{pmatrix}
            \begin{pmatrix}
                1    &   3    &   4    \\
                2    &   5    &   6    \\
                0    &   7    &   8
            \end{pmatrix}=
            \begin{pmatrix}
                0    &   1/2    &   1    \\
                2    &   5    &   6    \\
                0    &   7    &   8
            \end{pmatrix}.
        \end{equation}

    
    Maintenant nous faisons de même avec les colonnes (en renommant \( M\) la matrice obtenue à l'étape précédente) :
    \begin{equation}
        C_j\to C_j-\frac{ M_{i_1j} }{ M_{i_11} }C_1,
    \end{equation}
    qui revient à multiplier à droite par les matrices \( T_{1j}(\frac{ M_{i_1i} }{ M_{i_11} })\) avec \( j>1\). Encore une fois ce sont des matrices triangulaires supérieures.

    Dans l'exemple, pour traiter la seconde colonne, nous multiplions \eqref{EqyjXIYf} à droite par la matrice \( T_{12}(-5/2)\) :
    \begin{equation}
            \begin{pmatrix}
                0    &   1/2    &   1    \\
                2    &   5    &   6    \\
                0    &   7    &   8
            \end{pmatrix}
            \begin{pmatrix}
                1    &   -5/2    &   0    \\
                0    &   1    &   0    \\
                0    &   0    &   1
            \end{pmatrix}=
            \begin{pmatrix}
                0    &   1/2    &   1    \\
                2    &   0    &   6    \\
                0    &   7    &   8
            \end{pmatrix}.
    \end{equation}
    Appliquer encore la matrice \( T_{13}(-6/2)\) apporte la matrice
    \begin{equation}
        \begin{pmatrix}
            0    &   1/2    &   1    \\
            2    &   0    &   0    \\
            0    &   7    &   8
        \end{pmatrix}.
    \end{equation}
    Enfin nous multiplions la matrice obtenue par \( \frac{1}{ M_{i_11} }\mtu\) pour normaliser à \( 1\) l'élément «pivot» que nous avions choisit. Dans notre exemple nous multiplions par \( 1/2\) pour trouver
    \begin{equation}        \label{Eqduglwu}
        \begin{pmatrix}
            0    &   1/4    &   1/2    \\
            1    &   0    &   0    \\
            0    &   7/2    &   4
        \end{pmatrix}.
    \end{equation}

    La matrice obtenue jusqu'ici possède une ligne et une colonne de zéros avec un \( 1\) à leur intersection, et elle est de la forme
    \begin{equation}
        M'=T_1MT_2
    \end{equation}
    où \( T_1\) et \( T_2\) sont triangulaires supérieures et inversibles, produits de matrices de transvection (et d'une matrice scalaire pour la normalisation).

    Il reste à recommencer l'opération avec la seconde colonne (qui n'est pas toute nulle parce que le déterminant est encore non nul) puis la suivante etc. Dans notre exemple de l'équation \eqref{Eqduglwu}, nous éliminerions le \( 1/4\) et le \( 4\) en utilisant le \( 7/2\).

    Encore une fois tout cela se fait à l'aide de matrice supérieures parce qu'à chaque étape, les colonnes précédent le pivot sont déjà nulles (saut un \( 1\)) et ne doivent donc pas être touchées.

    À la fin de ce processus, ce qui reste est une matrice \( TMT'\) qui ne contient plus que un seul \( 1\) sur chaque ligne et chaque colonne, c'est à dire une matrice de permutation : \( P_{\sigma}=TMT'\) et donc
    \begin{equation}
        M=T^{-1}_{\sigma}(T')^{-1}.
    \end{equation}

        \item[Unicité]

            Soient \( \sigma,\sigma\in S_n'\) tels que \( T_1P_{\sigma}T_2=S_1P_{\tau}S_2\) avec \( T_i\) et \( S_i\) triangulaires supérieures et inversibles. En posant \( T=T_2S_2^{-1}\) et \( S=T_1^{-1}S_1\), nous avons
            \begin{equation}
                P_{\sigma}T=SP_{\tau}
            \end{equation}
            où \( S\) et \( T\) sont des matrices triangulaires supérieures et inversibles. Par les calculs de la preuve du lemme \ref{LemyrAXQs},
            \begin{subequations}
                \begin{numcases}{}
                    (P_{\sigma}T)_{kl}=T_{\sigma^{-1}(k)l}\\
                    (SP_{\tau})_{kl}=S_{k\tau(l)},
                \end{numcases}
            \end{subequations}
            et donc
            \begin{equation}    \label{EqKlmgOT}
                T_{\sigma^{-1}(k)l}=S_{k\tau(l)}.
            \end{equation}
            En écrivant cette équation avec \( k=\sigma(i)\) (nous rappelons que \( \sigma\) est bijective),
            \begin{equation}
                T_{il}=S_{\sigma(i)\tau(l)}.
            \end{equation}
            Nous savons que les termes diagonaux de \( T\) sont non nuls parce que \( T\) est triangulaire supérieure et inversible (donc pas de colonnes entières nulles). Nous avons donc, en prenant \( i=l=k\),
            \begin{equation}
                0\neq T_{kk}=S_{\sigma(k)\tau(k)}.
            \end{equation}
            La matrice étant triangulaire supérieure, cela implique 
            \begin{equation}    \label{EqEmiBTX}
                \sigma(k)\leq\tau(k).
            \end{equation}
            De la même manière en écrivant \eqref{EqKlmgOT} avec \( l=\tau^{-1}(i)\),
            \begin{equation}
                S_{ki}=T_{\sigma^{-1}(k)\tau^{-1}(i)}
            \end{equation}
            et donc
            \begin{equation}
                \sigma^{-1}(k)\leq \tau^{-1}(k).
            \end{equation}
            En écrivant cela avec \( k=\sigma(j)\), nous avons \( j\leq \tau^{-1}\sigma(j)\) et en appliquant enfin \( \tau\),
            \begin{equation}
                \tau(j)\leq \sigma(j).
            \end{equation}
            En comparant avec \eqref{EqEmiBTX}, nous avons \( \sigma=\tau\).
    \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Sous-groupes du groupe linéaire}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{lemma}[\cite{KXjFWKA}]       \label{LemOCtdiaE}
    Soit \( V\) un espace vectoriel de dimension finie muni d'une norme euclidienne \( \| . \|\). Soit \( K\) un compact convexe de \( V\) et \( G\), un sous groupe compact de \( \GL(V)\) tel que
    \begin{equation}
        u(K)\subset K
    \end{equation}
    pour tout \( u\in G\). Alors il existe \( a\in K\) tel que \( u(a)=a\) pour tout \( u\in G\).
\end{lemma}
\index{groupe!linéaire!sous-groupes compacts}
\index{compacité!sous-groupes du groupe linéaire}

\begin{proof}
    Avant de nous lancer dans la preuve, nous avons besoin d'un petit résultat.
    \begin{subproof}
        \item[Un pré-résultat]

        Nous commençons par prouver que si \( v\in \aL(V)\) vérifie \( v(K)\subset K\), alors \( v\) a un point fixe dans \( K\). Pour cela nous considérons \( x_0\in K\) et la suite
        \begin{equation}
            x_k=\frac{1}{ k+1 }\sum_{i=0}^kv^i(x_0).
        \end{equation}
        Étant donné que \( K\) est convexe et stable par \( v\), la suite \( (x_k)\) est contenue dans \( K\) et accepte une sous-suite convergente\footnote{C'est Bolzano-Weierstrass, théorème \ref{ThoBWFTXAZNH}.} que nous allons noter \( x_{\varphi(n)}\) avec \( \varphi\colon \eN\to \eN\) strictement croissante. Soit \( a\in K\) la limite :
        \begin{equation}
            \lim_{n\to \infty} x_{\varphi(n)}=a.
        \end{equation}
        Tant que nous y sommes nous pouvons aussi calculer \( v(x_k)\) :
        \begin{subequations}
            \begin{align}
                v(x_k)&=v\left( \frac{1}{ k+1 }\sum_{i=1}^kv^i(x_0) \right)\\
                &=\frac{1}{ k+1 }\sum_{i=0}^kv^{i+1}(x_0)\\
                &=x_k+\frac{1}{ k+1 }\Big( v^{k+1}(x_0)-x_0 \Big).      \label{EqUAfcaKG}
            \end{align}
        \end{subequations}
        La norme \( \| v^{k+1}(x_0)-x_0 \|\) est bornée par le diamètre de \( K\), donc en prenant la limite \( k\to \infty\) le second terme de \eqref{EqUAfcaKG} tend vers zéro. En prenant ces égalités en \( k=\varphi(n)\) et en prenant \( n\to\infty\), nous trouvons
        \begin{equation}
            v(a)=a,
        \end{equation}
        c'est à dire le résultat que nous voulions dans un premier temps.

    \item[Une norme sur \( V\)]

        Nous passons maintenant à la preuve du lemme. D'abord nous remarquons que le groupe \( G\) agit sur \( V\) par \( u\cdot x=u(x)\) et de plus, considérant la fonction continue
        \begin{equation}
            \begin{aligned}
                \alpha\colon G&\to V \\
                u&\mapsto u(x), 
            \end{aligned}
        \end{equation}
        nous voyons que les orbites de cette action sont compactes en tant qu'image par \( \alpha\) du compact \( G\) (théorème \ref{ThoImCompCotComp}). Nous posons
        \begin{equation}
            \begin{aligned}
                \nu\colon V&\to \eR^+ \\
                x&\mapsto \max_{u\in G}\| u(x) \|. 
            \end{aligned}
        \end{equation}
        Cette définition a un sens parce que l'orbite \( \{ u(x)\tq u\in G \}\) est compacte dans \( V\) et donc l'ensemble des normes est compact dans \( \eR\) et admet un maximum. De plus cela donne une norme sur \( V\) parce que nous vérifions les conditions de la définition \ref{DefNorme} :
        \begin{enumerate}
            \item
                Pour tout \( x,y\in V\) nous avons :
                \begin{equation}
                    \nu(x+y)=\max_{u\in G}\| u(x)+u(y) \|\leq \max_{u\in G}\left( \| u(x) \|+\| u(y) \| \right)\leq \nu(x)+\nu(y).
                \end{equation}
            \item
                Si \( \nu(x)=0\), alors l'égalité \( \max_{u\in G}\| u(x) \|=0\) nous enseigne que \( \| u(x) \|=0\) pour tout \( u\in G\) et donc en particulier avec \( u=\id\) nous trouvons \( x=0\).
            \item
                Pour tout \( \lambda\in \eR\) et \( x\in V\),
                \begin{equation}
                    \nu(\lambda x)=\max_{u\in G}\| u(\lambda x) \|=\max\| \lambda u(x) \|=\max| \lambda |\| u(x) \|=| \lambda |\nu(x).
                \end{equation}
        \end{enumerate}
        De plus la fonction \( \nu\) est constante sur les orbites de \( G\).

    \item[Un point fixe]

        Pour tout \( u\in G\) nous posons
        \begin{equation}
            F_u=\{ x\in K\tq u(x)=x \};
        \end{equation}
        par le pré-résultat, aucun de ces ensembles n'est vide. Ils sont de plus tous fermés par continuité de \( u\) (le complémentaire est ouvert). Nous devons prouver que \( \bigcap_{u\in G}F_u\neq \emptyset\) parce qu'une intersection serait un point fixe de tous les éléments de \( G\). Supposons donc que \( \bigcap_{u\in G}F_u=\emptyset\). Alors les complémentaires des \( F_u\) forment un recouvrement ouvert de \( K\) et nous pouvons en extraire un sous-recouvrement fini par compacité. Soient \( \{ u_i \}_{i=1,\ldots, p}\) les éléments qui réalisent ce recouvrement. Alors
        \begin{equation}
            \bigcap_{i=1}^pF_{u_i}=\emptyset.
        \end{equation}
        Nous considérons l'opérateur
        \begin{equation}
            v=\frac{1}{ p }\sum_{i=1}^pu_i\in\aL(V).
        \end{equation}
        Vu que \( K\) est convexe et stable sous chacun des \( u_i\), nous avons aussi \( v(K)\subset K\) et donc il existe \( a\in K\) tel que \( v(a)=a\). Pour ce \( a\), nous avons
        \begin{subequations}
            \begin{align}
                \nu\big( v(a) \big)&=\nu\left( \frac{1}{ p }\sum_{i=1}^pu_i(a) \right)      \label{EqDXSnwPb}\\
                &\leq \frac{1}{ p }\sum_{i=1}^p\nu\left( u_i(a) \right)\\
                &=\frac{1}{ p }\sum_{i=1}^p\nu(a)\\
                &=\nu(a)
            \end{align}
        \end{subequations}
        où nous avons utilisé la constance de \( \nu\) sur les orbites de \( G\). Par ailleurs nous savons que \( v(a)=a\), donc en réalité à gauche dans \eqref{EqDXSnwPb} nous avons \( \nu(a)\) et toutes les inégalités sont des égalités. Nous avons en particulier
        \begin{equation}        \label{EqBMjypoV}
                \nu\left( \sum_{i=1}^pu_i(a) \right) =\sum_{i=1}^p\nu\left( u_i(a) \right).
        \end{equation}
        Notons \( u_0\in G\) l'élément qui réalise le maximum de la définition de \( \nu\) pour le vecteur \( \sum_iu_i(a)\) :
        \begin{equation}
            \nu\left( \sum_i u_i(a) \right)=\| u_0\left( \sum_iu_i(a) \right) \|\leq\sum_i\| u_0u_i(a) \|\leq \sum_i\nu\big( u_i(a) \big).
        \end{equation}
        Mais nous venons de voir (équation \eqref{EqBMjypoV}) que l'expression de gauche est égale à celle de droite. Donc les inégalités sont des égalités et en particulier la première inégalité devient l'égalité
        \begin{equation}
            \| \sum_iu_0u_i(a)  \|=\sum_i\| u_0u_i(a) \|.
        \end{equation}
        En vertu du lemme \ref{LemLPOHUme}, il existe des nombres positifs \( \lambda_i\) tels que
        \begin{equation}
            u_0u_1(a)=\lambda_2u_0u_2(a)=\ldots =\lambda_pu_0u_p(a).
        \end{equation}
        Du fait que \( u_0\) est inversible nous avons aussi 
        \begin{equation}       \label{EqSTQwfIl}
            u_1(a)=\lambda_2u_2(a)=\ldots =\lambda_pu_p(a).
        \end{equation}
        Mais par constance de \( \nu\) sur les orbites nous avons \( \nu(u_i(a))=\nu(u_j(a))\) pour tout \( i\) et \( j\); en appliquant \( \nu\) à la série d'égalités \eqref{EqSTQwfIl}, nous trouvons que tous les \( \lambda_i\) doivent être égaux à \( 1\). En particulier
        \begin{equation}     
            u_1(a)=u_2(a)=\ldots =u_p(a).
        \end{equation}
        
        Nous récrivons maintenant l'équation \( v(a)=a\) avec la définition de \( v\) :
        \begin{equation}
            a=v(a)=\frac{1}{ p }\sum_{i=1}^pu_i(a)=u_j(a)
        \end{equation}
        pour n'importe quel \( j\). Donc
        \begin{equation}
            a\in\bigcap_{i=1}^pF_{u_i},
        \end{equation}
        ce qui contredit notre hypothèse de départ.
        \end{subproof}
\end{proof}

\begin{proposition}[\cite{NHXUsTa,KXjFWKA,RXvMqkd}]     \label{PropQZkeHeG}
    Soit \( G\) un sous-groupe compact de \( \GL(n,\eR)\). Alors 
    \begin{enumerate}
        \item
            Il existe une forme quadratique définie positive \( q\) sur \( \eR^n\) telle que \( G\subset \gO(q)\).
        \item
            Le groupe \( G\) est conjugué à un sous-groupe de \( \gO(n,\eR)\).
    \end{enumerate}
\end{proposition}
\index{groupe!action!utilisation}
\index{matrice!équivalence!dans le groupe linéaire}
\index{forme!quadratique!groupe orthogonal}
\index{groupe!orthogonal!d'une forme quadratique}
\index{endomorphisme!préservant une forme quadratique}

\begin{proof}
    Nous considérons le (pas tout à fait) morphisme de groupe
    \begin{equation}
        \begin{aligned}
            \rho\colon G&\to \GL\big( \gS(n,\eR) \big) \\
            u&\mapsto \rho_u\colon s\to  u^tsu,
        \end{aligned}
    \end{equation}
    et tant que nous y sommes à considérer, nous considérons l'ensemble
    \begin{equation}
        H=\{ M^tM\tq M\in G \}\subset \gS(n,\eR).
    \end{equation}
    Cet ensemble est constitué de matrices définies positives parce que si \( \langle M^tMx, x\rangle =0\), alors \(0= \langle Mx, Mx\rangle =\| Mx \|\), mais \( M\) étant inversible, cela implique que \( x=0\). Qui plus est cet ensemble est compact dans \( \GL(n,\eR)\) en tant qu'image du compact \( G\) par l'application continue \( M\mapsto M^tM\). L'enveloppe convexe \( K=\Conv(H)\) est alors également compacte par le théorème \ref{CorOFrXzIf}. Enfin nous considérons \( L=\rho(G)\), qui est un sous-groupe compact de \( \GL\big( \gS(n,\eR) \big)\) parce que \( \rho_u\rho_v=\rho_{vu}\in\rho(G)\). Nous remarquons que \( \rho_u\) étant linéaire, elle préserve les combinaisons convexes et donc pour tout \( u\in G\), \( \rho_u(K)\subset K\).

    Bref, \( L\) est un sous-groupe compact de \( \GL(n,\eR)\) préservant le compact \( K\) de \( \gS(n,\eR)\). Par le lemme \ref{LemOCtdiaE}, il existe \( s\in K\) tel que \( \rho_u(s)=s\) pour tout \( u\in G\). Ou encore :
    \begin{equation}
        u^tsu=s
    \end{equation}
    pour tout \( u\in G\). Fort de ce \( s\) bien particulier, nous considérons la forme quadratique associée : \( q(x)=x^tsx\). Cette forme est définie positive parce que \( s\) l'est. Nous avons \( G\subset \gO(q)\) parce que si \( u\in G\) alors
    \begin{equation}
        q\big( ux \big)=(ux)^tsux=x^t\underbrace{u^tsu}_{=s}x=q(x).
    \end{equation}
    Le premier point est prouvé.

    La matrice \( s\) est symétrique et définie positive. Elle peut donc être diagonalisée\footnote{Théorème \ref{ThoeTMXla}} en \( \diag(\lambda_1,\ldots, \lambda_n)\) avec \( \lambda_i>0\), et ensuite transformée en la matrice \( \mtu_n\) par la matrice \( \diag(1/\sqrt{\lambda_i})\). Nous avons donc une matrice \( a\in\GL(n,\eR)\) telle que \( a^tsa=\mtu_n\). Avec ça, si \( u\in G\), nous avons
    \begin{equation}
        (a^{-1}ua)^t(a^{-1} ua)=(a^{-1}ua)^t\mtu_n(a^{-1} ua)=a^tu^t(a^t)^{-1}a^tsaa^{-1}ua=a^tu^tsua=a^tsa=\mtu,
    \end{equation}
    ce qui prouve que \( a^{-1} ua\) est dans \( \gO(n,\eR)\), et donc que \( a^{-1} G a\subset \gO(n,\eR)\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Isométries de l'espace euclidien}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous considérons l'espace affine euclidien \( A=\affE_n(\eR)\) modelé sur \( \eR^n\) avec sa métrique usuelle. Un premier grand résultat sera le théorème \ref{ThoDsFErq} qui dira que les isométries de cet espace sont des applications linéaires\footnote{Regardez un coup dans le second tome du Landau et Lifchitz voir comment ils démontrent que les transformations de Lorentz doivent être linéaires. Ça vous donnera une idée à quel point notre théorème est cool.}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Structure du groupe  \texorpdfstring{\( \Isom(\eR^n)\)}{Isom(Rn)} }
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    La forme quadratique \( q(x)=x_1^2+x_2^2\) donne la norme euclidienne. La forme bilinéaire associée est \( b(x,y)=x_1y_1+x_2y_2\), qui est le produit scalaire usuel.
\end{example}

Il ne faudrait pas déduire trop vite que la formule \( \| x \|^2=q(x)\) donne une norme dès que \( q\) est non dégénérée. En effet \( q\) peut ne pas être définie positive. La forme \( q(x)=x_1^2-x_2^2\) prend des valeurs positives et négatives. A fortiori \( d(x,y)=q(x-y)\) ne donne pas toujours une distance.

\begin{definition}
    Une \defe{isométrie}{isométrie!de forme quadratique} pour la forme \( q\) est une application bijective \( f\colon V\to V\) telle que \( q(x-y)=q\big( f(x)-f(y) \big)\). Dans les cas où \( q\) donne une distance, alors c'est une isométrie au sens usuel.
\end{definition}

\begin{lemma}   \label{LemewGJmM}
    Soit \( q\) une forme quadratique et \( b\) la forme bilinéaire associée par le lemme \ref{LEMooLKNTooSfLSHt}.  Pour une application bijective \( f\colon E\to E\) telle que \( f(0)=0\), les conditions suivantes sont équivalentes: 
    \begin{enumerate}
        \item
            \( b\big( f(x),f(y) \big)=b(x,y)\) pour tout \( x,y\in E\);
        \item
            \( q\big( f(x)-f(y) \big)=q(x-y)\) pour tout \( x,y\in E\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Dans le sens direct, en posant \( x=y\) nous trouvons tout de suite \( q(f(x))=q(f)\); ensuite en utilisant la distributivité de \( b\),
    \begin{subequations}
        \begin{align}
            q\big( f(x)-f(y) \big)&=b\big( f(x)-f(y),f(x)-f(y) \big)\\
            &=q\big( f(x) \big)-2b\big( f(x),f(y) \big)+q\big( f(y) \big)\\
            &=q(x)+q(y)-2b(x,y)\\
            &=q(x-y).
        \end{align}
    \end{subequations}
    
    Dans l'autre sens, nous commençons par remarquer que l'hypothèse \( f(0)=0\) donne \( q(x)=q\big( f(x) \big)\). Ensuite nous utilisons l'identité de polarisation \eqref{EqMrbsop} :
    \begin{subequations}
        \begin{align}
            b\big( f(x),f(y) \big)&=\frac{ 1 }{2}\big[ q\big( f(x) \big)+q\big( f(y) \big)-q\big( f(x-y) \big) \big]\\
            &=\frac{ 1 }{2}\big[ q(x)+q(y)-q(x-y) \big]\\
            &=b(x,y).
        \end{align}
    \end{subequations}
\end{proof}

\begin{theorem}[\cite{ooQFKAooFnllQU}]     \label{ThoDsFErq}
    Soit \( f\colon E\to E\) une bijection telle que
    \begin{equation}
        q(x-y)=q\big( f(x)-f(y) \big)
    \end{equation}
    pour tout \( x,y\in E\). Alors
    \begin{enumerate}
        \item
            si \( f(0)=0\), alors \( f\) est linéaire;
        \item
            si \( f(0)\neq 0\) alors \( f\) est affine\footnote{Définition \ref{DEFooJWWYooNqfoPJ}.}.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Si \( f(0)=0\), nous savons par le lemme \ref{LemewGJmM} que \( b\big( f(x),f(y) \big)=b(x,y)\). Soit \( z\in E\); étant donné que \( f\) est bijective nous pouvons considérer l'élément \( f^{-1}(z)\in E\) et calculer
    \begin{subequations}
        \begin{align}
            b\big( f(x+y),z \big)&=b\big( f(x+y),f(f^{-1}(z)) \big)\\
            &=b(x+y,f^{-1}(z))\\
            &=b(x,f^{-1}(z))+b(y,f^{-1}(z))\\
            &=b(f(x),z)+b(f(y),z)\\
            &=b\big( f(x)+f(y),z \big),
        \end{align}
    \end{subequations}
    donc \( f(x+y)=f(x)+f(y)\) par le lemme \ref{LemyKJpVP}. 

    De la même façon on trouve \( b\big( f(\lambda x),z \big)=b\big( \lambda f(x),z \big)\) qui prouve que \( f(\lambda x)=\lambda f(x)\) et donc que \( f\) est linéaire.

    Si \( f(0)\neq 0\), alors nous posons \( g(x)=f(x)-f(0)\) qui vérifie \( g(0)=0\) et
    \begin{equation}
        q\big( g(x)-g(y) \big)=q\big( f(x)-f(0)-f(y)+f(0) \big)=q(x-y).
    \end{equation}
    Nous pouvons donc appliquer le premier point à \( g\), déduire que \( g\) est linéaire et donc que \( f\) est affine.
\end{proof}

\ifnumequal{\value{isAgreg}}{1}{}{
\begin{remark}
    Des preuves alternatives.
    \begin{enumerate}
        \item
            En utilisant un peut plus d'indices et un peu plus de mots comme «tenseurs», peut être trouvée  \href{http://physics.stackexchange.com/questions/12664/proving-that-interval-preserving-transformations-are-linear}{ici}. Le fait que la preuve donnée soit tensorielle me fait penser que le résultat peut encore être généralisé.
        \item
            Et encore une autre preuve, utilisant des techniques de groupes de Lie sera la proposition \ref{PROPooDVIWooAFDNPy}.
    \end{enumerate}
\end{remark}
}

Nous pouvons maintenant particulariser tout cela au cas de \( \eR^n\) pour voir quel résultat nous avons à peine prouvé. Nous notons ici \( T(n)\) le groupe des translations sur \( \eR^n\). Un élément de \( T(n)\) est une translation \( \tau_v\) donnée par un vecteur \( v\) et agissant sur \( \eR^n\) par
\begin{equation}
    \begin{aligned}
        \tau_v\colon \eR^n&\to \eR^{n} \\
        x&\mapsto x+v. 
    \end{aligned}
\end{equation}
Ce groupe est isomorphe au groupe abélien \( (\eR^n,+)\), et nous allons souvent identifier \( \tau_v\) à \( v\).

Si vous ne voulez pas savoir ce qu'est un produit semi-direct de groupes, vous pouvez lire seulement le point \ref{ITEMooLLUIooIGsknv} du théorème suivant, et passer directement à la remarque \ref{REMooLUEZooIwvTqu}.
\begin{theorem}     \label{THOooQJSRooMrqQct}
    Un peu de structure sur \( \Isom(\eR^n)\).
    \begin{enumerate}
        \item       \label{ITEMooLLUIooIGsknv}
            L'application
            \begin{equation}
                \begin{aligned}
                    \psi\colon T(n)\times \gO(n)&\to \Isom(\eR^n) \\
                    (v,\Lambda)&\mapsto \tau_v\circ\Lambda 
                \end{aligned}
            \end{equation}
            est une bijection. Ici,  \( T(n)\) est le groupe des translations de \( \eR^n\).
        \item
            Un couple \( (v,\Lambda)\in T(n)\times\SO(n)\) agit sur \( x\in \eR^n\) par
            \begin{equation}
                (v,\Lambda)x=\Lambda x+v
            \end{equation}
            au sens où \( \psi(v,\Lambda)x=\Lambda x+v\).
        \item
            En tant que groupes,
            \begin{equation}
                \Isom(\eR^n)\simeq T(n)\times_{\rho}\gO(n)
            \end{equation}
            où \( \rho\) représente l'action adjointe de \( \gO(n)\) sur \( T(n)\) et \( \times_{\rho}\) dénotes le produit semi-direct de la définition \ref{DEFooKWEHooISNQzi}.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Point par point.
    \begin{enumerate}
        \item
            Prouvons que l'application proposée est injective et surjective. Notons aussi que ce point ne parle pas de structure de groupe, mais seulement d'une bijection en tant qu'ensembles.    
            \begin{subproof}
                \item[Injection]
                    Si \( \psi(v,\Lambda)=\psi(w,\Lambda')\) alors en appliquant sur \( x=0\) nous avons tout de suite \( v=w\). Et ensuite \( \Lambda=\Lambda'\) est immédiat.
                \item[Surjection]
                    Une isométrie \( g\in\Isom(\eR^n)\) est une application \( g\colon \eR^n\to \eR^n\) telle que \( d(x,y)=d\big( g(x),g(y) \big)\). Dans le cas de \( \eR^n\) cela se traduit par
                    \begin{equation}
                        \| x-y \|=\big\| g(x)-g(y) \big\|,
                    \end{equation}
                    Vu que \( x\mapsto\| x \|\) est une forme quadratique, elle tombe sous le coup du théorème  \ref{ThoDsFErq}, ce qui nous permet de dire que \( g\) est affine. Or par définition une application est affine lorsqu'elle est la composée d'une translation et d'une application linéaire.
            \end{subproof}
        \item
            C'est seulement le fait que \( (\tau_v\circ\Lambda)x=\tau_v\big( \Lambda x \big)=\Lambda(x)+v\).
        \item
            Nous allons étudier l'application
            \begin{equation}
                \psi\colon T(n)\times_{\rho}O(n)\to \Isom(\eR^n).
            \end{equation}
            \begin{subproof}
            \item[Le produit semi-direct est bien définit]
                Il faut montrer que
                \begin{equation}
                    \begin{aligned}
                        \rho\colon O(n)&\to \Aut\big( T(n) \big) \\
                        \Lambda&\mapsto \AD(\Lambda) 
                    \end{aligned}
                \end{equation}
                est correcte.

                D'abord pour \( \Lambda\in O(n)\), nous avons bien \( \rho_{\Lambda}(\tau_v)\in T(n)\) parce qu'en appliquant à \( x\in \eR^n\),
                    \begin{equation}
                        (\Lambda\tau_v\Lambda^{-1})(x)=\Lambda\big( \tau_v(\Lambda^{-1} x) \big)=\Lambda\big( \Lambda^{-1}x+v \big)=x+\Lambda(v)=\tau_{\Lambda(v)}(x).
                    \end{equation}
                    Donc \( \rho_{\Lambda}(\tau_v)=\tau_{\Lambda(v)}\).

                    De plus, \( \rho_{\Lambda}\in\Aut\big( T(n) \big)\) parce que 
                    \begin{equation}
                        \rho_{\Lambda}\big( \tau_v\circ \tau_w \big)=\rho_{\Lambda}(\tau_v)\circ\rho_{\Lambda}(\tau_v),
                    \end{equation}
                    comme on peut aisément vérifier que les deux membres sont égaux à \( \tau_{\Lambda(v+w)}\).
                \item[\( \psi\) est une bijection]
                    Cela est déjà vérifié.
                \item[\( \psi\) est un homomorphisme]
                    Nous avons d'une part
                    \begin{equation}
                        \psi\big( (v,g)(w,h) \big)=\psi\big( v\rho_g(w),gh \big)=\tau_v\circ g\circ\tau_w\circ g^{-1}\circ g\circ h=\tau_v\circ g\circ\tau_w\circ h.
                    \end{equation}
                    Et d'autre part,
                    \begin{equation}
                        \psi(v,g)\circ\psi(w,h)=\tau_v\circ g\circ \tau_w\circ h,
                    \end{equation}
                    ce qui est la même chose.
            \end{subproof}
    \end{enumerate}
\end{proof}

\begin{remark}      \label{REMooLUEZooIwvTqu}
    Notons au passage la loi de groupe sur les couples qui est donnée, pour tout \( v,v'\in \eR^n\), \( \Lambda,\Lambda'\in\SO(n)\), par
    \begin{equation}    \label{EqDiHcut}
            (v,\Lambda)\cdot(v',\Lambda')=(\Lambda v'+v,\Lambda\Lambda')
    \end{equation}
    comme le montre le calcul suivant :
    \begin{subequations}
        \begin{align}
            (v,\Lambda)\cdot(v',\Lambda')x&=(v,\Lambda)(\Lambda'x+v')\\
            &=\Lambda\Lambda'x+\Lambda v'+v\\
            &=(\Lambda v'+v,\Lambda\Lambda')x.
        \end{align}
    \end{subequations}
\end{remark}

\begin{proposition}[\cite{ooZYLAooXwWjLa}]      \label{PROPooDHYWooXxEXvl}
    Soit \( n\geq 1\) et un élément \( R\) de \( \gO(n)\) de déterminant \( -1\) tel que \( R^2=\id\). En posant \( C_2=\{ \id,R \}\) nous avons
    \begin{equation}
        \gO(n)=\SO(n)\times_{\rho} C_2
    \end{equation}
\end{proposition}

\begin{proof}
    Notons que pour \( R\) nous pouvons prendre par exemple \( (x_1,\ldots, x_n)\mapsto (-x_1,x_2,\ldots, x_n)\). Ce que nous allons montrer être un isomorphisme est :
    \begin{equation}
        \begin{aligned}
            \psi\colon \SO(n)\times C_2&\to \gO(n) \\
            (A,h)&\mapsto Ah. 
        \end{aligned}
    \end{equation}
    \begin{subproof}
        \item[Injectif]
            Soient \( A,B\in \SO(n)\) et \( h,k\in C_2\) tels que \( \psi(A,h)=\psi(B,k)\), c'est à dire tels que \( Ah=Bk\). Vu que \( \det(A)=\det(B)=1\) nous avons \( \det(h)=\det(k)\). Mais comme \( C_2\) contient un élément de déterminant \( 1\) et un élément de déterminant \( -1\), nous avons \( h=k\). De là \( A=B\).
        \item[Surjectif]
            Soit \( X\in\gO(n)\). Si \( \det(X)=1\) alors \( X\in \SO(n)\) et \( X=\psi(X,\mtu)\). Si par contre \( \det(X)=-1\) alors \( XR\in\SO(n)\) parce que \( \det(XR)=1\) et nous avons
            \begin{equation}
                \psi(XR,R)=XR^2=X.
            \end{equation}
        \item[Homomorphisme]
            Nous avons
            \begin{equation}
                \psi\Big( (A,h)(B,k) \Big)=\psi\big( A\rho_h(B),hk \big)=A(hBh^{-1})hk=AhBk,
            \end{equation}
            tandis que
            \begin{equation}
                \psi(A,h)\psi(B,k)=AhBk,
            \end{equation}
            qui est la même chose.
    \end{subproof}
\end{proof}

\begin{lemma}[\cite{JGAdTA}]
    Si \( n\geq 3\), alors toute droite est intersection de deux plans non isotropes.
\end{lemma}

Pour en savoir plus sur le groupe des isométries, il faut lire le théorème de Cartan-Dieudonné dans \cite{JGAdTA}.

%--------------------------------------------------------------------------------------------------------------------------- 

\subsection{Théorème de Sylvester}
%---------------------------------------------------------------------------------------------------------------------------

% TODO : Il y a une démonstration sur wikipédia, à voir.

\begin{theorem}[de Sylvester]   \label{ThoQFVsBCk}
    Soit $Q$ une forme quadratique réelle de signature \( (p,q)\). Alors pour toute base orthonormée on a
    \begin{subequations}
        \begin{align}
            p&=\Card\{ i\tq Q(e_i)>0 \}\\
            q&=\Card\{ i\tq Q(e_i)<0 \}.
        \end{align}
    \end{subequations}
    Le rang de \( Q\) est \( p+q\).

    Si \( A\) est la matrice de \( Q\) dans une base, alors il existe une matrice inversible \( P\) telle que
    \begin{equation}
        P^tAP=\begin{pmatrix}
            -\mtu_q    &       &       \\
                &   \mtu_p    &       \\
                &       &   0
        \end{pmatrix}.
    \end{equation}
\end{theorem}
\index{théorème!Sylvester}
\index{rang}
\index{matrice!semblables}
\index{forme!quadratique}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Groupe diédral}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecHibJId}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Définition et générateurs : vue géométrique}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{definition}  \label{DEFooIWZGooAinSOh}
    Le \defe{groupe diédral}{groupe!diédral} \( D_n\)\nomenclature[R]{\( D_n\)}{groupe diédral} est le groupe des isométries de \( \eR^2\) laissant invariant un polygone régulier à \( n\) côtés. 
\end{definition}
Le groupe diédral peut être vu comme le stabilisateur de l'ensemble
\begin{equation}
    \{  e^{2ik\pi/n},k=0,\ldots, n-1 \}
\end{equation}
dans le groupe des isométries affines de \( \eC^*\).
\index{groupe!agissant sur un ensemble!diédral}
\index{groupe!en géométrie}
\index{groupe!fini!diédral}
\index{groupe!permutation!diédral}
% TODO : prouver que les racines de l'unité forment un polygone régulier.

Si \( f\in D_n\), alors \( f( e^{2ik\pi/n}) \) doit être l'un des \(  e^{2ik'\pi/n}\), et vu que \( f\) conserve les longueurs dans \( \eC\), nous devons avoir
\begin{equation}
    1=d(0, e^{2ik\pi/n})=d\big( f(0), e^{2ik'\pi/n} \big).
\end{equation}
Donc \( f(0)\) est à l'intersection de tous les cercles de rayon \( 1\) centrés en les \(  e^{2ik\pi/n}\), ce qui montre que \( f(0))0\) (dès que \( n\geq 3\)). Par conséquent notre étude du groupe diédral ne doit prendre en compte que les isométries vectorielles de \( \eR^2\). En d'autres termes
\begin{equation}
    D_n\subset O(2,\eR).
\end{equation}

\begin{proposition}[\cite{tzHydF}]
    Le groupe \( D_n\) contient un sous groupe cyclique d'ordre \( 2\) et un sous groupe cyclique d'ordre \( n\).
\end{proposition}

\begin{proof}
    Si \( s\) est la réflexion d'axe \( \eR\), alors \( s\) est d'ordre \( 2\). De plus \( s\) est bien dans \( D_n\) parce que
    \begin{equation}    \label{EqSUshknP}
        s\big(  e^{2ki\pi/n} \big)= e^{2(n-k)i\pi/n}.
    \end{equation}

    De la même façon, la rotations d'angle \(2\pi/n\), que l'on note \( r\), agit sur les racines de l'unité et engendre un le groupe d'ordre \( n\) des rotations d'angle \(2 k\pi/n\).
\end{proof}

Notons que la conjugaison complexe ne fait pas spécialement partie du groupe \( D_n\). En effet pour \( n=3\) par exemple les points fixes sont \( A_1=(1,0)\), \( A_2=(-\frac{ 1 }{2},\frac{ \sqrt{3} }{2})\) et \( A_3=(\frac{ 1 }{2},-\frac{ \sqrt{3} }{2})\). La conjugaison complexe envoie évidemment \( A_1\) sur \( A_1\), mais pas du tout \( A_2\) sur \( A_3\).
%TODO : un dessin du triangle équilatéral serait pas mal ici.

\begin{proposition}[\cite{tzHydF}]
    Nous avons \( (sr)^2=\id\).
\end{proposition}

\begin{proof}
    Si \( z^n=1\), alors
    \begin{equation}
        (srsr)z=srs e^{2 i\pi/n}z=sr\big( e^{-2\pi i/n\bar z}\big)=s\bar z=z.
    \end{equation}
\end{proof}

\begin{proposition}[\cite{tzHydF}] \label{PropLDIPoZ}
    Le groupe diédral \( D_n\) est engendré par \( s\) et \( r\). De plus tous les éléments de \( D_n\) s'écrivent sous la forme \( s\circ r^m\).
\end{proposition}
\index{groupe!diédral!générateurs (preuve)}
\index{racine!de l'unité}
\index{géométrie!avec nombres complexes}
\index{géométrie!avec des groupes}
\index{isométrie!de l'espace euclidien \( \eR^2\)}

\begin{proof}
    Nous considérons les points \( A_0=1\) et \( A_k= e^{2ki\pi/n}\) avec \( k\in\{ 1,\ldots, n-1 \}\). Par convention, \( A_n=A_0\). L'action des éléments \( s\) et \( r\) sur ces points est
    \begin{subequations}
        \begin{align}
            r(A_k)&=A_{k+1}\\
            s(A_k)&=A_{n-k}.
        \end{align}
    \end{subequations}
    Cette dernière est l'équation \eqref{EqSUshknP}.
    
    Soit \( f\in D_n\). Étant donné que c'est une isométrie de \( \eR^2\) avec un point fixe (le point \( 0\)), \( f\) est soit une rotation soit une réflexion.
    %TODO : il faut démontrer ce point et mettre un lien vers ici.

    Supposons pour commencer que un des \( A_k\) est fixé par \( f\). Dans ce cas \( f\) a deux points fixes : \( O\) et \( A_k\) et est donc la réflexion d'axe \( (OA_k)\). Dans ce cas, nous avons \( f=s\circ r^{n-2k}\). En effet
    \begin{equation}
        s\circ r^{n-2k}(A_k)=s(A_{k+n-2k})=s(A_{n-k})=A_k.
    \end{equation}
    Donc \( O\) et \( A_k\) sont deux points fixes de l'isométrie \( f\); donc \( f\) est bien la réflexion sur le bon axe.

    Nous passons à présent au cas où \( f\) ne fixe aucun des \( A_k\). 
    \begin{enumerate}
        \item
            Supposons que \( f\) soit une rotation. Si \( f(A_k)=A_m\), alors l'angle de la rotation est 
            \begin{equation}
                \frac{ 2(m-k)\pi }{ n },
            \end{equation}
            et donc \( f=r^{m-k}\), qui est de la forme demandée.
        \item
            Supposons à présent que \( f\) soit une réflexion d'axe \( \Delta\). Cette fois, \( \Delta\) ne passe par aucun des points \( A_k\), par contre \( \Delta\) passe par \( 0\). Nous commençons par montrer que \( \Delta\) doit être la médiatrice d'un des côtés \( [A_p,A_{p+1}]\) du polygone. Vu que \( \Delta\) passe par \( O\) et n'est aucune des droites \( (OA_k)\), cette droite passe par l'intérieur d'un des triangles \( OA_pA_{p+1}\) et intersecte donc le côté correspondant.

            Notre tâche est de montrer que \( \Delta\) coupe \( [A_p,A_{p+1}]\) en son milieu. Dans ce cas, \( \Delta\) sera automatiquement perpendiculaire parce que le triangle \( OA_pA_{p+1}\) est isocèle en \( O\). Nommons \( l\) la longueur des côtés du polygone, \( P=\Delta\cap[A_p,A_{p+1}]\), \( x=d(A_p,P)\) et \( \delta=d(A_p,\Delta)\). Vu que \( f\) est la symétrie d'axe \( \Delta\), nous avons aussi \( d\big( f(A_p),\Delta \big)=\delta\) et \( d\big( A_p,f(A_p) \big)=2\delta\). D'autre part, par la définition de la distance, \( \delta<x\). Si \( x<\frac{ l }{2}\), alors \( \delta<\frac{ \delta }{2}\) et donc \( d\big( A_p,f(A_p) \big)<l\). Or cela est impossible parce que le polygone ne possède aucun sommet à distance plus courte que \( l\) de \( A_p\).

            De la même manière si \( x>\frac{ l }{2}\), nous raisonnons avec \( A_{p+1}\) pour obtenir une contradiction. Nous en concluons que la seule possibilité est \( x=\frac{ l }{2}\), et donc \( f(A_p)=A_{p+1}\). Montrons alors que \( f=s\circ r^{n-2p-1}\). Il faut montrer que c'est une réflexion qui envoie \( A_p\) sur \( A_{p+1}\). D'abord c'est une réflexion parce que
            \begin{equation}
                \det(sr^{n-2p-1})=\det(s)\det(r^{n-2p-1})=-1
            \end{equation}
            parce que \( \det(s)=-1\) alors que \( \det(r^k)=1\) parce que \( r\) est une rotation dans \( \SO(2)\). Ensuite nous avons
            \begin{equation}
                s\circ r^{n-2p-1}(A_p)=s(A_{p+n-2p-1})=s(A_{n-p-1})=A_{n-(n-p-1)}=A_{p+1}.
            \end{equation}

            Donc \( s\circ r^{n-2p-1}\) est bien une réflexion qui envoie \( A_p\) sur \( A_{p+1}\).

    \end{enumerate}
\end{proof}

\begin{corollary}   \label{CorWYITsWW}
La liste des éléments de \( D_n\) est 
\begin{equation}
    D_n=\{ 1,r,\ldots, r^{n-1},s,sr,\ldots, sr^{n-1} \}
\end{equation}
et \( | D_n |=2n\).
\end{corollary}

\begin{proof}
    Nous savons par la proposition \ref{PropLDIPoZ} que tous les élément de \( D_n\) s'écrivent sous la forme \( r^k\) ou \( sr^k\). Vu que \( r\) est d'ordre \( n\), il ne faut considérer que \( k\in\{ 1,\ldots, n-1 \}\). Les éléments \( 1\), \( r\),\ldots, \( r^{n-1}\) sont tous différents, et sont (pour des raisons de déterminant) tous différents des \( sr^k\). Les isométries \( sr^k\) sont toutes différentes entre elles pour essentiellement la même raison :
    \begin{equation}
        sr^k(A_p)=s(A_{p+k})=A_{n-p+k}
    \end{equation}
    donc si \( k\neq k'\), \( sr^k(A_p)\neq sr^{k'}(A_p)\). La liste des éléments de \( D_n\) est donc
    \begin{equation}
        D_n=\{ 1,r,\ldots, r^{n-1},s,sr,\ldots, sr^{n-1} \}
    \end{equation}
    et donc \( | D_n |=2n\).
\end{proof}

\begin{example}     \label{EXooHNYYooUDsKnm}
    Nous considérons le carré \( ABCD\) dans \( \eR^2\) et nous cherchons les isométries de \( \eR^2\) qui laissent le carré invariant. Nous nommons les points comme sur la figure \ref{LabelFigIsomCarre}. La symétrie d'axe vertical est nommée \( s\) et la rotation de \( 90\) degrés est notée \( r\).
    \newcommand{\CaptionFigIsomCarre}{Le carré dont nous étudions le groupe diédral.}
    \input{Fig_IsomCarre.pstricks}

    Il est facile de vérifier que toutes les symétries axiales peuvent être écrites sous la forme \( r^is\). De plus le groupe engendré par \( s\) agit sur le groupe engendré par \( r\) parce que
    \begin{equation}
        (srs^{-1})(A,B,C,D)=sr(B,A,D,C)=s(A,D,C,B)=(B,C,D,A),
    \end{equation}
    c'est à dire \( srs^{-1}=r^{-1}\). Nous sommes alors dans le cadre du corollaire \ref{CoroGohOZ} et nous pouvons écrire que
    \begin{equation}
        D_4=\gr(r)\times_{\sigma}\gr(s).
    \end{equation}
\end{example}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Générateurs : vue abstraite}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Nous allons montrer que \( D_n\) peut être décrit de façon abstraite en ne parlant que de ses générateurs. Nous considérons un groupe \( G\) engendré par des éléments \( a\) et \( b\) tels que
\begin{enumerate}
    \item
        \( a\) est d'ordre \( 2\),
    \item
        \( b\) est d'ordre \( n\) avec \( n\geq 3\),
    \item
        \( abab=e\).
\end{enumerate}
Nous allons prouver que ce groupe doit avoir la même liste d'éléments que celle du corollaire \ref{CorWYITsWW}.

\begin{proposition}[\cite{tzHydF}]
    Le groupe \( G\) n'est pas abélien.
\end{proposition}

\begin{proof}
    Nous savons que \( abab=e\), donc \( abab^{-1}=b^{-2}\), mais \( b^{-2}\neq e\) parce que \( b\) est d'ordre \( n>2\). Donc \( abab^{-1}\neq e\). En manipulant un peu :
    \begin{equation}
        e\neq abab^{-1}=(ab)(ba^{-1})^{-1}=(ab)(ba)^{-1}
    \end{equation}
    parce que \( a^{-1}=a\). Donc \( ab\neq ba\).
\end{proof}

\begin{lemma}[\cite{tzHydF}]        \label{LemKKXdqdL}
    Pour tout \( k\) entre \( 1\) et \( n-1\) nous avons
    \begin{equation}
        \AD(a)b^k=ab^ka^{-1}=ab^ka=b^{-k}.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous faisons la démonstration par récurrence. D'abord pour \( k=1\), nous devons avoir \( aba=b^{-1}\), ce qui est correct parce que par construction de \( G\) nous avons \( abab=e\). Ensuite nous supposons que le lemme tient pour \( k\) et nous regardons ce qu'il se passe avec \( k+1\) :
    \begin{equation}
            ab^{k+1}ba=ab^kba=\underbrace{ab^ka}_{b^{-k}}\underbrace{aba}_{b^{-1}}=b^{-k}b^{-1}=b^{-(k+1)}.
    \end{equation}
\end{proof}

\begin{proposition}     \label{PROPooVQARooWuKHMZ}
    L'élément \( a\) n'est pas une puissance de \( b\).
\end{proposition}

\begin{proof}
    Supposons le contraire : \( a=b^k\). Dans ce cas nous aurions
    \begin{equation}
        e=(ab)(ab)=b^{k+1}b^{k+1}=b^{2k+2}=b^{2k}b^2=a^2b^2=b^2,
    \end{equation}
    ce qui signifierait que \( b\) est d'ordre \( 2\), ce qui est exclu par construction.
\end{proof}

\begin{proposition}[\cite{tzHydF}]      \label{PROPooEPVGooQjHRJp}
    La liste des éléments de \( G\) est donnée par
    \begin{equation}
        G=\{ 1,b,\cdots,b^{n-1},a,ab,\ldots, ab^{n-1} \}=\{ a^{\epsilon}b^k\}_{\substack{\epsilon=0,1\\k=0,\ldots, n-1}}
    \end{equation}
    Les éléments de ces listes sont distincts.
\end{proposition}

\begin{proof}
    Étant donné que \( a\) n'est pas une puissance de \( b\), les éléments \( 1\), \( a\), \( b\),\ldots, \( b^{n-1}\) sont distincts. De plus si \( k\) et \( m=k+p\) sont deux éléments distincts de \( \{ 1,\ldots, n-1 \}\), nous avons \( ab^k\neq ab^m\) parce que si \( ab^k=ab^{k+p}\), alors \( a=ab^p\) avec \( p<n\), ce qui est impossible. Pour la même raison, \( ab^k\neq e\), et \( ab^k\neq b^m\).

    Au final les éléments \( 1,a,b,\ldots, b^{n-1},ab,\ldots, ab^{n-1}\) sont tous différents. Nous devons encore voir qu'il n'y en a pas d'autres.

    Par définition le groupe \( G\) est engendré par \( a\) et \( b\), donc tout élément \( x\in G\) s'écrit $x=a^{m_1}b^{k_1}\ldots a^{m_r}b^{k_r}$ pour un certain \( r\) et avec pour tout \( i\), \( k_i\in\{ 1,\ldots, n-1 \}\) (sauf \( k_r\) qui peut être égal à zéro) et \( m_i=1\), sauf \( m_1\) qui peut être égal à zéro. Donc
    \begin{equation}
        x=a^mb^{k_1}ab^{k_2}a\ldots b^{k_{r-1}}ab^{k_r}
    \end{equation}
    où \( m\) et \( k_r\) peuvent éventuellement être zéro. En utilisant le lemme \ref{LemKKXdqdL} sous la forme \( b^{k_i}a=ab^{-k_i}\), quitte à changer les valeurs des exposants, nous pouvons passer tous les \( a \) à gauche et tous les \( b\) à droite pour finir sous la forme \( x=a^kb^m\). 

    Donc non, il n'existe pas d'autres éléments dans \( G\) que ceux déjà listés.
\end{proof}

\begin{lemma}[\cite{MonCerveau}]        \label{LemooNFRIooPWuikH}
    Tout élément de \( G\) s'écrit de façon unique sous la forme \( a^{\epsilon}b^k\) ou \( b^ka^{\epsilon}\) avec \( \epsilon=0,1\) et \( k=0,\ldots, n-1\).
\end{lemma}

\begin{proof}
    Nous commençons par la forme \( a^{\epsilon}b^k\). L'existence est la proposition \ref{PROPooEPVGooQjHRJp}. Pour l'unicité nous supposons \( a^{\epsilon}b^k=a^{\sigma}b^l\) et nous décomposons en \( 4\).
    \begin{subproof}
        \item[\( \epsilon=0\), \( \sigma=0\)]
            Alors \( b^k=b^l\). Mais \( b\) étant d'ordre \( n\) et \( k,l\) étant égaux au maximum à \( n-1\), cette égalité implique \( k=l\).
        \item[\( \epsilon=0\), \( \sigma=1\)]
            Alors \( b^k=ab^l\), ce qui donne \( a=b^{k-l}\), ce qui est interdit par la proposition \ref{PROPooVQARooWuKHMZ}.
        \item[\( \epsilon=1\), \( \sigma=0\)]
            Même problème.
        \item[\( \epsilon=1\), \( \sigma=1\)]
            Encore une fois \( b^k=b^l\) implique \( k=l\).
    \end{subproof}
    En ce qui concerne la forme \( b^ka^{\epsilon}\), l'existence est à montrer. Soit l'élément \( g=a^{\epsilon}b^k\) et cherchons à le mettre sous la forme \( b^la^{\sigma}\). Si \( \epsilon=0\) c'est évident. Sinon \( \epsilon=1\) et nous avons par le lemme \ref{LemKKXdqdL}
    \begin{equation}
        ab^k=b^{-k}a^{-1}=b^{-k}b^na=b^{-k}a.
    \end{equation}
    En ce qui concerne l'unicité, nous refaisons \( 4\) cas pour \( b^ka^{\epsilon}=b^la^{\sigma}\) comme précédemment et ils se traitement exactement comme précédemment.
\end{proof}

\begin{theorem}
    Les groupes \( G\) et \( D_n\) sont isomorphes.
\end{theorem}

\begin{proof}
        Nous utilisons l'application
    \begin{equation}
        \begin{aligned}
            \psi\colon G&\to D_n \\
            a^kb^m&\mapsto s^kr^m. 
        \end{aligned}
    \end{equation}
    C'est évidemment bien défini et bijectif, mais c'est également un homomorphisme parce que si nous calculons \( \psi\) sur un produit, nous devons comparer
    \begin{equation}        \label{EqBULPilp}
        \psi\big( a^{k_1}b^{m_1}a^{k_2}b^{m_2} \big)
    \end{equation}
    avec
    \begin{equation}        \label{EqIVEIphI}
        \psi\big( a^{k_1}b^{m_1}\big)\psi\big(a^{k_2}b^{m_2} \big)= s^{k_1}r^{m_1}s^{k_2}r^{m_2}.
    \end{equation}
    Vu que \( D_n\) et \( G\) ont les mêmes propriétés qui permettent de permuter \( a\) et \( b\) ou \( s\) et \( r\), l'expression à l'intérieur du \( \psi\) dans \eqref{EqBULPilp} se simplifie en \( a^kb^m\) avec les même \( k\) et \( n\) que l'expression à droite dans \eqref{EqIVEIphI} ne se simplifie en \( s^kr^m\).
\end{proof}

\begin{corollary}
    Toutes les propriétés démontrées pour \( G\) sont vraies pour \( D_n\). En particulier, avec quelques redites :
    \begin{enumerate}
        \item
            Le groupe \( D_n\) peut être défini comme étant le groupe engendré par un élément \( s\) d'ordre \( 2\) et un élément \( r\) d'ordre \( n-1\) assujettis à la relation \( srsr=e\).
        \item
            Le groupe \( D_n\) n'est pas abélien.
        \item
            Pour tout \( k\in\{ 1,\ldots, n-1 \}\) nous avons \( sr^ks=r^{-k}\).
        \item
            L'élément \( s\) ne peut pas être obtenu comme une puissance de \( r\).
        \item
            La liste des éléments de \( D_n\) est
            \begin{equation}
                D_n=\{ 1,r,\ldots, r^{n-1},s,sr,\ldots, sr^{n-1} \}
            \end{equation}
        \item
            Le groupe diédral \( D_n\) est d'ordre \( 2n\).
    \end{enumerate}
\end{corollary}

\begin{proposition}
    En posant \( C_n=\{ r^k \}_{k=0,\ldots, n-1}\) et \( C_2=\{ a^{\epsilon} \}_{\epsilon=0,1}\), nous pouvons exprimer \( D_n\) comme le produit semi-direct
    \begin{equation}
        D_n=C_n\times_{\rho}C_2
    \end{equation}
    où \( \rho\) désigne l'action adjointe.
\end{proposition}

\begin{proof}
    L'isomorphisme est :
    \begin{equation}
        \begin{aligned}
            \psi\colon C_n\times_{\rho}C_2&\to D_n \\
            (b^k,a^{\epsilon})&\mapsto b^ka^{\epsilon}.
        \end{aligned}
    \end{equation}
    \begin{subproof}
        \item[Action adjointe]
            L'application \( \rho_{a^{\epsilon}}=\AD(a^{\epsilon})\) est toujours un homomorphisme. Vu que \( a^{\epsilon}\) est soit \( e\) soit \( a\), nous allons nous restreindre à \( a\) et oublier l'exposant \( \epsilon\). Il faut montrer que\( \AD(a)\in\Aut(C_n)\). En utilisant le lemme \ref{LemKKXdqdL},
            \begin{equation}
                \AD(a)b^k=ab^ka^{-1}=b^{-k}=b^{n-k}.
            \end{equation}
            L'application \( \AD(a)\colon C_n\to C_n\) est donc bijective et homomorphique. Ergo isomorphisme.
        \item[Injectif]
            Si \( \psi(b^k,a^{\epsilon})=\psi(b^l,a^{\sigma})\), alors par unicité du lemme \ref{LemooNFRIooPWuikH} nous avons \( k=l\) et \( \epsilon=\sigma\).
        \item[Surjectif]
            Par la partie «existence»  du lemme \ref{LemooNFRIooPWuikH}.
        \item[Homomorphisme]
            L'homomorphisme est toujours de mise lorsque l'on prend deux sous-groupes d'un même groupe (ici le groupe des isométries de \( \eR^2\)) et que l'on tente de faire un produit semi-direct en utilisant l'action adjointe. Dans notre cas, le calcul est : 
            \begin{equation}
                \psi\big( (b^k,a^{\epsilon})(b^l,a^{\sigma}) \big)=b^k\rho_{a^{\epsilon}}(b^l)a^{\epsilon+\sigma}=b^ka^{\epsilon}b^la^{-\epsilon}a^{\epsilon+\sigma}=b^ka^{\epsilon}b^la^{\sigma}=\psi(b^k,a^{\epsilon})\psi(b^l,a^{\sigma}).
            \end{equation}
    \end{subproof}
     
\end{proof}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Classes de conjugaison}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{subsubsecZQnBcgo}

Pour les classes de conjugaison du groupe diédral nous suivons \cite{HRIMAJJ}.

D'abord pour des raisons de déterminants\footnote{Vous notez qu'ici nous utilisons un argument qui utilise la définition de \( D_n\) comme isométries de \( \eR^2\). Si nous avions voulu à tout prix nous limiter à la définition «abstraite» en termes de générateurs, il aurait fallu trouver autre chose.}, les classes des éléments de la forme \( r^k\) et de la forme \( sr^k\) ne se mélangent pas. Nous notons \( C(x)\) la classe de conjugaison de \( x\), et \( y\cdot x=yxy^{-1}\).

Les relations que nous allons utiliser sont 
\begin{subequations}
    \begin{align}
        sr^ks=r^{-k}\\
        rs=sr^{-1}=sr^{n-1}.
    \end{align}
\end{subequations}

La classe de conjugaison qui ne rate jamais est bien entendu \( C(1)={1}\). Nous commençons les vraies festivités \( C(r^{m})\). D'abord \( r^k\cdot r^m=r^m\), ensuite
\begin{equation}
    (sr^k)\cdot r^m=sr^kr^mr^{-k}s^{-1}=sr^ms^{-1}=r^{-m}.
\end{equation}
Donc
\begin{equation}    \label{EqVFfFxgi}
    C(r^m)=\{ r^m,r^{-m} \}.
\end{equation}
À ce niveau il faut faire deux remarques. D'abord si \( m>\frac{ n }{2}\), alors \( C(r^m)\) est la classe de \( C^{n-m}\) avec \( n-m<\frac{ n }{2}\). Donc les classes que nous avons trouvées sont uniquement à lister avec \( m<\frac{ n }{2}\). Ensuite si \( m=\frac{ n }{2}\) alors \( r^m=r^{-m}\) et la classe est un singleton. Cela n'arrive que si \( n\) est pair.

Nous passons ensuite à \( C(s)\). Nous avons
\begin{equation}
    r^k\cdot s=r^ksr^{-k}=ssr^ksr^{-k}=sr^{-k}r^{-k}=sr^{n-2k},
\end{equation}
et
\begin{equation}
    (sr^k)\cdot s=\underbrace{sr^ks}_{r^{-k}}r^{-k}s^{-1}=r^{-2k}s=r^{n-2k}s=sr^{(n-1)(n-2k)}=sr^{n^2-2kn-n+2k}=sr^{2k}.
\end{equation}
donc
\begin{equation}
    C(s)=\{ sr^{n-2k},sr^{2k} \}_{k=0,\ldots, n-1}.
\end{equation}
Ici aussi l'écriture n'est pas optimale : peut-être que pour certains \( k\) il y a des doublons. Nous reportons l'écriture exacte à la discussion plus bas qui distinguera \( n\) pair de \( n\) impair. Notons juste que si \( n\) est pair, l'élément \( sr\) n'est pas dans la classe \( C(s)\).

Nous en faisons donc à présent le calcul en gardant en tête le fait qu'il n'a de sens que si \( n\) est pair. D'abord
\begin{equation}
    s\cdot (sr)=ssrs=rs=sr^{n-1}.
\end{equation}
Ensuite
\begin{equation}
    (sr^k)\cdot (sr)=sr^ksrr^{-k}s=r^{-2k+1}s=sr^{2k-1}.
\end{equation}
Avec \( k=\frac{ n }{2}\), cela rend \( s\cdot (sr)\), donc pas besoin de le recopier. Nous avons
\begin{equation}
    C(sr)=\{ sr^{2k-1} \}_{k=1,\ldots, n-1}.
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Le compte pour $ n$ pair}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{SubsubsecROVmHuM}

Si \( n\) est pair, nous avons les classes
\begin{subequations}
    \begin{align}
        C(1)&=\{ 1 \}       &&&\text{\( 1\) élément}\\
        C(r^m)&=\{ r^m,r^{m-1} \}&\text{ pour }&0<m<\frac{ n }{2}   &\text{\( \frac{ n }{2}-1\) fois \( 2\) éléments}\\
        C(r^{n/2})&=\{ r^{n/2} \}   &&& \text{\( 1\) élément}\\ 
        C(s)&=\{ sr^{2k} \}_{k=0,\ldots, \frac{ n }{2}-1} &&& \text{\( \frac{ n }{2}\) éléments}\\
        C(sr)&=\{ sr^{2k+1} \}_{k=0,\ldots, \frac{ n }{2}-1} &&& \text{\( \frac{ n }{2}\) éléments}.
    \end{align}
\end{subequations}
Au total nous avons bien listé \( 2n\) éléments comme il se doit, dans \(  \frac{ n }{2}+3\) classes différentes.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Le compte pour $ n$ impair}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{Subsubsec*GJIzDEP}

Si \( n\) est impair, nous avons les classes
\begin{subequations}
    \begin{align}
        C(1)&=\{ 1 \}       &&&\text{\( 1\) élément}\\
        C(r^m)&=\{ r^m,r^{m-1} \}&\text{ pour }&0<m<\frac{ n-1 }{2}   &\text{\( \frac{ n-1 }{2}\) fois \( 2\) éléments}\\
        C(s)&=\{ sr^k \}_{k=0,\ldots, n-1} &&& \text{\( n\) éléments}
    \end{align}
\end{subequations}
Au total nous avons bien listé \( 2n\) éléments comme il se doit, dans \(  \frac{ n+3 }{2}\) classes différentes.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Applications : du dénombrement}
%---------------------------------------------------------------------------------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Le jeu de la roulette}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{pTqJLY}
\index{groupe!fini}
\index{groupe!de permutations}
\index{groupe!et géométrie}
\index{combinatoire}
\index{dénombrement}

Soit une roulette à \( n\) secteurs que nous voulons colorier en \( q\) couleurs\cite{HEBOFl}. Nous voulons savoir le nombre de possibilités à rotations près. Soit d'abord \( E\) l'ensemble des coloriages possibles sans contraintes; il y a naturellement \( q^n\) possibilités. Sur l'ensemble \( E\), le groupe cyclique \( G\) des rotations d'angle \( 2\pi/n\) agit. Deux coloriages étant identiques si ils sont reliés par une rotation, la réponse à notre problème est donné par le nombre d'orbites de l'action de \( G\) sur \( E\) qui sera donnée par la formule du théorème de Burnside \ref{THOooEFDMooDfosOw}. 

Nous devons calculer \( \Card\big( \Fix(g) \big)\) pour tout \( g\in G\). Soit \( g\), un élément d'ordre \( d\) dans \( G\). Si \( g\) agit sur la roulette, chaque secteur a une orbite contenant \( d\) éléments. Autrement dit, \( g\) divise la roulette en \( n/d\) secteurs. Un élément de \( E\) appartenant à \( \Fix(g)\) doit colorier ces \( n/d\) secteurs de façon uniforme; il y a \( q^{n/d}\) possibilités.

Il reste à déterminer le nombre d'éléments d'ordre \( d\) dans \( G\). Un élément de \( G\) est donné par un nombre complexe de la forme \(  e^{2ik\pi/n}\). Les éléments d'ordre \( d\) sont les racines primitives\footnote{Une racine non primitive \( 8\)ième de l'unité est par exemple \( i\). Certes \( i^8=1\), mais \( i^4=1\) aussi. Le nombre \( i\) est d'ordre \( 4\).} \( d\)ièmes de l'unité. Nous savons que --par définition-- il y a \( \varphi(d)\) telles racines primitives de l'unité. Bref il y a \( \varphi(d)\) éléments d'ordre \( d\) dans \( G\). 

La formule de Burnside nous donne maintenant le nombre d'orbites :
\begin{equation}
    \frac{1}{ n }\sum_{d|n}\varphi(d)q^{n/d}.
\end{equation}
Cela est le nombre de coloriage possibles de la roulette à \( n\) secteurs avec \( q\) couleurs.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{L'affaire du collier}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{siOQlG}

Nous avons maintenant des perles de \( q\) couleurs différentes et nous voulons en faire un collier à \( n\) perles. Cette fois non seulement les rotations donnent des colliers équivalents, mais en outre les symétries axiales (il est possible de retourner un collier, mais pas une roulette). Le groupe agissant sur \( E\) est maintenant le groupe diédral\footnote{Définition \ref{DEFooIWZGooAinSOh}.}\index{diédral}\index{groupe!diédral} \( D_n\) conservant un polygone a \( n\) sommets.

Nous devons séparer le cas \( n\) impair du cas \( n\) pair.

Si \( n\) est impair, alors les axes de symétries passent par un sommet par le milieu du côté opposé. Le groupe \( D_n\) contient \( n\) symétries axiales. Nous avons donc maintenant
\begin{equation}
    | G |=2n.
\end{equation}
Nous écrivons la formule de Burnside
\begin{equation}
    \Card(\Omega)=\frac{1}{ 2n }\sum_{g\in G}\Card\big( \Fix(g) \big).
\end{equation}
Si \( g\) est une rotation, le travail est déjà fait. Si \( g\) est une symétrie, nous avons le choix de la couleur du sommet par lequel passe l'axe et le choix de la couleur des \( (n-1)/2\) paires de sommets. Cela fait
\begin{equation}
    qq^{(n-1)/2}=q^{\frac{ n+1 }{2}}
\end{equation}
possibilités. Nous avons donc
\begin{equation}
    \Card(\Omega)=\frac{1}{ 2n }\left( \sum_{d|n}q^{n/d}\varphi(d)+nq^{\frac{ n+1 }{2}} \right).
\end{equation}

Si \( n\) est pair, le choses se compliquent un tout petit peu. En plus de symétries axiales passant par un sommet et le milieu du côté opposé, il y a les axes passant par deux sommets opposés. Pour colorier un collier en tenant compte d'une telle symétrie, nous pouvons choisir la couleur des deux perles par lesquelles passe l'axe ainsi que la couleur des \( (n-2)/2\) paires de perles. Cela fait en tout
\begin{equation}
    q^2q^{\frac{ n-2 }{2}}=q^{\frac{ n+2 }{2}}.
\end{equation}
Le groupe \( G\) contient \( n/2\) tels axes.

Notons que cette fois \( G\) ne contient plus que \( n/2\) symétries passant par un sommet et un côté. L'ordre de $G$ est donc encore \( 2n\). La formule de Burnside donne
\begin{equation}
    \Card(\Omega)=\frac{1}{ 2n }\left( \sum_{d\divides n}\varphi(d)q^{n/d}+\frac{ n }{2}q^{(n+2)/2}+\frac{ n }{2}q^{n/2} \right).
\end{equation}
