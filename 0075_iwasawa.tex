% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\section{The group \texorpdfstring{$\SO(2,1)$}{SO21} and its algebra}   \label{subsec_IwSOdu}
%-----------------------------------------------------------
\index{Iwasawa decomposition!of $\SO(2,1)$}

The condition for a matrix $A$ to belong to $\mathfrak{so}(2,1)$ is $A^tgA=g$ with $g=diag(1,1,-1)$. Hence considering the Cartan involution\label{pg:Cartan_SO} $\theta(X)=-X^t$, we find
\begin{equation}  \label{EqMtrsDansSOdu}
\lG=\mathfrak{so}(2,1)\leadsto\begin{pmatrix}
0&a&u_1\\-a&0&u_2\\u_1&u_2&0
\end{pmatrix},\quad
\lK\leadsto\begin{pmatrix}
0&1\\-1&0\\&&0
\end{pmatrix},
\quad
\lP\leadsto\begin{pmatrix}
&&u_1\\
&&u_2\\
u_1&u_2&0
\end{pmatrix}.
\end{equation}
As generator of $\lA$, we choose
\[ 
  J=\begin{pmatrix}
&0\\
0&0&1\\
&1
\end{pmatrix}.
\]
The condition $[J,X]=X$ which defines $\sN$ is satisfied by
\[ 
  L=\begin{pmatrix}
0&1&1\\-1\\1
\end{pmatrix}.
\]
If we denote by $X$ the general matrix of $\mathfrak{so}(2,1)$ given in equation \eqref{EqMtrsDansSOdu}, we have
\[ 
  \ad(J)X=
\begin{pmatrix}
0&-u_{1}&-a\\
u_{1}&0&0\\
-a&0&0
\end{pmatrix}
\]
which leads to the following root spaces\index{root!space!in $\mathfrak{so}(2,1)$}
\begin{equation}
S_{0}=
\begin{pmatrix}
0&0&0\\
0&0&1\\
0&1&0
\end{pmatrix},\quad
S_{1}=
\begin{pmatrix}
0&1&-1\\
-1&0&1\\
-1&1&0
\end{pmatrix},\quad
S_{-1}=
\begin{pmatrix}
0&1&1\\
-1&0&1\\
1&1&0
\end{pmatrix}
\end{equation}
with the relations
\begin{subequations}
\begin{align}
[S_{0},S_{1}]&=S_{1}\\
[S_{0},S_{-1}]&=-S_{-1}\\
[S_{1},S_{-1}]&=-2S_{0}.
\end{align}
\end{subequations}
Notice that the comparison with \eqref{subeq_rootSLR} shows that $SL(2,\eR)$ and $\SO(2,1)$ \emph{are not} isomorphic.

\subsection{Root spaces for \texorpdfstring{$\so(2,1)$}{so21}}
%--------------------------------------------------------------

\begin{proposition}
	The matrices of $\SO(p,q)$ satisfy the relation
	\[ 
		A^{-1}=\eta A^t\eta
	\]
	where $\eta$ is the diagonal matrix with $p$ times $1$ and $q$ times $-1$. That relation is often taken as the definition of $\SO(p,q)$.
\end{proposition}


The algebra $\so(2,1)$ is made up from $3\times 3$ matrices such that $X^t\eta+\eta X=0$ with vanishing trace. If we choice $\eta=diag(-,-,+)$, we find matrices of the form
\[ 
  \so(2,1)\leadsto\begin{pmatrix}
a	&u^t\\
u	&0
\end{pmatrix}
\]
where $a$ is an antisymmetric $2\times 2$ matrix and $u$ is any $1\times 2$ matrix. We find the Cartan decomposition
\[ 
  \iK\leadsto\begin{pmatrix}
\so(2)\\&0
\end{pmatrix},\qquad
\iP\leadsto\begin{pmatrix}
0&0&u_1\\
0&0&u_2\\
u_1&u_2&0
\end{pmatrix}.
\]
If one chooses
\[ 
  J=\begin{pmatrix}
0&0&1\\0\\1
\end{pmatrix}
\]
as generator for the abelian subalgebra of $\iP$, one finds
\[ 
	V_1=
	\begin{pmatrix}
		0&1&0\\
		-1&0&1\\
		0&1&0
	\end{pmatrix},\qquad
	V_{-1}=
	\begin{pmatrix}
		0&1&0\\
		-1&0&-1\\
		0&-1&0
	\end{pmatrix}
\]
as eigenvectors for $\ad(J)$ with eigenvalues $1$ and $-1$. The root space decomposition commutator table of $\so(2,1)$ is thus given by
\begin{subequations}
	\begin{align}
		 [V_0,V_1]&=V_1\\
		[V_0,V_{-1}]&=-V_{-1}\\
		[V_1,V_{-1}]&=-2V_0.
	\end{align}
\end{subequations}
Notice that the map $\phi(A_0)=2V_0$, $\phi(A_2)=V_1$, $\phi(A_{-2})=-V_{-1}$ provides an isomorphism between this table and the one of $\gsl(2,\eR)$, equations \eqref{subeq_rootSLR}. This fact assures a Lie algebra isomorphism $\gsl(2,\eR)\simeq\so(2,1)$. We actually have a stronger result: 

\begin{proposition}
The group $\SL(2,\eR)$ is a double-covering of the identity component $\SO_0(1,2)$.
\end{proposition}

\section{Iwasawa decomposition for \texorpdfstring{$\SOdn$}{SO2n}} \label{subsecIwasawa_un}
%-------------------------------------------------
\index{Iwasawa decomposition!of $\SO(2,n)$}

As seen in the general construction and in previous examples, the Iwasawa decomposition of a group or an algebra depends on several choices. We will study two out of them in the case of $\SO(2,n)$ and see that some ``compatibility conditions'' with the decomposition of $\SO(1,n)$ and the symmetric space structure of $AdS$ (see section \ref{SecSymeStructAdS}) fix most of choices. When the two decompositions will be used together, the unadapted one will be tilded (see subsection \ref{subsec:precision}).

\begin{lemma}
The Lie group $\SO(p,q)$ is semisimple.\nomenclature[G]{$\SO(p,q)$}{Isometry group of $\eR^{p,q}$}
\label{lem:SO_pq_ss}
\end{lemma}

\begin{proof}[Sketch of proof]
We are going to give some ideas of the proof. Remark that $\SO(p,q)$ essentially contains ``rotations''\ $J_{ij}(\lambda)$ of angle $\lambda$ in planes $(i,j)$. (If the coordinates $i$ and $j$ are not of the same type\footnote{i.e. if the metric in the plane $(i,j)$ has signature $(+,-)$.}, then it is not a true rotation but a boost and $\lambda$ is no more an angle but any real.) If one try to build an ideal $\mI$ containing $J_{ij}$, then the element $J_{kl}J_{ij}$ must also be in $\mI$. If we want $\mI$ to be abelian, we must have $[J_{ij},J_{ij}J_{kl}]$, or
\[
  J_{kl}J_{ij}=J_{ij}J_{kl}.
\]
It is really easy to find a $J_{kl}$ for which it is false. Thus $\SO(p,q)$ doesn't contain any abelian ideal, so that it is semisimple by lemma \ref{lem:ss_ideal}.
\end{proof}

\begin{probleme}
J'ai l'impression que c'est la simplicité de $\SO(p,q)$ que je démontre. À vérifier.
\end{probleme}

The Lie algebra $\sG=\mathfrak{so}(2,n)$ is the set 
\begin{equation}\label{def_sodn}
\big\{X\in M_{(2+n)\times (2+n)}\textrm{ such that }X^t\eta+\eta X=0\textrm{ and } \tr X=0\big\}
\end{equation}
where $\eta$ is the diagonal metric $\eta=diag(-,-,+,\ldots,+)$. An element of $\mathfrak{so}(2,n)$ can be written as
$
    X=\begin{pmatrix}
a & u ^t \\
v & B
\end{pmatrix}
$
with the matrices $a\in M_{2\times 2}$, $u\in M_{n\times 2}$, $v\in M_{n\times 2}$, and $B\in M_{n\times n}$. The conditions in \eqref{def_sodn} give: $a=-a^t$, $u=v$, and $B=-B^t$. Hence, a general matrix of $\sodn$ is given by
\begin{equation}	\label{eq:gene_sodn}
X=\begin{pmatrix}
a & u^t \\
u & B
\end{pmatrix}
\end{equation}
where $a,B$ are skew-symmetric.

\subsection{Cartan decomposition}		\label{SubSecCartandeuxN}
%//////////////////////////////

The Cartan decomposition of $\so(2,n)$ associated with the Cartan involution $\theta(X)=-X^t$ is
\begin{align}\label{K_et_P}
   \iK&\leadsto
\begin{pmatrix}
\mathfrak{so}(2) \\
 & \mathfrak{so}(n)
\end{pmatrix},
&\iP&\leadsto\begin{pmatrix}
0 & u^t \\
u & 0
\end{pmatrix}.
\end{align}
 Elements of $\SO(2)$ are represented by
\[ 
  \begin{pmatrix}
\cos\mu&\sin\mu\\
-\sin\mu&\cos\mu
\end{pmatrix}.
\]
A common abuse of notation in the text will be to identify the angle $\mu$ with the element of $\SO(2)$ itself. In the same spirit, when we speak about a matrix of $A\in \SO(2)$, we mean a matrix whose upper left corner is $A$ and the rest is the unit matrix. For example, for $AdS_3$, the matrix $-\mtu\in \SO(2)$ is
\[
\begin{pmatrix}
\begin{matrix}
-1&0\\
0&-1
\end{matrix}\\
&1\\
&&1
\end{pmatrix}.
\]
That matrix will be denoted\footnote{See also proposition \ref{LONGPropCartanExtExpo}.} by $k_{\theta}$ and has the property that
\begin{equation}
	\theta=\AD(k_{\theta}).
\end{equation}
Indeed, we have $k_{\theta}=\mtu$, so that $\AD(k_{\theta})^2=\id$. Since $k_{\theta}$ commutes with every element of $K$, $\AD(k_{\theta})|_{K}=\id$, and a simple computation shows that $\Ad(k_{\theta})|_{\sP}=-\id$.

Remark that $\iK$, the compact part of $\mG$ is made up from ``true'' rotations while $\iP$ contains boosts.  This remark allows us to guess a right choice of maximal abelian subalgebra in $\iP$. Indeed elements of $\iA$ must be boosts and the fact that there are only two time-like directions restricts $\sA$ to a two dimensional algebra. Up to reparametrization, it is thus generated by $t\partial_x+x\partial_t$ and $u\partial_y+y\partial_t$. Hence the following choice seems to be logical:
\begin{align}   \label{EqDevineA}
   J_1&=
\begin{pmatrix}
&0\\
0&0&0&1\\
&0\\
&1
\end{pmatrix}\in\sH,
&J_2&=q_1=
\begin{pmatrix}
0&0&1&0\\
0\\
1\\
0
\end{pmatrix}\in\sQ.
\end{align}

\subsection{Maximal abelian subalgebra}
%///////////////////////////////////////

We have to find an abelian subalgebra $\iA\subset\iP$. If we use the convention that the indices $a$, $b$ range from $1$ to $2$ and $i$, $j$ from $3$ to $n+2$, we can write
\begin{equation}
    u=\begin{pmatrix}
0 & u^t \\
u & 0
\end{pmatrix}=u^{ai}(E_{ai}+E_{ia}),
\end{equation}
where $E_{ij}$ denotes the matrix whose only non zero component is a $1$ in the place $ij$. Of course we use an abuse of notation between the $(n+2)\times(n+2)$ matrix of $\iP$ and $n\times 2$ matrix $u$ which defines it. We compute the commutator of two such matrices:
\begin{equation}
\begin{split}
[u,v]=u^{ai}v^{bj}(&\underline{\delta_{ib}E_{aj}}-\underline{\delta_{aj}E_{bi}}+\delE{ij}{ab}-\delE{ab}{ji}\\
&+ \delE{ab}{ij}-\delE{ij}{ba}+\underline{\delE{aj}{ib}}-\underline{\delE{ib}{ja}})
\end{split}
\end{equation}
Since $[\iP,\iP]\subseteq\iK$, the latter commutator takes the form $\begin{pmatrix}
a & 0 \\
0 & B
\end{pmatrix}$, thus the underlined terms must vanish. So we are left with:
\begin{eqnarray}
[u,v]&=&u^{ai}v^{bj}(\delta_{ij}(E_{ab}-E_{ba})+\delta_{ab}(E_{ij}-E_{ji}))\\
     &=&u^{ai}v^{bj} (E_{ab}-E_{ba})+u^{ai}v^{aj} (E_{ij}-E_{ji}).
\end{eqnarray}
Finally, $[u,v]=0$ if and only if 
\begin{equation} \label{EqCondSOdnsumia}
\sum_iu^{ai}v^{bi}=0\quad\text{and}\quad \sum_au^{ai}v^{aj}=0
\end{equation}
 with $a\neq b$, and $i\neq j$.

In order to build a maximal abelian subalgebra $\iA$ of $\iP$, we take one element in $\iP$ and then we extend it using the relations \eqref{EqCondSOdnsumia}. We choose $u^{ai}=\delE{a1}{i3}$.

If this $u$ lies in $\iA$, an other $v$ in $\iA$ must satisfy $v\in\iP$, $v^{23}=0$ and $v^{1j}=0$ for $j>3$. 
We choose the one with $u^{24}=1$. It is no difficult to see that these two are maximal. A basis of $\iA$ is thus $\tilde{H}_1\equiv u^{24}=1$ and
 $\tilde{H}_2\equiv u^{13}=1$. We immediately perform a change of basis by defining:
\begin{subequations}
\begin{align}
  H_1&=\tilde{H}_1-\tilde{H}_2\\
  H_2&=\tilde{H}_1+\tilde{H}_2,
\end{align}
\end{subequations}
and finally: $\iA=span\{H_1,H_2\}$. The matrices are given by
\begin{equation}
H_p=\left(\begin{array}{c|c}
N_p&0\\
\hline
0&0
\end{array}\right)
\end{equation}
with
\begin{equation}
N_p=\begin{pmatrix}
 & \begin{array}{cc}1&0\\0&(-1)^p\end{array} \\
\begin{array}{cc}1&0\\0&(-1)^p\end{array} & 
\end{pmatrix}.
\end{equation}
Up to a change of basis, they are exactly the ones guessed in equation \eqref{EqDevineA}. That matrix can be written under the more compact form

The generators of $\iA$ that we choose are the following linear combination of $J_1$ and $J_2$:
\begin{equation}\label{mtr_A}
       H_p=E_{13}+E_{31}+(-1)^p(E_{24}+E_{42}).
\end{equation}



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Compatibility conditions between \texorpdfstring{$\sH$}{H} and \texorpdfstring{$\sG$}{G}}
%---------------------------------------------------------------------------------------------------------------------------

The decomposition of $\SO(1,n)$ of section \ref{SecIwaSOunn} incites us to search for a decomposition of $\SOdn$ which satisfies certain compatibility conditions. If you want to learn how to compute Iwasawa decompositions, you should read the next section (\ref{SubSecANbarIwa}) where computations are more explicit.

We go back to the choice of $\iA$. Remark that $\iP_{\sH}\subset\sP$. This suggests us to try to get $\iA_{\sH}\subset\sA$. So we will choose $J$ as first matrix in $\iA$, and then extend it to a maximal abelian subalgebra of $\iP$. Note in particular that this don't affect the choice of the Cartan involution. Now we consider $\Phi$ and $\Psi$, the restricted roots for $G$ and $H$; for recall:
\begin{equation}
\begin{split}
  \sH_{\alpha}  &=\{ X\in\sH\tq \forall H'\in\iAH,(\ad H')X=\alpha(H')X \},\\
  \sG_{\lambda} &=\{ X\in\sG\tq \forall H\in\iA,(\ad H)X=\lambda(H)X \},\\
  \Phi        &=\{ \lambda\in\iA^*\tq \lambda\neq 0, \sG_{\lambda}\neq 0 \},\\
  \Psi        &=\{ \lambda\in\sA_{\sH}^*\tq \lambda\neq 0, \sH_{\lambda}\neq 0\}.
\end{split}
\end{equation}

\begin{lemma}
The root spaces of $G$ and $H$ are related by
\[
    \Phi|_{\iAH}=\Psi,
\]
in other words, elements of $\Psi$ are the restriction to $\iAH$ of the ones of $\Phi$
\end{lemma}

\begin{proof}
Let us consider a $\alpha\in\Phi$: there exists a non-zero subspace $\sG_{\alpha}$ of $\sG$ such that for any $H\in\iA$, $[H,X]=\alpha(H)X$. Clearly, $\alpha$ also fulfils $[H',X]=\alpha(H')X=\alpha|_{\iAH}(H')X$ for any $H'\in\iAH\subset\iA$. Then $\alpha|_{\sA_{\sH}}\in\Psi$.

In order to prove the inverse sense, consider $\beta\in\Psi$ and $X\in\sH_{\beta}$. We have $[H',X]=\beta(H')X$ for every $H'\in\sA_{\sH}$. Let us consider $H\in\iA$: 
\begin{equation}
  [H',[H,X]]=[H,[H',X]]
            =\beta(H')[H,X].
\end{equation}
Thus for any $H\in\iA$ and $X\in\sH_{\beta}$, $[H,X]\in\sH_{\beta}$. In other words,
\[  
   [\iA,\sH_{\beta}]\subset\sH_{\beta}.
\]
Now, $\ad\iA$ is a set of semisimple\quext{Il faudra encore voir ça\ldots} commuting operators on $\sH_{\beta}$; hence there exists a basis of $\sH_{\beta}$ of common eigenvectors. Thus we can write a decomposition of $\sH_{\beta}$ under the form
\begin{equation}\label{eq:decomp_sH}
 \sH_{\beta}=\bigoplus_{\alpha\in\Phi}\sH_{\beta}^{\alpha},
\end{equation}
with $\sH_{\beta}^{\alpha}=\sH_{\beta}\cap\sG_{\alpha}$. In order to see this last point, consider an eigenvector $h_{\beta}\in\sH_{\beta}$ of all the elements of $\ad\sA$: there exists $\alpha\in\sA^*$ such that $\forall A\in\sA$, $[A,h_{\beta}]=\alpha(A)h_{\beta}$. In other words, $h_{\beta}\in\sG_{\alpha}$.

This decomposition allows us to write $X\in\sH_{\beta}$ as
\[
    X=\bigoplus_{\alpha\in\Phi}X^{\alpha}.
\]
On $X\in\sH_{\beta}^{\alpha}$, $\ad A$ acts as $(\ad A)X=\alpha(A)X$.  So, for $X\in\sH_{\beta}$, $H'\in\iAH$,
\[
  [H',X]=\beta(H')X=\bigoplus_{\alpha\in\Phi}\alpha(H')X^{\alpha}.
\]
Then $\alpha(H')=\beta(H')$, i.e. $\alpha|_{\iAH}=\beta$.

\end{proof}

Now, we choose a clever positivity notion on $\Phi$. The one of $\Psi$ is clear; we extend it to $\Phi$, so that $\Psi^+\subset\Phi^+$ and $\iNH\subset\iN$. Finally for our new Iwasawa decomposition of $\SOdn$,
\[
   \iRH\subset\iR.
\]

In the decomposition \eqref{eq:decomp_sH}, it is possible that, for certain $\alpha$ and $\beta$, the set $\sH_{\beta}^{\alpha}$ reduces to $\{0\}$. We define
\begin{equation}
  \phi(\beta)=\{ \alpha\in\Phi\tq \sH_{\beta}^{\alpha}\neq\{0\} \};
\end{equation}
it defines a map $\phi$ from $\Psi$ to the parts of $\Phi$. Moreover, $\phi(\Psi)$ gives a partition of $\Phi$ because $\phi(\beta)\cap\phi(\beta')=\emptyset$ if $\beta\neq\beta'$. Indeed, $\phi(\beta)\cap\phi(\beta')$ is the set of all the $\alpha\in\Phi$ such that $\sH_{\beta}\cap\sG_{\alpha}\neq\{0\}$ and $\sH_{\beta'}\cap\sG_{\alpha}\neq\{0\}$. But for $\alpha\in\phi(\beta)$, $\alpha_{\iAH}=\beta$, then $\alpha(H')=\beta(H')$; for the same reason, $\alpha(H')=\beta'(H')$, so that $\beta=\beta'$.

We know the $\pm 1$ eigenspaces decompositions $\sG\stackrel{\sigma}{=}\sH\oplus\sQ\stackrel{\theta}{=}\sK\oplus\sP$ with $[\sigma,\theta]=0$, and the Iwasawa decomposition $\sA_{\sH}\oplus\sN_{\sH}\oplus\sK_{\sH}$ of $\sH$. 

For compatibility and simplicity purposes, we want the Iwasawa decompositions $\sG=\sA\oplus\sN\oplus\sK$ of $\sG$ in such a way that $\sA_{\sH}\subset\sA$ and $\sN_{\sH}\subset\sN$.
We denote by $A$, $N$, $K$, $A_H$, $N_H$ and $K_H$ the analytic connected subgroups of $G$ whose Lie algebras are $\sA$, $\sN$, $\sK$, $\sA_{\sH}$, $\sN_{\sH}$, and $\sK_{\sH}$ respectively. 


 For the $\sA$-part, we just perform a change of basis
\begin{subequations}
\begin{align}
J_{1}&=\frac{ 1 }{2}(H_{2}-H_{1})\\
J_{2}&=\frac{ 1 }{2}(H_{1}+H_{2}) 
\end{align}
\end{subequations}
in order to have $J_{1}\in\sP\cap\sH$ and $J_{2}\in\sP\cap\sQ$. The involution $\sigma$ has a simpler expression in this basis.

In order to get $\sA\subset\sA_{\sH}$, we take the element of $\sA_{\sH}$ as first basis element of $\sA$:
\begin{equation}		\label{EqGeueuleJun}
   J_1=
\begin{pmatrix}
&0\\
0&0&0&1\\
&0\\
&1
\end{pmatrix}.
\end{equation}
One can check from from \eqref{EqGeneRedQ} and \eqref{K_et_P} that $J_{1}$ belongs to $\sP\cap\sH$. Now we want to extend it to a maximal abelian subalgebra of $\sP$. In order to build an Iwasawa decomposition of $\SOdn$ compatible with $\sQ$, we search a $J_2\in\sP\cap\sQ$. We know that
\[
  \sP\cap\sQ\leadsto
\begin{pmatrix}
0&0&u^t\\
0&0\\
u
\end{pmatrix},
\]
so that the matrix
\begin{equation}		\label{EqgueueleJdeux}
J_2=
\begin{pmatrix}
0&0&1&0\\
0\\
1\\
0
\end{pmatrix}
\end{equation}
belongs to $\sP\cap\sQ$ and commutes with $J_1$. 

\begin{remark}
The new $\sA$ for $\SOdn$ is the abelian Lie algebra spanned by $J_1$ and $J_2$. This is just a change of basis in $\sA$: $J_1=\frac{1}{2}(H_2-H_1)$, $J_2=\frac{1}{2}(H_1+H_2)$. 
\end{remark}
Note that $J_1\in\sH$ and $J_2\in\sQ$; then the expression of $\sigma$ on $\sA$ should be rather simple. Let us first show the action of $\sigma$ on the root spaces.

If $\alpha\in\sA^*$, the notation $\sigma^*\alpha$ means $f\circ\alpha$, and $\sG_{\alpha}$ denotes the root space associated with the form $\alpha$.

\begin{lemma}			\label{LemSigmaThetaRootSpaces}
	The involutions $\sigma$ and $\theta$ act on the root spaces by 
	\begin{align}		\label{eq:sig_G_alpha}
		\sigma\sG_{\varphi}&=\sG_{\sigma^*\varphi},
		&\theta\sG_{\varphi}&=\sG_{-\varphi}
	\end{align}
	for all $\varphi\in\sA^*$,
\end{lemma}

\begin{proof}
Let $H\in \sA$ and $X\in\sG_{\varphi}$; by definition: $[H,X]=\varphi(H)X$. We have
\[ 
  \varphi(H)\sigma X=\sigma[H,X]=[\sigma H,\sigma X]=\varphi(\sigma H)\sigma X.
\]
We conclude that $\sigma X\in\sG_{\sigma^*\varphi}$. For the second equality, we take $X\in\sG_{\varphi}$ and 
\[ 
  [H,\theta X]=\theta[\theta H,X]=\theta\big( \varphi(\theta H)X \big)=-\varphi(H)\theta(X).
\]
\end{proof}

\begin{lemma}		\label{LemSigmaChangeDeux}
The involution $\sigma$ changes the sign of the $J_{2}^*$-part of the root spaces:
\[
	\sigma\sG_{(x,y)}=\sG_{(x,-y)}
\]
where $(x,y)$ denote the coordinates of a root in the basis $\{ J_1^*,J_2^* \}$ of $\sA$.
\end{lemma}

\begin{proof}
Since $\sigma$ is an involutive automorphism, it satisfies $[\sigma X,Y]=\sigma[X,\sigma Y]$. So when $X\in\sG_{(x,y)}$, we have $(\ad(J_{1}))(\sigma X)=x\sigma X$ and $\ad(J_{2})(\sigma X)=-y\sigma X$.

\end{proof}

It is also clear that $\sigma^*\alpha=0$ implies $\alpha(J_2)=0$. Indeed, $(\sigma^*\alpha)(J_2)=\alpha(-J_2) =-\alpha(J_2)$, because $J_{2}\in\sQ$.
Now we can give a precision about the decomposition $\sH_{\beta}=\oplus_{\alpha\in\Phi'}\sH_{\beta}^{\alpha}$ given by \eqref{eq:decomp_sH}. In fact, $\Phi'=\Phi_{\sigma}=\{\alpha\tq \sigma^*\alpha=\alpha\}$. Indeed $\sigma$ fixes $\sH$ and thus fixes $\sH_{\beta}^{\alpha}$. If $h_{\beta}^{\alpha}\in\sH_{\beta}^{\alpha}$, it satisfies $\sigma h_{\beta}^{\alpha}=h_{\beta}^{\alpha}$. But from equation \eqref{eq:sig_G_alpha}, $\sigma h_{\beta}^{\alpha}\in \sG_{\sigma^*\alpha}$.

\begin{proposition}
	The spaces $\sP$ and $\sK$ are Killing-orthogonal. The spaces $\sH$ and $\sQ$ are Killing-orthogonal.
\end{proposition}

\begin{proof}
	Let consider $X\in\sH$ and $Y\in\sQ$. Since $\sigma$ is an automorphism of the algebra $\sG$, the Killing form is $\sigma$-invariant and we have
	\begin{equation}
		B(X,Y)=B(\sigma X,\sigma Y)=B(X,-Y)=-B(X,Y),
	\end{equation}
	which proves that $B(X,Y)=0$. Exactly the same holds for $X\in\sK$ and $Y\in\sP$, using $\theta$ instead of $\sigma$.
\end{proof}

\subsection{First choice: \texorpdfstring{$AN$}{AN}}\label{subsecIwasawa_deux}
%---------------------------------------------------------

We turn now our attention to the Iwasawa decomposition. As before we are searching for matrices under the form $E=\begin{pmatrix}A&B\\C&D\end{pmatrix}$ with dimensions $4+(n-2)$. The condition which determines the root spaces reads
\begin{equation}
  (\ad J_i)E=
\begin{pmatrix}
  [j_i,A]&j_iB\\
  -Cj_i&0
\end{pmatrix}
\stackrel{!}{=}\lambda_i
\begin{pmatrix}A&B\\C&D\end{pmatrix}.
\end{equation}

% !! Si tu modifies des trucs ici, tu dois aussi le faire dans l'appendice sur les choses explicites. 

The computations are rather the same as the ones of the first time. The result is\label{pg:root_n}
\begin{equation}
\sG_{(0,0)}\leadsto
\begin{pmatrix}
&&x&0\\
&&0&y\\
x&0\\
0&y\\
&&&& D
\end{pmatrix},
\end{equation}
where $D\in M_{(n-2)\times(n-2)}$ is skew-symmetric. Notice that all but the $D$-part of this space is spanned by $q_1$ and $J_1$, so the $\sQ$-component of that matrix is a multiple of $q_1$. Other root spaces are given by
\begin{subequations}
\begin{align}
\sG_{(1,0)}&\leadsto W_i=E_{2i}+E_{4i}+E_{i2}-E_{i4}\in\sH,\\
\sG_{(-1,0)}&\leadsto Y_i=-E_{2i}+E_{4i}-E_{i2}-E_{i4},\\
\sG_{(0,1)}&\leadsto V_i=E_{1i}+E_{3i}+E_{i1}-E_{i3},\\
\sG_{(0,-1)}&\leadsto X_i=-E_{1i}+E_{3i}-E_{i1}-E_{i3}
\end{align}
\end{subequations}
with\footnote{Let us remember that we are dealing with $\SO(2,n)$ and that $AdS_{l}$ is a quotient of $\SO(2,l-1)$, so in the case of $AdS_{l}$ the index $j$ runs from $5$ to $l+1$. The first anti de Sitter space which contains such root spaces is $AdS_{4}$. More generally, remark that the table \eqref{EqTableSOIwa} of $\mathfrak{so}(2,n)$ gives the feeling that if something works with $AdS_4$, it will work for $AdS_{l\geq 4}$.} $\dpt{i}{5}{n+2}$. For example,
\begin{equation}		\label{EqGeueuleVWXY}
	\begin{aligned}[]
		\sG_{(1,0)}\leadsto W_5&
		=\begin{pmatrix}
			&&&&0\\
			&&&&1\\
			&&&&0\\
			&&&&1\\
			0&1&0&-1&0
		\end{pmatrix}\in\sH
		&\sG_{(-1,0)}\leadsto Y_5&
		=\begin{pmatrix}
			 	&		&		&		&	0\\ 
			 	&		&		&		&	-1\\ 
			 	&		&		&		&	0\\ 
			 	&		&		&		&	1\\ 
			0	&	-1	&	0	&	-1	&	0 
 		\end{pmatrix}\in\sH\\
		\sG_{(0,1)}\leadsto V_{5}&=
		\begin{pmatrix}
			&&&&1\\
			&&&&0\\
			&&&&1\\
			&&&&0\\
			1&0&-1&0&0
		\end{pmatrix}
		&\sG_{(0,-1)}\leadsto X_5&=
		\begin{pmatrix}
			 	&		&		&		&	-1\\ 
			 	&		&		&		&	0\\ 
			 	&		&		&		&	1\\ 
			 	&		&		&		&	0\\ 
			-1	&	0	&	-1	&	0	&	0 
		\end{pmatrix}\\
		\sG_{(1,1)}\leadsto M&=
		\begin{pmatrix}
			0&1&0&-1\\
			-1&0&1&0\\
			0&1&0&-1\\
			-1&0&1&0
		\end{pmatrix}
		&\sG_{(-1,-1)}\leadsto F&=
		\begin{pmatrix}
			0&1&0&1\\
			-1&0&-1&0\\
			0&-1&0&-1\\
			1&0&1&0
		\end{pmatrix}\\
		\sG_{(1,-1)}\leadsto L&=
		\begin{pmatrix}
			0&1&0&-1\\
			-1&0&-1&0\\
			0&-1&0&1\\
			-1&0&-1&0
		\end{pmatrix}
		&\sG_{(-1,1)}\leadsto N&=
		\begin{pmatrix}
			0&1&0&1\\
			-1&0&1&0\\
			0&1&0&1\\
			1&0&-1&0
		\end{pmatrix}.
	\end{aligned}
\end{equation}
These are the same spaces as the previous ones, but the reasoning at page \pageref{pg:subt_tilde} shows that there must be a difference. The subtlety is that we will choose an other notion of positivity, so that the space $\sN$ will be different. 

%\begin{figure}[ht]
%\begin{center}
%\begin{pspicture}(-2,-2)(2,2)
%  \psaxes[dotsep=1pt]{->}(0,0)(-1.9,-1.9)(1.9,1.9)
%  \psdots[dotscale=2](1,0)(1,1)(1,-1)(0,1)
%  \psdots[dotstyle=diamond,dotscale=2](-1,0)(-1,1)(-1,1)(0,-1)(-1,-1)
%  \rput(2.3,-0.3){$\sA^*_{\sH}$}
%\end{pspicture}
%\end{center}
%\caption{The root spaces}\label{fig:root}
%\end{figure}

%The result is on figure \ref{LabelFigHNxitLj}. % From file HNxitLj
\newcommand{\CaptionFigHNxitLj}{The root space}
\input{Fig_HNxitLj.pstricks}
 
Let us recall the aim of our new decomposition: we want to have $\sR_{\sH}\subset\sR$. For this purpose, the equation \eqref{eq:re_N_H} gives us a constraint on the choice of the positivity notion on $\sA^*$. First we must have $\sG_{(1,0)}\subset\sN$. The upper left $4\times 4$ corner of $\sN_{\sH}$ is spanned by $\sG_{(1,1)}-\sG_{(1,-1)}$. We complete our choice of $\sN$ with $\sG_{(0,1)}$. The underlying notion of positivity is that the element $\alpha(a,b)$ is positive in $\sA^*$ when $ (a>0) \LogOu(a=0 \LogEt b>0)$. 

The difference between decomposition and the previous one is the replacement of $N$ by $L\in\sG_{(1,-1)}$. Now\label{PgTablaIwa}
\begin{subequations}		\label{EqLeANEnDimAlg}
\begin{align}
	\sN&=\{W_i,V_j,M,L\}=\{ X_{1,0},X_{0,1},X_{1,1},X_{1,-1} \}\\
\sA&=\{ J_1, J_2\},
\end{align}
\end{subequations}
with the commutator table 
\begin{subequations}  \label{EqTableSOIwa}
\begin{align}
[V_i,W_j]&=\delta_{ij}M &[V_j,L]&=2W_j\\
[ J_1,W_j]&=W_j       &[ J_2,V_i]&=V_i\\
[ J_1,L]&=L           &[ J_2,L]&=-L\\
[ J_1,M]&=M           &[ J_2,M]&=M.
\end{align}
\end{subequations}
It is important to note that $W_{i}$, $J_{1}\in\sH$ and $J_{2}\in\sQ$. On the other hand, the vectors $V_i$ begin to appear in $\SO(2,3)$. The structure of $\SO(2,3)$ is thus slightly different to the one of $\SO(2,2)$, while the structures of $\SO(2,n)$ with $n\geq 3$ are more or less all the same. In the terminology of chapter \ref{ChapAdS}, theses special vectors arrive with $AdS_4$.

The following change of basis in $\sA$ reveals to be useful in some circumstances:
\begin{subequations}  \label{EqChmHJ}
\begin{align}
  	H_1&=J_1-J_2			&H_2&=J_1+J_2\\
	J_1&=\frac{ 1 }{2}(H_1+H_2)	&J_2&=-\frac{ 1 }{2}(H_1-H_2)
\end{align}
\end{subequations}
which leads to the table
\begin{subequations}		\label{TableSeconde}
\begin{align}
{}[V_i,W_j]&=\delta_{ij}M		&[V_j,L]&=2W_j\\
[H_1,V_i]&=-V_i				&[H_2,V_i]&=V_i\\
[H_1,W_i]&=W_i				&[H_2,W_i]&=W_i\\
[H_1,L]&=2L				&[H_2,M]&=2M.
\end{align}
\end{subequations}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{A companion : \texorpdfstring{$A\bar N$}{AN}}
%---------------------------------------------------------------------------------------------------------------------------

That Iwasawa immediately induces a new one by conjugation by the Cartan automorphism $\theta$. That new decomposition will play a central role when we will be interested in black holes on $\SO(2,n)/\SO(1,n)$. See chapter \ref{ChapAdS}, and more precisely the subsection \ref{SubSecGEneBHGrop} and the definition \ref{Singular}. 

The algebras $\sK$ and $\sA$ are not affected by $\theta$, but the algebra $\sN$ changes into a new algebra that we denote by $\bar\sN=\theta(\sN)$. The maximal parabolic algebra in that new Iwasawa decomposition is thus given by
\begin{equation}
	\begin{aligned}[]
		\sA&=\{ J_1,J_2 \}\\
		\bar\sN&=\{ X_{-1,0},X_{0,-1},X_{-1,1},X_{-1,-1} \}=\{ Y_i,X_i,N,F \}.
	\end{aligned}
\end{equation}
Indeed, using lemma \ref{LemSigmaThetaRootSpaces}, we have
\begin{equation}
	\begin{aligned}[]
		\theta(L)&\in\theta\big( \sG_{(1,-1)} \big)\in\sG_{(-1,1)}\leadsto N,\\
		\theta(M)&\in\theta\big( \sG_{(1,1)} \big)\in\sG_{(-1,-1)}\leadsto F,\\
		\theta(V_i)&\in\theta\big( \sG_{(0,1)} \big)\in\sG_{(0,-1)}\leadsto X_i,\\
		\theta(W_j)&\in\theta\big( \sG_{(1,0)} \big)\in\sG_{(-1,0)}\leadsto Y_j.
	\end{aligned}
\end{equation}


%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Second choice}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecANbarIwa}

Here, we show an other set of choices that can be done in order to get an Iwasawa decomposition. The one that we will create will not be compatible with the Iwasawa decomposition of $\SO(1,n)$.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Nilpotent part}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

We search the eigenvectors and eigenvalues of $\ad(H_p)$ under the form $E=\begin{pmatrix}A&B\\C&D\end{pmatrix}$ with $A\in M_{4\times 4}$, $B\in M_{(n-2)\times 4}$, $C\in M_{4\times (n-2)}$, $D\in M_{(n-2)\times (n-2)}$.  Remark that, thanks to \eqref{K_et_P}, the matrix $C$ is completely determined by $B$: 
\begin{equation}\label{C_de_B}
\left\{\begin{aligned}C^{i1}&=B^{1i},&C^{i2}&=B^{2i},\\
                    C^{i3}&=-B^{3i},&C^{i4}&=-B^{4i}.\end{aligned}\right.
\end{equation}
The equation to be solved is
\begin{equation}                                                                        \label{eq_gene}
(\ad H_p)E=\begin{pmatrix}[N_p,A]&N_pB\\-CN_p&0\end{pmatrix}\stackrel{!}{=}\lambda_pE,
\end{equation}
with the notation $\lambda_p=\lambda(H_p)$.


\subsubsection{Search for two non zero eigenvalues}

By ``two non zero eigenvalues'', we mean a $\lambda\in\iA^*$ such that $\lambda_{1}\neq 0$ and $\lambda_{2}\neq 0$. We immediately find $D=0$.

The next step is to determine $B$ by the condition $N_pB=\lambda_p B$. We change the range of the indices. Now, $a$, $b:1\rightarrow 4$, and $i$, $j:5\rightarrow n+2$ and a few computation give $\sum_{a}^{}N_p^{ca}B^{ai}=\lambda_pB^{ci}$ (with sum over $a$). Taking successively $c=1,2,3,4$ and taking into account $\lambda_p\neq 0$, we find:
\begin{subequations}\label{pour_B}
\begin{align}
B^{3i}&=\lambda_pB^{1i} \\
(-1)^pB^{4i}&=\lambda_pB^{2i}\\
\lambda_p&=\pm 1.
\end{align}
\end{subequations}
We can check that the equations obtained by $-CN_p=\lambda_pC$ are exactly the one that we can find directly using \eqref{C_de_B} and \eqref{pour_B}.

 Now, we determine $A$ by the condition $[N_p,A]=\lambda_pA$. We know that $A$ and $N_p$ are $4\times 4$ matrices. Again, we redefine the range of the indices: $a=1,2$ and $i=3,4$. Symmetry properties of $A$ are $A^{ai}=A^{ia}$, $A^{ij}=-A^{ji}$ and $A^{ab}=-A^{ba}$, so that
\[
 A=A^{12}(E_{12}-E_{21})+A^{ai}(E_{ai}+E_{ia})+A^{34}(E_{34}-E_{43})
\]
Using equation \eqref{mtr_A}, a quite tedious (but direct) computation give the following for $[N_p,A]$:
\begin{equation}\label{grosse_A}
\begin{pmatrix}
0&A^{23}-(-1)^p A^{14}&0&A^{34}-(-1)^p A^{12}\\
-A^{23}+(-1)^p A^{14}&0&A^{12}-(-1)^p A^{34}&0\\
0&A^{12}-(-1)^p A^{34}&0&A^{14}-(-1)^p A^{23}\\
A^{34}-(-1)^p A^{12}&0&A^{14}-(-1)^p A^{23}&0
\end{pmatrix}
\end{equation}
which has to be equated to $\lambda_pA$. We immediately have $A^{13}=A^{24}=A^{31}=A^{42}=0$. The others conditions are:
\begin{subequations}\label{pour_A}
\begin{align}
\lambda_pA^{12}=&A^{23}-(-1)^p A^{14}\\
\lambda_pA^{14}=&A^{34}-(-1)^p A^{12}
\end{align}
\end{subequations}
Since we are in the case $\lambda_p\neq 0$, using the fact that $\lambda_p=\pm 1$, we easily find $A=0$. Now, we define $\lambda_{\pm\pm}\in\iA^*$ by
\begin{eqnarray}
\begin{aligned}
\lambda_{++}(H_1)&=1 &\lambda_{++}(H_2)&=1,\\
\lambda_{+-}(H_1)&=1 &\lambda_{+-}(H_2)&=-1,\\
\lambda_{-+}(H_1)&=-1 &\lambda_{-+}(H_2)&=1,\\
\lambda_{--}(H_1)&=-1 &\lambda_{--}(H_2)&=-1.
\end{aligned}
\end{eqnarray}
Root spaces are (with $i:5\rightarrow n+2$):
\begin{eqnarray}
\begin{aligned}
\lambda_{++}&\leadsto V_i=E_{3i}+E_{1i}+E_{i1}-E_{i3},\\
\lambda_{-+}&\leadsto W_i=E_{4i}+E_{2i}-E_{i4}+E_{i2},\\
\lambda_{--}&\leadsto X_i=E_{3i}-E_{1i}-E_{i1}-E_{i3},\\
\lambda_{+-}&\leadsto Y_i=E_{4i}-E_{2i}-E_{i4}-E_{i2}.
\end{aligned}
\end{eqnarray}

\subsubsection{Search for eigenvalues with one zero}

We denote by $\lambda(a,b)$ the element of $\iA^*$ defined by $\lambda(x_1,x_2)(H_i)=x_i$. Equations \eqref{grosse_A} and \eqref{pour_A} with for example $a=0$ and $b\neq 0$ give $\lambda_2=\pm 2$. Serious computation give:
\begin{subequations}	\label{EqsTableRacinesSOdeuxn}
\begin{equation}
\lambda(0,-2)\leadsto F=
\begin{pmatrix}
                0&1&0&1\\
		-1&0&-1&0\\
		0&-1&0&-1\\
		1&0&1&0
              \end{pmatrix},\quad
\lambda(2,0)\leadsto N
=\begin{pmatrix}
                0&1&0&1\\
		-1&0&1&0\\
		0&1&0&1\\
		1&0&-1&0
              \end{pmatrix}
\end{equation}
\begin{equation}
\lambda(0,2)\leadsto M
=\begin{pmatrix}
                0&1&0&-1\\
		-1&0&1&0\\
		0&1&0&-1\\
		-1&0&1&0
              \end{pmatrix},\quad
\lambda(-2,0)\leadsto L=
\begin{pmatrix}
                0&1&0&-1\\
		-1&0&-1&0\\
		0&-1&0&1\\
		-1&0&-1&0
              \end{pmatrix}.
\end{equation}
\end{subequations}
In these expressions, we only wrote the upper left part of the matrices which are zero everywhere else.

\subsubsection{Two vanishing eigenvalues}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now search for a matrix $E$ of $\so(2,n)$ such that $(\ad H_p)E=0$ for $p=1,2$. Taking a look at \eqref{eq_gene}, we see that $D$ has no more constraints (apart the usual symmetries). The equation \eqref{grosse_A} gives us $A^{12}=A^{23}=A^{14}=A^{34}=0$, but $A^{13}$, $A^{24}$,$A^{31}$,$A^{42}$ are free. Therefore we can write:
\begin{equation}
  \mG_{\lambda(0,0)}=\{x(E_{13}+E_{31})+y(E_{42}+E_{24})+
    \left(\begin{array}{c|c}
    0&0\\
    \hline
    0&D
    \end{array}\right)
  \}.
\end{equation}
Remark that $\mG_{\lambda(0,0)}\cap\iP$ is spanned by matrices of the form
\[
\begin{pmatrix}
	0&0&x&0\\
	0&0&0&y\\
	x&0&0&0\\
	0&y&0&0
\end{pmatrix},
\]
so that $\mG_{\lambda(0,0)}\cap\iP=\iA$. This is a simple consequence of the very definition of $\iA$ as maximal abelian subalgebra of $\iP$.

\subsubsection{Choice of positivity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We are now able to write down the component $\iN$. We just have to ``elect''\ some $\mG_{\lambda_{(a,b)}}$ with a notion of positivity. Our choice is:
 \begin{equation}
     \iN=\{V_i,W_i,M,N\},\\
 \end{equation}
with $i$, $j:5\to n+2$. A basis of $\iK$ is given by $K_{rs}=E_{rs}-E_{sr}$, and $K_a=E_{12}-E_{21}$ with $\dpt{r,s}{3}{n+2}$.  The commutator with $\iA$ are:
\begin{equation}
\begin{split}
[H_p,K_a]&=-(-1)^p(E_{41}+E_{14})+E_{32}+E_{23}\\
[H_p,K_{rs}]&=\delta_{r3}(E_{1s}+E_{s1})-\delta_{s3}(E_{1r}+E_{r1})\\
           &\quad+(-1)^p(\delta_{4r}(E_{2s}+E_{s2})-\delta_{4s}(E_{2r}+E_{r2}))
\end{split}
\end{equation}

In the decomposition $\mG=\iA\oplus\iN\oplus\iK$, the matrix $E_{32}+E_{23}$ is obtained by
\[
   \frac{1}{2}(\lambda(0,2)+\lambda(2,0))=\begin{pmatrix}
0 & 1 & 0 & 0 \\
-1 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0
\end{pmatrix}
\]
where the upper left corner $\begin{pmatrix}0&1\\-1&0\end{pmatrix}$ can be removed by use of $K_a$:
\[
               E_{32}+E_{23}=\frac{1}{2}(\lambda(0,2)+\lambda(2,0))-K_a
\]
In the same way,
\[
             E_{41}+E_{14}=\frac{1}{2}(\lambda(2,0)-\lambda(0,2))-K_{34}
\]
We also have:
\begin{equation}
\begin{split}
         E_{1i}-E_{i1}&=V_i-K_{3i}\\
         E_{2i}+E_{i2}&=W_i-K_{4i}.
\end{split}
\end{equation} 
Let us summarize the result obtained.  The Iwasawa decomposition of $\SOdn$ is given by
\begin{subequations}\label{eq:Iwasawa_explicite}
\begin{align}
  \mA&=\{H_1,H_2\}\\
  \mN&=\{V_i,W_j,M,N\}\\
  \mK&=\{\begin{pmatrix}a&0\\0&B\end{pmatrix}\}.
\end{align}
\end{subequations}
with $i$, $j:5\to n+2$, $a\in M_{2\times 2}$, $B\in M_{n\times n}$ skew-symmetric, and
$H_p=E_{13}+E_{31}+(-1)^p(E_{24}+E_{42})$.

\label{pg:exp_AN}
We are going to write down a general element of $AN$ (forgetting for the moment $W_i$ and $V_j$). Since $[H_1,H_2]=[M,N]=0$, the only product to be considered is $e^{tH_1}e^{uH_2}e^{aM}e^{bM}$.  Our first task is to exponentiate the matrices of $\mA$ and $\mN$. The first is quite easy: since $H_p^2=\mtu$, 
\[
  e^{tH_p}=\sum_k\frac{t^k}{k!}\left(  \mtu_{\text{if $k$ is even}}, {H_p}\,_{\text{if $k$ is odd}}  \right)
            =(\cosh t)\mtu+(\sinh t)H_p;
\]
then
\begin{equation}
e^{tH_1}e^{uH_2}=\begin{pmatrix}
  \cosh\xi &    0      & \sinh\xi &    0  \\
    0       & \cosh\eta &    0      & \sinh\eta \\
  \sinh\xi &    0      & \cosh\xi &    0     \\
    0       & \sinh\eta &    0      & \cosh\eta \\
                 \end{pmatrix}
\end{equation}
with the change of variable 
\begin{subequations}\label{eq:chm_xi_eta}
\begin{align}   
   \xi &=t+u,\\
   \eta &=u-t.
\end{align}   
\end{subequations}

For the second, we remark that $e^{aM}=\mtu+aM$ and $e^{bN}=\mtu+bN$ because $M^2=N^2=0$. The result is:
\begin{equation}
  e^{aM}e^{bN}=\begin{pmatrix}
                 1-2ab & b+a & 2ab   & b-a \\
 		 -b-a  & 1   & b+a   &  0   \\
		 -2ab  & b+a & 1+2ab &  b-a \\
		  b-a  &  0  &  a-b  &  1  
	      \end{pmatrix}.
\end{equation}
Remark that, by construction, matrices of $\sN$ are nilpotent. But a sum of nilpotent matrices is nilpotent and exponential on nilpotent matrices is easy to compute. Hence there are no obstructions to compute an explicit general matrix of $AN$, this is done at the page \pageref{PgExplAN}.

Now, we compute some commutators of the different matrices that we had seen. 
\begin{subequations}
\begin{align}
[V_i,W_j]&=\delta_{ij}A_{(0,2)}\\
[V_i,X_j]&=2\delta_{ij}A_{(0,0)}(-1,0)+2(E_{ij}-E_{ji})\\
[X_i,Y_j]&=\delta_{ij}\lambda(0,-2)\\
[W_i,X_j]&=\delta_{ij}\lambda(-2,0)\\
[V_i,Y_j]&=\delta_{ij}\lambda(2,0)\\
[W_i,Y_j]&=-2\delta_{ij}A_{(0,0)}(0,1)\\
[V_i,V_j]&=0\\
[\lambda(0,2),\lambda(0,-2)]&=4A_{(0,0)}(-1,-1)\\
[\lambda(2,0),\lambda(-2,0)]&=4A_{(0,0)}(-1,1)
\end{align}
\end{subequations}
\begin{subequations}
\begin{equation}
\begin{split}
         [H_p,K_{rs}]&=\delta_{r3}(V_s-K_{3s})-\delta_{s3}(V_r-K_{3r})\\
 	             &\quad+(-1)^p[\delta_{4r}(W_s-K_{4s})-\delta_{4s}(W_r-K_{4r})],
\end{split}
\end{equation}
\begin{equation}
         [H_p,K_a]=(-1)^{p+1}(\frac{1}{2}(N-M)-K_{34})+\frac{1}{2}(N+M)-K_a.
\end{equation}
\end{subequations}
The non-zero commutators in $\mA\oplus\mN$ are:
\begin{subequations}		\label{TabelPrem}
\begin{align}
[V_i,W_j]&=\delta_{ij}M &[W_i,N]&=-2V_i\\
[H_1,V_i]&=-V_i          &[H_2,V_i]&=V_i\\
[H_1,N]&=2N          &[H_2,M]&=2M\\
[H_1,W_i]&=W_i   &[H_2,W_i]&=W_i.
\end{align}
\end{subequations}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Application: an isomorphism of Lie algebra}
%---------------------------------------------------------------------------------------------------------------------------
\label{sssIsomsoslplussl}

Root spaces are a powerful tool in proving isomorphisms of Lie algebras. Comparing the tables of $\gsl(2,\eR)$ and $\so(2,2)$ given in \ref{subeq_rootSLR} and \ref{EqsTableRacinesSOdeuxn}, it is easy to prove the following.

\begin{proposition}
We have
\begin{equation}
	\so(2,2)=\gsl(2,\eR)\oplus\gsl(2,\eR)
\end{equation}
as isomorphism of Lie algebras.
\end{proposition}

\begin{proof}
The algebra $\so(2,2)$ has the following root spaces:
\begin{align*}
\lambda(0,2)&&\lambda(2,0)\\
\lambda(0,-2)&&\lambda(-2,0)
\end{align*}
with $\iA=\{ H_1,H_2 \}$. So, by construction the vector space $\lG_1$ generated by $\{ H_1,\lambda(2,0),\lambda(-2,0) \}$ is a Lie subalgebra of $\so(2,2)$ with roots equal to $2$ and $-2$; the corresponding root spaces being one dimensional. Looking on the root spaces of $\gsl(2,\eR)$, we see that they are exactly the same. Thus $\lG_1=\gsl(2,\eR)$. The same is true for $\lG_2=\{ H_2,\lambda(0,2),\lambda(0,-2)  \}$ and we also have $[\lG_1,\lG_2]=0$ because of property \ref{enucii} of proposition \ref{PropPropRacincesReelles}. Thus we have
\[ 
	\so(2,2)=\lG_1\oplus\lG_2=\gsl(2,\eR)\oplus\gsl(2,\eR),
\]
as announced.
\end{proof}

\section{Iwasawa decomposition for \texorpdfstring{$\SO(1,n)$}{SO1n}}		\label{SecIwaSOunn}
%-----------------------------------------------
\index{Iwasawa!decomposition!of $\SO(1,n)$}

We saw in the section \ref{SecSymeStructAdS} that the quotient $\SO(2,n)/\SO(1,n)$ has a particular importance. Hence, we will work out the Iwasawa decompositions of these groups imposing certain compatibility conditions. We already build the Cartan involution $\theta$ in such a way that $[\sigma,\theta]=0$.

In this section, \( \sH\) is \( \SO(1,n)\) and we are searching for an Iwasawa decomposition \( \sA_{\sH}\oplus\sN_{\sH}\oplus\sK_{\sH}\). 

We want our Cartan involution $\dpt{\theta}{\sG}{\sG}$, $\theta(X)=-X^t$ and to be such that $[\theta,\sigma]=0$ with $\dpt{\sigma}{\sG}{\sG}$ given by $\sigma=id|_{\sH}\oplus(-id)|_{\sQ}$, see theorem \ref{tho:sigma_theta}.\label{pg:calcul_sigma_theta}

Consider a general matrix $h$ in $\sH$ (from \eqref{eq:gene_H}); we have
\[
  (\sigma\theta)(h)=-\sigma
  \begin{pmatrix}
  0&0&&0\\
  0&0&&v^t\\
  0&v&&-B
  \end{pmatrix}=
  \begin{pmatrix}
  0&0&&0\\
  0&0&&-v^t\\
  0&-v&&B
  \end{pmatrix},
\]
while
\[
  \theta\sigma(h)=\theta(h)=-
  \begin{pmatrix}
  0&0&&0\\
  0&0&&v^t\\
  0&v&&-B
  \end{pmatrix},
\] 
then $[\sigma,\theta]|_{\sH}=0$. The same computation with a matrix in $\sQ$ (see \eqref{EqGeneRedQ}) gives $[\sigma,\theta]|_{\sQ}=0$.

Now we show that $\theta$ descent to a Cartan involution on $H$. It is clear that the restriction of $\theta$ is an involutive automorphism of $H$. Lemma \ref{lem:Killing_ss_descent} assures us that the restriction of the Killing form of $G$ to $H$ is the Killing form of $H$, so that the condition of positivity of $B_{\theta}$ holds on $H$ as well as on $G$. Last, $\theta$ leaves $\sH$ invariant. Indeed suppose that  $\theta X_{\sH}=X'_{\sH}+X_{\sQ}\in\sH\oplus\sQ$. Then $\sigma\theta X_{\sH}=h'-q$ and $\theta\sigma h=h'+q$; since $[\theta,\sigma]=0$, we have $q=0$.

All that proves that we can use the same Cartan involution on $G$ as well as on $H$. Since $\theta=id|_{\iK}\oplus(-id)|_{\iP}$, it is clear that 
\begin{align}
   \iKH&=\iK\cap\sH,
   &\iPH&=\iP\cap\sH
\end{align}
is the Cartan decomposition of $\sH$. We can write explicit matrices as
\begin{equation}
\iKH=so(n)\leadsto
\begin{pmatrix}
  0&0&\cdots  \\
  0&0&\cdots  \\
  \vdots&\vdots& B
\end{pmatrix},
\end{equation}
where $B$ is skew-symmetric, and
\begin{equation}
\iPH\leadsto
\begin{pmatrix}
  0&0&\cdots  \\
  0&0& u^t  \\
  \vdots&u& 0
\end{pmatrix}
\end{equation}
where $u\in\eM_{n\times 1}(\eR)$ is a line. One remark that there are no two-dimensional subalgebra of $\iP_{\sH}$. So $\iA_{\sH}$ reduces to the choice of any element $J_1\in\iP_{\sH}$.  A positivity notion is easy to find: the form $\omega\in\iAH^*$ such that $\omega(J_1)=1$ is positive while $-\omega$ is negative. We choose
\[
  J_1=\begin{pmatrix}&0\\0&0&0&1\\&0\\&1\end{pmatrix}.
\]
The computation of $\iNH=\{X\in\sH\tq (\ad J_1)X=X\}$ yields the following :
\begin{equation}\label{eq:re_N_H}
\iNH\leadsto
\begin{pmatrix}
     \begin{pmatrix} &&0&0\\&&a&0\\0&a&0&-a\\0&0&a&0\end{pmatrix}
     & \begin{pmatrix}\cdots&0&\cdots\\\leftarrow&\overline{v}&\rightarrow\\
        \cdots&0&\cdots\\\leftarrow&\overline{v}&\rightarrow\end{pmatrix}\\
     \begin{pmatrix}\vdots&\uparrow&\vdots&\uparrow\\
                    0& \overline{v}&0&-\overline{v}\\
		    \vdots&\downarrow& \vdots&\downarrow \end{pmatrix}
     &0		    
\end{pmatrix}.
\end{equation}
Finally, we consider the algebra $\iRH=\iAH\oplus\iNH$.


\section{Explicit choices in \texorpdfstring{$\so(2,l-1)$}{so2l-1}}
%\label{app_calc}
%+++++++++++++++++++++++++++++++++++++++++++

In the case of $AdS_4$ the matrices are $5\times 5$. We will write them down, but the general form are entirely similar. Our choice of Iwasawa decomposition is
\begin{subequations}
\begin{align}
\sN&=\{W_i,V_j,M,L\}\\
\sA&=\{ J_1, J_2\}.
\end{align}
\end{subequations}


The basis of $\sodn$ in which we want to decompose all our elements is the root space one:
\begin{equation}
\sG=\Span\{J_1,q_1,X,Y,V,W,M,N,F,L\}
\end{equation}
note in particular that $\sG_{(0,0)}=\Span\{J_1,q_1\}$ and $W,J_1\in\sH$.
\begin{equation}
\frac{1}{2}(W-Y)=
\begin{pmatrix}
&0\\
0&0&0&0&1\\
&0\\
&0\\
&1
\end{pmatrix},
\qquad
\frac{1}{2}(V+X)=
\begin{pmatrix}
&&&&0\\
&&&&0\\
&&&&1\\
&&&&0\\
0&0&-1&0&0
\end{pmatrix},
\end{equation}



\begin{equation}
\frac{1}{2}(W+Y)=
\begin{pmatrix}
&&&&0\\
&&&&0\\
&&&&0\\
&&&&1\\
0&0&0&-1&0
\end{pmatrix},
\qquad
q_3=\frac{1}{2}(V-X)=
\begin{pmatrix}
0&0&0&0&1\\
0\\
0\\
0\\
1
\end{pmatrix}.
\end{equation}


\subsection{Decompositions and commutators for \texorpdfstring{$\sQ$}{Q}}
%-------------------------------------------------------------------------

First, the root space decomposition of the basis $\{q_i\}$ of $\sQ$:
\begin{subequations}
\begin{align} 
	q_0	&=\frac{1}{ 4 }(M+N+L+F)				&	q_2	&=\frac{1}{ 4 }(N+F-M-L)\\
		&=\frac{1}{ 4 }(X_{11}+X_{1,-1}+X_{-1,1}+X_{-1,-1})	&		&=\frac{1}{ 4 }(X_{-1,1}+X_{-1,-1}-X_{11}-X_{1,-1})\\
	q_1	&=q_1=J_2						&	q_3	&=\frac{1}{2}(V-X)\\
		&							&		&=\frac{ 1 }{2}(X_{01}-X_{0,-1}).
\end{align}
\end{subequations}
The commutators:
\begin{subequations}
\begin{align} 
[q_0,q_1]&=\us{4}(L+F-M-N) &[q_1,q_2]&=\us{4}(L+N-F-M)\\\
&=\frac{1}{ 4 }(X_{1,-1}+X_{-1,-1}-X_{11}-X_{-1,1})	&&=\frac{1}{ 4 }(X_{1,-1}+X_{-1,1}-X_{-1,1}-X_{11})\\
[q_0,q_2]&=-J_1            &[q_1,q_3]&=\frac{1}{2}(V+X)\\
[q_0,q_3]&=\frac{1}{2}(Y-W)      &[q_2,q_3]&=\frac{1}{2}(W+Y)
\end{align}
\end{subequations}

\subsection{Commutators between root spaces and \texorpdfstring{$\sQ$}{Q}}

\begin{subequations}
\begin{align}
[q_0,J_1]	&=\frac{1}{4}(N+F-M-L)					&[q_1,J_1]&=0	&[q_2,J_1]&=q_0\\
		&=\frac{1}{ 4 }(X_{-1,1}+X_{-1,-1}-X_{11}-X_{1,-1})					\\
		&=q_2\\
[q_0,q_1]	&=\frac{1}{4}(L+F-M-N)&       				&    &[q_2,q_1]&=\us{4}(F+M-L-N)\\
		&=\frac{ 1 }{ 4 }(X_{1,-1}+X_{-1,-1}-X_{11}-X_{-1,1})\\
[q_0,X]  &=\frac{1}{2}(W-Y)          &[q_1,X]  &=-X &[q_2,X]&=-\frac{1}{2}(W+Y)\\
[q_0,Y]  &=\frac{1}{2}(X-V)          &[q_1,Y]  &=0  &[q_2,Y]&=\frac{1}{2}(V-X)\\
[q_0,V]  &=\frac{1}{2}(Y-W)          &[q_1,V]  &=V  &[q_2,V]&=\frac{1}{2}(W+Y)\\
[q_0,W]  &=\frac{1}{2}(V-X)          &[q_1,W]  &=0  &[q_2,W]&=\frac{1}{2}(V+X)\\
[q_0,M]  &=q_1+J_1             &[q_1,M]  &=M  &[q_2,M]&=q_1+J_1\\
[q_0,N]  &=q_1-J_1             &[q_1,N]  &=N  &[q_2,N]&=-q_1+J_1\\
[q_0,L]  &=-q_1+J_1            &[q_1,L]  &=-L &[q_2,L]&=-q_1+J_1\\
[q_0,F]  &=-q_1-J_1            &[q_1,F]  &=-F &[q_2,F]&=q_1+J_1
\end{align}
\end{subequations}

\begin{subequations}
\begin{align}
 [q_3,J_1]&=0           &[q_3,M]&=W   &[q_3,V]&=-q_1\\
 [q_3,q_1]&=-\frac{1}{2}(V+X) &[q_3,N]&=-Y  &[q_3,W]&=\frac{1}{2}(M+L)\\
          &             &[q_3,L]&=W   &[q_3,X]&=-q_1\\
          &             &[q_3,F]&=-Y  &[q_3,Y]&=-\frac{1}{2}(N+F)
\end{align}
\end{subequations}

\subsection{Commutators in the root spaces}

\begin{subequations}
\begin{align}
[J_1,q_1]&=0\\
[J_1,X]&=0&[q_1,X]&=-X\\
[J_1,Y]&=-Y&[q_1,Y]&=0&[X,Y]&=F\\
[J_1,V]&=0&[q_1,V]&=V&[X,V]&=2q_1\\
[J_1,W]&=W&[q_1,W]&=0&[X,W]&=-L\\
[J_1,M]&=M&[q_1,M]&=M&[X,M]&=-2W\\
[J_1,N]&=-N&[q_1,N]&=N&[X,N]&=2Y\\
[J_1,F]&=-F&[q_1,F]&=-F&[X,F]&=0\\
[J_1,L]&=L&[q_1,L]&=-L&[X,L]&=0
\end{align}
\end{subequations}

\begin{subequations}
\begin{align}
[V,W]&=M\\
[V,M]&=0&[W,M]&=0\\
[V,N]&=0&[W,N]&=-2V&[M,N]&=0\\
[V,F]&=-2Y&[W,F]&=2X&[M,F]&=-4q_1-4J_1\\
[V,L]&=2W&[W,L]&=0&[M,L]&=0
\end{align}
\end{subequations}


\begin{subequations}
\begin{align}
[N,F]&=0\\
[N,L]&=-4q_1+4J_1&[F,L]&=0
\end{align}
\end{subequations}

\subsection{Killing form}
%++++++++++++++++++++
The adopted definition is $B(x,y)=\tr(\ad x\circ\ad y)$ with no one half or such coefficient.
\begin{equation}
\begin{aligned}
B(J_1,q_1)&=0	&B(V,X)&=-12\\
B(J_1,J_1)&=6	&B(N,L)&=-24\\
B(W,Y)&=-12	&B(M,F)&=-24
\end{aligned}
\end{equation}
Some easy computations show that for $g\in \SO(2)$,
\[
\begin{split}
dL_gq_0&=
\begin{pmatrix}
-\sin u&\cos u\\
-\cos u&\sin u
\end{pmatrix},
\quad
dL_g q_1=
\begin{pmatrix}
0&0&\cos u\\
0&&-\sin u\\
1
\end{pmatrix}\\
dL_g H_1&=
\begin{pmatrix}
0&0&\sin u\\
0&&\cos u\\
0&1
\end{pmatrix}\\
dR_g J_1&=
\begin{pmatrix}
0\\
0&0&0&1\\
0\\
-\sin u&\cos u
\end{pmatrix},
\quad
dR_g J_2=
\begin{pmatrix}
0&0&1\\
0\\
\cos u&\sin u
\end{pmatrix}
\end{split}
\]
So
\begin{subequations}
\begin{align}
dR_g J_1&=-\sin u\, dL_g q_2+\cos u\, dL_g H_2\\
dR_g J_2&=\sin u\, dL_g H_1+\cos u\, dL_g q_1.
\end{align}
\end{subequations}
and
\begin{subequations}
\begin{align}
  B_{[g]}(J_1^*,J_1^*)&=6\sin^2 u\\
B_{[g]}(J_2^*,J_2^*)&=6\cos^2 u.
\end{align}
\end{subequations}


\section{Iwasawa decomposition for \texorpdfstring{$\gsl(2,\eC)$}{sl2C}}		\label{SecIwasldeuxC}
%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\index{Iwasawa!decomposition!of $\SL(2,\eC)$}

Matrices of $\gsl(2,\eC)$ are acting on $\eC^2$ as 
\[ 
\begin{split}
  \begin{pmatrix}
\alpha&\beta\\\gamma&-\alpha
\end{pmatrix}&
\begin{pmatrix}
a+bi\\c+di
\end{pmatrix}\\
&=
\begin{pmatrix}
(\alpha_1a-\alpha_2b+\beta_1c-\beta_2d)+i(\alpha_2a+\alpha_1b+\beta_2c+\beta_1d)\\
(\gamma_1a-\gamma_2b-\alpha_1c+\alpha_2d)+i(\gamma_2a+\gamma_1b-\alpha_2c-\alpha_1d)
\end{pmatrix}
\end{split}
\]
if $\alpha=\alpha_1+i\alpha_2$.  Our aim is to embed $\SL(2,\eC)$ in $\SP(2,\eR)$ (see sections \ref{SecSympleGp} and \ref{SecDirADs}), so that we want a four dimensional realization of $\gsl(2,\eC)$. It is easy to rewrite the previous action under the form of $\begin{pmatrix}
\alpha&\beta\\\gamma&-\alpha
\end{pmatrix}$ acting of the vertical four component vector $(a,b,c,d)$. The result is that a general matrix of $\gsl(2,\eC)$ reads
\begin{equation}		\label{EqGenslMatr}
\gsl(2,\eC)\leadsto
\begin{pmatrix}
\boxed{
\begin{array}{cc}
\alpha_1&-\alpha_2\\
\alpha_2&\alpha_1
\end{array}
}&
\begin{array}{cc}
\beta_1&-\beta_2\\
\beta_2&\beta_1
\end{array}\\
\begin{array}{cc}
\gamma_1&-\gamma_2\\
\gamma_2&\gamma_1
\end{array}&
\boxed{
\begin{array}{cc}
-\alpha_1&\alpha_2\\
-\alpha_2&-\alpha_1
\end{array}
}
\end{pmatrix}.
\end{equation}
The boxes are drawn for visual convenience.  Using the Cartan involution $\theta(X)=-X^t$, we find the following Cartan decomposition:
\begin{equation}
\begin{split}
\iK_{\gsl(2,\eC)}&\leadsto
\begin{pmatrix}
\boxed{
\begin{array}{cc}
0&-\alpha_2\\
\alpha_2&0
\end{array}
}&
\begin{array}{cc}
\beta_1&-\beta_2\\
\beta_2&\beta_1
\end{array}\\
\begin{array}{cc}
-\beta_1&-\beta_2\\
\beta_2&-\beta_1
\end{array}&
\boxed{
\begin{array}{cc}
0&\alpha_2\\
-\alpha_2&0
\end{array}
}
\end{pmatrix},\\
\iP_{\gsl(2,\eC)}&\leadsto
\begin{pmatrix}
\boxed{
\begin{array}{cc}
\alpha_1&0\\
0&\alpha_1
\end{array}
}&
\begin{array}{cc}
\beta_1&-\beta_2\\
\beta_2&\beta_1
\end{array}\\
\begin{array}{cc}
-\beta_1&-\beta_2\\
\beta_2&-\beta_1
\end{array}&
\boxed{
\begin{array}{cc}
0&\alpha_2\\
-\alpha_2&0
\end{array}
}
\end{pmatrix}.
\end{split}
\end{equation}
We have $\dim\iP_{\gsl(2,\eC)}=3$ and $\dim\iP_{\gsl(2,\eC)}=3$. A maximal abelian subalgebra of $\iP_{\gsl(2,\eC)}$ is the one dimensional algebra generated by
\[ 
  A_1=
\begin{pmatrix}
1\\&1\\&&-1\\&&&-1
\end{pmatrix}.
\]
The corresponding root spaces are
\begin{itemize}
\item $\gsl(2,\eC)_0$:
\[ 
  I_1=
\begin{pmatrix}
1\\&1\\&&-1\\&&&-1
\end{pmatrix},\quad
I_2=
\begin{pmatrix}
0&-1\\
1&0\\
&&0&1\\
&&-1&0
\end{pmatrix}
\]
\item $\gsl(2,\eC)_2$:
\[ 
  D_1=\begin{pmatrix}
&&1&0\\
&&0&1\\
0&0\\
0&0
\end{pmatrix},\quad
D_2=
\begin{pmatrix}
&&0&-1\\
&&1&0\\
0&0&\\
0&0&
\end{pmatrix}
\]
\item $\gsl(2,\eC)_{-2}$
\[ 
  C_1=\begin{pmatrix}
&&0&0\\
&&0&0\\
1&0\\
0&1
\end{pmatrix},\quad
C_2=\begin{pmatrix}
&&0&0\\
&&0&0\\
0&-1\\
1&0
\end{pmatrix}.
\]
\end{itemize}
It is natural to choose $\gsl(2,\eC)_2$ as positive root space system. In this case, $\iN_{\gsl(2,\eC)}=\{ D_1,D_2 \}$, $\iA_{\gsl(2,\eC)}=\{ I_1 \}$ and the table of $\iA\oplus\iN$ is
\begin{align}
[I_1,D_1]&=2D_1&		[D_1,D_2]&=0\\
[I_1,D_2]&=2D_2&		
\end{align}

    The full table is
\begin{align}
[I_1,D_1]&=2D_1&	[I_2,D_1]&=2D_2&	[D_1,D_2]&=0\\
[I_1,D_2]&=2D_2&	[I_2,D_2]&=-2D_1&	[D_1,C_1]&=I_1\\
[I_1,C_1]&=-2C_1&	[I_2,C_1]&=-2C_2&	[D_1,C_2]&=I_2\\
[I_1,C_2]&=-2C_2&	[I_2,C_2]&=2C_1&	[D_2,C_1]&=I_2\\
	&	&		&     &		[D_2,C_2]&=-I_1.
\end{align}
\section{Symplectic group}		\label{SecSympleGp}
%+++++++++++++++++++++++++

\subsection{Iwasawa decomposition}
%-----------------------------
\index{Iwasawa!decomposition!of $\SP(2,\eR)$}

A simple computation shows that $4\times 4$ matrices subject to $A^t\Omega+\Omega A=0$ are given by
\[ 
  \begin{pmatrix}
A&B\\
C&-A^t
\end{pmatrix}
\]
where $A$ is any $2\times 2$ matrix while $B$ and $C$ are symmetric matrices. Looking at general form \eqref{EqGenslMatr}, we see that the operation to invert the two last column and then to invert the two last lines provides a homomorphism $\phi\colon \gsl(2,\eC)\to \gsp(2,\eR)$. The aim is now to build an Iwasawa decomposition of $\gsp(2,\eR)$ which ``contains'' the one of $\gsl(2,\eC)$.

Using the Cartan involution $\theta(X)=-X^t$, we find the Cartan decomposition
\begin{align}
\iK_{\gsp(2,\eR)}&\leadsto
\begin{pmatrix}
A&S\\-S&A
\end{pmatrix},
&\iP_{\gsp(2,\eR)}&\leadsto
\begin{pmatrix}
S&S'\\S'&-S
\end{pmatrix}
\end{align}
where $S$ and $S'$ are any symmetric matrices while $A$ is a skew-symmetric one. We have $\dim\iK_{\gsp(2,\eR)}=4$ and $\dim\iP_{\gsp(2,\eR)}=6$. It turns out that $\phi(\iK_{\gsl(2,\eC)})\subset\iK_{\gsp(2,\eR)}$ and $\phi(\iP_{\gsl(2,\eC)})\subset \iP_{\gsp(2,\eR)}$. A maximal abelian subalgebra of $\iP_{\gsp(2,\eR)}$ is spanned by the matrices $A'_1$ and $A'_2$ listed below and the corresponding root spaces are:
\begin{itemize}
\item $\gsp(2,\eR)_{(0,0)}$:
\[ 
  A'_1=
\begin{pmatrix}
1&0\\
0&1\\
&&-1&0\\
&&0&-1
\end{pmatrix},
\quad
A'_2=
\begin{pmatrix}
0&1\\
1&0\\
&&0&-1\\
&&-1&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(0,2)}$:
\[ 
 X'= \begin{pmatrix}
1&-1&\\
1&-1&\\
&&-1&-1\\
&&1&-1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(0,-2)}$:
\[ 
 V'= \begin{pmatrix}
1&1\\
-1&-1\\
&&-1&1\\
&&-1&1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(2,0)}$:
\[ 
 W'= \begin{pmatrix}
&&1&0\\
&&0&-1\\
0&0\\0&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(2,2)}$:
\[ 
  L'=
\begin{pmatrix}
&&1&1\\
&&1&1\\
0&0\\0&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(2,-2)}$:
\[ 
  M'=
\begin{pmatrix}
&&1&-1\\
&&-1&1\\
0&0\\0&0
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(-2,0)}$
\[ 
Y'=
\begin{pmatrix}
&&0&0\\&&0&0\\
1&0\\0&-1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(-2,2)}$:
\[ 
  N'=\begin{pmatrix}
&&0&0\\&&0&0\\
1&-1\\
-1&1
\end{pmatrix}
\]
\item $\gsp(2,\eR)_{(-2,-2)}$:
\[ 
  F'=\begin{pmatrix}
&&0&0\\
&&0&0\\
1&1\\
1&1
\end{pmatrix}
\]
\end{itemize}
It is important to notice how do the root spaces of $\gsl(2,\eC)$ embed:
\begin{align}
\phi(I_1)&=A'_1	&\phi(I_2)&=\frac{ V'-X' }{ 2 }\\
\phi(D_1)&=\frac{ L'-M' }{2}	&\phi(D_2)&=-W'\\
\phi(C_1)&=\frac{ F'-N' }{2}	&\phi(C_2)&=Y'. 
\end{align}
So $\iN_{\gsp(2,\eR)}$ must at least contain the elements $L'$, $M'$ and $W'$. We complete the notion of positivity by $V'$. The Iwasawa algebra reads
\[ 
\begin{split}
\iA_{\gsp(2,\eR)}&=\{ B_1,B_2 \}\\
\iN_{\gsp(2,\eR)}&=\{ L',M',W',V' \}
\end{split}  
\]
with
\begin{align*}
[L',V']&=-4W'	&[W',V']&=-2M'\\
[B_1',L']&=2L'	&[B'_2,M']&=2M'\\
[B_1',W']&=W'	&[B_2',W']&=W'\\
[B_1',V']&=-V'	&[B_2',V']&=V'
\end{align*}
where $B'_1=\frac{ 1 }{2}(A'_1+A'_2)$ and $B_2=\frac{ 1 }{2}(A_1'-A_2')$. The generators of $\iK_{\gsp(2,\eR)}$ are
\begin{align*}
K'_t&=
\begin{pmatrix}
&&1&0\\
&&0&1\\
-1&0\\
0&-1
\end{pmatrix}
	&K'_1&=
\begin{pmatrix}
0&1\\-1&0\\
&&0&1\\
&&-1&0
\end{pmatrix}\\
K'_2&=
\begin{pmatrix}
&&0&1\\&&1&0\\0&-1\\-1&0
\end{pmatrix}
	&K'_3&=
\begin{pmatrix}
&&1&0\\
&&0&-1\\
-1&0\\
0&1
\end{pmatrix}.
\end{align*}
Notice that $[K'_t,K'_i]=0$ for $i=1$, $2$, $3$.


\subsection{Isomorphism}		\label{SubSecIsosp}
%-----------------------

The following provides an isomorphism $\psi\colon \so(2,3)\to \gsp(2,\eR)$:
\begin{align*}
\psi(H_i)&=B'_i		&\psi(u)&=K'_t\\
\psi(W)&=W'		&\psi(R_1)&=\frac{ 1 }{2}K'_1\\
\psi(M)&=M'		&\psi(R_2)&=\frac{ 1 }{2}K'_2\\
\psi(L)&=L'		&\psi(R_3)&=\frac{ 1 }{2}K'_3\\
\psi(V)&=\frac{ 1 }{2}V'
\end{align*}
where the $R_i$'s are the generators of the $\so(3)$ part of $\sK_{\so(2,3)}$ satisfying the relations $[R_i,R_j]=\epsilon_{ijk}R_k$. It is now easy to check that the image of the embedding $\phi\colon \gsl(2,\eC) \to \gsp(2,\eR)$ is exactly $\so(1,3)$, so that
\begin{equation}
\psi^{-1}\circ\phi\colon \gsl(2,\eC)\to \sH
\end{equation}
is an isomorphism which realises $\sH$ as subalgebra of $\gsp(2,\eR)$. This circumstance will be useful in defining a spin structure on $AdS_4$.

One can prove that the kernel of the adjoint representation of $\SP(2,\eR)$ on its Lie algebra is $\pm\mtu$, in other words, $\Ad(a)=\id$ if and only if $a=\pm\mtu$. We define a bijective map $h\colon \SO(2,3)\to \SP(2,\eR)/\eZ_2$ by the requirement that
\begin{equation}		\label{Eqdefhspsl}
  \psi\big( \Ad(g)X \big)=\Ad\big( h(g) \big)\psi(X)
\end{equation}
for every $X\in\so(2,3)$. The following is true for all $\psi(X)$:
\[ 
\begin{split}
\Ad\big(h(gg'\big)) \psi(X)&=\psi\Big( \Ad(g)\big( \Ad(g')X \big) \Big)\\
			&=\Ad\big( h(g) \big)\psi\big( \Ad(g')X \big)\\
			&=\Ad\big( h(g)h(g') \big)\psi(X),
\end{split}
\]
 the map $h$ is therefore a homomorphism. If an element $a\in \SP(2,\eR)$ reads $a= e^{X_A} e^{X_N} e^{X_K}$ in the Iwasawa decomposition, the property $\Ad(a)\psi(X)=\psi\big( \Ad(g)X \big)$ holds for the element\label{PgSolhpsiSP} $g= e^{\psi^{-1}X_A} e^{\psi^{-1}X_N} e^{\psi^{-1}X_K}$ of $\SO(2,3)$. This shows that $h$ is surjective.

\subsection{Reductive structure on the symplectic group}		\label{SubSecRedspT}
%-------------------------------------------------------

A lot of structure of $\so(2,3)$, such as the reductive homogeneous space decomposition as $\sQ\oplus\sH$, can be immediately transported from $\so(2,3)$ to $\gsp(2,\eR)$. Indeed, let $\mT=\psi(\sQ)$ and $\mI=\phi\big( \gsl(2,\eC) \big)$. We have the direct sum decomposition 
\[ 
\gsp(2,\eR)=\mT\oplus\mI.
\]
 Let $X\in\mT\cap\mI$, then $\psi^{-1}X$ belongs to $\sQ\cap\sH$ which only contains $0$. The fact that $\psi$ is an isomorphism yields that $X=0$. Since $\psi$ preserves linear independence, a simple dimension counting shows that the sum actually spans the whole space.

Putting $g=h^{-1}(a)$ in the definition \eqref{Eqdefhspsl} of $h$, we find
\[ 
  \psi\left( \Ad\big( h^{-1}(a) \big)X \right)=\Ad(a)\psi(X).
\]
Considering a path $a(t)$ with $a(0)=e$, we differentiate this expression with respect to $t$ at $t=0$ we find
\[ 
  \ad(dh^{-1}\dot a)X=d\psi^{-1}\big( \ad(\dot a)\psi(X) \big)=\ad(d\psi^{-1}\dot a)(d\psi^{-1}\psi X),
\]
but $d\psi=\psi$ because $\psi$ is linear, hence $[dh^{-1}\dot a,X]=[\psi^{-1}\dot a,X]$ for all $X\in \so(2,3)$ and $\dot a\in \gsp(2,\eR)$. We deduce that $(dh^{-1})_e=\psi^{-1}$. We define 
\begin{align*}
	\theta_{\gsp}&=\id|_{\iK_{\gsp}}\oplus(-\id)|_{\iP_{\gsp}}\\
	\sigma_{\gsp}&=\id|_{\mT}\oplus(-\id)|_{\mI}.
\end{align*}
We can check that $\psi^{-1}\circ\theta_{\gsp}\circ\psi=\theta$ and $\psi^{-1}\circ\theta_{\gsp}\circ\psi=\theta$. Then it is clear that
\[ 
  [\sigma_{\gsp},\theta_{\gsp}]=0
\]
using the corresponding vanishing commutator in $\so(2,3)$. We denote $\mT_a=dL_a\mT$ and the fact that $dp= d\pi\circ dh^{-1}= d\pi\circ \psi^{-1}$ shows that $dp(\mT_a)$ is a basis of $T_{p(a)}(G/H)$. So we consider the basis $t_i=\psi(q_i)$ of $\mT$ and the corresponding left invariant vector fields $\tilde t_i(a)=dL_at_i$.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Heisenberg group and algebra}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


Let $V$ be a symplectic vector space with the symplectic form $\Omega$. The \hypertarget{HyperHeisenberg}{Heisenberg algebra} build on $V$ is the vector space
\begin{equation}
	\pH(V,\Omega)=V\oplus \eR E
\end{equation}
endowed with the bracket defined by
\begin{enumerate}

	\item
		$[\pH(V,\Omega),E]=0$,
	\item
		$[v,w]=\Omega(v,w)E$ for every $v,w\in V$.

\end{enumerate}
The first conditions makes $E$ central in $\pH$.

The Heisenberg group is, as set, the same as the algebra : $H=V\oplus\eR E$ with the product
\begin{equation}		\label{EqProduitHeisenbergGp}
	g_1\cdot g_2=g_1+g_2+\frac{ 1 }{2}[g_1,g_2]
\end{equation}
where the bracket is the one in the Lie algebra. Direct computations show that this product is associative, the neutral is $(0,0)$ and that the inverse is given by
\begin{equation}
	g^{-1}=-g.
\end{equation}
We are now going to prove that the Lie algebra of that group actually is $\pH(V,\Omega)$.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{The exponential mapping}
%---------------------------------------------------------------------------------------------------------------------------

Let us build the exponential map between the Heisenberg algebra and its group. Let $(x,\tau)\in T_eH$ and consider $g(s)= e^{s(x,\tau)}=\big( v(s),h(s) \big)$.  This map is subject to the following three relations:
\begin{enumerate}

	\item
		$g(s)g(t)=g(s+t)$,
	\item
		$g(0)=0$,
	\item
		$g'(01)=(x,t)$.

\end{enumerate}
Taking the derivative of the first one with respect to $s$ and taking into account $v'(0)=x$ and $h'(0)=\tau$, we find
\begin{subequations}
	\begin{align}
		\Dsdd{g(s)+g(t)+\frac{ 1 }{2}\big[ g(s),g(t) \big]  }{s}{0}&=\Dsdd{ \big( v(s+t),h(s+t) \big) }{s}{0}\\
		\Dsdd{ v(s)+v(t),h(s)+h(t)+\frac{ 1 }{2}\Omega\big( v(s),v(t) \big) }{s}{0}	&=\big( v'(t),h'(t) \big)\\
		\Big( x,\tau+\frac{ 1 }{2}\Omega\big( x,v(t) \big) \Big)&=\big( v'(t),h'(t) \big)
	\end{align}
\end{subequations}
We deduce that $v'(t)=x$ and $h'(t)=\tau+\frac{ 1 }{2}\Omega\big( x,v(t) \big)$, so that $v(t)=tx$ and $h(t)=t\tau$. The exponential mapping is thus given by the identity:
\begin{equation}
	\exp(x,\tau)=(x,\tau).
\end{equation}

In order to prove that the law \eqref{EqProduitHeisenbergGp} accepts the Heisenberg algebra as Lie algebra, we need to compute the adjoint action.
\begin{equation}
	\begin{aligned}[]
		\Ad( e^{t(x,\tau)})(x',\tau')&=\Dsdd{ \AD( e^{t(x,\tau)}) e^{s(x',\tau')} }{s}{0}\\
		&=\Dsdd{ (tx,t\tau)(sx',s\tau')(-tx,-t\tau) }{s}{0}\\
		&=\Dsdd{ \big(tx+sx',t\tau+s\tau'+\frac{ ts }{2}\Omega(x,x')\big)(-tx,t\tau) }{s}{0}\\
		&=\big( x',\tau'+t\Omega(x,x') \big).
	\end{aligned}
\end{equation}
Now, the Lie algebra bracket is given by
\begin{equation}
	\begin{aligned}[]
		\big[ (x,\tau),(x',\tau') \big]&=\Dsdd{ \Ad( e^{t(x,\tau)})(x',\tau') }{t}{0}\\
			&=\big( 0,\Omega(x,x') \big)\\
			&=\Omega(x,x')E,
	\end{aligned}
\end{equation}
which is the bracket of $\pH(V,\Omega)$.

\section{The group \texorpdfstring{$SU(2)$}{SU2}}
%--------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsection{Definition and properties}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The group $\SU(2)$ is made of unitary $2\times 2$ matrices with unit determinant: $\det U=1$ and $U^{\dag} U=\mtu$. Since it naturally acts on $\eC^2$, we will describe some ``matricial``\ notations for the main operations in $\SU(2)$ and $\eC^2$. An element of $\eC^2$ is written as a column matrix, while the adjoint is an horizontal one :
\[
   \xi=\begin{pmatrix}
\xi_1 \\ 
\xi_2
\end{pmatrix},\quad 
   \eta^{\dag}=\begin{pmatrix}
\oeta_1  &\oeta_2
\end{pmatrix}.
\]
This suggests us a scalar product\footnote{Be careful: it is \emph{not} $\scal{\xi}{\eta}=\xi^{\dag}\eta$.} :
\[
   \scal{\xi}{\eta}=\eta^{\dag}\xi= \begin{pmatrix}
                                     \oeta_1  &\oeta_2
                                   \end{pmatrix}
   \begin{pmatrix}
\xi_1 \\ 
\xi_2
\end{pmatrix}=\xi_1\oeta_1+\xi_2\oeta_2.
\]
This scalar product is hermitian and in particular, it satisfies $\scal{\xi}{\eta}=\overline{\scal{\eta}{\xi}}$. Elements of $\SU(2)$ preserves this product because 
\[
   \scal{A\xi}{A\eta}=(A\eta)^{\dag}(A\xi)=\eta^{\dag} A^{\dag} A\xi=\scal{\xi}{\eta}
\]
for all $A\in\SU(2)$. One can easily understand that $\SU(2)$ is a compact group. Indeed, the unitary property gives :
\[
UU^{\dag}=
\begin{pmatrix}
\alpha & \beta \\ 
\gamma & \delta
\end{pmatrix} 
\begin{pmatrix}
\oalpha & \ogamma \\ 
\obeta & \odelta
\end{pmatrix}
=
\begin{pmatrix}
\alpha\oalpha+\beta\obeta & x \\ 
x & \gamma\ogamma+\delta\odelta
\end{pmatrix}
\stackrel{!}{=}
\begin{pmatrix}
1  & 0 \\ 
0 & 1
\end{pmatrix}.
\]
If $\alpha=x+iy$ and $\beta=a+bi$, one immediately has  $x^2+y^2+a^2+b^2=1$ (the same is true for $\gamma$ and $\delta$), so that $\SU(2)$ is contained in a bounded subset of $\eR^8$, and it is clear that $\SU(2)$ is closed in $\eR^8$ because it is defined by equalities.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsection{Haar measure on \texorpdfstring{$\SU(2)$}{SU2}}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The quaternion\index{quaternion} field $\eH$ can be embed in $\eM_2(\eC)$ as a genera element reads
\begin{equation}
	q=
\begin{pmatrix}
  \alpha	&	\beta	\\ 
  -\bar\beta	&	\bar\alpha	
\end{pmatrix}
\end{equation}
with $\alpha$, $\beta\in\eC$. Under that isomorphism, we have
\[ 
	| q |^2=| \alpha |^2+| \beta |^2=\det q.
\]
Thus we have the identification
\begin{equation}
	\SU(2)=\{ q\in\eH\tq | q |=1 \}.
\end{equation}
We can act on $\eH$ by $\SU(2)\times \SU(2)$ by
\begin{equation}
	(u,v)\cdot q=uqv^{-1}
\end{equation}
for every $(u,v)\in \SU(2)\times\SU(2)$ and $q\in\eH$. That action defines an homomorphism from $\SU(2)\times\SU(2)$ onto $O(4)$.

\begin{proposition}
The previously defined homomorphism
\[ 
	\phi\colon \SU(2)\times\SU(2)\to O(4).
\]
is surjective over $\SO(4)$ (which is the identity component of $O(4)$) and, moreover, the kernel is $\big\{  (e,e),(-e,-e) \big\}$.
\end{proposition}

\begin{proof}
The group $\SU(2)\times \SU(2)$ being connected, its image can only be included in $\SO(4)$. Let us first determine the kernel of $\phi$. If $(u,v)\in\ker\phi$, we have $uqv^{-1}=q$ for every $q\in\eH$. In particular, with $q=1$, we find $u=v$. Then the relation $uqu^{-1}=q$ means that $u$ belongs to the center of $\eH$, which is $\eR$. We conclude that $u=\pm 1$. That proves that $\ker\phi=\big\{  (e,e),(-e,-e) \big\}$.

The differential $(d\phi)_{(e,e)}$ is an homomorphism
\[ 
	d\phi\colon \gsu(2)\oplus\gsu(2)\to \so(4).
\]
Let $(S,T)\in\gsu(2)\oplus\gsu(2)$, we have
\[ 
	d\phi(S,T)q=\Dsdd{ \phi( e^{t(S,T)})q }{t}{0}=\Dsdd{ \phi( e^{tS}, e^{tT})q }{t}{0}=\Dsdd{  e^{tS}q e^{-tT} }{t}{0}=Sq-qT,
\]
on which one sees that $d\phi$ is injective. Moreover we have $\dim\big( \gsu(2)\oplus\gsu(2) \big)=6=\dim\so(4)$. An injective map between vector space of same dimension being an isomorphism, the image of $\phi$ contains a neighborhood of identity in $\SO(4)$. From connectedness of $\SO(4)$, that neighborhood generates the whole group (see proposition \ref{PropUssGpGenere}), so that $\phi$ is in fact surjective.
\end{proof}

Since the map $\phi\colon \SU(2)\times \SU(2)\to \SO(4)$ is a surjective homomorphism with a discrete kernel, we have an isomorphism at the algebra level:
\[ 
	\so(4)\simeq \gsu(2)\oplus\gsu(2).
\]

\subsection{Building some representations for \texorpdfstring{$\SU(2)$}{SU2}}
%/////////////////////////////////////////////////////////////////////////////////

Since $\SU(2)$ acts on $\eC^2$, we can build a representation of $\SU(2)$ on functions on $\eC^2$. We define $\dpt{T}{SU(2)}{\End\big(\Cinf(\eC^2)\big)}$ by 
\[
  (T(U)f)(\xi)=f(U^{-1}\xi),
\]
if $f\in\Cinf\big(\eC^2\big)$, $\xi\in\eC^2$ and $U\in SU(2)$. 

Let $V_j$ be the space of the homogeneous polynomials of degree $j$ on $\eC^2$; a basis of this space is given by the $\phi_{pq}$, $p+q=2j$ defined by
\begin{equation}
   \phi_{pq}(\xi)=\xi_1^p\xi_2^q
\end{equation}
($\xi=\xi_1+i\xi_2$). If $j$ is fixed, we will often write $\phi_m$ instead of $\phi_{pq}$. The signification is $p=j+m$, $q=j-m$, and $m$ takes its values in $-j,\ldots,j$. Note that $p-q=2m$. It is clear that if $A$ is any invertible $2\times 2$ matrix , and $f\in V_j$, then 
\[
   \rho(A)f:=f(A^{-1} \cdot)
\]
 is still an element of $V_j$. This representation $\rho$ is defined on the whole $\Cinf(\eC^2)$. We will descent it to $V_j$ later.
Now, we fix $j$ and a $m$ between $-j$ and $j$. 

Consider the diagonal matrix
\[   U_{-\theta}=\begin{pmatrix}
e^{-i\theta} & 0 \\ 
0 & e^{i\theta}
\end{pmatrix} \in\SU(2).
\]
One has
\begin{equation}
  \left(\rho(U_{-\theta})\phi_{pq}\right)(\xi)=\phi_{pq}
\begin{pmatrix}
   e^{i\theta}\xi_1 \\ 
    e^{-i\theta}\xi_2
\end{pmatrix}
                                               = e^{pi\theta} e^{-qi\theta}\xi_1^p\xi_2^q
					       =e^{2mi\theta}\phi_{pq}(\xi).
\end{equation}
First conclusion: the $\phi$'s are eigenvectors of $\rho(U_{-\theta})$ because
\[
   \rho(U_{-\theta})\phi_m=e^{2mi\theta}\phi_m.
\]
Second, the trace of $\rho(U_{-\theta})$ is
\begin{equation}
   \chi_j(\theta)=\sum_{m=-s}^{s}e^{2mi\theta}.
\end{equation}
By the way, the $\chi_j$ are the characters of the representation $\rho$.

From considerations about the Haar\quextproj{} invariant measure on $\SU(2)$, one knows that the good notion product between functions is :
\begin{equation}
(f_1,f_2)_{\SU(2)}=\frac{2}{\pi}\int_0^{\pi}f_1(\theta)\overline{ f_2(\theta) }\sin^2\theta\,d\theta,
\end{equation}
so that $(\chi_j,\chi_j)=1$. This and the fact that $\SU(2)$ is compact make the theorem of Peter-Weyl (cf. \cite{Sternberg}) applicable, thus the restrictions of $\rho$ to the $V_j$'s are irreducible and moreover, these provide \emph{all} the irreducible representations.

\subsection{Special case: \texorpdfstring{$j=\frac{1}{2}$}{j=1/2}}
%//////////////////////////////////////////////////////////////////////

Consider a matrix $A\in\SU(2)$ : 
\begin{equation}
A=\begin{pmatrix}
\oalpha & -\beta \\ 
\obeta & \alpha
\end{pmatrix},\qquad
A^{-1}=\begin{pmatrix}
\alpha & \beta \\ 
-\obeta & \oalpha
\end{pmatrix}.
\end{equation}
A basis of $V_{\frac{1}{2}}$ is given by $\phi_{10}$ and $\phi_{01}$. Let us see how $\rho(A)$ acts on. Since
\[
A^{-1}\begin{pmatrix}
\xi_1 \\ 
\xi_2
\end{pmatrix}=
\begin{pmatrix}
\alpha\xi_1+\beta\xi_2 \\ 
-\obeta\xi_1+\oalpha\xi_2
\end{pmatrix},
\]
we find
\begin{equation}
\begin{split}
  (\rho(A)\phi_{10})(\xi)&=\alpha\xi_1+\beta\xi_2=(\alpha\phi_{10}+\beta\phi_{01})(\xi)\\
  (\rho(A)\phi_{01})(\xi)&=-\obeta\xi_1+\oalpha\xi_2=(-\obeta\phi_{10}+\oalpha\phi_{01})(\xi).
\end{split}
\end{equation}
Thus in the basis $\{\phi_{10},\phi_{01}\}$, the matrix of $\rho(A)$ is given by
\begin{equation}
\rho(A)=\begin{pmatrix}
\alpha & -\obeta \\ 
\beta & \oalpha
\end{pmatrix}=\overline{A}.
\end{equation}

Up to here, we were looking at the representation $\rho$ of $\SU(2)$ on the whole set of functions on $\eC^2$, and more precisely, its restriction to $V_j$. We could define the representation $\rho_{\frac{1}{2}}$ as $\rho_{\frac{1}{2}}=\rho|_{V_{\frac{1}{2}}}$, but we will not do it. Our definition is
\begin{equation}
  \rho_{\frac{1}{2}}(A)=\rho(\overline{A})|_{V_{\frac{1}{2}}}.
\end{equation}
Note that 
\[
\begin{pmatrix}
0 & -1 \\ 
1 & 0
\end{pmatrix}
A 
\begin{pmatrix}
0 & 1 \\ 
-1 & 0
\end{pmatrix}=\overline{A},
\]
thus the representation $A\to\rho(\overline{A})|_{V_{\frac{1}{2}}}$ is equivalent to $A\to\rho(A)|_{V_{\frac{1}{2}}}$. This equivalence can also be seen because these two representations have the same characters\quextproj.

The basis $\phi_{pq}$ is orthogonal; we will build an orthonormal one: $e_m(\xi)$ is the vector whose coordinates are
\begin{equation}
e_m^j(\xi)=\frac{ \xi_1^{j+m}\xi_2^{j-m} }{\sqrt{ (j+m)!(j-m)! }}
\end{equation}
for $m=-j,-j+1,\ldots,j$. The metric to take in order to define $(e_m,e_n)$ is the unique one on $V_j$ which is $\SU(2)$-invariant.

The Newton's formula for the binomial yields :
\begin{equation}
\begin{split}
  \sum_{m=-s}^s e_m(\xi)\overline{e_m(\eta)}
        &=\sum\frac{ \xi_1^{j+m}\xi_2^{j-m}\oeta_1^{j+m}\oeta_2^{j-m} }{(j+m)!(j-m)!}\\
	&=\us{(2j)!}(\xi_1\oeta_1+\xi_2\oeta_2)^2\\
	&=\us{(2j)!}\scal{\xi}{\eta}^{2j}.
\end{split}
\end{equation}
But we know that $A\in\SU(2)$ preserves the scalar product: $\scal{A\xi}{A\eta}=\scal{\xi}{\eta}$. Therefore :
\begin{equation}\label{eq:produit_e_m}
\sum (\rho_j(A)e_m)(\xi)\overline{ (\rho_j(A)e_m)(\eta) }=\sum e_m(\xi)\overline{e_m(\eta)}.
\end{equation}
Now, instead of considering the matrices $\rho_j(A)$ on $V_j$ for the basis $\phi_m$, we looks at the ones with respect to the basis $e_m$ :
\begin{equation}
\rho_j(A)e_m=r(A)^k_me_k;
\end{equation}
in others words, we looks at the representation $A\to r(A)$. The equations \eqref{eq:produit_e_m} makes
\[
  \sum_{m=-j}^j\left(
                      r(A)^l_me_l(\xi)\overline{ r(A)^k_me_k(\eta)   }
		        -\delta^l_me_l(\xi)\delta^k_me_k(\eta)
                \right)=0.
\]
Since the functions
\begin{equation}
\begin{aligned}
 e_k\otimes\overline{e_l}\colon \eC^2\times\eC^2 &\to \eC \\ 
(\xi,\eta) &\mapsto  e_k(\xi)\overline{e_l(\eta)}
\end{aligned}
\end{equation}
 are linearly independent, one gets $\sum_m r(A)^k_l\overline{r(A)^l_m}=\delta^{kl}$, or
\begin{equation}
r(A)r(A)^*=\mtu,
\end{equation}
the conclusion is that in this basis, the matrices $\rho_j(A)$ are unitary.

\subsection{Clebsch-Gordan}
%//////////////////////////////////////////////////////////////////////

From the knowledge of the characters of $\rho_j$, one can decompose the product $\rho_s\otimes\rho_r$ into irreducible representations. For example,
\[
   V_{\frac{1}{2}}\otimes V_{\frac{1}{2}}=V_0\oplus V_1.  
\]
More generally,
\begin{equation}
  V_s\otimes V_r=V_{|r-s|} \oplus V_{|r-s|+1}\oplus\ldots\oplus V_{r+s}.
\end{equation}
For this reason, the representation $\rho_j$ is sometimes called the \defe{spin $j$}{spin!representation!of $\SU(2)$} representation of $\SU(2)$.

%---------------------------------------------------------------------------------------------------------------------------
					\section{The complex algebra \texorpdfstring{$\protect\gsl(2,\eC)$}{sl2C} and its representations}
%---------------------------------------------------------------------------------------------------------------------------
\label{SecsldeuxCandrepres}

The book \cite{Kassel} contains the representations of \( \gsl(2,\eC)\).

The algebra $\gsl(2,\eC)$ is the complex algebra of complex $2\times 2$ matrices with vanishing trace. As generating matrices, one can take the elements $u_i$ of \eqref{EqGenssudeux} and complete them by
\begin{align*}
v_1&=\frac{ 1 }{2}
\begin{pmatrix}
  -1	&	0	\\ 
  0	&	1	
\end{pmatrix},
&v_2&=\frac{ 1 }{2}
\begin{pmatrix}
  0	&	i	\\ 
  -i	&	0	
\end{pmatrix},
&v_3&=\frac{ 1 }{2}
\begin{pmatrix}
  0	&	-1	\\ 
  -1	&	0	
\end{pmatrix}
\end{align*}
which satisfy the commutation relations
\begin{subequations}
\begin{align}
	[v_i,v_j]&=-\epsilon_{ikj}u_k\\
	[v_i,u_j]&=\epsilon_{ikj}v_k.
\end{align}
\end{subequations}

\begin{remark}
    This is not the algebra \( \gsl(2,\eC)\) used in physics. The latter is the \emph{four}-dimensional \emph{real} algebra of trace vanishing \( 2\times 2\) complex matrices. There is one more generator and the representation theory is different. Moreover the physics works with the \emph{group} instead of the \emph{algebra}.
\end{remark}

The change of basis
\begin{align}
	x_j&=\frac{ 1 }{2}(u_j+iv_j),	&y_j&=\frac{ 1 }{2}(u_j-iv_j)
\end{align}
provides the simplification
\begin{align}
[x_i,x_j]&=\epsilon_{ijk}x_k	&[y_i,y_j]&=\epsilon_{ijk}y_k	&[x_i,y_j]&=0,
\end{align}
so that, as algebras, we have the isomorphism
\begin{equation}
	\gsl(2,\eC)=\gsu(2)\oplus\gsu(2).
\end{equation}
Thus the representation theory of $\gsl(2,\eC)$ is determined by the one of $\gsu(2)$.


\index{representation!of $\gsl(2,\eC))$}
We follow the presentation of \cite{GpAlgLie_Faraut}. Consider the space $\mP_m$ of homogeneous polynomials of degree $m$ in two variables with complex coefficients. The dimension of $\mP_m$ is $m+1$ and we have the following representation of $\SL(2,\eC)$ thereon:
\begin{equation}
	\big( \pi_m(g)f \big)(u,v)=f\big( 
g
\begin{pmatrix}
u\\v
\end{pmatrix}
 \big)
=
f(au+bv,cu+dv)
\end{equation}
if $g=\begin{pmatrix}
  a	&	b	\\ 
  c	&	d	
\end{pmatrix}$. We are going to determine the corresponding representation $\rho_m$ of the Lie algebra $\gsl(2,\eC)$ as algebra over complex numbers. A basis of $\gsl(2,\eC)$ over $\eC$ is given by the matrices $\{ H,E,F \}$ of page \pageref{PgBaseSLdeuxRHEF} and are subject to the commutation relations
\begin{align}
[H,E]&=2E\\
[H,F]&=-2F\\
[E,F]&=H.
\end{align}

The only positive root is \( \alpha(H)=2\). The Cartan matrix reduces to one number: 
\begin{equation}
    A=A_{11}=\frac{ 2(\alpha,\alpha) }{ (\alpha,\alpha) }=2.
\end{equation}
Notice that the relations \eqref{EqChevalleySimple} are fulfilled. The Killing form, in the basis \( \{ H,E,F \}\) is given by
\begin{equation}
    B=\begin{pmatrix}
        8    &   0    &   0    \\
        0    &   0    &   4    \\
        0    &   4    &   0
    \end{pmatrix}
\end{equation}
and the element \( t_{\alpha}\) is then
\begin{equation}
    t_{\alpha}=\frac{1}{ 4 }H.
\end{equation}
The inner product on \( \lH^*\) is then
\begin{equation}        \label{Eqinnerhstarsldc}
    (\alpha,\alpha)=B(t_{\alpha},t_{\alpha})=\frac{ 1 }{2}.
\end{equation}

The element \( t_{\alpha}\) is defined by 

Using the exponentiation \eqref{EqExpMatrsSLdeuxR}, we find
\[ 
	\big( \pi_m( e^{tH})f \big)(u,v)=f( e^{t}u, e^{-t}v),
\]
so that
\begin{equation}
	\big( \rho_m(H)f \big)(u,v)=u\frac{ \partial f }{ \partial u }-v\frac{ \partial f }{ \partial v }.
\end{equation}
In the same way, we find
\begin{equation}
	\big( \rho_m(E)f \big)(u,v)=\Dsdd{ \big( \pi_m( e^{tE})f \big)(u,v) }{t}{0}=v\frac{ \partial f }{ \partial u },
\end{equation}
and
\begin{equation}
	 \big( \rho_m(F)f \big)(u,v)=u \frac{ \partial f }{ \partial v }.
\end{equation}
A natural basis of $\mP_m$ is given by the monomials $f_j(u,v)=u^jv^{m-j}$ with $j=0,\ldots,m$. The representation $\rho_m$ on this basis reads
\begin{equation}        \label{EqReprezgsldeuxC}
\begin{split}
	\rho_m(H)f_j&=(2j-m)f_j\\
    \rho_m(E)f_j&=(m-j)f_{j+1}\\
    \rho_m(F)f_j&=jf_{j-1}.
\end{split}
\end{equation}

\begin{proposition}		\label{ProprhomirredsldeuxC}
The representation $\rho_m$ is irreducible.
\end{proposition}

\begin{proof}
Let $W\neq\{ 0 \}$ be an invariant subspace of $\mP_m$. If $p\in W$, from invariance, $\rho_m(H)(p)\in W$. If $p$ is a linear combination of $\{ f_j \}_{j\in I}$ ($I\subseteq \{ 0,\ldots m \}$), then $\rho_m(H)p$ is still a linear combination $q$ of elements in the same set. Thus there exists a linear combination of $p$ and $\rho_m(H)p$ which is a linear combination of $\{ f_j \}_{j\in J}$ with $J\subset I$ (strict inclusion). Using the same trick with $q$ and $\rho_m(H)q$, we still reduce the number of basis elements. Proceeding in the same way at most $m$ times, we find that one of the $f_j$ belongs to $W$. From there, acting with $\rho_m(E)$ and $\rho_m(F)$, one generates the whole $\mP_m$. That proves that $W=\mP_m$ and thus that $\rho_m$ is irreducible. 
\end{proof}

\begin{theorem}
Every $\eC$-linear irreducible finite dimensional representation of $\gsl(2,\eC)$ is equivalent to one of the $\rho_m$.
\end{theorem}

For a proof, see \cite{GpAlgLie_Faraut}. These results will be proved also in the quantum case in theorems \ref{ThoVfintemofdsldcun} and \ref{ThoVfintemofdslddeux}.

%---------------------------------------------------------------------------------------------------------------------------
					\section{Representations of \texorpdfstring{$\SL(2,\eR)$}{SL2R} and \texorpdfstring{$\SU(2)$}{SU2}}
%---------------------------------------------------------------------------------------------------------------------------

The representation $\pi_m$ of $\SL(2,\eC)$ restricts to $\SL(2,\eR)$.

\begin{lemma}
The representation $\pi_m$ of $\SL(2,\eR)$ is irreducible.
\end{lemma}

\begin{proof}
If $W$ is an invariant space under $\pi_m\big( \SL(2,\eR) \big)$, then is is invariant under the derived representation $\rho_m\big( \gsl(e,\eR) \big)$. The proof of proposition \ref{ProprhomirredsldeuxC} still holds here, so that $W=\mP_m$.
\end{proof}

\begin{theorem}
Let $\pi$ be an irreducible representation of $G=\SL(2,\eR)$ or $\SU(2)$ in a complex finite dimensional vector space $V$. Then $\pi$ is equivalent to one of the $\pi_m$.
\end{theorem}

\begin{proof}
Let $\lG$ be the Lie algebra of $G$. One important property shared by $\SL(2,\eR)$ and $\SU(2)$ is that $G=\exp(\lG)$. It is clear that the representation $d\pi$ on $\gsl(2,\eR)$ extends $\eC$-linearly to a representation $\rho$ of $\gsl(2,\eC)$. Looking on the basis \eqref{EqGenssudeux}, one sees that in fact the same is true for $\gsu(2)$ which $\eC$-linearly extends to $\gsl(2,\eC)$.

Let us prove that $\rho$ is irreducible. Let $W\neq\{ 0 \}$ be a subspace of $V$ invariant under $\rho(\lG)$. Then $W$ is invariant under $ e^{\rho(X)}=\pi( e^{X})$ for every $X\in\lG$. Since $\exp(\lG)=G$, the space $W$ is in fact invariant under $\pi(G)$, and is therefore equal to $V$.

Since $\rho$ is irreducible, we have $\rho=\rho_m$ for a certain $m$. Thus there exists an intertwining operator $A\colon V\to \mP_m$ such that
\[ 
	A\rho(X)=\rho_m(X)A
\]
for every $X\in\lG$. By linearity, for every $N\in\eN$, we have $A\rho\big( \sum_{k=1}^n X^k/k! \big)=\rho_m\big( \sum_{k=1}^n X^k/k! \big)A$, and at the limit, we have
\begin{equation}
	A e^{\rho(X)}= e^{\rho_m(X)A}.
\end{equation}
From that we deduce that $A\pi( e^{X})=\pi_m( e^{X})A$ which means that
\[ 
	A\pi(g)=\pi_m(g)A.
\]
That shows that $A$ intertwines $\pi$ and $\pi_m$, so that $\pi$ is equivalent to $\pi_m$.
\end{proof}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
					\section{Representations of \texorpdfstring{$\so(2,d-1)$}{so2d}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Verma module}
%---------------------------------------------------------------------------------------------------------------------------

One can find text about these representations in \cite{Ferrata,Dolan_son,SingString}, while we will mainly follow the developments of \cite{SingletonCompposites,HowMassless,Teschner}. We are going to study representations of the algebra $\so(2,d-1)$ which fulfills the commutation relations described in lemma \ref{LemCommsopqAlg}:
\begin{equation}		\label{EqCommsodeuxdmoinsun}
	[M_{ab},M_{cd}]=-i\eta_{ac}M_{bd}+i\eta_{ad}M_{bc}+i\eta_{bc}M_{ad}-i\eta_{bd}M_{ac}.
\end{equation}
Notice that $M_{ab}=-M_{ba}$. As convention, the indices $a$, $b$,\ldots run over $\{ 0,0',1,2,\ldots d-1 \}$ while $r$, $s$, \ldots run over $\{ 1,2,\ldots,d-1 \}$. As on page \pageref{PgDefsGenre}, we choose the convention
\begin{equation}
	\eta =
	\begin{pmatrix}
		\mtu_{2\times 2}\\
		&-\mtu_{(d-1)\times (d-1)}
	\end{pmatrix}.
\end{equation}
Notice that this convention numerically holds for the matrix $\eta_{st}$ as well as for its inverse $\eta^{st}$.

The algebra separates into two parts: the compact and the non compact part. The maximal compact subalgebra is $\so(2)\oplus\so(d-1)$ which is generated by $E=M_{00'}$ and $J_{rs}=M_{rs}$. The non compact generators are $M_{0'r}$ and $M_{0r}$ that we rearrange into ladder operators
\begin{equation}
	L^{\pm}_r=M_{0r}\mp iM_{0'r}.
\end{equation}
Using commutation relations \eqref{EqCommsodeuxdmoinsun}, one computes the commutators in the new basis. For example
\[ 
	[E,L^{\pm}_r]=[M_{0'0},M_{0r}]\mp i[E_{0'0},M_{0'r}]=\pm M_{0r}-iM_{0'r}=\pm L^{\pm}_r.
\]
The table of $\so(2,d-1)$ in this basis is
\begin{subequations}		\label{SubEqsCommssodeuxd}
	\begin{align}
		[E,L^{\pm}_r]&=\pm L^{\pm}_r\\
		[J_{rs},L_t^{\pm}]&=-i(\delta_{rt}L_s^{\pm}-\delta_{st}L_r^{\pm})\\
		[L_r^{-},L_s^+]&=2(iJ_{rs}+\delta_{rs}E)\\
		[J_{rs},J_{tu}]&=-i\delta_{ac}M_{bd}+i\delta_{ad}M_{bc}+i\delta_{bc}M_{ad}-i\delta_{bd}M_{ac}.
	\end{align}
\end{subequations}
The unitary properties are $(M_{rs})^{\dag}=M_{rs}$, $E^{\dag}=E$ and $(L^{\pm}_r)^{\dag}=L_r^{\mp}$. From these commutators, we deduce the following rules that will be always used
\begin{subequations}
	\begin{align}
		L^-_rL^+_s&=L^+_sL^-_r+2(iJ_{rs}+\delta_{rs}E)\\
		J_{rs}L^+_t&=L^+_tJ_{rs}-i(\delta_{rt}L^+_s-\delta_{st}L^+_r)\\
		EL^+_r&=L^+_rE+L^+_r.
	\end{align}
\end{subequations}

The Cartan algebra of $\so(d)$ is given by the elements $A_p=M_{2p-1,2p}$ with $p=1,\ldots, r$ for $\so(2r)$ and $\so(2r+1)$. 

The unitary irreducible representations of $\so(2,n)$ have the form $\mD(e_0,\bar\jmath)$. It is given by a basis vector $| e_0,\bar\jmath \rangle$ on which $E$ and $J_{rs}$ act by their respective representations (of $\so(n)$ and $\so(2)$). The \defe{energy}{energy!in the representations of $\so(2,d-1)$} of the vector $\ket{e,\overline{ m }}$ is its eigenvalue for the operator $E$, namely $e$:
\begin{equation}
	E\ket{e,\overline{ m }}=e\ket{e,\overline{ m }}.
\end{equation}
Using the commutators \eqref{SubEqsCommssodeuxd}, we find $L_r^{\pm}=(E\pm 1)L_r^{\pm}$, so that
\begin{equation}
	E L^{\pm}_r\ket{e,\overline{ m }} =(e\pm 1)\ket{e,\overline{ m }}.
\end{equation}
We see that the ladder operator $L_r^+$ raises the value of the energy of one unit, while the operator $L_r^-$ lower the energy of one unit. The vector $\ket{e_0,\bar\jmath}$ is the \defe{vacuum vector}{vacuum!vector}, it has the lowest energy in the sense that $L^{-}_r| e_0,\jmath \rangle=0$. A \defe{scalar representation}{scalar!representation} is a representation with $\bar\jmath=0$. They are, logically, denoted by $\mD(e_0)$ and its vacuum is $| e_0 \rangle$ which satisfies
\begin{align}		\label{Eqaldefketezerovac}
	J_{rs}| e_0 \rangle & = 0	& (E-e_0)| e_0 \rangle&=0	&L_r^{-}| e_0 \rangle&=0.
\end{align}
Then one build the generalised Verma module
\begin{equation}	\label{EqmVVermaldots}
	\mV(e_0,0)\equiv \big\{   L_{r_1}^+\ldots L_{r_n}^+| e_0 \rangle   \big\}_{n=0}^{\infty}.
\end{equation}
Notice that the Verma module is not automatically irreducible. We will soon build irreducible representations by taking quotient of the Verma module by its singular module.

In order to compute the norm of $L_s^+L_r^+\ket{e_0}$, we compute $L^-_rL^-_sL^+_sL^+_r\ket{e_0} =4\big(E+E^2+\delta_{rs}E^2+(J_{rs})^2\big)\ket{e_0}$. In order to get that result, we moved all the $L^-$ on the right using the commutation relation, and we taken into account the simplifications induced by the definition relations \eqref{Eqaldefketezerovac}. Now, using the relation $J_{rs}\ket{e_0}=0$, we have
\begin{equation}
	4\big(E+E^2+\delta_{rs}E^2\big)\ket{e_0}.
\end{equation} We also have
\begin{equation}
	(E-e_0)L^+_sL^+_s\ket{e_0}=0.
\end{equation}

\begin{proposition}
The vectors $L^+_{r_1}\ldots L^+_{r_k}\ket{e_0}$ and $L^+_{t_1}\ldots L^+_{t_l}\ket{e_0}$ are orthogonal if $k\neq l$.
\end{proposition}
That proposition says that different layers are orthogonal\quext{À justifier en analysant qui est exactement $\lH$ et les racines simples, mais ça me semble ok.}

\begin{proof}
We proceed by induction. We suppose that the result is proved for $k,l\geq n$, and we prove that 
\begin{equation}
	L^-_{t_1}\ldots L^-_{t_{n+1}}L^+_{r_1}\ldots L^+_{r_n}\ket{e_0}=0.
\end{equation}
First, remark that, using the commutation relations and the fact that $J_{rs}\ket{e_0}=0$ and $E\ket{e_0}=e_0\ket{e_0}$, the vectors
\begin{subequations}		\label{SubEqsJELLket}
	\begin{align}
		J_{st}L^+_{r_1}\ldots L^+_{r_k}\ket{e_0}\\
		EL^+_{r_1}\ldots L^+_{r_k}\ket{e_0}
	\end{align}
\end{subequations}
are combinations of vectors of the form $L^+_{a_1}\ldots L^+_{a_k}\ket{e_0}$. Now, we have
\begin{equation}
	L^-_{t_1}\ldots L^-_{t_{n+1}}L^+_{r_1}\ldots L^+_{r_n}\ket{e_0}= L^-_{t_1}\ldots L^-_{t_n}\big( L^+_{r_1}L^-_{t_{n+1}}+2i(J_{t_{n+1},r_1} + \delta_{t_{n+1},r_1}E ) \big)L^+_{r_2}\ldots L^+_{r_n}\ket{e_0}
\end{equation}
which decomposes in three terms. The first one is
\begin{equation}
	L^-_{t_1}\ldots L^-_{t_n}L^+_{r_1}L^-_{t_{n+1}}L^+_{r_2}\ldots L^+_{r_n}\ket{e_0},
\end{equation}
and according to equations \eqref{SubEqsJELLket}, the two other terms reduce to zero. Continuing that way, the operator $L^-_{t_{n+1}}$ advance of one position at each step and finishes to kill himself on $\ket{e_0}$.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Singular module}
%---------------------------------------------------------------------------------------------------------------------------

Let us compute the norm of the general vector $L^+_{r_1}\ldots L^+_{r_k}\ket{s_0,s}$. We have
\begin{equation}
	\begin{aligned}[]
		L^-_{r_k}\ldots L^-_{r_1}L^+_{r_1}\ldots L^+_{r_k}\ket{e_0,s}	
				&= L^-_{r_k}\ldots L^-_{r_2} (L^+_{r_1}L^-_{r_2}+2E) L^+_{r_2}\ldots L^+_{r_k}\ket{e_0,s}\\
				&= L^-_{r_k}\ldots L^-_{r_2}L^+_{r_1}L^-_{r_2}L^+_{r_2}\ldots L^+_{r_k}\ket{e_0,s}\\
				&\quad +2(e_0+k-1)L^-_{r_k}\ldots L^-_{r_2} L^+_{r_2}\ldots L^+_{r_k}\ket{e_0,s}
	\end{aligned}
\end{equation}
Using again and again the commutation relations, we eliminate all the operators $L^+_r$ and we obtain a sum of terms of the form $(e_0+k-l)$. It will, obviously be positive for large enough $e_0$. Thus, unitarity of the representation is enforced for large values of $e_0$, and there exists a lower bound $E_0(s)$ such that negative norm states appears when $e_0<E_0(s)$. If $e_0=E_0(s)$, then these vectors have a vanishing norm.

Let us consider a limit representation: $e_0=E_0$; there are vectors of vanishing norm, but no vectors with negative norm. In that case, if $v\cdot v=0$, then $v\cdot w=0$ for every other vector $w$. Indeed, if $v\cdot w\neq 0$, we have
\begin{equation}
	(v-w)\cdot(v-w)=w\cdot w - v\cdot w-w\cdot v,
\end{equation}
which holds for every positive multiple $\lambda v$ and $\mu w$. Choosing a big $\lambda$ and a small $\mu$, the norm of $\lambda v-\mu w$ becomes negative. What we proved is
\begin{lemma}
	If the energy $e_0$ of a representation saturates the unitary condition, then a vector with vanishing norm is orthogonal to every other vectors. Moreover, the vectors with vanishing norm form an invariant subspace.
\end{lemma}
The second part is the fact that, if $A\in\lG$, and $\| \ket{\psi} \|=0$  then $\| A\ket{\psi} \|=0$, because it is the scalar product of $\ket{\psi}$ with the vector $A^{\dag}A\ket{\psi}$. The submodule made of vectors of zero norm is the \defe{singular submodule}{singular!submodule}, and is denoted by $\mS(e_0,s)$.

\begin{proposition}		\label{PropSinModRedSSIADesNuls}
A module is reducible if and only if it possesses a vector $\ket{v}$ (different from $\ket{e_0,s}$) such that $L^-_r\ket{v}=0$ for every $r$. Such a vector is said to be \defe{null}{null vector}.
\end{proposition}

\begin{proof}
Since the energy is bounded from bellow, applying several times the lowering operators $L^-_r$ on any vector ends up on zero. Thus, any submodule contains a vector $\ket{v}$ such that $L^-\ket{v}=0$ for every $r$. If that vector is not $\ket{e_0,s}$, then the submodule is a proper submodule.

If $\ket{v}\neq\ket{e_0,s}$, then it is of the form $L^+_{\bar r}\ket{e_0,s}$ and its norm is given by
\begin{equation}
	\| L^+_{\bar r}\ket{e_0,s} \|=\bra{e_0,s} L^-_{\bar r}L^+_{\bar r}\ket{e_0,s}=0
\end{equation}
because $L^-_{\bar r}L^+_{\bar r}\ket{e_0,s}=0$ by assumption.
\end{proof}
From the Verma module \eqref{EqmVVermaldots}, we thus extract the irreducible representation taking the quotient by the singular module:
\begin{equation}
	\mH(e_0) = \mV(e_0)/\mS(e_0).
\end{equation}

Most of time, we have only one extra vacuum, let $\ket{e'_0,s'}$, and in this case, the whole singular module is generated by vectors of the form
\begin{equation}
	L^+_{r_1}\ldots L^+_{r_k}\ket{e'_0,s'}.
\end{equation}
Let $\ket{e_0,s}$ be the vacuum with $s=(s_1,s_2,0,\ldots,0)$, corresponding to the Young diagram
\begin{equation}
   \input{Fig_AIFsOQO.pstricks}
%	\input{image_Young_sssSing.pstricks}
\end{equation}
where the first line has $s_1$ boxes and the second one has $s_2$ boxes. Thanks to theorem \ref{ThoOpqrepreTens}, it can be realized with the tensor
\begin{equation}
	v_{a_1,\ldots a_{s_2}b_1\ldots s_1}(e_0)
\end{equation}
which is separately symmetric in the indices $a$ and $b$, in the same time as being antisymmetric in the couples $a_i$, $b_i$ when $i\leq s_2$, for example,
\begin{equation}
	v_{a_1\ldots a_{s_2},b_1\ldots b_{s_1}} = -v_{b_1 a_1\ldots a_{s_2},b_2\ldots b_{s_1}}.
\end{equation}
In particular, if we symmetrise $v$ on $s_1+1$ indices, we always found zero. Moreover, all the traces vanishes. If $\eta$ is the metric of $O(D-1)$, we have for example
\begin{equation}
	\eta^{b_1b_2}v_{a_1\ldots a_{s_2},b_1,\ldots b_{s_2}}=0.
\end{equation}

The vectors of the first level are the ones of the form	$L^+_r\ket{e_0,s}$. As far as notations are concerned, we have
\begin{equation}
	L^+_rv_{a_1\ldots a_{s_2},b_1\ldots b_{s_2}}=(L^+_rv)_{a_1\ldots a_{s_2},b_1\ldots b_{s_2}}.
\end{equation}
Remark that the operators $\{ L_r^+ \}_{r=1,\ldots,D-1}$ carry a representation of $o(D-1)$, namely the vector representation. Thus, the states of the first level form the representation given by the tensor product of $(s_1,s_2,0,\ldots)$ and the vector representation. In order to see the irreducible components of that representation, we have to know what are the symmetry properties that we can give to the indices
\begin{equation}
	r,a_1,\ldots,a_{s_2},b_1,\ldots,b_{s_1}.
\end{equation}
There are three possibilities: we can contract the $r$ with one of the $a_i$ (by symmetry, all of these contractions are equivalent), or with one of the $b_i$, or add one box in the Young diagram. The latter possibility splits into three cases: the diagram $(s_1,s_2,0,\ldots)$ can be transformed in $(s_1+1,s_2,0,\ldots)$, $(s_1,s_2+1,0,\ldots)$ or $(s_1,s_2,1,0,\ldots)$. So we have $5$ irreducible component in the $o(D-1)$ representation carried by the level one.

The question that naturally arises is to know if one of these have a singular vacuum. In other words, if $\Pi_{\beta}$ are the projections to the irreducible components, do we have
\begin{equation}
	L^-_{t}\Pi_{\alpha}\big( L^+_rv_{\bar a,\bar b}(e_0) \big)=0
\end{equation}
for a certain $e_0$ ?

Notice that the contraction with the last $b_i$'s is not the same as the one with the firsts ones because of the symmetry properties with respect to the $a_i$'s. The first representation with cell cut is given by
\begin{equation}
	v^1_{\bar a,b_1\ldots b_{s_1-1}}	=\eta^{rt}L^+_r\big\{ v_{\bar a,b_1 \ldots b_{s_1-1}t}(e_0)
							+\frac{ s_2 }{ s_1-s_2+1 } v_{ca_1\ldots a_{s_2-1},b_1\ldots b_{s_1-1}a_{s_2}(e_0)}\big\},
\end{equation}
while the second representation with cell cut is easier:
\begin{equation}
	v^2_{a_1\ldots a_{s_{2}-1},\bar b}=\eta^{rt}L^+_rv_{ta_1\ldots a_{s_2-1},\bar b}(e_0).
\end{equation}
Now, the sport is to compute $L^-_qv^1_{\bar a,b_1\ldots b_{s_1-1}}$ and $L^-_q v^2_{a_1\ldots a_{s_{2}-1},\bar b}$.

\begin{probleme}
Il y a du calcul non terminé, ici.
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{The quotient for the scalar singleton}
%---------------------------------------------------------------------------------------------------------------------------

The value of the energy which saturates the unitary condition is $1$ when $s=\frac{1}{ 2 }$ and $\frac{ 1 }{2}$ when $s=0$. That is the reason why we consider the two special representations
\begin{equation}
	\begin{aligned}[]
		\rDi&=\mD(1,\frac{ 1 }{2})		&& \rRac&=\mD(\frac{ 1 }{2},0).
	\end{aligned}
\end{equation}
We are now interested in the scalar case, the $Rac$.

We know that, when $\epsilon_0$ is the value of the energy which saturates the unitary condition $e_0\geq \frac{ d-3 }{ 2 }$ (in the scalar case, then
\begin{enumerate}
\item the vectors $L^+_sL^+_s\ket{\epsilon_0}$ are singular vectors,
\item the vectors $L^+_{r_1}\cdots L^+_{r_n}L^+_sL^+_s\ket{\epsilon_0}$ is orthogonal to all other states, it is a null vector.
\end{enumerate}
On the other hand, we know from proposition \ref{PropSinModRedSSIADesNuls} that a module is reducible if and only if it has a vector $\ket{v}\neq\ket{e_0,s}$ such that $L^-_r\ket{v}=0$ for every $r$. Thus one constructs irreducible representations by taking the quotient of the Verma module by the singular module. 

What is the dimension of the scalar singleton ? We have to count how many different vectors we have in the Verma module $\mV(e_0,0)\equiv \big\{   L_{r_1}^+\ldots L_{r_n}^+| e_0 \rangle   \big\}_{n=0}^{\infty}$, and which \emph{are not} build over $L^+_sL^+_s\ket{e_0}$. In the case of $\SO(2,3)$, we have the generators $L^+_1$, $L^+_2$ and $L^+_3$ (which are commuting), so the only vectors that are left after removing the singular modules are the seven following ones: $L^+_1\ket{e_0}$, $L^+_2\ket{e_0}$,$L^+_3\ket{e_0}$, $L^+_1L^+_2\ket{e_0}$, $L^+_1L^+_3\ket{e_0}$,$L^+_2L^+_3\ket{e_0}$, and $L^+_1L^+_2L^+_3\ket{e_0}$. The scalar singleton representation is thus $7$ dimensional.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Singletons as sections of bundle}
%---------------------------------------------------------------------------------------------------------------------------

If one wants to see singletons as a field theory, we have to build the following bundle, we have to follow the path of section \ref{subsec:incl_Lorentz}. So we consider the frame bundle over $M=AdS_4$, and an associated bundle for the scalar singleton representation:
\begin{equation}
 \xymatrix{
	\SO(2,3)  \ar@{~>}[r]	& L(G/H) \ar[d]_{\displaystyle \pi}	&	& E=L(G/H)\times_{\rho} V \\
				& M \ar[urr]_{\displaystyle\phi}	  
  }
\end{equation}
where $(V,\rho)$ is the scalar singleton representation of $\SO(2,3)$, and $L(G/H)$ is the frame bundle over $AdS_4$, as build page \pageref{PgFrameHomo}.

