% This is part of Mes notes de mathématique
% Copyright (c) 2011-2014
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Polynôme d'endomorphismes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme caractéristique}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( A\) un anneau commutatif et \( \eK\), un corps commutatif. L'injection canonique \( A\to A[X]\) se prolonge en une injection
\begin{equation}
   \eM(A)\to\eM\big( A[X] \big).
\end{equation}

\begin{definition}  \label{DefOWQooXbybYD}
    Si \( u\in\eM_n(A)\), nous définissons le \defe{polynôme caractéristique de \( u\)}{polynôme!caractéristique}\index{caractéristique!polynôme} :
    \begin{equation}    \label{Eqkxbdfu}
        \chi_u(X)=\det(X\mtu_n-u).
    \end{equation} 
    Ce faisons nous assimilons la matrice \( u\) et l'endomorphisme \( u\colon E\to E\) qu'elle définit. 
\end{definition}

\begin{lemma}
    Si \( u\) est un endomorphisme
    \begin{equation}
        I_u=\{ P\in \eK[X] \tq P(u)=0\}
    \end{equation}
    n'est pas vide.
\end{lemma}

\begin{proof}
    Nous avons un morphisme d'algèbre
    \begin{equation}
        \begin{aligned}
            \varphi_u\colon\eK[X]&\to \End(E) \\
            P&\mapsto P(u). 
        \end{aligned}
    \end{equation}
    Cet endomorphisme ne peut pas être injectif parce que \(\eK[X]\) est de dimension infinie tandis que \( \End(E)\) est de dimension finie. Il possède donc un noyau, c'est à dire qu'il existe \( P\in\eK[X]\) tel que \( P(X)=0\).
\end{proof}

\begin{definition}
    Le \defe{polynôme minimal}{polynôme!minimal!d'un endomorphisme}\index{minimal!polynôme!d'endomorphisme} de \( u\) est le générateur unitaire de \( I_u\). C'est le polynôme unitaire de plus petit degré qui annule \( u\). Nous le notons \( \mu_u\)\nomenclature[A]{\( \mu_A\)}{polynôme minimal de \( A\)} :
\begin{equation}
    \mu_u(u)=0.
\end{equation}
\end{definition}

\begin{lemma}
    Le polynôme \( \chi_u\) est unitaire et de degré \( n\).
\end{lemma}

\begin{lemma}       \label{LemjcztYH}
    Soit \( u\) un endomorphisme et \( E_{\lambda}(u)\)\nomenclature[A]{\( E_{\lambda}(u)\)}{Espace propre de \( u\)} ses espaces propres. La somme des \( V_{\lambda}\) est directe.
\end{lemma}

\begin{proof}
    Soit \( v_i\in V_{\lambda_i}\) un choix de vecteurs propres de \( u\). Si la somme n'est pas directe, nous pouvons considérer une combinaison linéaire des \( v_i\) qui soit nulle :
    \begin{equation}
        v_1+\ldots+v_p=0.
    \end{equation}
    Appliquons \( (A-\lambda_1\mtu)\) à cette égalité :
    \begin{equation}
        (\lambda_2-\lambda_1)v_1+\ldots+(\lambda_p-\lambda_1)v_p=0.
    \end{equation}
    En appliquant encore successivement les opérateurs \( (A-\lambda_i\mtu)\) nous réduisons le nombre de termes jusqu'à obtenir \( v_p=0\).
\end{proof}

\begin{theorem}     \label{ThoNhbrUL}
    Soit \( E\) un \(\eK\)-espace vectoriel de dimension finie \( n\) et un endomorphisme \( u\in\End(E)\). Alors
    \begin{enumerate}
        \item
            Le polynôme caractéristique divise \( (\mu_u)^n\) dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes facteurs irréductibles dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes racines dans \(\eK[X]\).
        \item
            Le polynôme caractéristique est scindé si et seulement si le polynôme minimal est scindé.
    \end{enumerate}
\end{theorem}

Si \( \lambda\in\eK\) est une racine de \( \chi_u\), l'ordre de l'annulation est la \defe{multiplicité algébrique}{multiplicité!algébrique d'une valeur propre} de la valeur propre \( \lambda\) de \( u\). À ne pas confondre avec la \defe{multiplicité géométrique}{multiplicité!géométrique} qui sera la dimension de l'espace propre.

\begin{example} \label{ExICOJcFp}
    Sur \( \eR^2\), nous considérons la matrice \( A=\begin{pmatrix}
        1    &   0    \\ 
        1    &   1    
    \end{pmatrix}\) qui a pour polynôme caractéristique le polynôme \( \chi_A=(X-1)^2\). Le nombre \( \lambda=1\) est une racine double de ce polynôme, et pourtant il n'y a qu'une seule dimension d'espace propre :
    \begin{equation}
        \begin{pmatrix}
            1    &   0    \\ 
            1    &   1    
        \end{pmatrix}\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}=\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}
    \end{equation}
    entraine \( x=0\).

    Ici la multiplicité algébrique est différente de la multiplicité géométrique.
\end{example}

\begin{theorem} \label{ThoWDGooQUGSTL}
    Soit \( u\in\End(E)\) et \( \lambda\in\eK\). Les conditions suivantes sont équivalentes
    \begin{enumerate}
        \item\label{ItemeXHXhHi}
            \( \lambda\in\Spec(u)\)
        \item\label{ItemeXHXhHii}
            \( \chi_u(\lambda)=0\)
        \item\label{ItemeXHXhHiii}
            \( \mu_u(\lambda)=0\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    \ref{ItemeXHXhHi} \( \Leftrightarrow\) \ref{ItemeXHXhHii}. Dire que \( \lambda\) est dans le spectre de \( u\) signifie que l'opérateur \( u-\lambda\mtu\) n'est pas inversible, ce qui est équivalent à dire que \( \det(u-\lambda\mtu)\) est nul ou encore que \( \lambda\) est une racine du polynôme caractéristique de \( u\). 

    \ref{ItemeXHXhHii} \( \Leftrightarrow\) \ref{ItemeXHXhHiii}. Cela est une application directe du théorème \ref{ThoNhbrUL} qui précise que le polynôme caractéristique a les mêmes racines dans \(\eK\) que le polynôme minimal.
\end{proof}

\begin{lemma}
    Une matrice triangulaire supérieure avec des \( 1\) sur la diagonale n'est diagonalisable que si elle est diagonale (c'est à dire si elle est la matrice unité).
\end{lemma}

\begin{proof}
    Si \( A\) est une matrice triangulaire supérieure de taille \( n\) telle que \( A_{ii}=1\), alors \( \det(A-\lambda\mtu)=(1-\lambda)^n\), ce qui signifie que \( \Spec(A)=\{ 1 \}\). Pour la diagonaliser, il faudrait une matrice \( P\in\GL(n,\eK)\) telle que \( \mtu=P^{-1}AP\), ce qui est uniquement possible si \( A=\mtu\).
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices semblables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[matrices semblables] \label{DefCQNFooSDhDpB}
    Sur l'ensemble \( \eM_n(\eK)\) des matrices \( n\times n\) à coefficients dans \(\eK\) nous introduisons la relation d'équivalence \( A\sim B\) si et seulement si il existe une matrice \( P\in\GL(n,\eK)\) telle que \( B=P^{-1}AP\). Deux matrices équivalentes en ce sens sont dites \defe{semblables}{semblables!matrices}.
\end{definition}

Le polynôme caractéristique est un invariant sous les similitudes. En effet si \( P\) est une matrice inversible,
\begin{subequations}
    \begin{align}
        \chi_{PAP^{-1}}&=\det(PAP^{-1}-\lambda X)\\
        &=\det\big( P^{-1}(PAP^{-1}-\lambda X)P^{-1} \big)\\
        &=\det(A-\lambda X).
    \end{align}
\end{subequations}

La permutation de lignes ou de colonnes ne sont pas de similitudes, comme le montrent les exemples suivants :
\begin{equation}
    \begin{aligned}[]
        A&=\begin{pmatrix}
            1    &   2    \\ 
            3    &   4    
        \end{pmatrix}&
        B&=\begin{pmatrix}
            2    &   1    \\ 
            4    &   3    
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Nous avons \( \chi_A=x^2-5x-2\) tandis que \( \chi_B=x^2-5x+2\) alors que le polynôme caractéristique est un invariant de similitude.

\begin{definition}  \label{DefCNJqsmo}
    Une matrice est \defe{diagonalisable}{diagonalisable} si elle est semblable à une matrice diagonale.
\end{definition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes d'endomorphismes}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( u\in\End(E)\) où \( E\) est un \( \eK\)-espace vectoriel. Nous considérons l'application
\begin{equation}    \label{EqOVKooeMJuv}
    \begin{aligned}
        \varphi_u\colon \eK[X]&\to \End(E) \\
        P&\mapsto P(u). 
    \end{aligned}
\end{equation}
L'image de \( \varphi_u\) est un sous-espace vectoriel. En effet si \( A=\varphi_u(P)\) et \( B=\varphi_u(Q)\), alors \( A+B=\varphi_u(P+Q)\) et \( \lambda A=(\lambda P)(u)\). En particulier c'est un espace fermé.

Soit \( u\) un endomorphisme d'un \( \eK\)-espace vectoriel \( E\) et \( P\), un polynôme. Nous disons que \( P\) est un polynôme \defe{annulateur}{polynôme!annulateur} de \( u\) si \( P(u)=0\) en tant que endomorphisme de \( E\).

\begin{lemma}       \label{LemQWvhYb}
    Si \( P\) et \( Q\) sont des polynômes dans \( \eK[X]\) et si \( u\) est un endomorphisme d'un \( \eK\)-espace vectoriel \( E\), nous avons
    \begin{equation}
        (PQ)(u)=P(u)\circ Q(u).
    \end{equation}
\end{lemma}

\begin{proof}
    Si \( P=\sum_i a_iX^i\) et \( Q=\sum_j b_jX^j\), alors le coefficient de \( X^k\) dans \( PQ\) est
    \begin{equation}        \label{EqCoefGPyVcv}
        \sum_la_lb_{k-l}.
    \end{equation}
    Par conséquent \( (PQ)(u)\) contient \( \sum_la_lb_{k-l}u^k\). Par ailleurs \( P(u)\circ Q(u)\) est donné par
    \begin{equation}
        \sum_ia_iu^i\left( \sum_jb_ju^j \right)(x)=\sum_{ij}a_ib_ju^{i+j}(x).
    \end{equation}
    Le coefficient du terme en \( u^k\) est bien le même que celui donné par \eqref{EqCoefGPyVcv}.
\end{proof}

\begin{theorem}[Décomposition des noyaux ou lemme des noyaux]       \label{ThoDecompNoyayzzMWod}
    Soit \( u\) un endomorphisme du \( \eK\)-espace vectoriel \( E\). Soit \( P\in\eK[X]\) un polynôme tel que \( P(u)=0\). Nous supposons que \( P\) s'écrive comme le produit \( P=P_1\ldots P_n\) de polynômes deux à deux étrangers. Alors
    \begin{equation}
        E=\ker P_1(u)\oplus\ldots\oplus\ker P_n(u).
    \end{equation}
    De plus les projecteurs associés à cette décomposition sont des polynômes en \( u\).
\end{theorem}
\index{lemme!des noyaux}
Ce résultat est utilisé pour prouver que toute représentation est décomposable en représentations irréductibles, proposition \ref{PropHeyoAN} ainsi que pour le théorème \ref{ThoDigLEQEXR} qui dit que si le polynôme minimal d'un endomorphisme est scindé à racine simple alors il est diagonalisable.

\begin{proof}
    Nous posons 
    \begin{equation}
        Q_i=\prod_{j\neq i}P_i.
    \end{equation}
    Par le lemme \ref{LemuALZHn} ces polynômes sont étrangers entre eux et le théorème de Bézout (théorème \ref{ThoBezoutOuGmLB}) donne l'existence de polynômes \( R_i\) tels que
    \begin{equation}
        R_1Q_1+\ldots+R_nQ_n=1.
    \end{equation}
    Si nous appliquons cette égalité à \( u\) et ensuite à \( x\in E\) nous trouvons
    \begin{equation}        \label{EqqVcpUy}
        \sum_{i=1}^n(R_iQ_i)(u)(x)=x,
    \end{equation}
    et en particulier si nous posons \( E_i=\Image\big(P_iQ_i(u)\big)\) nous avons
    \begin{equation}
        E=\sum_{i=1}^nE_i.
    \end{equation}
    Cette dernière somme n'est éventuellement pas une somme directe. Si \( i\neq j\), alors \( Q_iQ_j\) est multiple de \( P\) et nous avons, en utilisant le lemme \ref{LemQWvhYb}, 
    \begin{equation}
        (R_iQ_i)(u)\circ (R_jQ_j)(u)=\big( R_iQ_iR_jQ_j \big)(u)=S_{ij}(u)\circ P(u)=0
    \end{equation}
    où \( S_{ij}\) est un polynôme. 

    Nous pouvons voir \( E\) comme un \( \eK\)-module et appliquer le théorème \ref{ThoProjModpAlsUR}. Les opérateurs \( R_iQ_i(u)\) ont l'identité comme somme et sont orthogonaux, et nous avons donc la décomposition en somme directe :
    \begin{equation}
        E=\bigoplus_{i=1}^nR_iQ_i(u)E.
    \end{equation}

    Afin de terminer la preuve, nous devons montrer que \( R_iQ_i(u)E=\ker P_i(u)\). D'abord nous avons
    \begin{equation}
        P_iR_iQ_i(u)=(R_iP)(u)=R_i(u)\circ P(u)=0,
    \end{equation}
    par conséquent \( \Image(R_iQ_i(u))\subset \ker P_i(u)\). Pour obtenir l'inclusion inverse, nous reprenons l'équation \eqref{EqqVcpUy} avec \( x\in\ker P_i(u)\). Elle se réduit à
    \begin{equation}
        (R_iQ_i)(u)x=x.
    \end{equation}
    Par conséquent \( x\in\Image\big( R_iQ_i(u) \big)\).
\end{proof}

\begin{corollary}   \label{CorKiSCkC}
    Soit \( E\), un \( \eK\)-espace vectoriel de dimension finie et \( f\), un endomorphisme semi-simple dont la décomposition du polynôme minimal \( \mu_f\) en facteurs irréductibles sur \( \eK[X]\) est \( \mu_f=M_1^{\alpha_1}\cdots M_r^{\alpha_r}\). Si \( F\) est un sous-espace stable par \( f\), alors
    \begin{equation}
        F=\bigoplus_{i=1}^r\ker M_i^{\alpha_i}(f)\cap F
    \end{equation}
\end{corollary}

\begin{proof}
    Nous posons \( E_i=\ker M_i^{\alpha_i}(f)\) et \( F_i=E_i\cap F\). Les polynômes \( M_i^{\alpha_i}\) sont deux à deux étrangers et \( \mu_f(f)=0\), donc le lemme des noyaux (\ref{ThoDecompNoyayzzMWod}) s'applique et
    \begin{equation}
        E=E_1\oplus\ldots\oplus E_r.
    \end{equation}
    Nous pouvons décomposer \( x\in F\) en termes de cette somme :
    \begin{equation}     \label{EqbBbrdi}
        x=x_1+\ldots +x_r
    \end{equation}
    avec \( x_i\in E_i\). Toujours selon le lemme des noyaux, les projections sur les espaces \( E_i\) sont des polynômes en \( f\). Par conséquent \( F\) est stable sous toutes ces projections \( \pr_i\colon E\to E_i\), et en appliquant \( \pr_i\) à \eqref{EqbBbrdi}, \( \pr_i(x)=x_i\). Vu que \( x\in F\), le membre de gauche est encore dans \( F\) et \( x_i\in E_i\cap F\). Nous avons donc
    \begin{equation}
        F\subset\bigoplus_{i=1}^rF_i.
    \end{equation}
    L'inclusion inverse est immédiate parce que \( F_i\subset F\) pour chaque \( i\).
\end{proof}

\begin{lemma}   \label{LemVISooHxMdbr}
    Si \( x\) est un vecteur propre de valeur propre \( \lambda\) pour l'endomorphisme \( u\) et si \( P\) est un polynôme, alors \( x\) est vecteur propre de \( u\) pour la valeur propre \( P(\lambda)\).
\end{lemma}

\begin{proof}
    C'est un simple calcul de \( P(u)x\) en ayant noté \( P(X)=\sum_{k=0}^nc_kX^n\) :
    \begin{equation}
        P(u)x=\sum_{k=0}^nc_ku^k(x)=\sum_{k=0}^nc_k\lambda^ku=P(\lambda)x.
    \end{equation}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme minimal ponctuel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{Decyyumy}
    Soit \( E\), un espace vectoriel et \( f\colon E\to E\) un endomorphisme de \( E\). Pour chaque \( x\in E\) nous considérons l'idéal
    \begin{equation}
        I_{f,x}=\{ P\in \eK[X]\tq P(f)x=0 \}.
    \end{equation}
    C'est l'ensemble des polynômes qui annulent \( f\) en \( x\). Le générateur unitaire de \( I_{f,x}\) est le \defe{polynôme minimal ponctuel}{polynôme!minimal!ponctuel}\index{polynôme!minimal!relativement à un point} de \( f\) en \( x\). Il sera noté \( \mu_{f,x}\).
\end{definition}
Ces définitions sont légitimées par les faits suivants. L'idéal \( I_{f,x}\) n'est pas réduit à \( \{ 0 \}\) parce que le polynôme minimal de \( f\) fait partie de \( I_{f,x}\). C'est le théorème \ref{ThoCCHkoU} qui nous assure l'existence d'un unique générateur unitaire dans~\( I_{f,x}\). 

\begin{lemma}\label{LemSYsJJj}
    Soit \( f\colon E\to E\) un endomorphisme de l'espace vectoriel \( E\). Il existe un élément \( x\in E\) tel que \( \mu_{f,x}=\mu_f\).
\end{lemma}

\begin{proof}
    Nous savons que pour tout \( x\in E\), \( \mu_f\in I_{f,x}\), donc le polynôme \( \mu_{f,x}\) divise \( \mu_f\) pour tous les \( x\). Nous en déduisons que l'ensemble
    \begin{equation}
        \{ \mu_{f,x}\tq x\in E \}
    \end{equation}
    est en réalité un ensemble fini, sinon \( \mu_f\) ne serait pas un polynôme. Soient donc les points \( x_1,\ldots, x_l\) tels que
    \begin{equation}
        \{ \mu_{f,x}\tq x\in E \}=\{ \mu_{f,x_1},\ldots, \mu_{f,x_l} \}.
    \end{equation}
    Étant donné que \( x\in \ker\mu_{f,x}\) nous avons \( \mu_{f,x}\in I_{f,x}\) et donc \( \mu_{f,x}(f)x=0\). Par conséquent
    \begin{equation}
        E=\bigcup_{1\leq i\leq l}\ker\mu_{f,x_i(f)}.
    \end{equation}
    En vertu de la proposition \ref{PropTVKbxU}, un des termes de l'union doit être l'espace \( E\) entier. Il existe donc un \( x_i\) tel que
    \begin{equation}
        E=\ker\big( \mu_{f,x_i}(f) \big).
    \end{equation}
    Le polynôme \( \mu_{f,x_i}\) annule \( f\) et est donc divisé par le polynôme minimal de \( f\). Nous avons donc montré que \( \mu_{f,_{x_i}}\) divise et est divisé par \( \mu_f\). Par conséquent \( \mu_f=\mu_{f,x_i}\).
\end{proof}

\begin{lemma}   \label{LemrFINYT}
    Si le polynôme minimal d'un endomorphisme est irréductible, alors il est semi-simple.
\end{lemma}

\begin{proof}
    Soit \( f\), un endomorphisme dont le polynôme minimal est irréductible et \( F\), un sous-espace stable par \( f\). Nous devons en trouver un supplémentaire stable. Si \( F=E\), il n'y a pas de problèmes. Sinon nous considérons \( u_1\in E\setminus F\) et
    \begin{equation}
        E_{u_1}=\{ P(f)u_1\tq P\in \eK[X] \},
    \end{equation}
    qui est un espace stable par \( f\). 

    Montrons que \( E_{u_1}\cap F=\{ 0 \}\). Pour cela nous regardons l'idéal
    \begin{equation}
        I_{u_1}=\{ P\in \eK[X]\tq P(f)u_1=0 \}.
    \end{equation}
    Cela est un idéal non réduit à \( \{ 0 \}\) parce que le polynôme minimal de \( f\) par exemple est dans \( I_{u_1}\). Soit \( P_{u_1}\) un générateur unitaire de \( I_{u_1}\). Étant donné que \( \mu_f\in I_{u_1}\), nous avons que \( P_{u_1}\) divise \( \mu_f\) et donc \( P_{u_1}=\mu_f\) parce que \( \mu_f\) est irréductible par hypothèse.

    Soit \( y\in E_{u_1}\cap F\). Par définition il existe \( P\in\eK[X]\) tel que \( y=P(f)u_1\) et si \( y\neq 0\), ce la signifie que \( P\notin I_{u_1}\), c'est à dire que \( P_{u_1} \) ne divise pas \( P\). Étant donné que \( P_{u_1}\) est irréductible cela implique que \( P_{u_1}\) et \( P\) sont premiers entre eux (ils n'ont pas d'autre \( \pgcd\) que \( 1\)).

    Nous utilisons maintenant Bézout (théorème \ref{ThoBezoutOuGmLB}) qui nous donne \( A,B\in \eK[X]\) tels que 
    \begin{equation}
        AP+BP_{u_1}=1.
    \end{equation}
    Nous appliquons cette égalité à \( f\) et puis à \( u_1\):
    \begin{equation}
        u_1=A(f)\circ \underbrace{P(f)u_1}_{=y}+B(f)\circ \underbrace{P_{u_1}(u_1)}_{=0}=A(f)y.
    \end{equation}
    Mais \( y\in F\), donc \( A(f)y\in F\). Nous aurions donc \( u_1\in F\), ce qui est impossible par choix. Nous avons maintenant que l'espace \( E_{u_1}\oplus F\) est stable sous \( f\). Si cet espace est \( E\) alors nous arrêtons. Sinon nous reprenons le raisonnement avec \( E_{u_1}\oplus F\) en guise de \( F\) et en prenant \( u_2\in E\setminus(E_{u_1}\oplus F)\). Étant donné que \( E\) est de dimension finie, ce procédé s'arrête à un certain moment et nous aurons
    \begin{equation}
        E=F\oplus E_{u_1}\oplus\ldots\oplus E_{u_k}
    \end{equation}
    où chacun des \( E_{u_i}\) sont stables.
\end{proof}

\begin{theorem} \label{ThoFgsxCE}
    Un endomorphisme est semi-simple si et seulement si son polynôme minimal est produit de polynômes irréductibles distincts deux à deux.
\end{theorem}
\index{anneau!principal}

\begin{proof}

    Supposons que \( f\) soit semi-simple et que son polynôme minimal soit donné par \( \mu_f=M_1^{\alpha_1}\ldots M_r^{\alpha_r}\) où les \( M_i\) sont des polynômes irréductibles deux à deux distincts. Nous devons montrer que \( \alpha_i=1\) pour tout \( i\). Soit \( i\) tel que \( \alpha_i\geq 1\) et \( N\in \eK[X]\) tel que \( \mu_f=M^2N\) où l'on a noté \( M=M_i\). Nous étudions l'espace
    \begin{equation}
        F=\ker M(f)
    \end{equation}
    qui est stable par \( f\), et qui possède donc un supplémentaire \( S\) également stable par \( f\). Nous allons montrer que \( MN\) est un polynôme annulateur de \( f\).

    D'abord nous prenons \( x\in S\). Étant donné que \( F\) est le noyau de \( M(f)\),
    \begin{equation}
        M(f)\big( MN(f)x \big)=\mu_f(f)x=0,
    \end{equation}
    ce qui signifie que \( MN(f)x\in F\). Mais vu que \( S\) est stable par \( f\) nous avons aussi que \( MN(f)x\in S\). Finalement \( MN(f)x\in F\cap S=\{ 0 \}\). Autrement dit, \( MN(f)\) s'annule sur \( S\).

    Prenons maintenant \( y\in F\). Nous avons
    \begin{equation}
        MN(f)=N(f)\big( M(f)y \big)=0
    \end{equation}
    parce que \( y\in F=\ker M(f)\).

    Nous avons prouvé que \( MN(f)\) s'annule partout et donc que \( MN(f)\) est un polynôme annulateur de \( f\), ce qui contredit la minimalité de \( \mu_f=M^2N\).

    Nous passons au sens inverse. Soit \( m_f=M_1\ldots M_r\) une décomposition du polynôme minimal de l'endomorphisme \( f\) en irréductibles distincts deux à deux. Soit \( F\) un sous-espace vectoriel stable par \( f\). Nous notons
    \begin{equation}
        E_i=\ker(M_i(f))
    \end{equation}
    et \( f_i=f|_{E_i}\). Par le lemme \ref{CorKiSCkC} nous avons
    \begin{equation}
        F=\bigoplus_{i=1}^r(F\cap E_i).
    \end{equation}
    Les espaces \( E_i\) sont stables par \( f\) et étant donné que \( M_i\) est irréductible, il est le polynôme minimal de \( f_i\). En effet, \( M_i\) est annulateur de \( f_i\), ce qui montre que le minimal de \( f_i\) divise \( M_i\). Mais \( M_i\) étant irréductible, \( M_i\) est le polynôme minimal. Étant donné que \( \mu_{f_i}=M_i\), l'endomorphisme \( f_i\) est semi-simple par le lemme \ref{LemrFINYT}.

    L'espace \( F\cap E_i\) étant stable par l'endomorphisme semi-simple \( f_i\), il possède un supplémentaire stable que nous notons \( S_i\)~:
    \begin{equation}
        E_i=S_i\oplus(F\cap E_i).
    \end{equation}
    Étant donné que sur chaque \( S_i\) nous avons \( f|_{S_i}=f_i\), l'espace \( S=S_1\oplus\ldots\oplus S_r\) est stable par \( f\). Du coup nous avons
    \begin{subequations}
        \begin{align}
            E&=E_1\oplus\ldots\oplus E_r\\
            &=\big( S_1\oplus(F\cap E_1) \big)\oplus\ldots\oplus\big( S_r\oplus(F\cap E_r) \big)\\
            &=\big( \bigoplus_{i=1}^rS_i \big)\oplus\big( \bigoplus_{i=1}^rF\cap E_i \big)\\
            &=S\oplus F,
        \end{align}
    \end{subequations}
    ce qui montre que \( F\) a bien un supplémentaire stable par \( f\) et donc que \( f\) est semi-simple.
\end{proof}

\begin{proposition}     \label{PropAnnncEcCxj}
    Si \( P\) est un polynôme tel que \( P(u)=0\), alors le polynôme minimal \( \mu_u\) divise \( P\). Autrement dit, le polynôme minimal engendre l'idéal des polynômes annulateurs.
\end{proposition}

\begin{proof}
    L'ensemble \( I=\{ Q\in \eK[X]\tq Q(u)=0 \}\) est un idéal par le lemme \ref{LemQWvhYb}. Le polynôme minimal de \( u\) est un élément de degré plus bas dans \( I\) et par conséquent \( I=(\mu_u)\) par le théorème \ref{ThoCCHkoU}. Nous concluons que \( \mu_u\) divise tous les éléments de \( I\).
\end{proof}

\begin{example}[L'espace engendré par \( \mtu\), \( A\), \( A^2\),\ldots]
    Soit \( A\) une matrice, et 
    \begin{equation}
        V=\Span\{A^k\tq k\in \eN \}.
    \end{equation}
    Nous montrons que \( \dim(V)\) est le degré du polynôme minimal de \( A\).

    D'abord l'idéal annulateur de \( A\) est engendré par le polynôme minimal\footnote{Proposition \ref{PropAnnncEcCxj}.} que nous notons
        $\mu=\sum_{k=0}^pa_kX^k$.
    La partie \( \{ \mtu,\ldots, A^{p-1} \}\) est libre parce qu'une combinaison linéaire nulle de cela serait un polynôme annulateur en \( A\) de degré plus petit que \( p\). Donc \( \dim(V)\geq p\).

    La partie \( \{ \mtu,A,\ldots, A^p \}\) est liée à cause du polynôme minimal. Isoler \( A^p\) dans \( \mu(A)=0\) donne un polynôme \( f\) de degré \( p-1\) tel que \( A^p=f(A)\).

    Nous allons montrer à présent que la famille \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice (alors \( \dim(V)\leq p\)). Soit un entier \( q\geq p\)et de division euclidienne\footnote{Théorème \ref{ThoDivisEuclide}.} \( np+r=q\) avec \( r<p\). Nous avons \( A^q=A^{np}A^r\). D'une part
    \begin{equation}
        A^{np}=(A^p)^n=f(A)^n
    \end{equation}
    est de degré \( n(p-1)\). Par conséquent
    \begin{equation}
        A^q=f(A)^nA^r
    \end{equation}
    qui est de degré \( n(p-1)+r=q-n\). Autrement dit il existe un polynôme \( g_1\) de degré \( q-n\) tel que \( A^q=g_1(A)\). Si \( q-n>p-1\) alors nous pouvons recommencer et obtenir un polynôme \( g_2\) de degré strictement inférieur à celui de \( g_1\) tel que \( A^q=g_2(A)\). Au bout du compte, il existe un polynôme \( g\) de degré au maximum \( p-1\) tel que \( A^q=g(A)\). Cela prouve que la partie \( \{ \mtu,A,\ldots, A^{p-1} \}\) est génératrice de \( V\).

    La dimension de \( V\) est donc \( p\), le degré du polynôme minimal.
\end{example}

\begin{lemma}
    Soit \( f\) un endomorphisme cyclique d'un espace vectoriel \( E\) de dimension finie et \( y\), un vecteur cyclique de \( f\). Alors le polynôme minimal de \( f\) en \( y\) est le polynôme minimal de \( f\).
\end{lemma}

\begin{proof}
    En utilisant les notations de la définition \ref{Decyyumy}, nous devons démontrer que \( \mu_{f,y}=\mu_f\). Bien entendu, \( \mu_f\in I_{y,f}\), donc \( \mu_{f,y}\) divise \( \mu_f\). Montrons que \( \mu_{f,y}\) est un polynôme annulateur de \( f\). Dans ce cas \( \mu_f\) divisera \( \mu_{f,y}\) et le lemme sera démontré.

    Le vecteur \( y\) étant cyclique, tout élément de \( E\) s'écrit sous la forme \( x=P(f)y\) où \( P\) est un polynôme (de degré égal à la dimension de \( E\)). En utilisant le lemme \ref{LemQWvhYb} nous avons
    \begin{equation}
            \mu_{f,y}(f)x=\big( \mu_{f,y}(f)\circ P(f) \big)y
            =\big( P(f)\circ \mu_{f,y}(f) \big)y
            =0.
    \end{equation}
\end{proof}

Si \( f\) est un endomorphisme de l'espace vectoriel \( E\) et si \( x\in E\), nous notons 
\begin{equation}
    E_{f,x}=\Span\{ f^k(x)\tq k\in \eN \}.
\end{equation}

\begin{proposition}[\cite{RombaldiO}]\label{PropNrZGhT}
    Soit \( f\), un endomorphisme de \( E\) et \( x\in E\). Alors
    \begin{enumerate}
        \item
            L'espace \( E_{f,x}\) est stable par \( f\).
        \item\label{ItemfzKOCo}
            L'espace \( E_{f,x}\) est de dimension
            \begin{equation}
                p_{f,x}=\dim E_{f,x}=\deg(\mu_{f,x})
            \end{equation}
            où \( \mu_{f,x}\) est le générateur unitaire de \( I_{f,x}\).
        \item   \label{ItemKHNExH}
            Le polynôme caractéristique de \( f|_{E_{f,x}}\) est \( \mu_{f,x}\).
        \item   \label{ItemHMviZw}
            Nous avons
            \begin{equation}
                \chi_{f|_{E_{f,x}}}(f)x=\mu_{f,x}(f)x=0.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le fait que \( E_{f,x}\) soit stable par \( f\) est classique. Le point \ref{ItemHMviZw} est un une application du point \ref{ItemKHNExH}. Les deux gros morceaux sont donc les points \ref{ItemfzKOCo} et \ref{ItemKHNExH}.

    Étant donné que \( \mu_{f,x}\) est de degré minimal dans \( I_{f,x}\), l'ensemble
    \begin{equation}
        B=\{ f^k(x)\tq 0\leq k\leq p_{f,x}-1 \}
    \end{equation}
    est libre. En effet une combinaison nulle des vecteurs de \( B\) donnerait un polynôme en \( f\) de degré inférieur à \( p_{f,x}\) annulant \( x\). Nous écrivons
    \begin{equation}
        \mu_{f,x}(X)=X^{p_{f,x}}-\sum_{i=0}^{p_{f,x}-1}a_iX^k. 
    \end{equation}
    Étant donné que \( \mu_{f,x}(f)x=0\) et que la somme du membre de droite est dans \( \Span(B)\), nous avons \( f^{p_{f,x}}(x)\in\Span(B)\). Nous prouvons par récurrence que \( f^{p_{f,x}+k}(x)\in\Span(B)\). En effet en appliquant \( f^k\) à l'égalité
    \begin{equation}
        0=f^{p_{f,x}}(x)-\sum_{i=0}^{p_{f,x}-1}a_if^i(x)
    \end{equation}
    nous trouvons
    \begin{equation}
        f^{p_{f,x}+k}(x)=\sum_{i=0}^{p_{f,x}-1}a_if^{i+k}(x),
    \end{equation}
    alors que par hypothèse de récurrence le membre de droite est dans \( \Span(B)\). L'ensemble \( B\) est alors générateur de \( E_{f,x}\) et donc une base d'icelui. Nous avons donc bien \( \dim(E_{f,x})=p_{f,x}\).

    Nous montrons maintenant que \( \mu_{f,x}\) est annulateur de \( f\) au point \( x\). Nous savons que
    \begin{equation}
        \mu_{f,x}(f)x=0.
    \end{equation}
    En y appliquant \( f^k\) et en profitant de la commutativité des polynômes sur les endomorphismes (proposition \ref{LemQWvhYb}), nous avons
    \begin{equation}
        0=f^k\big( \mu_{f,x}(f)x \big)=\mu_{f,x}(f)f^k(x),
    \end{equation}
    de telle sorte que \( \mu_{f,x}(f)\) est nul sur \( B\) et donc est nul sur \( E_{f,x}\). Autrement dit,
    \begin{equation}
        \mu_{f,x}\big( f|_{E_{f,x}} \big)=0.
    \end{equation}
    Montrons que \( \mu_{f,x}\) est même minimal pour \( f|_{E_{f,x}}\). Sot \( Q\), un polynôme non nul de degré \( p_{f,x}-1\) annulant \( f|_{E_{f,x}}\). En particulier \( Q(f)x=0\), alors qu'une telle relation signifierait que \( B\) est un système lié, alors que nous avons montré que c'était un système libre. Nous concluons que \( \mu_{f,x}\) est le polynôme minimal de \( f|_{E_{f,x}}\).
\end{proof}

Cette histoire de densité permet de donner une démonstration alternative du théorème de Cayley-Hamilton.
\begin{theorem}[Cayley-Hamlilton]   \label{ThoCalYWLbJQ}
    Le polynôme caractéristique est un polynôme annulateur.
\end{theorem}
\index{théorème!Cayley-Hamilton}

Une démonstration plus simple via la densité des diagonalisables est donnée en théorème \ref{ThoHZTooWDjTYI}.
\begin{proof}
    Nous devons prouver que \( \chi_f(f)x=0\) pour tout \( x\in E\). Pour cela nous nous fixons un \( x\in E\), nous considérons l'espace \( E_{f,x}\) et \( \chi_{f,x}\), le polynôme caractéristique de \( f|_{E_{f,x}}\). Étant donné que \( E_{f,x}\) est stable par \( f\), le polynôme caractéristique de \( f|_{E_{j,x}}\) divise \( \chi_f\), c'est à dire qu'il existe un polynôme \( Q_x\) tel que
    \begin{equation}
        \chi_f=Q_x\chi_{f,x},
    \end{equation}
    et donc aussi
    \begin{equation}
        \chi_f(f)x=Q_x(f)\big( \chi_{f,x}(f)x \big)=0
    \end{equation}
    parce que la proposition \ref{PropNrZGhT} nous indique que \( \chi_{f,x}\) est un polynôme annulateur de \( f|_{E_{f,x}}\).
\end{proof}

Le polynôme de Cayley-Hamilton donne un moyen de calculer l'inverse d'un endomorphisme inversible pourvu que l'on sache son polynôme caractéristique. En effet, supposons que
\begin{equation}
    \chi_f(X)=\sum_{k=0}^na_kX^k.
\end{equation}
Nous aurons alors
\begin{equation}
    0=\chi_f(f)=\sum_{k=0}^na_kf^k.
\end{equation}
Nous appliquons \( f^{-1}\) à cette dernière égalité en sachant que \( f^{-1}(0)=0\) :
\begin{equation}
    0=a_0f^{-1}+\sum_{k=1}^na_kf^{k-1},
\end{equation}
et donc
\begin{equation}
    u^{-1}=-\frac{1}{ \det(f) }\sum_{k=1}^na_kf^{k-1}
\end{equation}
où nous avons utilisé le fait que \( a_0=\chi_f(0)=\det(f)\).

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Diagonalisation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Ici encore \( \eK\) est un corps commutatif.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes diagonalisables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LemgnaEOk}
    Soit \( F\) un sous-espace stable par \( u\). Soit une décomposition du polynôme minimal
    \begin{equation}
        \mu_u=P_1^{n_1}\ldots P_r^{n_r}
    \end{equation}
    où les \( P_i\) sont des polynômes irréductibles unitaires distincts. Si nous posons \( E_i=\ker P_i^{n_i}\), alors
    \begin{equation}
        F=(F\cap E_1)\oplus\ldots \oplus(F\cap E_r).
    \end{equation}
\end{lemma}

\begin{theorem}     \label{ThoDigLEQEXR}
    Soit \( E\), un espace vectoriel de dimension \( n\) sur le corps commutatif \( \eK\) et \( u\in\End(E)\). Les propriétés suivantes sont équivalentes.
    \begin{enumerate}
        \item\label{ItemThoDigLEQEXRiv}
            L'endomorphisme \( u\) est diagonalisable.
        \item       \label{ItemThoDigLEQEXRi}
            Il existe un polynôme \( P\in\eK[X]\) non constant, scindé sur \(\eK\) dont toutes les racines sont simples tel que \( P(u)=0\).
        \item\label{ItemThoDigLEQEXRii}
            Le polynôme minimal \( \mu_u\) est scindé sur \(\eK\) et toutes ses racines sont simples\footnote{Le polynôme \emph{caractéristique}, lui, n'a pas spécialement ses racines simples; il peut encore être de la forme
            \begin{equation}
                \chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i},
        \end{equation}
        mais alors \( \dim(E_{\lambda_i})=\alpha_i\). }.
        \item\label{ItemThoDigLEQEXRiii}
            Tout sous-espace de \( E\) possède un supplémentaire stable par \( u\).
    \end{enumerate}
\end{theorem}
\index{diagonalisable!et polynôme minimum scindé}

\begin{proof}
    \ref{ItemThoDigLEQEXRi}\( \Rightarrow\)\ref{ItemThoDigLEQEXRii}. Étant donné que \( P(u)=0\), il est dans l'idéal des polynôme annulateurs de \( u\), et le polynôme minimal \( \mu_u\) le divise parce que l'idéal des polynôme annulateurs est généré par \( \mu_u\) par le théorème \ref{ThoCCHkoU}.

    \ref{ItemThoDigLEQEXRii}\( \Rightarrow\)\ref{ItemThoDigLEQEXRiv}. Étant donné que le polynôme minimal est scindé à racines simples, il s'écrit sous forme de produits de monômes tous distincts, c'est à dire
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots(X-\lambda_r)
    \end{equation}
    où les \( \lambda_i\) sont des éléments distincts de \( \eK\). Étant donné que \( \mu_u(u)=0\), le théorème de décomposition des noyaux (théorème \ref{ThoDecompNoyayzzMWod}) nous enseigne que
    \begin{equation}
        E=\ker(u-\lambda_1)\oplus\ldots\oplus\ker(u-\lambda_r).
    \end{equation}
    Mais \( \ker(u-\lambda_i)\) est l'espace propre \( E_{\lambda_i}(u)\). Donc \( u\) est diagonalisable.

    \ref{ItemThoDigLEQEXRiv}\( \Rightarrow\)\ref{ItemThoDigLEQEXRiii}. Soit \( \{ e_1,\ldots, e_n \}\) une base qui diagonalise \( u\), soit \( F\) un sous-espace de \( E\) un \( \{ f_1,\ldots, f_r \}\) une base de \( F\). Par le théorème \ref{ThoBaseIncompjblieG} (qui généralise le théorème de la base incomplète), nous pouvons compléter la base de \( F\) par des éléments de la base \( \{ e_i \}\). Le complément ainsi construit est invariant par \( u\).

    \ref{ItemThoDigLEQEXRiii}\( \Rightarrow\)\ref{ItemThoDigLEQEXRiv}. En dimension un, tout endomorphisme est diagonalisable, nous supposons donc que \( \dim E=n\geq 2\). Nous procédons par récurrence sur le nombre de vecteurs propres connus de \( u\). Supposons avoir déjà trouvé \( p\) vecteurs propres \( e_1,\ldots, e_p\) de \( u\). Considérons \( H\), un hyperplan qui contient les vecteurs \( e_1,\ldots, e_p\). Soit \( F\) un supplémentaire de \( H\) stable par \( u\); par construction \( \dim F=1\) et si \( e_{p+1}\in F\), il doit être vecteur propre de \( u\).

    \ref{ItemThoDigLEQEXRiv}\( \Rightarrow\)\ref{ItemThoDigLEQEXRi}. Nous supposons maintenant que \( u\) est diagonalisable. Soient \( \lambda_1,\ldots, \lambda_r\) les valeurs propres deux à deux distinctes, et considérons le polynôme
    \begin{equation}
        P(x)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Alors \( P(u)=0\). En effet si \( e_i\) est un vecteur propre pour la valeur propre \( \lambda_i\), 
    \begin{equation}
        P(u)e_i=\prod_{j\neq i}(u-\lambda_j)\circ(u-\lambda_i)e_i=0
    \end{equation}
    par le lemme \ref{LemQWvhYb}. Par conséquent \( P(u)\) s'annule sur une base.
\end{proof}

\begin{corollary}       \label{CorQeVqsS}
    Si \( u\) est diagonalisable et si \( F\) est une sous-espace stable par \( u\), alors
    \begin{equation}
        F=\sum_{\lambda}E_{\lambda}(u)\cap F
    \end{equation}
    où \( E_{\lambda}(u)\) est l'espace propre de \( u\) pour la valeur propre \( \lambda\). En particulier la restriction de \( u\) à \( F\), \( u|_F\) est diagonalisable.
\end{corollary}

\begin{proof}
    Par le théorème \ref{ThoDigLEQEXR}, le polynôme \( \mu_x\) est scindé et ne possède que des racines simples. Notons le
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Les espaces \( E_i\) du lemme \ref{LemgnaEOk} sont maintenant les espaces propres.

    En ce qui concerne la diagonalisabilité de \( u|_F\), notons que nous avons une base de \( F\) composée de vecteurs dans les espaces \( E_{\lambda}(u)\). Cette base de \( F\) est une base de vecteurs propres de \( u\).
\end{proof}

\begin{lemma}
    Soit \( E\) un \( \eK\)-espace vectoriel et \( u\in\End(E)\). Si \( \Card\big( \Spec(u) \big)=\dim(E)\) alors \( u\) est diagonalisable.
\end{lemma}

\begin{proof}
    Soient \( \lambda_1,\ldots, \lambda_n\) les valeurs propres distinctes de \( u\). Nous savons que les espaces propres correspondants sont en somme directe (lemme \ref{LemjcztYH}). Par conséquent \( \Span\{ E_{\lambda_i}(u) \}\) est de dimension \( n\) est \( u\) est diagonalisable.
\end{proof}

Voici un résultat de diagonalisation simultanée. Nous donnerons un résultat de trigonalisation simultanée dans le lemme \ref{LemSLGPooIghEPI}.
\begin{proposition}[Diagonalisation simultanée]     \label{PropGqhAMei}
    Soit \( (u_i)_{i\in I}\) une famille d'endomorphismes qui commutent deux à deux.
    \begin{enumerate}
        \item       \label{ItemGqhAMei}
            Si \( i,j\in I\) alors tout sous-espace propre de \( u_i\) est stable par \( u_j\). Autrement dit \( u_j\big(E_{\lambda}(u)\big)\subset E_{\lambda}(u)\).
        \item
            Si les \( u_i\) sont diagonalisables, alors ils le sont simultanément.
    \end{enumerate}
\end{proposition}
\index{diagonalisation!simultanée}

\begin{proof}
    Supposons que \( u_i\) et \( u_j\) commutent et soit \( x\) un vecteur propre de \( u_i\) : \( u_ix=\lambda x\). Nous montrons que \( u_jx\in E_{\lambda}(u)\). Nous avons
    \begin{equation}
        u_i\big( u_j(x) \big)=u_j\big( u_i(x) \big)=\lambda u_j(x).
    \end{equation}
    Par conséquent \( u_j(x)\) est vecteur propre de \( u_i\) de valeur propre \( \lambda\).

    Montrons maintenant l'affirmation à propos des endomorphismes simultanément diagonalisables. Si \( \dim E=1\), le résultat est évident. Nous supposons également qu'aucun des \( u_i\) n'est multiple de l'identité. Nous effectuons une récurrence sur la dimension.

    Soit \( u_0\) un des \( u_i\) et considérons ses valeurs propres deux à deux distinctes \( \lambda_1,\ldots, \lambda_r\). Pour chaque \( k\) nous avons
    \begin{equation}
        E_{\lambda_k}(u_0)\neq E,
    \end{equation}
    sinon \( u_0\) serait un multiple de l'identité. Par contre le fait que \( u_0\) soit diagonalisable permet de décomposer \( E\) en espaces propres de \( u_0\) :
    \begin{equation}
        E=\bigoplus_{k}E_{\lambda_k}(u_0).
    \end{equation}
    Ce que nous allons faire est de simultanément diagonaliser les \( (u_i)_{i\in I}\) sur chacun des \( E_{\lambda_k}\) séparément. Par le point \ref{ItemGqhAMei}, nous avons \( u_i\colon E_{\lambda_k}(u_0)\to E_{\lambda_k}(u_0)\), et nous pouvons considérer la famille d'opérateurs
    \begin{equation}
        \left( u_i|_{E_{\lambda_k}(u_0)} \right)_{i\in I}.
    \end{equation}
    Ce sont tous des opérateurs qui commutent et qui agissent sur un espace de dimension plus petite. Par hypothèse de récurrence nous avons une base de \( E_{\lambda_k}(u_0)\) qui diagonalise tous les \( u_i\).
\end{proof}

\begin{example}     \label{ExewINgYo}
    Soit un espace vectoriel sur un corps \( \eK\). Un opérateur \defe{involutif}{involution} est un opérateur différent de l'identité dont le carré est l'identité. Typiquement une symétrie orthogonale dans \( \eR^3\). Le polynôme caractéristique d'une involution est \( X^2-1=(X+1)(X-1)\).
    
    Tant que \( 1\neq -1\), \( X^1-1\) est donc scindé à racines simples et les involutions sont diagonalisables (\ref{ThoDigLEQEXR}). Cependant si le corps est de caractéristique \( 2\), alors \( X^2-1=(X+1)^2\) et l'involution n'est plus diagonalisable.

    Par exemple si le corps est de caractéristique \( 2\), nous avons
    \begin{subequations}
        \begin{align}
            A&=\begin{pmatrix}
                1    &   1    \\ 
                0    &   1    
            \end{pmatrix}\\
            A^1&=\begin{pmatrix}
                1    &   2    \\ 
                0    &   1    
            \end{pmatrix}=\begin{pmatrix}
                1    &   0    \\ 
                0    &   1    
            \end{pmatrix}.
        \end{align}
    \end{subequations}
    Ce \( A\) est donc une involution mais n'est pas diagonalisable.
\end{example}


\begin{proposition} \label{PropleGdaT}
    Soit \( p\) un nombre premier et \( P\) un élément de \( \eF_p[X]\). L'anneau \( \eF_p[X]/(P)\) est intègre si et seulement si \( P\) est irréductible dans \( \eF_p[X]\).
\end{proposition}

\begin{proof}
    Supposons que \( P\) soit réductible dans \( \eF_p[X]\), c'est à dire qu'il existe \( Q,R\in \eF_p[X]\) tels que \( P=QR\). Dans ce cas, \( \bar Q\) est diviseur de zéro dans \( \eF_p[X]/(P)\) parce que \( \bar Q\bar R=0\).

    Nous supposons maintenant que \( \eF_p[X]/(P)\) ne soit pas intègre : il existe des polynômes \( R,Q\in \eF_p[X]\) tels que \( \bar Q\bar R=0\). Dans ce cas le polynôme \( QR\) est le produit de \( P\) par un polynôme : \( QR=PA\). Tous les facteurs irréductibles de \( A \) étant soit dans \( Q\) soit dans \( R\), il est possible de modifier un peu \( Q\) et \( R\) pour obtenir \( QR=P\), ce qui signifie que \( P\) n'est pas irréductible.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Un peu de structure dans \texorpdfstring{$ \eZ[i]$}{Zi}}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}   \label{LemSCAlICY}
     L'application
     \begin{equation}
         \begin{aligned}
             N\colon \eZ[i]&\to \eN \\
             a+bi&\mapsto a^2+b^2 
         \end{aligned}
     \end{equation}
     est un stathme euclidien pour \( \eZ[i]\).
\end{lemma}
\index{stathme!sur \( \eZ[i]\)}

\begin{proof}
    Soient \( t,t\in \eZ[i]\setminus\{ 0 \}\) et 
    \begin{equation}
        \frac{ z }{ t }=x+iy
    \end{equation}
    dans \( \eC\). Nous considérons \( q=a+bi\) où \( a\) et \( b\) sont les entiers les plus proches de \( x\) et \( y\). Si il y a \emph{ex aequo}, on prend au hasard\footnote{Dans l'exemple \ref{ExwqlCwvV}, nous prenions toujours l'inférieur parce que le stathme tenait compte de la positivité.}. Alors nous avons
    \begin{equation}
        | \frac{ z }{ t }-q |\leq \frac{ | 1+i | }{ 2 }=\frac{ \sqrt{2} }{2}<1.
    \end{equation}
    On pose \( r=z-qt\) qui est bien un élément de \( \eZ[i]\). De plus
    \begin{equation}
        | r |=| z-qt |=| t | |\frac{ z }{ t }-q |<| t |,
    \end{equation}
    c'est à dire que \( | r |^2<| t |^2\) et donc \( N(r)<N(t)\). 
\end{proof}
Étant donné que \( \eZ[i]\) est euclidien, il est principal (proposition \ref{Propkllxnv}).

\begin{lemma}   \label{LemBMEIiiV}
    Les éléments inversibles de \( \eZ[i]\) sont \( \{ \pm 1,\pm i \}\).
\end{lemma}

\begin{proof}

    Déterminons les éléments inversibles de \( \eZ[i]\). Si \( z\in \eZ[i]^*\), alors il existe \( z'\in \eZ[i]^*\) tel que \( zz'=1\). Dans ce cas nous aurions
    \begin{equation}
        1=N(zz')=N(z)N(z'),
    \end{equation}
    ce qui est uniquement possible avec \( N(z)=N(z')=1\), c'est à dire \( z=\pm 1\) ou \( z=\pm i\). Nous avons donc
    \begin{equation}
        \eZ[i]^*=\{ \pm 1,\pm i \}.
    \end{equation}
\end{proof}

Nous notons \( \Sigma=\{ a^2+b^2\tq a,b\in \eN \}\). 
\begin{lemma}   \label{LemIBDPzMB}
    L'ensemble \( \Sigma\) est un sous-monoïde de \( \eN\).
\end{lemma}

\begin{proof}
    Il suffit de prouver que si \( m,n\in \Sigma\), alors le produit \( mn\) est également dans \( \Sigma\). Si \( N\) est le stathme euclidien sur \( \eZ[i]\), alors  \( n\in \Sigma\) si et seulement si il existe \( z\in \eZ[i]\) tel que \( N(z)=n\). Si \( z,z'\in \eZ[i]\), alors \( zz'\in \eZ[i]\) et de plus
    \begin{equation}
        N(zz')=N(z)N(z')=nm.
    \end{equation}
    Donc \( nm\) est l'image de \( zz'\) par \( N\), ce qui prouve que \( nm\in \Sigma\).
\end{proof}

\begin{theorem}[Théorème des deux carrés, version faible]   \label{ThospaAEI}
    Un nombre premier est somme de deux carrés si et seulement si \( p=2\) ou \( p\in[1]_4\).
\end{theorem}
\index{anneau!principal}
\index{nombre!premier}
\index{théorème!des deux carrés!version faible}

\begin{remark}
    Il n'est pas dit que les nombres dans \( [1]_4\) sont premiers (\( 9=8+1\) ne l'est pas par exemple). Le théorème signifie que (à part \( 2\)), si un nombre premier est dans \( [1]_4\) alors il est somme de deux carrés, et inversement, si un nombre premier est somme de deux carrés, il est dans \( [1]_4\).
\end{remark}

\begin{proof}
    Soit \( p\) un nombre premier dans \( \Sigma\). Si \( a=2k\), alors \( a^2=4k^2\) et \( a^2=0\mod 4\). Si au contraire \( a\) est impair, \( a=2k+1\) et \( a^2=4k^2+1+4k=1\mod 4\). La même chose est valable pour \( b\). Par conséquent, \( a^2+b^2\) est automatiquement \( [0]_4\), \( [1]_4\) ou \( [2]_4\). Évidemment les nombres de la forme \( 0\mod 4\) ne sont pas premiers; parmi les \( 2\mod 4\), seul \( p=2\) est premier (et vaut \( 1^2+1^2\)).

    Nous avons démontré que les seuls premiers de la forme \( a^2+b^2\) sont \( p=2\) et les \( p=1\mod 4\). Il reste à faire le contraire : démontrer que si un nombre premier \( p\) vaut \( 1\mod 4\), alors il est premier. Nous considérons l'anneau
    \begin{equation}
        \eZ[i]=\{ a+bi\tq a,b\in \eZ \}.
    \end{equation}
    puis l'application
    \begin{equation}
        \begin{aligned}
            N\colon \eZ[i]&\to \eN \\
            a+bi&\mapsto a^2+b^2. 
        \end{aligned}
    \end{equation}
    Un peu de calcul dans \( \eC\) montre que pour tout \( z,z'\in \eZ[i]\), \( N(zz')=N(z)N(z')\).


    Nous savons que les éléments inversibles de \( \eZ[i]\) sont \( \pm 1\) et \( \pm i\) (lemme \ref{LemBMEIiiV}).

    Le lemme \ref{LemSCAlICY} montre que \( \eZ[i]\) est un anneau euclidien parce que \( N\) est un stathme. L'anneau \( \eZ[i]\) étant euclidien, il est principal (proposition \ref{Propkllxnv}).

   

    Pour la suite, nous allons d'abord montrer que \( p\in\Sigma\) si et seulement si \( p\) n'est pas irréductible dans \( \eZ[i]\), puis nous allons voir quels sont les irréductibles de \( \eZ[i]\).

    Soit \( p\), un nombre premier dans \( \Sigma\). Si \( p=a^2+b^2\), alors nous avons \( p=(a+ib)(a-bi)\), mais étant donné que \( p\) est premier, nous avons \( a\neq 0\) et \( b\neq 0\). Du coup \( p\) n'est pas inversible dans \( \eZ[i]\), mais il peut être écrit comme le produit de deux non inversibles. Le nombre \( p\) est donc non irréductible dans \( \eZ[i]\).

    Dans l'autre sens, nous supposons que \( p\) est un nombre premier non irréductible dans \( \eZ[i]\). Nous avons alors \( p=zz'\) avec ni \( z\) ni \( z'\) dans \( \{ \pm 1,\pm i \}\). En appliquant \( N\) nous avons
    \begin{equation}
        p^2=N(p)=N(z)N(z').
    \end{equation}
    Vu que \( p\) est premier, cela est uniquement possible avec \( N(z)=N(z')=p\) (avoir \( N(z)=1\) est impossible parce que cela dirait que \( z\) est inversible). Si \( z=a+ib\), alors \( p=N(z)=a^2+b^2\), et donc \( p\in \Sigma\).

    Nous savons déjà que \( \eZ[i]\) est un anneau principal et n'est pas un corps; la proposition \ref{PropomqcGe} s'applique donc et \( p\) sera non irréductible si et seulement si l'idéal \( (p)\) sera non premier. Le fait que \( (p)\) soit un idéal non premier implique que le quotient \( \eZ[i]/(p)\) est non intègre (c'est la définition d'un idéal premier). Nous cherchons donc les nombres premiers pour lesquels le quotient \( \eZ[i]/(p)\) n'est pas intègre.

    Nous commençons par écrire le quotient \( \eZ[i]/(p)\) sous d'autres formes. D'abord en remarquant que si \( I\) et \( J\) sont deux idéaux, on a \( (\eA/I)/J\simeq (\eA/J)/I\), du coup, en tenant compte du fait que \( \eZ[i]=\eZ[X]/(X^2+1)\), nous avons
    \begin{equation}
        \eZ[i]/(p)=(\eZ[X]/(p))/(X^2+1)=\eF_p[X]/(X^2+1).
    \end{equation}
    Nous avons donc équivalence des propositions suivantes :
    \begin{subequations}
        \begin{align}
            p\in\Sigma\\
            \eF_p[X]/(X^2+1)\text{ n'est pas intègre}\\
            X^2+1\text{ n'est pas irréductible dans \( \eF_p\)} \label{EqZkdrvh}\\
            \text{\( X^2+1\) admet une racine dans \( \eF_p\)}\\
            -1\in (\eF_p^*)^2\\
            \exists y\in \eF_p^*\tq y^2=-1.
        \end{align}
    \end{subequations}
    Le point \eqref{EqZkdrvh} vient de la proposition \ref{PropleGdaT}. Maintenant nous utilisons le fait que \( p\) soit un premier impair (parce que le cas de \( p=2\) est déjà complètement traité), donc \( (p-1)/2\in \eN\) et nous avons, pour le \( y\) de la dernière ligne,
    \begin{equation}
        (-1)^{(p-1)/2}=(y^2)^{(p-1)/2}=y^{p-1}=1
    \end{equation}
    parce que dans \( \eF_p\) nous avons \( y^{(p-1)}=1\) par le petit théorème de Fermat (théorème \ref{ThoOPQOiO}). Du coup \( p\) doit vérifier
    \begin{equation}
        1=(-1)^{(p-1)/2},
    \end{equation}
    c'est à dire \( \frac{ p-1 }{2}=0\mod 2\) ou encore \( p=1\mod 4\).
\end{proof}

\begin{theorem}[Théorème des deux carrés\cite{KXjFWKA}]
    Soit \( n\geq 2\) un nombre dont nous notons
    \begin{equation}    \label{EqBMHTzCT}
        n=\prod_{p\in\pP}p^{v_p(n)}
    \end{equation}
    où \( \pP\) est l'ensemble des nombres premiers. Alors \( n\in \Sigma\) si et seulement si pour tout \( p\in\pP\cap[3]_4\), nous avons \( v_p(n)\in [0]_2\) (c'est à dire \( v_p(n)\) est pair).
\end{theorem}
\index{théorème!des deux carrés}
\index{nombre!premier!théorème des deux carrés}
\index{anneau!principal!utilisation}
%TODO : il y a un lien entre le théorème des deux carrés et les triplets pytagoritiens http://fr.wikipedia.org/wiki/Triplet_pythagoricien

\begin{proof}
    \begin{subproof}
    \item[Condition suffisante.]
        
        Le produit \eqref{EqBMHTzCT} est évidemment un produit fini que nous pouvons alors regrouper en quatre parties : \( \pP\cap[0]_4\), \( \pP\cap[1]_4\), \( \pP\cap[2]_4\) et \( \pP\cap[3]_4\).

        \begin{itemize}
            \item Il n'y a pas de nombres premiers dans \( [0]_4\).
            \item Les nombres premiers de \( [1]_4\) sont dans \( \Sigma\). Le produit d'éléments de \( \Sigma\) étant dans \( \Sigma\), nous avons
                \begin{equation}
                    \prod_{p\in\pP\cap[1]_4}p^{v_p(n)}\in \Sigma.
                \end{equation}
            \item
                Le seul nombre premier dans \( [2]_4\) est \( 2\). C'est un élément de \( \Sigma\).
            \item
                Le produit
                \begin{equation}
                    \prod_{p\in\pP\cap[3]_4}p^{v_p(n)}
                \end{equation}
                est par hypothèse un produit de carrés (\( v_p(n)\) est pair), et est donc un carré.
        \end{itemize}
        Au final le produit \( \prod_{p\in\pP}p^{v_p(n)}\) est un produit d'un carré par un élément de \( \Sigma\), ce qui est encore un élément de \( \Sigma\).

        Pour cette partie, nous avons utilisé et réutilisé le lemme \ref{LemIBDPzMB}.

    \item[Condition nécessaire.] 

        Soit \( p\), un nombre premier. Nous voulons montrer que
        \begin{equation}
            \{ v_p(n)\tq n\in \Sigma \}\subset [2]_2.
        \end{equation}
        Pour montrer cela nous allons procéder par récurrence sur les ensembles
        \begin{equation}
            E_k=\{ v_p(n)\tq n\in \Sigma \}\cap\{ 0,\ldots, k \}.
        \end{equation}
        Il est évident que les éléments de \( E_0\) sont pairs, vu qu'il n'y a que zéro, qui est pair.

        Supposons que \( E_k\subset[0]_2\), et montrons que \( E_{k+1}\subset[0]_2\). Soit un élément de \( E_{k+1}\), c'est à dire \( v_p(n)\leq k+1\) avec \( n=a^2+b^2\). Si \( v_p(n)=0\) alors l'affaire est réglée; sinon c'est que \( p\) divise \( n\). Mais dans \( \eZ[i]\) nous avons
        \begin{equation}
            n=a^2+b^2=(a+bi)(a-bi)
        \end{equation}
        Vu que \( \eZ[i]\) est principal, le lemme de Gauss \ref{LemSdnZNX} nous dit que si \( p\) divise \( n\), alors il doit diviser soit \( a+bi\), soit \( a-bi\) (et du coup en fait les deux). Nous avons alors \( p\divides a\) et \( p\divides b\) en même temps. Du coup
        \begin{equation}
            p^2\divides a^2+b^2=n.
        \end{equation}
        Posons \( a=pa'\) et \( b=pb'\) avec \( a',b'\in \eN\). Nous avons
        \begin{equation}
            \frac{ n }{ p^2 }=\frac{ p^2a'^2+p^2b'^2 }{ p^2 }=a'^2+b'^2\in \Sigma.
        \end{equation}
        Mais par construction,
        \begin{equation}
            v_p\left( \frac{ n }{ p^2 } \right)=v_p(n)-2<k.
        \end{equation}
        Donc \( v_p(\frac{ n }{ p^2 })\) est pair et du coup \( v_p(n)\) doit également être pair.

    \end{subproof}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Théorème de Burnside}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LemwXXzIt}
    Soit \( P\), un polynôme sur \( \eK\). Une racine de \( P\) est une racine simple si et seulement si elle n'est pas racine de \( P'\).
\end{lemma}

\begin{theorem}     \label{ThoBurnsideoPuCtS}
    Toute représentation d'un groupe d'exposant fini sur \( \eC^n\) a une image finie.
\end{theorem}

Dans le cas d'un groupe abélien, la démonstration est facile. Étant donné que \( G\) est d'exposant fini, il existe \( \alpha\in \eN^*\) tel que \( g^{\alpha}=e\) pour tout \( g\in G\). Le polynôme \( P(X)=X^{\alpha}-1\) est scindé à racines simples. En effet tout polynôme sur \( \eC\) est scindé. Le fait qu'il soit à racines simples provient du lemme \ref{LemwXXzIt} parce que si \( a^{\alpha}=1\), alors il n'est pas possible d'avoir \( \alpha a^{\alpha-1}=0\).

Par ailleurs \( P(g)=0\). Le fait que nous ayons un polynôme annulateur de \( g\) scindé à racines simples implique que \( g\) est diagonalisable (théorème \ref{ThoDigLEQEXR}). Le fait que \( G\) soit abélien montre qu'il existe une base de \( \eC^n\) dans laquelle tous les éléments de \( G\) sont diagonaux. Nous devons par conséquent montrer qu'il existe un nombre fini de matrices de la forme
\begin{equation}
    \begin{pmatrix}
        \lambda_1    &       &       \\
            &   \ddots    &       \\
            &       &   \lambda_n
    \end{pmatrix}.
\end{equation}
Nous savons que \( \lambda_i^{\alpha}=1\) parce que \( g^{\alpha}=\mtu\), par conséquent chacun des \( \lambda_i\) est une racine de l'unité dont il n'existe qu'un nombre fini.

Le résultat reste vrai si \( G\) n'est pas abélien, mais la preuve devient plus compliquée. C'est le \wikipedia{fr}{Théorème_de_Burnside_(problème_de_1902)}{théorème de Burnside}.\index{théorème!Burnside}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons maintenant le cas de l'espace \( E=\eC^n\) comme espace vectoriel de dimension \( n\) sur \( \eC\). Il est muni d'une forme sesquilinéaire
\begin{equation}    \label{EqFormSesqQrjyPH}
    \langle x, y\rangle =\sum_{k=1}^nx_k\bar y_k
\end{equation}
pour tout \( x,y\in\eC^n\).

\begin{lemma}
    Pour un opérateur hermitien,
    \begin{enumerate}
        \item
            le spectre est réel,
        \item
            deux vecteurs propres à des valeurs propres distinctes sont orthogonales\footnote{Pour la forme \eqref{EqFormSesqQrjyPH}.}.
    \end{enumerate}
\end{lemma}
\index{spectre!matrice hermitienne}

\begin{proof}
    Soit \( v\) un vecteur de valeur propre \( \lambda\). Nous avons d'une part 
    \begin{equation}
        \langle Av, A\rangle =\lambda\langle v, v\rangle =\lambda\| v \|^2,
    \end{equation}
    et d'autre part, en utilisant le fait que \( A\) est hermitien,
    \begin{equation}
        \langle Av, v\rangle =\langle v, A^*v\rangle =\langle v, Av\rangle =\bar\lambda\| v \|^2,
    \end{equation}
    par conséquent \( \lambda=\bar\lambda\) parce que \( v\neq 0\).

    Soient \( \lambda_i\) et \( v_i\) (\( i=1,2\)) deux valeurs propres de \( A\) avec leurs vecteurs propres correspondants. Alors d'une part
    \begin{equation}
        \langle Av_1, v_2\rangle =\lambda_1\langle v_1, v_2\rangle ,
    \end{equation}
    et d'autre part
    \begin{equation}
        \langle Av_1, v_2\rangle =\langle v_1, Av_2\rangle =\lambda_2\langle v_1, v_2\rangle .
    \end{equation}
    Nous avons utilisé le fait que \( \lambda_2\) était réel. Par conséquent, soit \( \lambda_1=\lambda_2\), soit \( \langle v_1, v_2\rangle =0\).
\end{proof}

Le lemme suivant est utile en soi et dit que toute matrice complexe est trigonalisable. Une démonstration alternative passant par le polynôme caractéristique sera présentée dans la remarque \ref{RemXFZTooXkGzQg} utilisant la proposition \ref{PropKNVFooQflQsJ}.
\begin{lemma}[Lemme de Schur complexe\cite{NormHKNPKRqV}]  \label{LemSchurComplHAftTq}
    Si \( A\in\eM(n,\eC)\), il existe une matrice unitaire \( U\) telle que \( UAU^{-1}\) soit triangulaire supérieure.
\end{lemma}
\index{lemme!Schur complexe}
%TODO : Le lemme de Schur est souvent énoncé en disant que si p est une représentation irréductible, alors les seuls endomorphismes de V commutant avec tous les p(g) sont les multiples de l'idenditié. Quel est le lien avec ceci ?

\begin{proof}
    Étant donné que \( \eC\) est algébriquement clos, nous pouvons toujours considérer un vecteur propre \( v_1\) de \( A\), de valeur propre \( \lambda_1\). Nous pouvons utiliser un procédé de Gram-Schmidt pour construire une base orthonormée \( \{ v,u_2,\ldots, u_n \}\) de \( \eR^n\), et la matrice (unitaire)
    \begin{equation}
        Q=\begin{pmatrix}
             \uparrow   &   \uparrow    &       &   \uparrow    \\
             v   &   u_2    &   \cdots    &   u_n    \\ 
             \downarrow   &   \downarrow    &       &   \downarrow
         \end{pmatrix}.
    \end{equation}
    Nous avons \( Q^{-1}AQe_1=Q^{-1} Av=\lambda Q^{-1} v=\lambda e_1\), par conséquent la matrice \( Q^{-1} AQ\) est de la forme
    \begin{equation}
        Q^{-1}AQ=\begin{pmatrix}
            \lambda_1    &   *    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( *\) représente une ligne quelconque et \( A_1\) est une matrice de \( \eM(n-1,\eC)\). Nous pouvons donc répéter le processus sur \( A_1\) et obtenir une matrice triangulaire supérieure (nous utilisons le fait qu'un produit de matrices orthogonales est une matrice orthogonale).  
\end{proof}
En particulier les matrices hermitiennes, anti-hermitiennes et unitaires sont trigonalisables par une matrice unitaire, qui peut être choisie de déterminant \( 1\).

\begin{corollary}   \label{CorUNZooAZULXT}
    Le polynôme caractéristique\footnote{Définition \ref{DefOWQooXbybYD}.} sur \( \eC\) d'une matrice s'écrit sous la forme
    \begin{equation}
        \chi_A(X)=\prod_{i=1}^r(X-\lambda_i)^{m_i}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres distinctes de \( A\) et \( m_i\) sont les multiplicités correspondantes.
\end{corollary}
\index{polynôme!caractéristique}

\begin{proof}
    Le lemme \ref{LemSchurComplHAftTq} nous donne l'existence d'une base de trigonalisation; dans cette base les valeurs propres de \( A\) sont sur la diagonale et nous avons 
    \begin{equation}
        \chi_A(X)=\det(A-X\mtu)=\det\begin{pmatrix}
            X-\lambda_1    &   *    &   *    \\
            0    &   \ddots    &   *    \\
            0    &   0    &   X-\lambda_r
        \end{pmatrix},
    \end{equation}
    qui vaut bien le produit annoncé.
\end{proof}

\begin{theorem}[Théorème spectral pour les matrices normales\cite{LecLinAlgAllen,OMzxpxE,HOQzXCw}]\index{théorème!spectral!matrices normales}  \index{diagonalisation!cas complexe}  \label{ThogammwA}
    Soit \( A\in\eM(n,\eC)\) une matrice de valeurs propres \( \lambda_1,\ldots, \lambda_n\) (non spécialement distinctes). Alors les conditions suivantes sont équivalentes :
    \begin{enumerate}
        \item   \label{ItemJZhFPSi}
            \( A\) est normale,
        \item   \label{ItemJZhFPSii}
            \( A\) se diagonalise par une matrice unitaire,
        \item
            \( \sum_{i,j=1}^n| A_{ij} |^2=\sum_{j=1}^n| \lambda_j |^2\),
        \item
            il existe une base orthonormale de vecteurs propres de \( A\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous allons nous contenter de prouver \ref{ItemJZhFPSi}\( \Leftrightarrow\)\ref{ItemJZhFPSii}.
    %TODO : le reste.

    Soit \( Q\) la matrice unitaire donnée par la décomposition de Schur (lemme \ref{LemSchurComplHAftTq}) : \( A=QTQ^{-1}\). Étant donné que \( A\) est normale nous avons
    \begin{equation}
        QTT^*Q^{-1}=QT^*TQ^{-1},
    \end{equation}
    ce qui montre que \( T\) est également normale. Or une matrice triangulaire supérieure normale est diagonale. En effet nous avons \( T_{ij}=0\) lorsque \( i>j\) et
    \begin{equation}
        (TT^*)_{ii}=(T^*T)_{ii}=\sum_{k=1}^n| T_{ki} |^2=\sum_{k=1}^n| T_{ik} |^2.
    \end{equation}
    Écrivons cela pour \( i=1\) en tenant compte de \( | T_{k1} |^2=0\) pour \( k=2,\ldots, n\),
    \begin{equation}
        | T_{11} |^2=| T_{11} |^2+| T_{12} |^2+\ldots+| T_{1n} |^2,
    \end{equation}
    ce qui implique que \( T_{11}\) est le seul non nul parmi les \( T_{1k}\). En continuant de la sorte avec \( i=2,\ldots, n\) nous trouvons que \( T\) est diagonale.

    Dans l'autre sens, si \( A\) se diagonalise par une matrice unitaire, \( UAU^*=D\), nous avons
    \begin{equation}
        DD^*=UAA^*U^*
    \end{equation}
    et 
    \begin{equation}
        D^*D=UA^*AU^*,
    \end{equation}
    qui ce prouve que \( A\) est normale.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Lemme de Schur réel]\index{lemme!Schur réel}  \label{LemSchureRelnrqfiy}
    Soit \( A\in\eM(n,\eR)\). Il existe une matrice orthogonale \( Q\) telle que \( Q^{-1}AQ\) soit de la forme
    \begin{equation}        \label{EqMtrTSqRTA}
        QAQ^{-1}=\begin{pmatrix}
            \lambda_1    &   *    &   *    &   *    &   *\\  
            0    &   \ddots    &   \ddots    &   \ddots    &   \vdots\\  
            0    &   0    &   \lambda_r    &   *    &   *\\  
            0    &   0    &   0    &   \begin{pmatrix}
                a_1    &   b_1    \\ 
                c_1    &   d_1    
            \end{pmatrix}&   *\\  
            0    &   0    &  0     &   0    &   \begin{pmatrix}
                a_s    &   b_s    \\ 
                c_s    &   d_s    
            \end{pmatrix}
        \end{pmatrix}.
    \end{equation}
    Le déterminant de \( A\) est le produit des déterminants des blocs diagonaux et les valeurs propres de \( A\) sont les \( \lambda_1,\ldots, \lambda_r\) et celles de ces blocs.
\end{lemma}

\begin{proof}
    Si la matrice \( A\) a des valeurs propres réelles, nous procédons comme dans le cas complexe. Cela nous fournit le partie véritablement triangulaire avec les valeurs propres \( \lambda_1,\ldots, \lambda_r\) sur la diagonale. Supposons donc que \( A\) n'a pas de valeurs propres réelles. Soit donc \( \alpha+i\beta \) une valeur propre (\( \beta\neq 0\)) et \( u+iv\) un vecteur propre correspondant où \( u\) et \( v\) sont des vecteurs réels. Nous avons
    \begin{equation}
        Au+iAv=A(u+iv)=(\alpha+i\beta)(u+iv)=\alpha u-\beta v+i(\alpha v+\beta v),
    \end{equation}
    et en égalisant les parties réelles et imaginaires,
    \begin{subequations}
        \begin{align}
            Au&=\alpha u-\beta v\\
            Av&=\alpha v+\beta u.
        \end{align}
    \end{subequations}
    Sur ces relations nous voyons que ni \( u\) ni \( v\) ne sont nuls. De plus \( u\) et \( v\) sont linéairement indépendants (sur \( \eR\)), en effet si \( v=\lambda u\) nous aurions \( Au=\alpha u-\beta\lambda u=(\alpha-\beta\lambda)u\), ce qui serait une valeur propre réelle alors que nous avions supposé avoir déjà épuisé toutes les valeurs propres réelles.

    Étant donné que \( u\) et \( v\) sont deux vecteurs réels non nuls et linéairement indépendants, nous pouvons trouver une base orthonormée \( \{ q_1,q_2 \}\) de \( \Span\{ u,v \}\). Nous pouvons étendre ces deux vecteurs en une base orthonormée \( \{ q_1,q_2,q_3,\ldots, q_n \}\) de \( \eR^n\). Nous considérons à présent la matrice orthogonale dont les colonnes sont formées de ces vecteurs : \( Q=[q_1\,q_2\,\ldots q_n]\).

    L'espace \( \Span\{ e_1,e_2 \}\) est stable par \( Q^{-1} AQ\), en effet nous avons
    \begin{equation}
        Q^{-1} AQe_1=Q^{-1} Aq_1=Q^{-1}(aq_1+bq_2)=ae_1+be_2.
    \end{equation}
    La matrice \( Q^{-1}AQ\) est donc de la forme
    \begin{equation}
        Q^{-1} AQ=\begin{pmatrix}
            \begin{pmatrix}
                \cdot    &   \cdot    \\ 
                \cdot    &   \cdot    
            \end{pmatrix}&   C_1    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( C_1\) est une matrice réelle \( 2\times (n-1)\) quelconque et \( A_1\) est une matrice réelle \( (n-2)\times (n-2)\). Nous pouvons appliquer une récurrence sur la dimension pour poursuivre.

    Notons que si \( A\) n'a pas de valeurs propres réelles, elle est automatiquement d'ordre pair parce que les valeurs propres complexes viennent par couple complexes conjuguées.

    En ce qui concerne les valeurs propres, il est facile de voir en regardant \eqref{EqMtrTSqRTA} que les valeurs propres sont celles des blocs diagonaux. Étant donné que \( QAQ^{-1}\) et \( A\) ont même polynôme caractéristique, ce sont les valeurs propres de \( A\).
\end{proof}

\begin{theorem}[Théorème spectral, matrice symétrique\cite{KXjFWKA}] \label{ThoeTMXla}
    Une matrice symétrique réelle,
    \begin{enumerate}
        \item
            a un spectre contenu dans \( \eR\)
        \item
            est diagonalisable par une matrice orthogonale.
    \end{enumerate}
    Si \( M\) est une matrice symétrique réelle alors \( \eR^n\) possède une base orthonormée de vecteurs propres de \( M\).
\end{theorem}
\index{diagonalisation!cas réel}
\index{rang!diagonalisation}
\index{endomorphisme!diagonalisation}
\index{spectre!matrice symétrique réelle}
\index{théorème!spectral!matrice symétrique}

\begin{proof}
    Soit \( A\) une matrice réelle symétrique. Si \( \lambda\) est une valeur propre complexe pour le vecteur propre complexe \( v\), alors d'une part \( \langle Av, v\rangle =\lambda\langle v, v\rangle \) et d'autre part \( \langle Av, v\rangle =\langle v, Av\rangle =\bar\lambda\langle v, v\rangle \). Par conséquent \( \lambda=\bar\lambda\).
    
    Le lemme de Schur réel \ref{LemSchureRelnrqfiy} donne une matrice orthogonale qui trigonalise \( A\). Les valeurs propres étant toutes réelles, la matrice \( QAQ^{-1}\) est même triangulaire (il n'y a pas de blocs dans la forme \eqref{EqMtrTSqRTA}). Prouvons que \( QAQ^{-1}\) est symétrique :
    \begin{equation}
        (QAQ^{-1})^t=(Q^{-1})^tA^tQ^t=QA^tQ^{-1}=QAQ^{-1}
    \end{equation}
    où nous avons utilisé le fait que \( Q\) était orthogonale (\( Q^{-1}=Q^t\)) et que \( A\) était symétrique (\( A^t=A\)). Une matrice triangulaire supérieure symétrique est obligatoirement une matrice diagonale.

    En ce qui concerne la base de vecteurs propres, soit \( \{ e_i \}_{i=1,\ldots, n}\) la base canonique de \( \eR^n\) et \( Q\) une matrice orthogonale e telle que \( A=Q^tDQ\) avec \( D\) diagonale. Nous posons \( f_i=Q^te_i\) et en tenant compte du fait que \( Q^t=Q^{-1}\) nous avons \( Af_i=Q^tDQQ^te_i=Q^t\lambda_i e_i=\lambda_if_i\). Donc les \( f_i\) sont des vecteurs propres de \( A\). De plus ils sont orthonormés parce que
    \begin{equation}
        \langle f_i, f_j\rangle =\langle Q^te_i, Q^te_j\rangle =\langle e_i, Q^tQe_j\rangle =\langle e_i, e_j\rangle =\delta_{ij}.
    \end{equation}
\end{proof}
Le théorème spectral pour les opérateurs auto-adjoints sera traité plus bas parce qu'il a besoin de choses sur les formes bilinéaires, théorème \ref{ThoRSBahHH}.
% et les choses sur la dégénérescences utilisent le théorème spectral, cas réel. Donc l'enchaînement est très loumapotiste.

\begin{remark}  \label{RemGKDZfxu}
    Une matrice symétrique est diagonalisable par une matrice orthogonale. Nous pouvons en réalité nous arranger pour diagonaliser par une matrice de \( \SO(n)\). Plus généralement si \( A\) est une matrice diagonalisable par une matrice \( P\in\GL^+(n,\eR)\) alors elle est diagonalisable par une matrice de \( \GL^-(n,\eR)\) en changeant le signe de la première ligne de \( P\). Et inversement.

    En effet, si nous avons \( P^tDP=A\), alors en notant \( *\) les quantités qui ne dépendent pas de \( a\), \( b\) ou~\( c\),
    \begin{equation}
        \begin{aligned}[]
        \begin{pmatrix}
            a    &   *    &   *    \\
            b    &   *    &   *    \\
            c    &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1    &       &       \\
                &   \lambda_2    &       \\
                &       &   \lambda_3
            \end{pmatrix}
            \begin{pmatrix}
                a    &   b    &   c    \\
                *    &   *    &   *    \\
                *    &   *    &   *
            \end{pmatrix}&=
        \begin{pmatrix}
            a    &   *    &   *    \\
            b    &   *    &   *    \\
            c    &   *    &   *
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1a    &   \lambda_1b    &   \lambda_1c    \\
            *    &   *    &   *    \\
            *    &   *    &   *
        \end{pmatrix}\\
        &=\begin{pmatrix}
            \lambda_1 a^2+*   &   \lambda_1ab+*    &   \lambda_1ac  +*  \\
            \ldots    &   \ldots    &   \ldots    \\
            \ldots    &   \ldots    &   \ldots
        \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Nous voyons donc que si nous changeons les signes de \( a\), \( b\) et \( c\) en même temps, le résultat ne change pas.
\end{remark}

\begin{definition}[Matrice définie positive]    \label{DefAWAooCMPuVM}
    Une matrice symétrique est \defe{définie positive}{matrice!définie positive} si toutes ses valeurs propres sont strictement positives. Elle est \defe{semi-définie positive}{semi-définie positive} si ses valeurs propres sont positives ou nulles.
\end{definition}
Afin d'éviter l'une ou l'autre confusion, nous disons souvent \emph{strictement} définie positive pour positive.

Nous notons \( S^+(n,\eR)\)\nomenclature[A]{\( S^+(n,\eR)\)}{matrices symétriques semi-définies positives} l'ensemble des matrices réelles \( n\times n\) semi-définies positives. L'ensemble \( S^{++}(n,\eR)\)\nomenclature[A]{\( S^{++}(n,\eR)\)}{matrices symétriques strictement définies positives} est l'ensemble des matrices symétriques strictement définies positives.

\begin{remark}
    Nous ne définissons pas la notion de matrice définie positive pour une matrice non symétrique.
\end{remark}

\begin{lemma}   \label{LemWZFSooYvksjw}
    La matrice symétrique \( M\) est définie positive si et seulement si \( \langle x, Mx\rangle >0\) pour tout \( x\in \eR^n\). 
\end{lemma}

\begin{proof}
    Soit \( \{ e_i \}_{i=1,\ldots, n}\) une base orthonormée de vecteurs propres de \( M\) dont l'existence est assurée par le théorème spectral \ref{ThoeTMXla}. Nous nommons \( x_i\) les coordonnées de \( x\) dans cette base. Alors,
    \begin{equation}
        \langle x,Mx \rangle =\sum_{i,j}x_i\langle e_i, x_jMe_j\rangle =\sum_{i,j}x_ix_j\langle e_i, \lambda_je_j\rangle =\sum_{ij}x_ix_j\lambda_j\delta_{ij}=\sum_i\lambda_ix_i^2
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( M\). Cela est strictement positif pour tout \( x\) si et seulement si tous les \( \lambda_i\) sont positifs.
\end{proof}

\begin{corollary}[Pseudo-réduction simultanée\cite{JMYQgLO}]  \label{CorNHKnLVA}
    Soient \( A,B\in \gS(n,\eR)\) avec \( A\) définie positive\footnote{Définition \ref{DefAWAooCMPuVM}.}. Alors il existe \( Q\in \GL(n,\eR)\) tell que \( Q^tBQ\) soit diagonale et \( Q^tAQ=\mtu\).
\end{corollary}

\begin{proof}
    Nous allons noter \( x\cdot y\) le produit scalaire usuel de \( \eR^n\) et \( \{ e_i \}_{i=1,\ldots, n}\) sa base canonique.

    Vu que \( A\) est définie positive, nous avons que l'expression\footnote{On peut aussi l'écrire de façon plus matricielle sous la forme \( \langle x, y\rangle =x^tAy\).} \( \langle x, y\rangle =x\cdot Ay\) est un produit scalaire sur \( \eR^n\). Autrement dit, \( E\) muni de cette forme bilinéaire symétrique est un espace euclidien, ce qui fait dire à la proposition \ref{PropUMtEqkb} qu'il existe une base de \( \eR^n\) orthonormée \( \{ f_i \}_{i=1,\ldots, n}\) pour ce produit scalaire, c'est à dire qu'il existe une matrice \( P\in \GL(n,\eR)\) telle que \( P^tAP=\mtu\). Ici, \( P\) est la matrice de changement de base de la base canonique à notre base orthonormée, c'est à dire la matrice qui fait \( Pe_i=f_i\) pour tout \( i\). Voyons cela avec un peu de détails.

    Pour savoir ce que valent les éléments de la matrice \( P^tAP\), nous nous souvenons que \( P^tAPe_j\) est un vecteur dont les coordonnées sont les éléments de la \( j\)\ieme colonne de \( P^tAP\). Nous avons donc \( (P^tAP)_{ij}=e_i\cdot P^tAPe_i\). Calculons :
    \begin{equation}
            (P^tAP)_{ij}=e_i\cdot P^tAPe_i
            =Pe_i\cdot APe_j
        =f_i\cdot Af_j
        =\langle f_i, f_j\rangle 
        =\delta_{ij}
    \end{equation}
    où nous avons utilisé le fait que \( A\) était auto-adjointe pour la passer de l'autre côté du produit scalaire (usuel). Au final nous avons effectivement \( P^tAP=\mtu\).

    La matrice \( P^tBP\) est une matrice symétrique, donc le théorème spectral \ref{ThoeTMXla} nous donne une matrice \( R\in \gO(n,\eR)\) telle que \( R^tP^tBPR\) soit diagonale. En posant maintenant \( Q=PR\) nous avons la matrice cherchée.
\end{proof}
Note : nous avons prouvé la pseudo-réduction simultanée comme corollaire du théorème de diagonalisation des matrices symétriques \ref{ThoeTMXla}. Il aurait aussi pu être vu comme un corollaire du théorème spectral \ref{ThoRSBahHH} sur les opérateurs auto-adjoints via son corollaire \ref{CorSMHpoVK}.

