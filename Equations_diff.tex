% This is part of Mes notes de mathématique
% Copyright (c) 2011-2012
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

Une équation différentielle ordinaire est la recherche de toutes les fonctions définie sur une partie de $\eR$ satisfaisant à une certaine égalité, faisant intervenir les dérivées de la fonction recherchée.

Dans la suite, $I$ désignera un intervalle de $\eR$. Une fonction sera \defe{dérivable sur $I$}{dérivalble!fonction} si elle est dérivable au sens usuel sur l'intérieur de $I$, et si elle est dérivable à droite (resp. à gauche) sur l'éventuel bord gauche (resp. droit) de $I$.

\begin{definition}
  Une \defe{équation différentielle ordinaire d'ordre $n$ sur $I$}{Équation différentielle!ordinaire!ordre 1} est la recherche d'une fonction $y : I \to \eR$ dérivable $n$ fois, satisfaisant à une équation du type
  \begin{equation}\label{eqequadiff}
    F(t, y(t), y^\prime(t), \ldots, y^{n\prime}(t)) = 0 \quad \text{pour tout $t \in I$}
  \end{equation}
  où $I$ est un intervalle de $\eR$ et \begin{math}F : (I \times D) \subset (\eR\times\eR^{n+1})\to \eR\end{math} est une fonction donnée.
\end{definition}

\begin{remark}
L'équation différentielle~(\ref{eqequadiff}) sera raccourcie sous la forme
  \begin{equation}
    F(t, y, y^\prime, \ldots, y^{n\prime}) = 0
  \end{equation}
  où la dépendance en $t$ est sous-entendue.
\end{remark}

\begin{example}
	Soit $f : I \to \eR$ une fonction continue fixée. L'équation différentielle
	\begin{equation}
		y^\prime = f(t)
	\end{equation}
	se ramène à la recherche des primitives de $f$ sur l'intervalle $I$.
\end{example}

Le lemme suivant sert de temps en temps.
\begin{lemma}[Lemme de Grönwall]\index{Grönwall (lemme)}\index{lemme!Grönwall}. \label{LemuBVozy}
    Soient \( \phi\) et \( \psi\) deux fonctions telles que pour tout \( t\in\mathopen[ t_0 , t_1 \mathclose]\), \( \phi(t)\geq 0\), \( \psi(t)\geq 0\) et
    \begin{equation}
        \phi(t)\leq +L\int_{t_0}^f\psi(s)\phi(s)ds
    \end{equation}
    où \( K\) et \( L\) sont des constantes positives. Alors
    \begin{equation}
        \phi(t)\leq K\exp\big( L\int_{t_0}^t\psi \big).
    \end{equation}
\end{lemma}
%TODO : la preuve.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
					\section{Équations différentielles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équations à variables séparées}
%---------------------------------------------------------------------------------------------------------------------------

Ce sont les équations pour lesquelles on peut mettre tous les $y$ d'un côté. Elles se présentent sous la forme
\begin{equation}
	y'=u(x)f(y).
\end{equation}
On peut évidement mettre tous les $y$ et $y'$ d'un côté :
\begin{equation}
	\frac{ y' }{ f(y) }=u(x).
\end{equation}
Une fois que cela est fait, on écrit $y'=\frac{ dy }{ dx }$, et on envoie le $dx$ du côté des $x$ :
\begin{equation}
	\frac{ dy }{ f(y) }=u(x)dx.
\end{equation}
Maintenant il suffit de prendre l'intégrale des deux côtés : comme la position des $dx$ et $dy$ l'indiquent, il faut intégrer par rapport à $y$ d'un côté et par rapport à $dx$ de l'autre côté.

L'intégrale à gauche est facile : c'est $\ln(y)$. À droite, par contre, ça dépend tout à fait de $u$.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équations homogènes}
%---------------------------------------------------------------------------------------------------------------------------

Une équation différentielle homogène se présente sous la forme
\begin{equation}
	y'=\frac{ \text{degré $n$ en $x,y$} }{  \text{degré $n$ en $x,y$}  },
\end{equation}
avec pas de $y'$ à droite : juste du $y$ et du $x$.

Pour traiter une équation différentielle homogène, le bon plan est de changer de fonction inconnue et poser
\begin{equation}		\label{EqSubstHomouyp}
	u(x)=\frac{ y(x) }{ x },
\end{equation}
ce qui fait $y=ux$ et $y'(x)=u(x)+xu'(x)$, à replacer dans l'équation de départ.


%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équations linéaires}
%---------------------------------------------------------------------------------------------------------------------------

Tant qu'il n'y a pas de second membre, c'est facile. Prenons l'exemple suivant :
\begin{equation}
	y'+2xy=0.
\end{equation}
Nous mettons tous les $x$ d'un côté et tous les $y$ et $y'$ de l'autre :
\begin{equation}
	\frac{ y' }{ y }=-2x,
\end{equation}
et puis on intègre sans oublier la constante d'intégration :
\begin{equation}
	\ln(y)=-x^2+C,
\end{equation}
et donc $y(x)=K e^{-x^2}$.

Lorsqu'il y a un second membre, il y a une astuce. Prenons par exemple
\begin{equation}		\label{EqDiffExLin}
	y'+2xy=4x.
\end{equation}
L'astuce est de commencer par résoudre l'équation sans le second membre (l'équation homogène associée). Nous notons $y_H$ la solution. Ici, la réponse est
\begin{equation}
	y_H(x)=K e^{-x^2}.
\end{equation}
Ensuite le truc est d'essayer de trouver la solution de l'équation \eqref{EqDiffExLin} sous la forme
\begin{equation}		\label{EqEssaiLin}
	y(x)=K(x) e^{x^2}.
\end{equation}
L'idée est de prendre la même que la solution de l'équation homogène (sans second membre), mais en disant que $K$ est une fonction. Afin de trouver la fonction $K$ qui donne la solution, il suffit de remettre l'essai \eqref{EqEssaiLin} dans l'équation \eqref{EqDiffExLin} :
\begin{equation}
	\underbrace{K' e^{-x^2}-2xK e^{-x^2}}_{y'(x)}+\underbrace{2xK e^{-x^2}}_{2xy(x)}=4x
\end{equation}
Les deux termes avec $K$ se simplifient et il reste
\begin{equation}
	K'(x)=4x e^{x^2},
\end{equation}
ce qui signifie $K(x)=2 e^{x^2+C}$. Nous avons donc déterminé la fonction qui fait fonctionner l'essai, et la solution à l'équation est
\begin{equation}
	y(x)=\big( 2 e^{x^2}+C \big) e^{-x^2}=2+C e^{-x^2}.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Résolution de systèmes d'équations différentielles}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons l'équation différentielle
\begin{equation}    \label{EqOOsXZJ}
    f'(t)=Af(t)
\end{equation}
pour la fonction \( f\colon \eR\to \eR^n\) et \( A\) est une matrice ne dépendant pas de \( t\). Nous supposons que \( A\) est diagonalisable pour les vecteurs propres \( v_i\) et les valeurs propres \( \lambda_i\) correspondantes.

La matrice 
\begin{equation}
    R(t)=\big[  e^{\lambda_1t}v_1\, \ldots  e^{\lambda_nt}v_n \big]
\end{equation}
est la \defe{matrice résolvante}{résolvante} du système. Alors la solution du système \eqref{EqOOsXZJ} pour la condition initiale \( f(0)=f_0\) est 
\begin{equation}
    f(t)=R(t)f_0.
\end{equation}
En effet
\begin{equation}
    AR(t)=\left[  A\begin{pmatrix}
        \uparrow    \\ 
        e^{\lambda_1t}v_1    \\ 
        \downarrow    
    \end{pmatrix}\,\ldots\,A\begin{pmatrix}
        \uparrow    \\ 
        e^{\lambda_nt}v_n    \\ 
            \downarrow
    \end{pmatrix}\right]=R'(t).
\end{equation}
Par conséquent \( f'(t)=R'(t)f_0=AR(t)f_0=Af(t)\).


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Équations différentielles du second ordre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Avec second membre}
%---------------------------------------------------------------------------------------------------------------------------

Une équation différentielle du second ordre avec un second membre se présente sous la forme
\begin{equation}
	ay''(x)+by'(x)+cy(x)=v(x)
\end{equation}
où $v(x)$ est une fonction donnée. Le truc est de commencer par résoudre l'équation différentielle sans second membre, c'est à dire trouver la fonction $y_H(x)$ telle que
\begin{equation}
	ay''_H(x)+by_H'(x)+cy_H(x)=0.
\end{equation}
Cela se fait en utilisant la méthode du polynôme caractéristique.

Ensuite, il faut trouver une solution particulière $y_P(x)$ de l'équation avec le second membre. Une seule. Pour y parvenir, il faut du doigté et un peu de technique. Il faut faire des essais en fonction de ce à quoi ressemble le $v(t)$ :
\begin{enumerate}

	\item
		Si $v(x)$ est un polynôme, alors il faut essayer un polynôme,

	\item
		Si $v(x)=\cos(\omega x)$ ou bien $v(x)=\sin(\omega x)$, alors essayer $y_P(x)=A\cos(x)+B\sin(\omega x)$,

	\item
		Si $v(x)= e^{\omega x}$, alors essayer $y_P(x)=A e^{\omega x}$.

\end{enumerate}


%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équations à variables séparées}
%---------------------------------------------------------------------------------------------------------------------------

Une \defe{équation à variables séparées}{Équation!différentielle!à variables séparées} est une équation  de la forme
\begin{equation}		\label{EqDiffSeparee}
	y'=u(t)f(y).
\end{equation}
Les propositions \ref{ProJLykrK} et \ref{PropOkmXmC} résolvent ce cas.
\begin{proposition}     \label{ProJLykrK}
Nous considérons l'équation \eqref{EqDiffSeparee} avec $u(t)$ continue sur $I$ et $f$ continue sur $J$ avec $f(\eta)\neq 0$ pour tout $\eta\in J$. Soit $U$, une primitive de $u$ sur $I$, et $G$, une primitive de $1/f$ sur $J$.

Si $y\colon Y'\to J$ est une fonction sur un intervalle $I'\subset I$, alors $y$ est solution de l'équation \eqref{EqDiffSeparee} si et seulement si il existe $C\in\eR$ tel que
\begin{equation}		\label{EqSoluceEqDiffSep}
	G\big( y(t) \big)=U(t)+C.
\end{equation}
\end{proposition}
Cette proposition dit que toutes les solutions qui ne s'annulent jamais sur un intervalle ont la forme $G\big( y(t) \big)=U(t)+C$ et peuvent donc être trouvées en calculant des primitives.

La formule \eqref{EqSoluceEqDiffSep} peut être obtenue de la façon heuristique suivante, en écrivant $y'=dy/dt$, et en passant le $dt$ à droite. Nous trouvons successivement
\begin{equation}
	\begin{aligned}[]
		y'&=u(t)f(y)\\
		dy&=u(t)f(y)dt\\
		\frac{ dy }{ f(y) }&=u(t)dt\\
		\int\frac{ dy }{ f(y) }&=\int u(t)dt\\
		G(y)&=U(t)+C.
	\end{aligned}
\end{equation}

\begin{proposition} \label{PropOkmXmC}
Soient $u$ continue sur $I$ et $f$ continue sur $J$, et $f(\eta)\neq 0$ sur $J$. Soient $t_0\in I$ et $y_0\in J$. Alors il existe $I'\subset I$ avec $t_0\in I'$ et $f\in C^1(I'\to J)$ tels que
\begin{enumerate}

\item
$y$ est solution de \eqref{EqDiffSeparee} sur $I'$ et vérifie $y(t_0)=y_0$,
\item
si $z$ est une solution de \eqref{EqDiffSeparee} sur $I''\subset I'$ avec $t_0\in I''$ et $z(t_0)=y_0$, alors $I''\subset I'$ et $z(t)=y(t)$ pour tout $t\in I''$.

\end{enumerate}
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Équation \texorpdfstring{$y''+q(t)y=0$}{y'+q(t)y=0}}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecSyTwyM}

Soient deux solutions \( y_1\) et \( y_2\) de l'équation différentielle. Le \defe{Wronskien}{Wronskien} de ces deux solutions est le déterminant
\begin{equation}
    W(t)=\begin{vmatrix}
        y_1    &   y_2    \\ 
        y'_1    &   y'_2    
    \end{vmatrix}.
\end{equation}
Si nous considérons l'équation différentielle
\begin{equation}
    y''+py'+qy=0,
\end{equation}
le Wronskien peut être déterminé sans savoir explicitement \( y_1\) et \( y_2\) parce que \( W=y_1y'_2-y'_1y_2\), et en dérivant,
\begin{subequations}
    \begin{align}
        W'&=y_1y_2''+y'_1y'_2-y''_1y_2-y'_1y'_2\\
        &=y_1(-py'_2-qy_2)-(-py'_1-qy_1)y_2\\
        &=-p\begin{vmatrix}
            y_1    &   y_2    \\ 
            y'_1    &   y'_2    
        \end{vmatrix},
    \end{align}
\end{subequations}
c'est à dire
\begin{equation}    \label{EqHEMRgM}
    W'=-pW.
\end{equation}
Il suffit donc de savoir une condition initiale pour obtenir une équation différentielle pour \( W\).

Nous allons donner quelque propriété des solutions de l'équation
\begin{equation}
    y''+qy=0
\end{equation}
en fonction de telle ou telle hypothèse sur \( q\).

\begin{proposition}
    Si \( q\colon \eR^+\to \eR\) est continue et si
    \begin{equation}
        \int_0^{\infty}| q(t) |dt
    \end{equation}
    converge, alors
    \begin{enumerate}
        \item
            toute solution bornée de \( y''+qy=0\) vérifie \( \lim_{t\to \infty} y'(t)\),
        \item
            l'équation \( y''+qy=0\) admet des solutions non bornées.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Soit \( y\) une solution bornée, et intégrons l'équation différentielle entre \( 0\) et \( \infty\) :
    \begin{equation}
        \int_0^{\infty}y''(t)dt=-\int_0^{\infty}q(t)y(t)dt.
    \end{equation}
    La fonction \( y\) étant bornée, l'hypothèse sur \( q\) permet de dire que l'intégrale de droite existe. Par ailleurs,
    \begin{equation}
        \int_0^{\infty}y''=\lim_{a\to \infty}\int_0^ay''=\lim_{a\to \infty}y'(a)-y'(0).
    \end{equation}
    Cela justifie que la limite \( \lim_{t\to \infty} y'(t)\) existe. Posons \( \alpha=\lim_{t\to \infty} y'(t)\) et supposons par l'absurde que \( \alpha\neq 0\). Soit \( \epsilon>0\) et \( \lambda\) assez grand pour que
    \begin{equation}
        \| y'-\alpha \|_{\mathopen[ \lambda , \infty [}<\epsilon.
    \end{equation}
    Soit aussi \( x>\lambda\). Nous avons
    \begin{subequations}
        \begin{align}
            y(x)&=y(\lambda)+\int_{\lambda}^xy'(t)dt\\
            &\geq y(\lambda)\int_{\lambda}^x(\alpha-\epsilon)\\
            &=y(\lambda)+\alpha x-\epsilon\lambda.
        \end{align}
    \end{subequations}
    En prenant la limite des deux côtés on voit que \( y(x)\to \infty\) dès que \( \alpha\neq 0\), ce qui est contraire aux hypothèses. Donc \( \alpha=0\).

    Pour la seconde partie de la proposition, nous devons prouver que l'équation \( y''+qy=0\) possède des solutions non bornées. Si l'équation a seulement des solutions bornées et si \( \{ u,v \}\) est une base de solutions, alors nous avons \( u',v'\to 0\). Si nous reprenons l'équation \eqref{EqHEMRgM} avec \( p=0\) nous savons que dans notre cas le Wronskien satisfait à \( W'=0\), c'est à dire qu'il est constant. Mais vu que \( u\) et \( v\) sont bornées et que les dérivées tendent vers zéro, nous avons \( W(t)\to 0\) et donc \( W(t)=0\).

    Or l'annulation identique du Wronskien contredit que \( \{ u,v \}\) serait une base de solutions. Donc il existe des solutions non bornées.
\end{proof}

\begin{proposition} \label{PropMYskGa}
    Soit l'équation différentielle \( y''+qy=0\). Si \( q\) est \( C^1\), strictement positive et croissante, alors toutes les solutions sont bornées.
\end{proposition}

\begin{proof}
    Soit \( y\) une solution et multiplions l'équation par \( 2y'\) (qui est non nulle par hypothèse) :
    \begin{equation}
        2y'y''+2qy'y=0.
    \end{equation}
    Nous allons intégrer cela en nous souvenant que \( 2y'y''\) est la dérivée de \( (y')^2\). Pour tout \( x>0\) nous avons
    \begin{subequations}
        \begin{align}
            0&=y'(x)^2-y'(0)^2+2\underbrace{\int_0^xq(t)y'(t)y(t)dt}_{\text{par partie}}\\
            &=y'(x)^2-y'(0)^2+2\left( [qy^2]_0^x-\int_0^xq'y^2 \right)\\
        \end{align}
    \end{subequations}
    Le terme qui nous intéresse est celui qui contient \( y(x)\) :
    \begin{equation}
        2q(x)y(x)^2=-y'(x)^2+y'(0)^2+2q(0)y(0)^2+2\int_0^x q'y^2
    \end{equation}
    Nous pouvons majorer \( -y'(x)^2\) par zéro et remplace toutes les constantes pas \( K\) :
    \begin{equation}
        q(x)y(x)^2<\int_0^xq'y^2+K=\int_0^x\frac{ q' }{ q }qy^2.
    \end{equation}
    C'est le moment d'utiliser le lemme de Grönwall (\ref{LemuBVozy}) avec \( \phi=qy^2\) et \( \psi=q'/q\). Les hypothèses de croissance et de positivité ont été posées exprès. Bref, on a
    \begin{subequations}
        \begin{align}
            qy^2&\leq K\exp\left( \int_0^x\frac{ q'(s) }{ q(s) }ds \right)\\
            &=K\exp\left( \ln\frac{ q(x) }{ q(0) } \right)\\
            &=K\frac{ q(x) }{ q(0) }.
        \end{align}
    \end{subequations}
    Notons que \( q(0)\) est strictement positif. Nous déduisons que
    \begin{equation}
        y^2\leq \frac{ K }{ q(0) }
    \end{equation}
    et donc \( y\) est bornée.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équations linéaires}
%---------------------------------------------------------------------------------------------------------------------------

Une \defe{équation différentielle linéaire}{Équation différentielle!linéaire} est une équation de la forme
\begin{equation}
	y'+u(t)y=v(t).
\end{equation}
La technique pour résoudre cette équation est de commencer par résoudre l'équation homogène associée. Si $U(t)$ est une primitive de $u(t)$, nous avons
\begin{equation}
	\begin{aligned}[]
		y'_H(t)+u(t)y_H(t)&=0\\
		\frac{ y'_H }{ y_H }&=-u(t)\\
		\ln(y_H)&=-U(t)+C\\
		y_H(t)&= e^{-U(t)+C}=K e^{-U(t)}
	\end{aligned}
\end{equation}
où $K= e^{C}$.

Cela fournit la solution générale de l'équation homogène. Il existe un truc génial qui permet d'en tirer la solution générale du système non homogène. Lorsque nous avons trouvé $y_H(t)=K e^{-U(t)}$, le symbole $K$ désigne une constante. La méthode de \defe{variation des constantes}{variation des constantes} consiste à essayer la solution
\begin{equation}		\label{EqEssayVarSctr}
	y(t)=K(t) e^{-U(t)},
\end{equation}
c'est à dire à dire que la constante est en réalité une fonction. Afin de trouver quelle fonction $K(t)$ fait en sorte que l'essai \eqref{EqEssayVarSctr} soit une solution, nous la remplaçons dans l'équation de départ $y'+uy=v$. Maintenant,
\begin{equation}
	y'(t)=K'(t) e^{-U(t)}-K(t)u(t) e^{-U(t)}.
\end{equation}
En remettant dans l'équation,
\begin{equation}
	y'+uy=K' e^{-U}-Ku e^{-U}+uK e^{-U}=K' e^{-U}=v.
\end{equation}
Notez que les termes en $K$ se sont miraculeusement simplifiés. Cela est directement dû au fait que $ e^{-U}$ est solution de l'équation homogène. Nous restons avec l'équation
\begin{equation}
	K'=\frac{ v }{  e^{-U} }
\end{equation}
pour $K(t)$. La solution générale du problème non homogène est donc finalement donnée par
\begin{equation}
	y(t)=\big( W(t)+C \big) e^{-U(t)}
\end{equation}
si $W(t)$ est une primitive de $v(t)e^{U(t)}$.

Tout ceci est un peu heuristique. La proposition suivante dit dans quels cas ça fonctionne.
\begin{proposition}
Soient $u$ et $v$ continues sur $I$ et $U$, une primitive de $u$ sur $I$ et $W$ une primitive de $v e^{-U}$ sur $I$. Une fonction $y\colon I\to \eR$ est solution de $y'+u(t)y=v(t)$ si et seulement si il existe une constante $C\in \eR$ telle que
\begin{equation}
	y(t)=\big( W(t)+C \big) e^{U(t)}
\end{equation}
pour tout $t\in I$.
\end{proposition}
Le corollaire de la page $310$ lui règle son compte au problème de Cauchy. Si $t_0\in I$ et si $y_0\in\eR$, alors il existe une unique solution $y$ sur $I$ à l'équation linéaire telle que $y(t_0)=y_0$.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{La magie de l'exponentielle\ldots}
%---------------------------------------------------------------------------------------------------------------------------

Prenons l'équation différentielle très simple
\begin{equation}
	y'=ay.
\end{equation}
La solution est $y(t)=A e^{at}$. Et si on a la donnée que Cauchy $y(t_0)=y_0$, alors
\begin{equation}		\label{EqytexposimpleProp}
	y(t)=A e^{at} e^{-at_0} e^{at_0}= e^{a(t-t_0)}y(t_0).
\end{equation}
Donc on a le facteur multiplicatif $ e^{a(t-t_0)}$ qui sert à faire passer de $y(0)$ à $y(t)$. C'est un peu un opérateur d'évolution. Ce qui fait la magie  de l'exponentielle, c'est son développement en série
\begin{equation}		\label{EqDevExpoMag}
	e^x=1+x+\frac{ x^2 }{ 2 }+\frac{ x^3 }{ 3! }+\frac{ x^4 }{ 4! }+\ldots
\end{equation}
qui est tel que chaque terme est la dérivée du terme suivant.

Maintenant, si on a un système
\begin{equation}
	\bar y'=A\bar y,
\end{equation}
il n'est pas du tout étonnant d'avoir comme solution $\bar y(t)= e^{At}$ où l'exponentielle de la matrice est définie exactement par la série \eqref{EqDevExpoMag}. C'est un peu longuet, mais dans le cours, c'est effectivement ce qui est prouvé. La matrice résolvante $R(t,t_0)\colon \bar y_0\to \bar y(t;t_0,y_0)$ est donné par
\begin{equation}
	R(t,t_0)= e^{(t-t_0)A},
\end{equation}
exactement comme dans l'équation \eqref{EqytexposimpleProp}.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{\ldots mais la difficulté}
%---------------------------------------------------------------------------------------------------------------------------

Maintenant, il est suffisant de calculer des exponentielles de matrices pour résoudre des systèmes. Hélas, il est en général très difficile de calculer des exponentielles. Tu peux essayer de prouver les deux suivantes :
\begin{equation}
	\begin{aligned}[]
		A=\begin{pmatrix}
	0	&	a	\\ 
	-a	&	0	
\end{pmatrix}	&\leadsto  e^{A}=\begin{pmatrix}
	\cos(a)	&	\sin(a)	\\ 
	-\sin(a)	&	\cos(a)	
\end{pmatrix}\\
		S=\begin{pmatrix}
	0	&	a	\\ 
	a	&	0	
\end{pmatrix}	&\leadsto  e^{S}=\begin{pmatrix}
	\cosh(a)	&	\sinh(a)	\\ 
	\sinh(a)	&	\cosh(a)	
\end{pmatrix}.
	\end{aligned}
\end{equation}
La première, tu vas la revoir si tu fais de la géométrie différentielle ou de la mécanique quantique : l'algèbre de Lie du groupe des matrices orthogonales de déterminant $1$ est l'algèbre des matrices antisymétriques.

La seconde se retrouve en relativité parce que $e^S$ est la matrice qui préserve $x^2-y^2$, tout comme $e^A$ préserve $x^2+y^2$. Si tu as besoin d'une rafraîchissure de mémoire sur les fonctions hyperboliques, ou bien si leur utilité en relativité t'intéresse, tu peux aller voir la partie sur la relativité \href{http://student.ulb.ac.be/~lclaesse/physique-math.pdf}{ici}\footnote{ \url{http://student.ulb.ac.be/~lclaesse/physique-math.pdf}}.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{La recette}
%---------------------------------------------------------------------------------------------------------------------------

Afin d'éviter de devoir calculer explicitement des exponentielles de matrices, nous faisons appel à toutes sortes de trucs, dont la forme de Jordan. Le résultat final est la méthode suivante. Soit le système homogène
\begin{equation}
	\bar y'=A\bar y.
\end{equation}

\let\oldTheEnumi\theenumi
\renewcommand{\theenumi}{\arabic{enumi}.}
\begin{enumerate}

\item 
D'abord, nous calculons les valeurs propres de $A$.

\item
Ensuite les vecteurs propres.

\item\label{ItemRapSystDc}
Une bonne valeur propre, c'est une valeur propre dont l'espace propre a une dimension égale à sa multiplicité. C'est à dire que si $\lambda$ est de multiplicité $m$, alors on a, dans les bon cas,  $m$ vecteur propres linéairement indépendants.

Dans ce cas, si $v_1,\ldots,v_m$ sont les vecteurs, alors on a les solutions linéairement indépendantes suivantes :
\begin{equation}
	\begin{pmatrix}
	\vdots	\\ 
	v_1	\\ 
		\vdots	
\end{pmatrix} e^{\lambda t},\ldots,
\begin{pmatrix}
	\vdots		\\ 
	v_m	\\ 
	\vdots		
\end{pmatrix} e^{\lambda t}.
\end{equation}
Pour chaque bonne valeur propre, ça nous fait un tel paquet de solutions linéairement indépendantes.

\item
Si $\lambda$ n'est pas une bonne valeur propre, alors les choses se compliquent. Mettons que $\lambda$ ait $k$ vecteurs propres en moins que sa multiplicité. Dans ce cas, il faut chercher des solutions sous la forme
\begin{equation}		\label{EqEqRapAsTestPolk}
	 \begin{pmatrix}
	a^{(k)}_1t^k+\ldots+a_1^{(0)}	\\ 
	\vdots	\\ 
	a^{(k)}_1t^k+\ldots+a_n^{(0)}		
\end{pmatrix} e^{\lambda t}.
\end{equation}
C'est à dire qu'on prend comme coefficient de $ e^{\lambda t}$, un vecteur de polynômes de degré $k$. Il faut mettre cela dans l'équation de départ pour voir quelles sont les contraintes sur les constantes $a_i^{(j)}$ introduites.

\item\label{ItemRapSystDe}
Nous avons un cas particulier du cas précédent. Si $\lambda$ est une valeur propre de multiplicité $m$ qui n'a que un seul vecteur propre $v$, alors il faut chercher des polynômes de degré $m-1$, et on peut directement fixer le coefficient de $t^{m-1}$, ce sera l'unique vecteur propres :
\begin{equation}
\left[
	\begin{pmatrix}
	\vdots	\\ 
	v	\\ 
	\vdots	
\end{pmatrix}+
\begin{pmatrix}
	a_1^{(m-2)}	\\ 
	\vdots	\\ 
	a_n^{(m-2)}	
\end{pmatrix}t^{m-2}+\ldots
\right] e^{\lambda t}.
\end{equation}
Cela économise quelque calculs par rapport à poser brutalement \eqref{EqEqRapAsTestPolk}.

\end{enumerate}
\let\theenumi\oldTheEnumi

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
					\section{Équations résolubles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\subsection{Équation à variables séparées}
\label{secvarsep}


Une équation différentielle à variables séparées est une équation différentielle d'ordre $1$ qui peut s'écrire sous la forme
\begin{equation}		\label{EqGeneTheSep}
	y'=u(t)f(y)
\end{equation}
où $u\colon I\to \eR$ et $f\colon J\to \eR$ sont deux fonctions continues données. Voyons rapidement comment l'appliquer en pratique la méthode générale de résolution. 

Rappelons nous que le problème est de trouver les fonction $y\colon I'\to \eR$ telles que pour tout $t \in I'$,
\begin{equation}
	y'(t)=u(t)f\big( y(t) \big).
\end{equation}
Nous considérons $U$, une primitive de $u$ sut $I$ et $G$, une primitive de $1/f$ sur $J$. Si $I'\subseteq I$ et $y\colon I'\to J$, alors $y$ est solution de \eqref{EqGeneTheSep} si et seulement si il existe une constante $C$ telle que
\begin{equation}		\label{EqSolSepThe}
	G\big( y(t) \big)=U(t)+C.
\end{equation}
La recherche des solutions de l'équation différentielle se ramène donc à la recherche de primitives et de solutions d'une équation algébrique (il faut isoler $y(t)$ dans \eqref{EqSolSepThe}). Réciproquement toute solution régulière de cette dernière relation est solution de l'équation différentielle.

Remarque : lorsque nous cherchons $U$ et $G$, nous ne cherchons que \emph{une} primitive. Il ne faut pas considérer des constantes d'intégration à ce niveau.


%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équations et systèmes linéaire à coefficients constants}
%---------------------------------------------------------------------------------------------------------------------------

Nous regardons l'équation
\begin{equation}	\label{EqLinConstantRappels}
	y^{(n)} + a_1 y^{(n-1)} + \ldots + a_{n-1} y^\prime + a_n y = v(t)
\end{equation}
où les coefficients $a_k$ sont maintenant des constantes. La méthode est donnée à la page $311$ du cours. Il faut commencer par résoudre le polynôme caractéristique
\begin{equation}
	r^n+a_1 r^{n-1}+\ldots +a_n=0.
\end{equation}
Si $\lambda_1,\ldots,\lambda_k$ sont les solutions avec multiplicité $\mu_1,\ldots,\mu_k$, alors le \defe{système fondamental}{Système fondamental} de solutions linéairement indépendantes est l'ensemble suivant de solutions à l'équation homogène :
\begin{equation}
	\begin{aligned}[]
		 e^{\lambda_1 t},t e^{\lambda_1 t},	&	\ldots,t^{\mu_1-1} e^{\lambda_1  t}\\
							&\vdots\\
		 e^{\lambda_k t},t e^{\lambda_k t},	&\ldots,t^{\mu_k-1} e^{\lambda_k  t}.
	\end{aligned}
\end{equation}
Nous notons $y_i$ ces solutions. La solution générale de l'équation homogène est donc donnée par
\begin{equation}
	y_H=\sum_i c_i y_i.
\end{equation}
Afin de trouver la solution générale de l'équation non homogène, nous appliquons la méthode de variation des constantes, en imposant les $n-1$ conditions
\begin{equation}		\label{EqVarCstSubtil}
	\sum_{i=1}^n c'_i(t)y_i^{(l)}(t)=0
\end{equation}
avec $l=0,\ldots,n-2$. Ces condition plus l'équation de départ \eqref{EqLinConstantRappels} forment un système de $n$ équations différentielles pour les $n$ fonctions inconnues $c_i(t)$.

Cette condition peut paraître mystérieuse. Elle est posée à la page 337 du cours de première année. Il est cependant encore possible de travailler sans poser la condition \eqref{EqVarCstSubtil} en suivant la recette, en calculant des déterminants de Wronskien. Des exemples sont donnés dans les exercices sur le second ordre.


\paragraph{Si les coefficients ne sont pas constants ?}

Une équation différentielle linéaire d'ordre $n$ sur $I$ est une
équation de la forme
\begin{equation}	\label{EqLinRappels}
	y^{(n)} + u_1(t) y^{(n-1)} + \ldots + u_{n-1}(t) y^\prime + u_n(t) y = v(t)
\end{equation}
où $v$ et $u_k$ sont des fonctions continues fixées de $I$ vers $\eR$.

Pour résoudre cette équation, il faut commencer par résoudre l'équation homogène correspondante (c'est à dire celle que l'on obtient en posant $v(t)=0$). Ensuite, nous trouvons la solution de l'équation \eqref{EqLinRappels} en appliquant la méthode de la \defe{variation des constantes}{variations des constantes}.

Donnons un exemple du pourquoi la méthode de variations des constantes est efficace. Soit l'équation 
\begin{equation}		\label{EqDiffExempleVarCst}
	u'+f(t)u=g(t),
\end{equation}
 et disons que $u_H$ est une solution de l'équation homogène. La méthode de variations des constantes consiste à poser $u(t)=K(t)u_H(t)$, et donc $u'(t)=K'u_H+Ku_H'$. En remettant dans l'équation de départ,
\begin{equation}
	K'u_H+Ku_H'+fKu_H=g.
\end{equation}
La somme $Ku_H'+fKu_H$ est nulle, par définition de $u_H$. Par conséquent, il ne reste que
\begin{equation}
	K'=\frac{ g(t) }{ u_H }.
\end{equation}
Lorsqu'on utilise la méthode de variation des constantes, nous trouvons toujours une simplification \og miraculeuse\fg.

Dans l'immédiat, nous ne considérons que le cas où les \( u_i\) sont des constantes. Le cas où les \( u_i\) deviennent des fonctions de \( t\) sera vu plus tard.


%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation homogène}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecEqDiffHomo}

C'est une équation de la forme
\begin{equation}
	y'=f(t,y)
\end{equation}
où $f(\lambda t,\lambda y)=f(t,y)$ pour tout $\lambda\neq 0$.
\begin{lemma}
L'équation $y'=f(t,y)$ est homogène si et seulement si $f(t,y)$ est une fonction de $y/t$ seulement.
\end{lemma}
Pour résoudre l'équation homogène, on pose
\begin{equation}		\label{EqDiffHomoPoser}
	z(t)=\frac{ y(t) }{ t },
\end{equation}
donc $tz=y$, et 
\begin{equation}
	y'(t)=tv'(t)+v(t),
\end{equation}
à remettre dans l'équation de départ.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation de Bernoulli}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecBernh}

C'est une équation du type
\begin{equation}	\label{EqBerNDiffalp}
	y'=a(t)y+b(t)y^{\alpha}
\end{equation}
où $\alpha\neq 0$ ou $1$. Pour la résoudre, on divise l'équation par $y^{\alpha}$, et on pose $u=y^{1-\alpha}$, et on tombe sur une équation linéaire
\begin{equation}
	u'=(1-\alpha)\big( a(t)u+b(t) \big).
\end{equation}


%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation de Riccati}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecRicatti}

Contrairement à ce qui est écrit dans le cours, l'orthographe est \emph{\href{http://fr.wikipedia.org/wiki/Jacopo_Riccati}{Riccati}}, et non \emph{Ricatti}.

C'est une équation de la forme 
\begin{equation}		\label{EqDiffGFeneRicatti}
	y'=a(t)y^2+b(t)y+c(t).
\end{equation}

En général, on ne peut pas la résoudre, mais si on en connaît \emph{a priori} des solutions particulières, alors on peut s'en sortir.

\begin{enumerate}

\item 
Si on sait que $y_1(t)$ est une solution, alors on pose
\begin{equation}
	y(t)=y_1(t)+\frac{1}{ u(t) },
\end{equation}
et on obtient une équation linéaire
\begin{equation}
	u'=-\big( 2y_1(t)a(t)+b(t) \big)u-a(t).
\end{equation}

\item
Si $y_1$ et $y_2$ sont solutions, alors nous avons $y$ sous forme implicite
\begin{equation}
	\frac{ y-y_1 }{ y-y_2 }=K e^{\int a(t)\big( y_1(t)-y_2(t) \big)dt}.
\end{equation}
\end{enumerate}

Pour résoudre une équation de Ricatti, il faut donc d'abord deviner une ou deux solutions.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation différentielle exacte}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecEqDiffExacte}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Résolution lorsque tout va bien}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Avant de vous lancer dans les équations différentielles exacte, vous devez lire la section sur les formes différentielles \ref{SecFormDiffRappel}. Une équation différentielle exacte est de la forme $P(t,y)+Q(t,y)y'=0$ que nous allons écrire sous la forme
\begin{equation}		\label{EqExacteDiff}
	P(t,y)dt+Q(t,y)dy=0.
\end{equation}
Nous savons que si $\partial_yP=\partial_tQ$, alors il existe une fonction $f(t,y)$ telle que $Pdt+Qdy=df$. Pour trouver une telle fonction, nous pouvons simplement intégrer la forme $Pdt+Qdy$. En effet, si $\gamma\colon [0,1]\to \eR^2$ est un chemin tel que $\gamma(0)=(0,0)$ et $\gamma(1)=(t,y)$, alors en définissant
\begin{equation}
	f(t,y)=\int_{\gamma}[Pdt+Qdt]=\int_{0}^1\big[ (P\circ\gamma)(u)dt+(Q\circ\gamma)(u) \big]\big( \gamma'(u) \big)du,
\end{equation}
nous avons $df=Pdt+Qdy$. N'importe quel chemin fait l'affaire. Calculons avec $\gamma(u)=(tu,yu)$. La dérivée de ce chemin est donnée par
\begin{equation}
	\gamma'(u)=t\begin{pmatrix}
	1	\\ 
	0	
\end{pmatrix}+y\begin{pmatrix}
	0	\\ 
	1	
\end{pmatrix}.
\end{equation}
Étant donné que $dt\begin{pmatrix}
	a	\\ 
	b	
\end{pmatrix}=a$ et $dy\begin{pmatrix}
	a	\\ 
	b	
\end{pmatrix}=b$, nous avons
\begin{equation}
	\begin{aligned}[]
	f(t,y)&=\int_0^1[Pdt+Qdy]\big( \gamma(u) \big)\left( t\begin{pmatrix}
	1	\\ 
	0	
\end{pmatrix}+y\begin{pmatrix}
	0	\\ 
	1	
\end{pmatrix} \right)du\\
		&=\int_0^1P\big( \gamma(t) \big)tdu+\int_0^1Q\big( \gamma(t) \big)ydu\\
		&=\int_0^1\big[ tP(tu,uy)+yQ(tu,yu) \big]du.
	\end{aligned}
\end{equation}
Nous retrouvons exactement la formule \eqref{EqFormI33Fffdd} de l'exercice \ref{exo_I-3-3}. Si ça t'étonne, c'est que tu n'as pas compris ;) Dans le cas où nous avons la fonction $f$ qui vérifie $P=\partial_tf$ et $Q=\partial_yf$, l'équation \eqref{EqExacteDiff} devient
\begin{equation}
	\frac{ \partial f }{ \partial t }+\frac{ \partial f }{ \partial y }\frac{ dy }{ dt }=0,
\end{equation}
c'est à dire 
\begin{equation}
	\frac{ d }{ dt }\Big[ f\big( t,y(t) \big) \Big]=0,
\end{equation}
dont la solution
\begin{equation}
	f\big( t,y(t) \big)=C
\end{equation}
donne la solution $y(t)$ sous forme implicite.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Facteur intégrant (quand tout ne va pas bien)}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Si la forme $Pdt+Qdy$ n'est pas exacte, il n'existe pas de fonction $f$ qui résolve l'affaire. Nous pouvons toutefois essayer de trouver un \defe{facteur itnégrant}{Facteur!intégrant}. Nous cherchons une fonction $M$ telle que
\begin{equation}
	(MP)dt+(MQ)dy
\end{equation}
soit exacte. Nous cherchons donc $M(t,y)$ telle que $\partial_y(MP)=\partial_t(MQ)$. En utilisant la règle de Leibnitz, nous trouvons l'équation suivante pour $M$ :
\begin{equation}		\label{EqDuFacteurIntegrant}
	M(\partial_yP-\partial_tQ)=Q(\partial_tM)-P(\partial_yM).
\end{equation}
Cette équation est en générale extrêmement difficile à résoudre, mais dans certains cas particuliers, il est possible d'en trouver une solution à tâtons.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Réduction de l'ordre}
%---------------------------------------------------------------------------------------------------------------------------

Afin de diminuer l'ordre d'une équation dans laquelle le paramètre n'apparaît pas, il y a deux changements de variables très utiles. Le premier, le plus simple, est simplement de poser $z(t)=y'(t)$, ce qui donne $z'(t)=y''(t)$. Le second, \emph{qui n'est pas le même}, est $z\big( y(t) \big)=y'(t)$, qui entraîne $y''(t)=z'\big( y(t) \big)z(t)$. Dans ce second cas, il faut également changer de variable, et utiliser $y(t)$ comme variable au lieu de $t$.

