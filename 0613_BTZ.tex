% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013-2014
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

\section{The causally singular structure}
\label{LONGSecBlacHole}

\subsection{Closed orbits}

The singularity in $AdS_l$ is defined as the closed orbits of $AN$ and $A\bar N$ in $G/H$. This subsection is intended to identify them.

\begin{proposition}		\label{LONGPropCartanExtExpo}
	The Cartan involution $\theta\colon \sG\to \sG$ is an inner automorphism, namely it is given by $\theta=\Ad(k_{\theta})$ where $k_{\theta}= e^{\pi q_0}$.
\end{proposition}

%
\begin{proof}
	The operator $\Ad(k_{\theta})$ acts as the identity on $\sK$ because $q_0$ is central in $\sK$ by definition.  Looking at the decompositions \eqref{LONGEqDecomptsNkKP} and \eqref{LONGEqDecomptsNTroisKP}, and taking into account that the result is already guaranteed on $\sK$, we have to check the action of $\Ad(k_{\theta})$ on $J_1$, $J_2$, $q_k$, $p_k$ and $p_1$. It is done in setting $x=\pi$ in equations
	%
	%
	\eqref{LONGSubEqsAdxqzJJ},
	%
	%
	\eqref{LONGEqExpAdqkpk}
	%
	and \eqref{LONGEqAdqzpUn}.
	What we get is that $\Ad(k_{\theta})$ changes the sign on $\sP$.
	
\end{proof}


\begin{proposition}		\label{LONGPropKanUnicAbarN}
	For each $an\in AN$, there exists one and only one $k\in K$ such that $kan\in A\bar N$. There also exists one and only one $k\in K$ such that $ank\in A\bar N$.
\end{proposition}

\begin{proof}
	For unicity, let $an\in AN$ and suppose that $k_1^{-1}an$ and $k_2^{-1}an$ both belong to $A\bar N$. Then there exist $a_1$, $a_2$, $\bar n_1$ and $\bar n_2$ such that $k_1^{-1}an=a_1\bar n_1$ and $k_2^{-1}an=a_2\bar n_2$ and we have
	\begin{equation}
		an=k_1a_1\bar n_1=k_2a_2\bar n_2.
	\end{equation}
	By unicity of the decomposition $KA\bar N$, we conclude that $k_1=k_2$.

	For the existence, let $an\in AN$ and consider the $KAN$ decomposition $\theta(an)=ka'n'$. We claim that $k^{-1}$ answers the question. Indeed, $\theta$ is the identity on $K$, so that $an=k\theta(a'n')$, and then
	\begin{equation}
		k^{-1}an=\theta(a'n')\in A\bar N.
	\end{equation}
	
	One checks the statement about $ank\in A\bar N$ in much the same way.
\end{proof}%
In the following results, we use the fact that the group $K$ splits into the commuting product $K=\SO(2)\times\SO(l-1)$.
\begin{lemma}		\label{LONGLemExistxTqansAbarN}
	For every $an\in AN$, there exists $x\in\mathopen[ 0 , 2\pi [$ such that $[an e^{xq_0}]\in[A\bar N]$.
\end{lemma}

\begin{proof}
	Let $k\in K$ such that $ank\in A\bar N$. The element $k$ decomposes into $k=st$ with $s= e^{xq_0}\in \SO(2)$ and $t\in\SO(n)\subset H$. Thus $[ans]\in[A\bar N]$.
\end{proof}

\begin{lemma}		\label{LONGLemansse}
	If $[an]=[s]$ with $s\in\SO(2)$, then $s=e$.
\end{lemma}

\begin{proof}
	The assumption implies that there exists a $h\in H$ such that $an=sh$. Using the $KAN$ decomposition of $H$, such a $h$ can be written under the form $h=ta'n'$ with $t\in\SO(n)$. Thus we have $an=sta'n'$. By unicity of the decomposition $kan$, we must have $st=e$, and then $s=e$.
\end{proof}


\begin{lemma}		\label{LONGLemANksk}
	If $an\in AN$ and if $[ank_{\theta}]=[s]$ with $s\in\SO(2)$, then $s=k_{\theta}$.
\end{lemma}

%
The hypothessis implies that there exists a $h\in H$ such that $ank_{\theta}=sh$. The element $h$ can be decomposed as
\begin{equation}
	h=h_Kh_{AN}k_{\theta}
\end{equation}
with $h_K\in K$ and $h_{AN}\in AN$ but with no warranty that $h_K$ or $h_{AN}$ belong to $H$. We have
\begin{equation}
	an=\underbrace{sh_K}_{\in K}h_{AN},
\end{equation}
so that $sh_K=e$ by unicity of the $KAN$ decomposition. In particular $h_K\in\SO(2)$.

Now we want to prove that $h_K=k_{\theta}$ because it would implies $s=k_{\theta}$ by the relation $sh_K=e$. If $h_K= e^{yq_0}k_{\theta}$ we have
\begin{equation}
	h=h_Kh_{AN}k_{\theta}= e^{yq_0}k_{\theta}h_{AN}k_{\theta}= e^{yq_0}h_{A\bar N}.
\end{equation}
where $h_{A\bar N}=\AD(k_{\theta})h_{AN}$. By unicity of the $KA\bar N$ decomposition in $H$, the latter relation shows that $ e^{yq_0}$ has to be the compact part of $H$ and then belong to $K_H$ which is only possible when $y=0$.
%

\begin{theorem}		\label{LONGThoOrbitesOuverttes}
	The closed orbits of $AN$ in $AdS_l$ are $[AN]$ and $[AN k_{\theta}]$ where $k_{\theta}$ is the element of $K$ such that $\theta=\Ad(k_{\theta})$. The closed orbits of $A\bar N$ are $[A\bar N]$ and $[A\bar N k_{\theta}]$. The other orbits are open.
\end{theorem}

\begin{proof}
	Let us deal with the $AN$-orbits in order to fix the ideas. First, remark that each orbit of $AN$ pass trough $[SO(2)]$. Indeed, each $[ank]$ is in the same orbit as $[k]$ with $k\in K=\SO(2)\otimes\SO(n)$. Since $\SO(n)\subset H$, we have $[k]=[s]$ for some $s\in\SO(2)$.

	We are thus going to study openness of the $AN$-orbit of elements of the form $[e^{x q_0}]$ because these elements are ``classifying'' the orbits. Using the isomorphism $  dL_{g^{-1}}\colon T_{[g]}(G/H)\to \sQ$, we know that a set $\{ X_1,\ldots X_l \}$ of vectors in $T_{[ e^{x q_0}]}AdS_l$ is a basis if and only if the set $\{ dL_{ e^{-xq_0}}X_i \}_{i=1,\ldots l}$ is a basis of $\sQ$. We are thus going to study the elements 
	\begin{equation}
		\begin{aligned}[]
			dL_{ e^{-xq_0}}X^*_{[ e^{xq_0}]}	&=dL_{ e^{-xq_0}}\Dsdd{ \pi\big(  e^{-t X} e^{xq_0} \big) }{t}{0}\\
								&=\Dsdd{ \pi\big(  \AD( e^{-xq_0} ) e^{-tX} \big) }{t}{0}\\
								&=-\pr_{\sQ} e^{-\ad(xq_0)}X
		\end{aligned}
	\end{equation}
    when $X$ runs over the elements of $\sA\oplus \sN$ and $\pr_{\sQ}$ stands for the projection on $\sQ$ parallel to $\sH$.
	%
	The projections on $\sQ$ of equations \eqref{LONGSubEqsAdxqzJJ}, \eqref{LONGEqExpoQzSurNk} and \eqref{LONGEqExpoQzN} are
	\begin{subequations}		\label{LONGSubEqsExpAdxqDivers}
		\begin{align}
			\pr_{\sQ}\Big( e^{\ad(xq_0)}J_1\Big)&=\sin(x)q_2\\
			\pr_{\sQ}\Big( e^{\ad(xq_0)}J_2\Big)&=\cos(x)q_1\\
			\pr_{\sQ}\Big(  e^{xq_0}X_{++} \Big)&=q_0+\sin(x)q_1-\cos(x)q_2\\
			\pr_{\sQ} \Big(   e^{\ad(xq_0)}X_{+-} \Big)&=q_0-\sin(x)q_1-\cos(x)q_2\\
			\pr_{\sQ}\Big(  e^{\ad(xq_0)}(s_k-p_k) \Big)&= \sin(x)q_k\\
			\pr_{\sQ}\Big(  e^{\ad(xq_0)}(q_k+r_k) \Big)&=\cos(x)q_k.
		\end{align}
	\end{subequations}
	It is immediately visible
	%
	that an orbit trough $[ e^{xq_0}]$ is open if and only if $\sin(x)\neq 0$. It remains to study the orbits of $[ e^{\pi q_0}]$ and $[e]$. Lemma \ref{LONGLemansse} shows that these two orbits are disjoint.

	Let us now prove that $[AN]$ is closed. A point outside $\pi(AN)$ reads $\pi(ans)$ where $s$ is an elements of $\SO(2)$ which is not the identity. Let $\mO$ be an open neighborhood of $ans$ in $G$ such that every element of $\mO$ read $a'n's't'$ with $s'\neq e$. The set $\pi(\mO)$ is then an open neighborhood of $\pi(ans)$ which does not intersect $[AN]$. This proves that the complementary of $[AN]$ is open. The same holds for the orbit $[A\bar N]$.
	
	The orbit $[ANk_{\theta}]$ and $[A\bar Nk_{\theta}]$ are closed too because $ANk_{\theta}=k_{\theta}A\bar N$.

\end{proof}

%
\begin{lemma}
	We have $[AN]\cap[ANk_{\theta}]=\emptyset$.
\end{lemma}

\begin{proof}
	A representative of an element in $[AN]\cap[ANk_{\theta}]$ can be written $an=a'n'k_{\theta}h$, and then we have $k_{\theta}h\in AN$. Decomposing $h$ into its components $h_K\in K$ and $h_{AN}\in AN$, we see that $k_{\theta}h_K\in AN$, which is impossible because $k_{\theta}\in\SO(2)$ while $K_H=\SO(l-1)$.
\end{proof}

\begin{proposition}		\label{LONGPropUniquexxxxANANbarktheta}
	Let $an\in AN$. Then there exist unique $x_0$, $x_1$, $x_2$, $x_3$ in $\mathopen[ 0 , 2\pi [$ such that
	\begin{enumerate}
		\item
			$[an e^{x_0q_0}]\in[AN]$, and $an e^{x_0q_0}=an$,
		\item
			$[an e^{x_1q_0}]\in[ANk_{\theta}]$, and $an e^{x_1q_0}=ank_{\theta}$,
		\item
			$[an e^{x_2q_0}]\in[A\bar N]$, and $an e^{x_2q_0}=a'\bar n h_K$ (lemma \ref{LONGLemExistxTqansAbarN}),
		\item
			$[an e^{x_3q_0}]\in[A\bar Nk_{\theta}]$, and $an e^{x_3q_0}=a'\bar nk_{\theta}h_K$.
	\end{enumerate}
	These numbers satisfy $x_0=0$, $x_1=\pi$ and $x_3=x_2+\pi$ modulo $2\pi$. 

	Moreover if $P$ does not belong to $[AN]\cap[A\bar N]$ nor to $[AN]\cap[A\bar Nk_{\theta}]$, these are four different numbers.
\end{proposition}

\begin{proof}
	 Existence comes from lemma \ref{LONGLemExistxTqansAbarN}. Now we discuss the unicity.
	 \begin{enumerate}

		 \item
			 Let $x'_0$ such that $[an e^{x'_0q_0}]\in[AN]$. In that case, there exist $a'n'\in AN$ and $h\in H$ such that $ e^{x'_0q_0}=a'n'$. If we decompose $h=h_{AN}h_K$, unicity of the $ANK$ decomposition show that $ e^{x'_0q_0}=h_K$, which is only possible when $x'_0=0$ and $h_K=e$.
		\item
			If $an e^{x'_1q_0}=a'n'k_{\theta}h$, then there exist $a''n''$ such that $ e^{x'_1q_0}=a''n''k_{\theta}h$. Taking the class, $[a''n''k_{\theta}]=[ e^{x'_1q_0}]\in[\SO(2)]$. Then $ e^{x'_1q_0}=k_{\theta}$ by lemma \ref{LONGLemANksk}.
		\item
			Let $x_2$ such that $an e^{x_2q_0}t=a'\bar n$ with $t\in\SO(l-1)$ (proposition \ref{LONGPropKanUnicAbarN}), and suppose that we have an other $x'_2$ such that $[an e^{x'_2}]\in[A\bar N]$. We have the system
			\begin{subequations}
				\begin{numcases}{}
					an e^{x_2q_0}t=a'\bar n\\
					an e^{x'_2q_0}=a''\bar n' h.
				\end{numcases}
			\end{subequations}
			We extract $an$ from the first equation and we put the result in the second one. Taking into account the fact that $t$ commutes with $ e^{xq_0}$ and that $t\in H$ and renaming $h\to ht$, if we decompose $h=h_{A\bar N}h_K$ we find
			\begin{equation}
				a'\bar n e^{(x'_2-x_2)q_0}=a''\bar n'h_{A\bar N}h_K.
			\end{equation}
			By unicity of the decomposition $A\bar NK$ we find $h_K= e^{(x'_2-x_2)q_0}$. The left hand side belongs to $\SO(l-1)$ and the right hand side belongs to $\SO(2)$, so that one has to have $x'_2=x_2$.
		\item
			If we consider $x_3=x_2+\pi$ we have $an e^{x_3q_0}t=a'\bar nk_{\theta}$ (with $t\in\SO(l-1)$) and if we consider an other $x'_3$ such that $[an e^{x'_3q_0}]\in[A\bar Nk_{\theta}]$, we have the system
			\begin{subequations}
				\begin{numcases}{}
					an e^{(x_2+\pi)q_0}t=a'\bar nk_{\theta}\\
					an e^{x'_3q_0}=a'' \bar n'k_{\theta}h.
				\end{numcases}
			\end{subequations}
			Some manipulations including a redefinition of $h$ to include $t$ yield
			\begin{equation}
				e^{(x'_3-x_3)q_0}=k_{\theta}a_0\bar n_0k_{\theta}h,
			\end{equation}
			but $k_{\theta}A\bar Nk_{\theta}\subset AN$, thus
			\begin{equation}
				e^{(x'_3-x_3)q_0}=a_1n_1h_{AN}h_K
			\end{equation}
			so that unicity of the decomposition $ANK$ implies, $ e^{(x'_3-x_3)q_0=h_K}$, so that $x'_3=x_3$. 
	\end{enumerate}
	Let us now prove the second part. We suppose that $[P]$ does not belong to $[AN]\cap[A\bar N]$. If $x_0=x_2=0$, we have $[P]\in[AN]\cap[A\bar N]$. If $x_0=x_3$, then we have $an=a'\bar nk_{\theta}h_K$, and then $[an]\in[AN]\cap[A\bar Nk_{\theta}]$ since $h_K$ commutes with $k_{\theta}$.

	 If $x_1=x_2$, then $an=a'\bar nk_{\theta}h_K$, so that $[an]\in[AN]\cap[A\bar Nk_{\theta}]$. If $x_1=x_3$, then $ank_{\theta}=a'\bar nk_{\theta}h_K$, so that $an\in[AN]\cap[A\bar N]$.
\end{proof}

%
\subsection{Vanishing norm criterion}
%

In the preceding section, we defined the singularity by means of the action of an Iwasawa group. We are now going to give an alternative way of describing the singularity, by means of the norm of a fundamental vector of the action. This ``new'' way of describing the singularity is, in fact, much more similar to the original BTZ black hole where the singularity was created by identifications along the integral curves of a Killing vector field\cite{these_Detournay}. The vector $J_1$ in theorem \ref{LONGThosSequivJzero} plays here the role of that ``old'' Killing vector field.

Discrete identifications along the integral curves of $J_1$ would produce the causally singular space which is at the basis of our black hole.

What we will prove is the
\begin{theorem}		\label{LONGThosSequivJzero}
	We have $\sS\equiv \| J_1^* \|_{[g]}=\| \pr_{\sQ}\Ad(g^{-1})J_1 \|=0$.
\end{theorem}

Thanks to this theorem, our strategy will be to compute $\| \pr_{\sQ}\Ad(g^{-1})J_1 \|$ in order to determine if $[g]$ belongs to the singularity or not. The proof will be decomposed in three steps. The first step is to obtain a manageable expression for $\| J_1^* \|$.

\begin{lemma}		\label{LONGLemExpressionCoolNormJUn}
	We have $\| (J_1^*)_{[g]} \|=\| \pr_{\sQ}\Ad(g^{-1})J_1 \|$ for every $[g]\in AdS_l$.
\end{lemma}




\begin{proof}
	By definition, 
	\begin{equation}
		(J_1^*)_{[g]}=\Dsdd{ \pi( e^{-tJ_1}g) }{t}{0}=-d\pi dR_g J_1.
	\end{equation}
	The norm of this vector is the norm induced from the Killing form on $\sG$\cite{Kerin}. First we have to put $dR_g J_1$ under the form $dL_g X$ with $X\in \lG$. One obviously has $dR_g J_1=dL_g\Ad(g^{-1})J_1$, and the norm to be computed is
	\begin{equation}
		\begin{aligned}[]
			\| J_1^* \|_{[g]}=\| d\pi_gdL_g\Ad(g^{-1})J_1\|_{[g]}&=  \|  d\pi_gdL_g\pr_{\sQ}\Ad(g^{-1})J_1\|_{[g]}\\ 
			&= \| dL_g\pr_{\sQ}\Ad(g^{-1})J_1 \|_{g} \\
			&=\| \pr_{\sQ}\Ad(g^{-1})J_1 \|_e
		\end{aligned}
	\end{equation}
\end{proof}


\begin{proposition}		\label{LONGPropPtpsSjzero}
	If $p\in\hS$, then $\| J_1^* \|_p=0$.
\end{proposition}

\begin{proof}
	We are going to prove that $\pr_{\sQ}\Ad(g^{-1})J_1$ is a light like vector in $\sQ$ when $g$ belongs to $[AN]$ or $[A\bar N]$. A general element of $AN$ reads $g=a^{-1}n^{-1}$ with $a\in A$ and $n\in N$. Since $\Ad(a)J_1=J_1$, we have $\Ad(g^{-1})J_1=\Ad(n)J_1$. We are going to study the development
	\begin{equation}
		\Ad( e^{Z})J_1= e^{\ad(Z)}J_1=J_1+\ad(Z)J_1+\frac{ 1 }{2}\ad(Z)^2J_1+\ldots
	\end{equation}
	where $Z=\ln(n)\in\sN$. The series is finite because $Z$ is nilpotent (see theorem \ref{LONGEtOrdreDeux} for more informations) and begins by $J_1$ while all other terms belong to $\sN$. Notice that the same remains true if one replaces $\sN$ by $\bar \sN$ everywhere. 
	
	Moreover, $\Ad( e^{Z})J_1$ has no $X_{0+}$-component (no $X_{0-}$-component in the case of $Z\in\bar\sN$) because $[X_{0+},J_1]=0$, so that the term $[Z,J_1]$ is a combination of $X_{+0}$, $X_{++}$ and $X_{+-}$.  Since the action of $\ad(X_{+\pm})$ on such a combination is always zero, the next terms are produced by action of $\ad(X_{0+})$ on a combination of $X_{+0}$, $X_{++}$ and $X_{+-}$. Thus we have
	%
\begin{equation}		\label{LONGEqAdanJUnabck}
	\Ad( e^{Z})J_1=J_1+aX_{++}+bX_{+-}+c_kX_{+0}^k
\end{equation}
for some\footnote{One can show that every combinations of these elements are possible, but that point is of no importance here.} constants $a$, $b$ and $c_k$.

The projection of $\Ad( e^{Z})J_1$ on $\sQ$ is made of a combination of the projections of $X_{+0}$, $X_{++}$ and $X_{+-}$. From the definitions \eqref{LONGEqBasQQzi}, we have $\pr_{\sQ}X_{++}=q_0+q_2$, lemma \ref{LONGLemNonHXaz} implies $\pr_{\sQ}X_{+0}=0$ and lemma \ref{LONGLemSigmaXppEgalXPm} yields $\pr_{\sQ}X_{+-}=-\sigma\pr_{\sQ}X_{++}=q_0+q_2$. The conclusion is that $\pr_{\sQ}\big( e^{\ad(Z)}J_1\big)$ is a multiple of $q_0+q_2$, which is light like. The conclusion still holds with $\bar\sN$, but we get a multiple of $q_0-q_2$ instead of $q_0+q_2$.

	Now we have $\Ad(k_{\theta})J_1=J_1$ and $\Ad(k_{\theta})(q_0\pm q_2)=-(q_0\pm q_2)$, so that the same proof holds for the closed orbits $[ANk_{\theta}]$ and $[A\bar N k_{\theta}]$.
\end{proof}

\begin{remark}		\label{LONGRemANANbarYapas}
	The coefficients $a$, $b$ and $c_k$ in equation \eqref{LONGEqAdanJUnabck} are continuous functions of the starting point $an\in AN$. More precisely, they are polynomials in the coefficients of $X_{++}$, $X_{+-}$, $X_{0+}$ and $X_{0+}$ in $Z$. The vector $\pr_{\sQ}\Ad(g^{-1})J_1=(a+b)(q_0+q_1)$ is thus a continuous function of the point $[g]\in[AN]$.

	If $[g]\in[AN]\cap[A\bar N]$, then $\pr_{\sQ}\Ad(g^{-1})J_1$ has to vanish as it is a multiple of $q_0+q_1$ and of $q_0-q_1$ in the same time. We conclude that in each neighborhood in $[AN]$ of an element of $[AN]$, there is an element which does not belong to $[A\bar N]$.
\end{remark}

\begin{proposition}
	If $\| J_1^* \|_p=0$, then $p\in\sS$.
\end{proposition}

\begin{proof}
	As before we are looking at a point $[g]=[(an)^{-1}s^{-1}]$ with $s= e^{xq_0}$. The norm $\| J_1^* \|$ vanishes if
	\begin{equation}
		\| \pr_{\sQ} \Ad( e^{xq_0})\Ad(an)J_1 \|=0.
	\end{equation}
	We already argued in the proof of proposition \ref{LONGPropPtpsSjzero} that $\Ad(an)J_1$ is equal to $J_1$ plus a linear combination of $X_{++}$, $X_{+-}$ and $X_{+0}$. Using the relations \eqref{LONGSubEqsExpAdxqDivers}, we see that
	\begin{equation}		\label{LONGEqprQexpxqzXanroots}
		\begin{aligned}[]
			\pr_{\sQ} e^{\ad(xq_0)}(J_1&+aX_{++}+bX_{+-}+\sum_k c_kX^k_{+0})\\
								&=(a+b)q_0+(a-b)\sin(x)q_1+\big( \sin(x)-(a+b)\cos(x) \big)q_2+\sum _k c_k\sin(x)q_k.
		\end{aligned}
	\end{equation}
	The norm of this vector, as function of $x$, is given by
	\begin{equation}
		n(x)=(a+b)\sin(2x)+(4ab-C^2-1)\big( 1-\cos(2x) \big),
	\end{equation}
	where $C^2=\sum_kc_k^2$. Using the variables $u=a+b$ and $v=(1+c^2-4ab)/2$,
	\begin{equation}
		n(x)=u\sin(2x)+v\cos(2x)-v.
	\end{equation}
	Following $u=0$ or $u\neq 0$, the graph of that function vanishes two or four times between $0$ and $2\pi$, see figure \ref{LabelFigRLuqsrr}. Points of $[AN]$ are divided into two parts: the \emph{red points} which correspond to $u\neq 0$, and the \emph{blue points} which correspond to $u=0$. By continuity, the red part is open.


%The result is on figure \ref{LabelFigRLuqsrr}. % From file RLuqsrr
\newcommand{\CaptionFigRLuqsrr}{In red, the function $n(x)$ with $u\neq 0$ and in blue, the function with $u=0$.}
\input{Fig_RLuqsrr.pstricks}

	Let $P=an\in AN$. 
	By proposition \ref{LONGPropUniquexxxxANANbarktheta}, we consider the unique  $x_0$, $x_1$, $x_2$ and $x_3$ in $\mathopen[ 0 , 2\pi [$ such that 
\begin{subequations}
	\begin{align}
		[P e^{x_0q_0}]&\in[AN]\\
		[P e^{x_1q_0}]&\in[ANk_{\theta}]\\
		[P e^{x_2q_0}]&\in[A\bar N]\\
		[P e^{x_3q_0}]&\in[A\bar Nk_{\theta}].
	\end{align}
\end{subequations}
They satisfy $x_0=0$, $x_1=\pi$ and $x_3=x_2+\pi$ modulo $2\pi$. Now, we divide $[AN]$ into two parts. The elements of $[AN]\cap [A\bar N]$ and $[AN]\cap[A\bar Nk_{\theta}]$ are said to be of \emph{type I}, while the other are said to be of \emph{type II}. We are going to prove that type I points are exactly blue points, while type II points are the red ones.

If $P$ is a point of type II, we know that the $x_i$ are four different numbers so that the norm function $n_P(x)$ vanishes \emph{at least} four times on the interval $\mathopen[ 0 , 2\pi [$, each of them corresponding to a point in the singularity. But our division of $[AN]$ into red and blue points shows that $n_P(x)$ can vanish \emph{at most} four times. We conclude that a point of type II is automatically red, and that the four roots of $n_P(x)$ correspond to the four values $x_i$ for which $P e^{x_iq_0}$ belongs to the singularity. The proposition is thus proved for points of type II.

Let now $P$ be of type I (say $P\in [AN]\cap [A\bar N]$) and let us show that $P$ is blue. We consider a sequence of points $P_k$ of type II which converges to $P$ (see remark \ref{LONGRemANANbarYapas}). We already argued that $P_k$ is red, so that $x_0(P_k)\neq x_2(P_k)$ and $x_1(P_k)\neq x_3(P_k)$, but
\begin{subequations}
	\begin{align}
		x_0(P_k)-x_2(P_k)\to 0\\
		x_1(P_k)-x_3(P_k)\to 0.
	\end{align}
\end{subequations}
The continuity of $n_Q(x)$ with respect to both $x\in\mathopen[ 0 , 2\pi [$ and $Q\in[AN]$ implies that $P$ has to be blue, and then $n_P(x)$ vanishes for exactly two values of $x$ which correspond to the points $P e^{xq_0}$ in the singularity.

Let us now prove that everything is done. We begin by points of type I. Let $P$ be of type $I$ and say $P\in[AN]\cap[A\bar N]$. The curve $n_P(x)$ vanishes exactly two times in $\mathopen[ 0 , 2\pi [$. Now, if $P e^{x_1 q_0}\in[ANk_{\theta}]$, thus $x_1 = \pi$ and we also have $P e^{x_1q_0}\in[A\bar Nk_{\theta}]$, but $P$ does not belong to $[ANk_{\theta}]$, which proves that $n_P(x)$ vanishes \emph{at least} two times which correspond to the points $P e^{xq_0}$ that are in the singularity. Since the curve vanishes in fact exactly two times, we conclude that $n_P(x)$ vanishes if and only if $P e^{xq_0}$ belongs to the singularity.

If we consider a point $P$ of type II, we know that the values of $x_i$ are four different numbers, so that the curve $n_P(x)$ vanishes \emph{at least} four times, corresponding to the points $P e^{xq_0}$ in the singularity. Since the curve is in fact red, it vanishes \emph{exactly} four times in $\mathopen[ 0 , 2\pi [$ and we conclude that the curve $n_P(x)$ vanishes if and only if $P e^{xq_0}$ belongs to the singularity.

The conclusion follows from the fact that 
\begin{equation}
	AdS_l=\Big\{ [P e^{xq_0}] \tq \text{$P$ is of type I or II and $x\in\mathopen[ 0 , 2\pi [$} \Big\}.
\end{equation}

\end{proof}
Proof of theorem \ref{LONGThosSequivJzero} is now complete.
%TODO : la remettre en ordre.


\subsection{Existence of the black hole}
%
\label{LONGSubSecExistenceTrouNoir}

We know that the geodesic trough $[g]$ in the direction $X$ is given by
\begin{equation}
	\pi\big( g e^{sX} \big)
\end{equation}
and that a geodesics is light-like when the \defe{direction}{Direction} $X$ is given by a nilpotent element in $\sQ$\cite{lcTNAdS}.


%The result is on figure \ref{LabelFigDTIYKkP}. % From file DTIYKkP
\newcommand{\CaptionFigDTIYKkP}{We are looking at a geodesics issued from one point of the line $[\SO(2)]=\{ e^{xq_0}\}_{x\in\mathopen[ 0 , 2\pi [}$. Here, $E(w)=q_0+w_1q_1+w_2q_2+\sum_{k\geq 3}w_kq_k$ with $\sum_{k}w_k^2=1$.}
\input{Fig_DTIYKkP.pstricks}

Let us study the geodesics issued from the point $[ e^{-xq_0}]$, see figure \ref{LabelFigDTIYKkP}. They are given by
\begin{equation}
	l^w_x(s)=\pi\big(    e^{-xq_0} e^{sE(w)} \big)
\end{equation}
where $E(w)=q_0+\sum_iw_iq_i$ with $\| w \|=1$. According to our previous work, the point $l^w_x(s)$ belongs to the singularity if and only if 
\begin{equation}		\label{LONGEqNormAFaireZeroOuPas}
	n_x^w(s)=\left\|   \pr_{\sQ} e^{-\ad(sE(w))} e^{\ad(xq_0)}J_1  \right\|^2=0.
\end{equation}
%
We already computed that $ e^{\ad(xq_0)}J_1=\cos(x)J_1+\sin(x)q_2$. By construction, $E(w)$ is nilpotent and $\ad(E)^3=0$ by proposition \ref{LONGEtOrdreDeux}. Using the fact that $[\sQ,\sH]\subset\sQ$ and $[\sQ,\sQ]\subset\sH$, we collect the terms in $\sQ$ in the development of the exponential. The $\sQ$ component of
\begin{equation}
	e^{-s\ad(E)}\big( \cos(x)J_1+\sin(x)q_2 \big)
\end{equation}
is 
\begin{equation}
	\ell=\frac{ s^2 }{ 2 }\sin(x)\ad(E)^2q_2-s\cos(x)\ad(E)J_1+\sin(x)q_2.
\end{equation}
The square norm of that expression is \emph{a priori} a polynomial of order $4$. Hopefully, the coefficient of $s^4$ contains
\begin{equation}
	B\big( \ad(E)^2q_2,\ad(E)^2q_2 \big),
\end{equation}
while the coefficient of $s^3$ is given by
\begin{equation}
	B\big( \ad(E)J_2,\ad(E)^2q_2 \big).
\end{equation}
Both of these two expressions are zero because the $\ad$-invariance of the Killing form makes appear $\ad(E)^3$. Equation \eqref{LONGEqNormAFaireZeroOuPas} is thus the second order polynomial given by
%
%
\begin{equation}		\label{LONGEqnwxBBB}
	\begin{aligned}[]
		n_x^w(s)	&=s^2\sin^2(x)B\big( \ad(E)^2q_2,q_2 \big)\\
				&\quad+s^2\cos^2(x)B\big( \ad(E)J_1,\ad(E) J_1\big)\\
				&\quad-2s\cos(x)\sin(x)B\big( \ad(E)J_1,q_2 \big)\\
				&\quad+\sin^2(x)B(q_2,q_2).
	\end{aligned}
\end{equation}
The problem now reduces to the evaluation of the three Killing products in this expression. %
%
Let us begin with $B\big( \ad(E)^2q_2,q_2 \big)$. For this one, we need to know the $q_2$-component of $\ad(E)^2q_2$. We have to review all the possibilities $\ad(q_i)\ad(q_j)q_2$ and determine which one(s) have a $q_2$-component.

In this optic, let us recall that $q_2$ is characterised by
\begin{equation}
	q_2\in\sP\cap\sQ\cap\tilde\sN_2.
\end{equation}

All the combinations $\ad(q_i)^2$ work. Since $\sG=\sK\oplus\sP$ is reductive and since $q_0$ is the only basis element of $\sQ$ to belong to $\sK$, none of the combinations $\ad(q_i)\ad(q_0)$ or $\ad(q_0)\ad(q_i)$ work. Using the relations
%
\begin{equation}
	\begin{aligned}[]
		q_1&\in\sP\cap\sQ\cap\sA	,	&&	[\tilde\sN_k,\tilde\sN_2]\subset\tilde\sN_k\\
		q_2&\in\sP\cap\sQ\cap\tilde\sN_2,	&&	[\tilde\sN_2,\tilde\sN_2]\subset\sA\\
		q_k&\in\sP\cap\sQ\cap\tilde\sN_k,	&&	[\tilde\sN_k,\tilde\sN_k]\subset\sA\oplus\tilde\sN_2,
	\end{aligned}
\end{equation}%
%
we check that the only working combinations are $\ad(q_i)^2$.

%

%

%

%

Thus, the only elements $\ad(q_i)\ad(q_j)q_2$ which have a $q_2$-component are $\ad(q_i)^2q_2$, while theorem \ref{LONGThoAdESqqq} says that this component is $q_2$ for $2\neq i\neq 0$ and $-q_2$ for $i=0$. Therefore, the $q_2$-component of $\ad(E)^2q_2$ is
\begin{equation}
	\ad(q_0)^2q_2+w_1^2\ad(q_1)^2q_2+\sum_{k\geq 3}w_k^2\ad(q_k)^2q_2=-w_2^2q_2
\end{equation}
where we used the fact that $\sum_i w_i^2=1$. Thus we have
\begin{equation}		\label{LONGEqBeDeuxqqwDeux}
	B\big( \ad(E)^2q_2,q_2 \big)=-w_2^2B(q_2,q_2).
\end{equation}

Let us now search for the $q_2$-component of $\ad(E)J_1$. We have $[q_1,J_1]\in[\sA,\sA]=0$, $[q_k,J_1]=0$ (equation \eqref{LONGEqJUnqkzero}), and $[q_2,J_1]=-q_0$, $[q_0,J_1]=-q_2$ (equation \eqref{LONGEqCalculBBBJUnUnNirme}). Then, we have
\begin{equation}
	\ad(E)J_1=w_2q_0+q_2.
\end{equation}
That implies
\begin{equation}
	B\big( \ad(E)J_1,q_2 \big)=B(q_2,q_2),
\end{equation}
and
\begin{equation}
	B\big( \ad(E)J_1,\ad(E)J_1 \big)=B(q_2,q_2)+w_2^2B(q_0,q_0).
\end{equation}
Equation \eqref{LONGEqnwxBBB} now reads
%
%
\begin{equation}
	\begin{aligned}[]
		\frac{ n_x^w(s) }{ B(q_2,q_2) }=\big( \cos^2(x)-w^2_2 \big)s^2-2\cos(x)\sin(x)s+\sin^2(x).
	\end{aligned}
\end{equation}
We have $n_x^w(s)=0$ when $s$ equals
\begin{equation}
	s_{\pm}=\frac{ \cos(x)\sin(x)\pm| w_2\sin(x) | }{ \cos^2(x)-w_2^2 }.
\end{equation}
If $w_2\sin(x)\geq 0$, we have\footnote{The solutions \eqref{LONGEqRacinesellTN} were already deduced in \cite{lcTNAdS} in a quite different way; these are equations \eqref{eq:tempssingul}.}
\begin{equation}				\label{LONGEqRacinesellTN}
	\begin{aligned}[]
		s_+	&=\frac{ \sin(x) }{ \cos(x)-w_2 }&\text{and}&&
		s_-	&=\frac{ \sin(x) }{ \cos(x)+w_2 },
	\end{aligned}
\end{equation}
and if $w_2\sin(x)<0$, we have to exchange $s_+$ with $s_-$.

If we consider a point $ e^{xq_0}$ with $\sin(x)>0$ and $\cos(x)<0$, the directions $w$ with $| w_2 |<| \cos(x) |$ escape the singularity as the two roots \eqref{LONGEqRacinesellTN} are simultaneously negative. Such a point does not belong to the black hole. That proves that the black hole is not the whole space.

If we consider a point $ e^{xq_0}$ with $\sin(x)>0$ and $\cos(x)>0$, we see that for every $w_2$, we have $s_+>0$ or $s_->0$ (or both). That shows that for such a point, every direction intersect the singularity. Thus the black hole is actually larger than only the singularity itself.

The two points with $\sin(x)=0$ belong to the singularity. At the points $\cos(x)=0$, $\sin(x)=\pm1$, we have $s_+=-1/w_2$ and $s_-=1/w_2$. A direction $w$ escapes the singularity only if $w_2=0$ (which is a closed set in the set of $\| w \|=1$). 


%The result is on figure \ref{LabelFigSFdgHdO}. % From file SFdgHdO
\newcommand{\CaptionFigSFdgHdO}{Points in $\pi(K)$ are classified by their angle in $\SO(2)$. Red points are part of the singularity, points in the black zone belong to the black hole and points in the green zone are free. The upper and lower boundaries belong to the horizon.}
\input{Fig_SFdgHdO.pstricks}


%
\section{Some more computations}
%
\label{LONGSecMoreComputations}

As we saw, the use of theorem \ref{LONGThosSequivJzero} leads us to study the function
\begin{equation}			\label{LONGEqprSqbignor}
	n_{[g]}^w(s)=\|   \pr_{\sQ} \Ad\big(  e^{-s E(w)} \big)X  \|^2=0
\end{equation}
where $E(w)=q_0+w_1q_1+\ldots+w_{l-1}q_{l-1}$ ($w\in S^{l-2}$) and (see equation \eqref{LONGEqAdanJUnabck})
\begin{equation}
	X=e^{\ad(xq_0)}\Ad(na)J_1=e^{\ad(xq_0)}\big(  J_1+aX_{++}+bX_{+-}+\sum_{k=3}^{l-1}c_kX_{+0}^k\big).
\end{equation}

%
From proposition \ref{LONGEtOrdreDeux} we have $\ad(E)^3=0$ and the exponential in equation \eqref{LONGEqprSqbignor} contains only three terms. Using the fact that $\pr_{\sQ}\ad(E)X=\ad(E)X_{\sH}$ and  we are then lead to study the norm of
\begin{equation}
	X_{\sQ}-s\ad(E)X_{\sH}+\frac{ s^2 }{2}\ad(E)^2X_{\sQ}.
\end{equation}
Notice that, since $\sQ$ is Killing-orthogonal to $\sH$, we have $B(X_{\sQ},Y_{\sQ})=B(X,Y_{\sQ})$. Thus we have
\begin{equation}
	n_{[g]}^w(s)=\|   \pr_{\sQ} \Ad\big(  e^{-s E(w)} \big)X  \|^2=a(E)s^2+b(E)s+c
\end{equation}
where
\begin{subequations}		\label{LONGSubEqsabcEBBB}
	\begin{align}
		a(E)&=-B\big( \ad(E)X,\sigma\ad(E)X \big)		\label{LONGEqCoefaEBX}\\
		b(E)&=-2B\big( X_{\sQ},\ad(E)X_{\sH} \big)\\
		c&=B(X_{\sQ},X_{\sQ}).
	\end{align}
\end{subequations}
%
%
It is convenient to decompose $X$ into $X=X_2+\sum_kc_kX_k$ with $X_2\in\tilde\sN_2$ and $X_k\in\tilde\sN_k$ as well as $E=E_2+\sum_kE_k$. Using the decompositions \eqref{LONGSubeqsDecompXqps}, the first part is
\begin{equation}
	\begin{aligned}[]
		X_2&=\Ad( e^{xq_0})\big( a(q_0-q_2)+b(p_1+s_1) \big)	\label{LONGEqXsiAdSTrois}\\
		X_k&=\Ad( e^{xq_0})(p_k+s_k)
	\end{aligned}
\end{equation}
%
where we have renamed $a$, $b$ and $c_k$. Using the exponentials \eqref{LONGSubEqsexpxzqpsdzuu} and \eqref{LONGEqExpoQzSurNkb} we have
\begin{subequations}
	\begin{align}
		X_2 &= aq_0-b\sin(x)q_1-a\cos(x)q_2 +a\sin(x)J_1+b s_1+b\cos(x)p_1	\label{LONGsubEqXtroisdonne}\\
		X_k &= -s_k-\cos(x)p_k+\sin(x)q_k					\label{LONGsubEqXkdonne}
	\end{align}
\end{subequations}

Let us first look at the case we encounter in $AdS_3$, i.e. with $X_k=0$ and $E_k=0$. In this case, a general direction is given by
\begin{equation}
	E_2(w)=E(\theta)=q_0+\cos(\theta)q_1+\sin(\theta)q_2.
\end{equation}
We achieve the computation of $ e^{\ad(E(\theta))}X_2$ using the known commutators:
\begin{subequations}		\label{LONGSubEqsEXtrois}
\begin{equation}
	\begin{aligned}[]
		\ad(E_2)X_{\sQ}&=J_1\big( a\sin(\theta)+a\cos(x) \big)\\
				&\quad+p_1\big( -a\cos(\theta)-b\sin(x) \big)\\
				&\quad+s_1\big( b\sin(x)\sin(\theta)-a\cos(x)\cos(\theta) \big).
	\end{aligned}
\end{equation}
and
\begin{equation}
	\begin{aligned}[]
		\ad(E_2)X_{\sH}&=q_0\big( a\sin(x)\sin(\theta)-b\cos(x)\cos(\theta) \big)\\
				&\quad-q_1 b\big( \sin(\theta)+\cos(x) \big)\\
				&\quad+q_2\big( a\sin(x)+b\cos(\theta) \big).
	\end{aligned}
\end{equation}
\end{subequations}
Putting together and writing $\big( \cos(\theta),\sin(\theta) \big)=(w_1,w_2)$,
\begin{equation}\label{LONGEqadEtroissurXtrois}
	\begin{aligned}[]
		\ad(E_2)X_2 &= J_1\big( a\cos(x)+aw_2 \big)\\
			&\quad p_1\big( -b\sin(x)-aw_1 \big)\\
			&\quad s_1\big( -a\cos(x)w_1+b\sin(x)w_2 \big)\\
			&\quad q_0\big( -b\cos(x)w_1+a\sin(x)w_2 \big)\\
			&\quad q_1\big( -bw_2-b\cos(x) \big)\\
			&\quad q_2\big( bw_1+a\sin(x) \big)
	\end{aligned}
\end{equation}
Using the expression \eqref{LONGEqadEtroissurXtrois} among with the norms and collecting the terms with respect to the dependence in $\theta$, we have
\begin{equation}			\label{LONGEqKillingEXQEXQ}
	\begin{aligned}[]
		\frac{ B\big( \ad(E)X_{\sQ},\ad(E)X_{\sQ} \big)}{B(q_0,q_0)}&=a^2\cos^2(x)-a^2\\
				&\quad +\sin(\theta)\big( -2a^2\cos(x) \big)\\
				&\quad +\cos(\theta)\big( -2ab\sin(x) \big)\\
				&\quad +\cos^2(\theta)\big( a^2\cos^2(x)-b^2\sin^2(x) \big)\\
				&\quad +\sin(\theta)\cos(\theta)\big( -2ab\sin(x)\cos(x) \big),
	\end{aligned}
\end{equation}
and
\begin{equation}
	\begin{aligned}[]
		\frac{ B\big( \ad(E)X_{\sH},\ad(E)X_{\sH} \big)}{B(q_0,q_0)}&= -b^2(1+\cos^2(x))\\
							&\quad +\sin(\theta)\big( -2b^2\cos(x) \big)\\
							&\quad +\cos(\theta)\big( -2ab\sin(x) \big)\\
							&\quad +\cos^2(\theta)\big( b^2\cos^2(x)-a^2\sin^2(x) \big)\\
							&\quad +\sin(\theta)\cos(\theta)\big( -2ab\sin(x)\cos(x) \big)
	\end{aligned}
\end{equation}
and finally, in the case of $AdS_3$ we have
\begin{equation}		\label{LONGEqaEdansAdsTrois}
	\begin{aligned}[]
		a(E)&=B\big( \ad(E)X_{\sH},\ad(E)X_{\sH} \big)-B\big( \ad(E)X_{\sQ},\ad(E)X_{\sQ} \big)\\
			&=(a^2-b^2)\big( \cos^2(x)+\sin(\theta)\cos(x)+\sin^2(\theta) \big).
	\end{aligned}
\end{equation}

For investigating the higher dimensional cases, we decompose $\ad(E)X$ into the four parts $\ad(E_2)X_2$, $\ad(E_2)X_k$, $\ad(X_k)X_2$ and $\ad(E_k)X_k$. 

The action of $E_2$ on $\tilde\sN_k$ is given by
\begin{subequations}
	\begin{align}
		\ad(E_2)r_k&=w_1q_k\\
		\ad(E_2)p_k&=-q_k\\
		\ad(E_2)q_k&=p_k+w_1r_k-w_2s_k\\
		\ad(E_2)s_k&=-w_2q_k.
	\end{align}
\end{subequations}
Thus 
\begin{equation}\label{LONGadEsurXki}
		\ad(E_2)X_k	=	q_k\big( w_2+\cos(x) \big)
				 + p_k\big( \sin(x) \big)
				 + r_k \big( w_1\sin(x) \big)
				 + s_k \big( -w_2\sin(x) \big)
\end{equation}
The same way we find
\begin{equation}
	\begin{aligned}[]
		\ad(E_k)X_2 = \sum_{k=3}^{l-1}\Big[ & p_k(-aw_k)
						+ r_k\big( b\sin(x)w_k\big)
						+ s_k\big( -a\cos(x)w_k \big)
		\Big].
	\end{aligned}
\end{equation}
Using the relations
\begin{equation}
	\ad(q_k)X_k=-q_2+\cos(x)q_0.
\end{equation}
and
\begin{subequations}
	\begin{align}
		\ad(E)q_0&=-w_1p_1+w_2J_1-\sum_{k=1}^lw_kp_k\\
		\ad(E)q_1&=p_1-w_2s_1-\sum_{k=3}^lw_kr_k\\
		\ad(E)q_2&=-J_1+w_1s_1+\sum_{k=3}^ls_k\\
		\ad(E)q_k&=p_k+w_1r_k-w_2s_k,
	\end{align}
\end{subequations}
we find
\begin{equation}
	\ad(E_k)X_{k'}=\begin{cases}
		c_{k'}w_k\sin(x)	&	\text{if $k\neq k'$}\\
		-c_kw_k\cos(q_0)-c_kw_kq_2	&	 \text{if $k=k'$}
	\end{cases}
\end{equation}
Putting all together, we find
\begin{equation}
	\begin{aligned}[]
		\ad(E)X	& = J_1\big( a\cos(x)+aw_2 \big)\\
			&\quad +  p_1\big( -b\sin(x)-aw_1 \big)\\
			&\quad+  s_1\big( -a\cos(x)w_1+bw_2\sin(x) \big)\\
			&\quad  +  q_0\big( -bw_1\cos(x)+aw_2\sin(x)+\sum_{k}c_kw_k\cos(x) \big)\\
			&\quad +  q_1\big( -bw_2-b\cos(x) \big)\\
			&\quad +  q_2\big( bw_1+a\sin(x)-\sum_kc_kw_k \big)\\
			&\quad +  \sum_kq_kc_k\big( w_2+\cos(x) \big)\\
			&\quad +  \sum_kp_k\big( c_k\sin(x)-aw_k \big)\\
			&\quad +  \sum_k r_k\big( c_kw_1+bw_k \big)\sin(x)\\
			&\quad +  \sum s_k\big( -c_kw_2\sin(x)-aw_k\cos(x) \big)\\
			&\quad + \sum_{k\geq 3}\sum_{k'>k} r_{kk'}\big( c_kw_{k'}-c_{k'}w_k \big)\sin(x)
	\end{aligned}
\end{equation}
It is quite easy but long to compute $a(E)$, $b(E)$ and $c$ from that expression. The results are
%

\begin{subequations}\label{LONGEqCoefsabcBE}
	\begin{align}
		\frac{ a(E) }{ B(q_0,q_0) }&=M\Big( w_2^2+\cos(x)w_2+\cos^2(x)\Big)	   \label{LONGEqCoeffaE}\\
		\frac{ b(E) }{ B(q_0,q_0) }&=-2M\sin(x)\big( w_2+\cos(x)\big))\\
		\frac{ c }{ B(q_0,q_0) }&=M\sin^2(x)
	\end{align}
\end{subequations}
where $M=\big( a^2-b^2-\sum_kc_k^2 \big)$. The important point to notice is that these expressions only depend on the $w_2$-component of the direction. Notice that $c=0$ if and only if the point $[g]$ belongs to the singularity because $s=0$ is a solution of \eqref{LONGEqprSqbignor} only in the case $[g]\in\hS$.

%
We can avoid the computation of a certain number of terms by exploiting the properties of the decomposition $\sG=\mZ_{\sK}(\sA)\oplus\tilde\sN_2\oplus\tilde\sN_k\oplus\sA$. The dependence in $w_1^2$ of $a(E)$ is given by the term
\begin{equation}
	B\big( \ad(q_1)X,\sigma\ad(q_1)X \big)=B\big( \ad(q_1)^2X,\sigma X \big).
\end{equation}
This is easily computed using the theorems \ref{LONGThoAdSqIouZero} and \ref{LONGThoBaisXXorthoigher}. The result is that the coefficient of $w_1^2$ in $a(E)/B(q_0,q_0)$ is
\begin{equation}
	-\sin^2(x)(a^2-b^2- C^2)
\end{equation}
where $C^2=\sum_{k\geq 3}c_k^2$.

The term which does not depend on $w$ is 
\begin{equation}
	B\big( \ad(q_0)X,\sigma\ad(q_0)X \big)=B\big( \ad(q_0)^2X,\sigma X \big).
\end{equation}
The result is that the independent term in $a(E)/B(q_0,q_0)$ is
\begin{equation}
	\big( 1-2\cos^2(x) \big)(a^2-b^2-C^2).
\end{equation}
In the same way, the coefficient of $w_2^2$ is $B\big( \ad(q_2)^2X,\sigma X \big)$ and we find
\begin{equation}
	-\big( \sin^2(x)+1 \big)(a^2-b^2-C^2).
\end{equation}


\begin{remark}	\label{LONGRemImapoabcE}
	Importance of the coefficients \eqref{LONGEqCoefsabcBE}. If $v\in\hF_l$, there is a direction $E_0$ in $AdS_l$ which escapes the singularity from $v$. Thus the polynomial $a(E_0)s^2+b(E_0)s+c$ has only non positive roots. From the expressions \eqref{LONGEqCoefsabcBE}, we see that the polynomial corresponding to $\iota(v)$ is the same, so that the direction $E_0$ escapes the singularity from $\iota(v)$ as well. This is the main ingredient of the next section.
\end{remark}

%
\section{Description of the horizon}
%
\label{LONGSecHorizonSansMatrices}

%
\subsection{Induction on the dimension}
%

The horizon in $AdS_3$ is already well understood \cite{Keio}. We are not going to discuss it again. We will study how does the causal structure (black hole, free part, horizon) of $AdS_{l}$ includes itself in $AdS_{l+1}$ by the inclusion map\cite{BTZ_horizon}
\begin{equation}
	\iota\colon AdS_l\to AdS_{l+1}.
\end{equation}

We will use the following notations. The symbols $\hS_l$, $BH_l$, $\hF_l$ and $\hH_l$ respectively denote the singularity, the black hole, the free part and the horizon in $AdS_l$. If $D\subset AdS_l$, then $\Int(D)$ and $\Adh(D)$ are the interior and the closure of $D$.

%
\begin{lemma}		\label{LONGLemMemeQueLemQuatre}
	Let $[g]\in\iota(AdS_3)\subset AdS_l$ be outside the singularity. We suppose that there is an open set $\mO$ in $S^1$ of directions escaping the singularity from $[g]$. Then there exists an open set $\mO'$ in $S^{l-2}$ of directions escaping the singularity.
\end{lemma}

\begin{proof}
	The proof is a consideration about the coefficients \eqref{LONGSubEqsabcEBBB}. The hypothesis means that the points
\begin{equation}
	\pi\left( g e^{sE(\theta)} \right)
\end{equation}
do not belong to $\hS$ for $s\geq 0$ when $E(\theta)=q_0+\cos(\theta)q_1+\sin(\theta)q_2$ and $\theta$ belongs to the given open set $\mO\subset\mathopen[ 0 , 2\pi \mathclose]$. If $a(E_0)\neq 0$ for some $E_0\in\mO$, the solutions are given by
\begin{equation}
	s_{\pm}=\frac{ -b\pm\sqrt{b^2-4ac} }{ 2a }.
\end{equation}
In such a direction, there are two values, both outside\footnote{When we say ``outside'' of $\eR^+$, we include the case of complex solutions.} of $\eR^+$, of $s$ such that $[g e^{sE_0}]\in \hS$. By continuity, we can find a neighborhood of $E_0$ in $S^{l-2}$ such that $[g e^{sE}]$ belongs to the singularity only for non positive numbers.

A problem arises when $a(E)=0$ for every direction $E$ in the open set $\mO$. In that case the equation \eqref{LONGEqprSqbignor} has only one solution which is negative by hypothesis. But it could appear that in every neighborhood of $E$, a second solution, positive, appears. If we write $X=\Ad(g^{-1})J_1$, what we have to prove is that the quantity
\begin{equation}
	a(E)=B\big( \ad(E)X,\sigma\ad(E)X \big)
\end{equation}
is not constant when $E$ runs over $\mO$, in particular, there exists a direction $\theta_0\in\mO$ such that $a(\theta_0)\neq 0$. We supposed that $[g]\in \iota(AdS_3)$, so that $X=\Ad(an)J_1$ is given by $X_3$ of equation \eqref{LONGsubEqXtroisdonne}.


The function $a(E)$ \eqref{LONGEqaEdansAdsTrois} is analytic with respect to $\theta$, thus if it vanishes on an open set $\mO$, it has to vanish everywhere. This can only be achieved with $a=\pm b$. Now, simple computation show that
\begin{equation}
	c=a^2-b^2\sin^2(x)-a^2\cos^2(x)=(a^2-b^2)\sin^2(x)
\end{equation}
which vanishes when $a=\pm b$, so that $a(E)$ can only be constant with respect to $E$ on the singularity. Thus we conclude that $a(E)$ is not constant with respect to $E\in S^1$ outside the singularity.

This concludes the proof of lemma \ref{LONGLemMemeQueLemQuatre}.
\end{proof}
%

\begin{lemma}
	The direction $E_0$ in $AdS_l$ escapes the singularity from $v\in AdS_l$ if and only if it escapes the singularity from $\iota(v)$ in $AdS_{l+1}$.
\end{lemma}

\begin{proof}
	The fact for $v$ to escape the singularity in the direction $E_0$ means that the equation
	\begin{equation}
		a_{v}(E_0)s^2+b_{v}(E_0)+c_{v}=0
	\end{equation}
	where the coefficients are given by \eqref{LONGEqCoefsabcBE} has no positive solutions with respect to $s$. Since these coefficients are the same for $v$ and $\iota(v)$, the equation for $\iota(v)$ is in fact the same and has the same solutions.
\end{proof}

%
As a warm up, let us prove the following, which is a particular case of lemma \ref{LONGLemDueiINtlIntlpu}.
\begin{lemma}
	Let $[g]\in AdS_l$ be such that there exists an open set $\mO\in S^1$ of directions that escape the singularity. Then there is an open set in $S^{l-1}$ that escapes the singularity from $i[g]\in AdS_{l+1}$.
\end{lemma}

\begin{proof}
	With the notations of section \ref{LONGSecMoreComputations}, we only have to compute $a(E)$ when $E=E_2$ and $X$ is general. So we pick the expression \eqref{LONGEqCoeffaE} and we put $w_2=\cos(\theta)$ while $w_k=0$ for every $k\geq 3$. What we have is
	\begin{equation}
		a(E)=M\Big( \cos^2(\theta)+\cos(x)\cos(\theta)+\cos^2(x) \Big)
	\end{equation}
	If $a(E)=0$ for every $\theta\in\mO$, then $M=0$ which is impossible since we suppose that the starting point does not belong to the singularity.
\end{proof}

\begin{lemma}		\label{LONGLemDueiINtlIntlpu}
	We have
	\begin{equation}
		\iota\big( \Int(\hF_l) \big)\subset\Int(\hF_{l+1})
	\end{equation}
	or, equivalently, 
	\begin{equation}
		\Adh(BH_{l+1})\cap\iota(AdS_l)\subset\iota\big( \Adh(BH_l) \big).
	\end{equation}
\end{lemma}

\begin{proof}
	Let $v'\in\Int(\hF_l)$ and $\mO$, an open set of directions in $AdS_l$ that escape the singularity. The coefficient $a_l(E)$ is not constant on $\mO$ because the coefficient $M=a^2-b^2-C^2$ is only zero on the singularity (see equation \eqref{LONGEqCoeffaE}). Thus we can choose $E_0\in\mO$ such that $a_l(E_0)\neq 0$. We consider $a_{l+1}(E_0)$, the coefficient of $s^2$ for the point $\iota(v')$ in the direction $E_0$. From the expression \eqref{LONGEqCoeffaE} we know that $a_{l+1}(E_0)=a_l(E_0)$. The coefficients $b(E_0)$ and $c$ are also the same for $v'$ and $\iota(v')$. 
	
	Since $a(E_0)\neq 0$ and $v'\in\Int(\hF_l)$, we have two solutions to the equation $a(E_0)s^2+b(E_0)s+c=0$ and both of these are outside $\eR^+_0$. This conclusion is valid for $v'\in AdS_l$ as well as for $\iota(v')\in AdS_{l+1}$. Then there is a neighborhood of $\iota(v')$ on which the two solutions keep outside $\eR^+_0$. That proves that $\iota(v')\in\Int(\hF_{l+1})$.

	For the second line, suppose that $v\in\iota(AdS_l)$ does not belong to $\iota\big( \Adh(BH_l) \big)$, thus $v\in\iota\big( \Int(\hF_l) \big)\subset\Int(\hF_{l+1})$. In that case $v$ does not belong to $\Adh(BH_{l+1})$.

\end{proof}

\begin{proposition}		\label{LONGProphFdanshF}
	We have
	\begin{equation}
		\hF_{l+1}\cap\iota(AdS_l)\subset\iota(\hF_l)
	\end{equation}
\end{proposition}

\begin{proof}
	If $v=\iota(v')\in\hF_{l+1}$, there is a direction $E_0$ in $AdS_{l+1}$ which escape the singularity from $v$. That direction is given by a vector $(w_1,\cdots v_l)\in S^{l}$. Since the coefficients $a(E)$, $b(E)$ and $c$ do only depend on $w_2$, a direction $(w'_1,\cdots,w'_{l-1},0)$ with $w'_2=w_2$ escapes the singularity from $v'$. This proves that $v'\in\hF_l$.
\end{proof}

\begin{lemma}		\label{LONGLemHiH}
	We have
	\begin{equation}
		\hH_{l+1}\cap\iota(AdS_l)\subset\iota(\hH_{l}).
	\end{equation}
\end{lemma}

\begin{proof}
	First,
	\begin{equation}
		v\in\hH_{l+1}\cap\iota(AdS_l)\subset\hF_{l+1}\cap\iota(AdS_l)\subset\iota(\hF_l)
	\end{equation}
	from proposition \ref{LONGProphFdanshF}. Now, let's take $v'\in\hF_l$ such that $v=\iota(v')$. We have to prove that $v'\in\hH_l$. Let us suppose that $v'\in\Int(\hF_l)$, so $v\in\Int(\hF_{l+1})$ because of lemma \ref{LONGLemDueiINtlIntlpu}. This is in contradiction with the fact that $v\in\hH_{l+1}$.
\end{proof}

\begin{corollary}		\label{LONGCorDeuxTrucsBHhH}
	We have
	\begin{multicols}{2}
		\begin{enumerate}

		\item
			$\iota(\hS_l)\subset\hS_{l+1}$,
		\item
			$\iota(\hF_l)\subset\hF_{l+1}$,
		\item
			$\iota(BH_l)\subset BH_{l+1}$,
		\item
			$\iota(\hH_l)\subset \hH_{l+1}$.

		\end{enumerate}
	\end{multicols}
\end{corollary}

\begin{proof}
	We have $\Ad\big( \iota(g^{-1}) \big)J_1=\Ad(g^{-1})J_1$, so that the condition of theorem \eqref{LONGThosSequivJzero} is invariant under $\iota$. Thus one immediately has $\iota(\hS_l)\subset\hS_{l+1}$ and $\iota(\hF_l)\subset\hF_{l+1}$.

	An element $v$ which does not belong to $BH_{l+1}$ belongs to $\hF_{l+1}$, but if $v$ belongs to $\iota(AdS_l)\cap\hF_{l+1}$, it belongs to $\iota(\hF_l)$ by proposition \ref{LONGProphFdanshF} and then does not belong to $\iota(BH_l)$. Thus $\iota(BH_{l})\subset BH_{l+1}$.

	Now if $v'\in\hH_l$, let us consider $\mO$, a neighborhood of $v=\iota(v')$ in $AdS_{l+1}$. The set $\iota^{-1}\big( \mO\cap\iota(AdS_l) \big)$ contains a neighborhood $\mO'$ of $v'$ in $AdS_{l}$. Since $v'\in\hH_l$, there is $\bar v\in\mO'$ such that $\bar v\in BH_l$. Thus $\iota(\bar v)\in\mO$ belongs to $BH_{l+1}$ by the first item.

\end{proof}

Let $X\in\sG_{l+1}$ such that $[X,J_1]=0$, and let $R$ be the group generated by $X$. The following results are intended to show that such a group can be used in order to transport the causal structure from $AdS_l$ to $AdS_{l+1}$

The key ingredient will be the fact that, since $X$ commutes with $J_1$, we have
\begin{equation}		\label{LONGEqAdOkSurJun}
	\Ad\big( (g e^{sE})^{-1} \big)J_1=\Ad\big( ( e^{\alpha X}g e^{sE})^{-1} \big)J_1.
\end{equation}

\begin{lemma}		\label{LONGLemRSsubsetS}
	A group $R$ as described above preserves the causal structure in the sense that
	\begin{multicols}{2}
	\begin{enumerate}

		\item
			$R\cdot\hS\subset\hS$
		\item
			$R\cdot BH\subset BH$
		\item
			$R\cdot \hF\subset\hF$
		\item
			$R\cdot \hH\subset\hH$.

	\end{enumerate}
	\end{multicols}
\end{lemma}

\begin{proof}
	From equation \eqref{LONGEqAdOkSurJun}, we deduce that a direction $E_0$ will escape the singularity from the point $[g]$ is and only if it escapes the singularity from the points $r[g]$ for every $r\in R$. The first three points follow.

	Now let $v\in\hH$ and $r\in R$ and let us prove that $r\cdot v\in BH$. By the third point, $r\cdot v\in\hF$. Let now $\mO$ be a neighborhood of $r\cdot v$. The set $r^{-1}\cdot \mO$ is a neighborhood of $v$ and we can consider $\bar v\in BH\cap r^{-1}\cdot\mO$. By the second point, $r\cdot \bar v$ is a point of the black hole in $\mO$.
\end{proof}

\begin{remark}		\label{LONGRemdqnqRSlsubsetSlpu}
	Combining corollary \ref{LONGCorDeuxTrucsBHhH} and lemma \ref{LONGLemRSsubsetS} we have
	\begin{multicols}{2}
	\begin{enumerate}
		\item
			$ R\cdot\iota(\hS_l)\subset\hS_{l+1}$
		\item
			$ R\cdot\iota(\hF_l)\subset\hF_{l+1}$
		\item
			$ R\cdot\iota(BH_{l})\subset BH_{l+1}$
		\item
			$ R\cdot\iota(\hH_{l})\subset\hH_{l+1}$
	\end{enumerate}
	\end{multicols}
    where the dot stands for the action.
\end{remark}


\begin{theorem}		\label{LONGThoCausalPasseParR}
	If moreover the one parameter group $R$ has the property to generate $AdS_{l+1}$ (in the sense that $R\cdot\iota(AdS_l)=AdS_{l+1}$), then we have
	\begin{multicols}{2}
	\begin{enumerate}
		\item
			$ R\cdot\iota(\hS_l)=\hS_{l+1}$
		\item
			$ R\cdot\iota(\hF_l)=\hF_{l+1}$
		\item\label{LONGItemStrucalpb}
			$ R\cdot \iota(BH_{l})=BH_{l+1}$
		\item
			$ R\cdot\iota(\hH_{l})=\hH_{l+1}$.
	\end{enumerate}
	\end{multicols}
\end{theorem}

\begin{proof}
	The inclusions in the direct sense are already done in the remark \ref{LONGRemdqnqRSlsubsetSlpu}.

	Let $r= e^{\alpha X}$ be an element of $R$. Since, by assumption, we have $[X,J_1]=0$, the action of $r$ leaves invariant the condition of theorem \eqref{LONGThosSequivJzero}:
	\begin{equation}		\label{LONGEqalpharjnagitpas}
		\Ad\big( (g e^{sE})^{-1} \big)J_1=\Ad\big( ( e^{\alpha X}g e^{sE})^{-1} \big)J_1.
	\end{equation}
	\begin{enumerate}
		\item
			Let $[g]\in\hS_{l+1}$, there exists a $r\in R$ such that $r[g]\in \iota(AdS_l)$. There exists an element $g'\in \SO(2,l-1)$ such that $rg=\iota(g')$. Now $[g']\in\hS_l$ because
			\begin{equation}
				\Ad(g'^{-1})J_1=\Ad(\iota(g'^{-1}))J_1=\Ad\big(  (rg)^{-1} \big)J_1=\Ad(g^{-1})J_1,
			\end{equation}
			but by assumption the norm of the projection on $\sQ$ of the right hand side is zero.

		\item
			If $v$ is free in $AdS_{l+1}$, there is a direction $E_0$ escaping the singularity from $v$ and an element $r\in R$ such that $v'=r\cdot v\in\iota(AdS_l)$. The point $v'$ is also free in $AdS_{l+1}$ as the direction $E_0$ works for $r\cdot v$ as well as for $v$. Thus by proposition \ref{LONGProphFdanshF} we have
			\begin{equation}
				v'\in\hF_{l+1}\cap\iota(AdS_l)\subset\iota(\hF_l)
			\end{equation}
			and $v\in R\cdot\iota(\hF_l)$.
		\item
			If $v\in BH_{l+1}$, the point $r\cdot v\in\iota(AdS_l)$ also belongs to $BH_{l+1}$. If $r\cdot v=\iota(v')$, then $v'\in BH_l$ from if $v'\in\hF_l$, then $\iota(v')\in\hF_{l+1}$.
		\item
			If $v\in\hH_{l+1}$, there exists a $r\in R$ such that $v'=r\cdot v\in\iota(AdS_l)$ and, moreover, $v'$ belongs to the horizon in $AdS_{l+1}$ since the horizon is invariant under $R$. Thus $v'$ belongs to $\iota(AdS_l)\cap\hH_{l+1}\subset \iota(\hH_l)$ by lemma \ref{LONGLemHiH}.  Now, $v\in R\cdot\iota(\hH_l)$.

	\end{enumerate}
\end{proof}

%
\subsection{Examples of surjective groups}
%

Theorem \ref{LONGThoCausalPasseParR} describes the causal structure in $AdS_l$ by induction on the dimension provided that one knows a group $R$ such that $AdS_{l+1}=R\cdot \iota(AdS_l)$. Can one provide examples of such groups? The following proposition provides a one.

\begin{proposition}		\label{LONGPropSurjectif}
	If $R$ is the one parameter subgroup of $\SO(2,l)$ generated by $r_{l,l+1}$, then we have
	\begin{equation}
		R\cdot \iota(AdS_l)= AdS_{l+1}.
	\end{equation}
\end{proposition}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{proof}
	If one realises $AdS_n$ as the set of vectors of length $1$ in $\eR^{2,n-1}$, $AdS_l$ is included in $AdS_{l+1}$ as the set of vectors with vanishing last component and the element $r_{l,l+1}$ is the rotation in the plane of the two last coordinates. In that case, we have to solve
	\begin{equation}
		e^{\alpha r_{l,l+1}}\begin{pmatrix}
			u'	\\ 
			t'	\\ 
			x'_1	\\ 
			\vdots	\\ 
			x'_{l-2}	\\ 
			0	
		\end{pmatrix}=
		\begin{pmatrix}
			u	\\ 
			t	\\ 
			x_1	\\ 
			\vdots	\\ 
			x_{l-2}	\\ 
			x_{l-1}	
		\end{pmatrix}
	\end{equation}
	with respect to $\alpha$, $u'$, $t'$ and $x'_i$. There are of course exactly two solution if $x_{l-2}^2+x_{l-1}^2\neq 0$.
\end{proof}

In fact, many others groups are available, as the one showed at the end of \cite{BTZ_horizon}. In fact, since, in the embedding of $AdS$ in $\eR^{2,n}$, the singularity is given by $t^2-y^2=0$, almost every group which leaves invariant the combination $t^2-y^2$ can be used to propagate the causal structure. One can found lot of them for example by looking at the matrices given in \cite{These}.

%
\subsection{Backward induction}
%

Using proposition \ref{LONGProphFdanshF}, lemma \ref{LONGLemHiH}, corollary \ref{LONGCorDeuxTrucsBHhH} and the fact that the norm of $J_1^*$ is the same in $AdS_l$ as in $\iota(AdS_l)\subset AdS_{l+1}$, we have
\begin{enumerate}
	\item
		$\iota(\hF_l)=\hF_{l+1}\cap\iota(AdS_l)$,
	\item
		$\iota(\hH_l)=\hH_{l+1}\cap\iota(AdS_l)$,
	\item 
		$\iota(\hS_l)=\hS_{l+1}\cap\iota(AdS_l)$.
\end{enumerate}
These equalities hold for $l\geq 3$. For $l=2$ we can take the latter as a definition and set
\begin{equation}
	\hS_2=\{ v\in AdS_2\tq \iota(v)\in\hS_3 \}.
\end{equation}
The Iwasawa decomposition of $\SO(2,1)$ is given by $\sA=\langle J_2\rangle$, $\sN=\langle X_+\rangle$, $\sK=\langle q_0\rangle$ where $X_+=p_1-q_0$. Notice that we \emph{do not} have $\iota(X_+)=X_{++}$. Instead we have $\iota(X_+)=\frac{ 1 }{2}(X_{++}+X_{-+})$. Thus $\hS_2$ is not given by the closed orbits of $AN$ in $AdS_2$.

The light-like directions are given by the two vectors $E=q_0\pm q_1$. In order to determine if the point $[e^{-\alpha J_2}e^{-aX_+} e^{-xq_0}]$ belongs to the black hole, we follow the same way as in section \ref{LONGSubSecExistenceTrouNoir}: we compute the norm
\begin{equation}
	\big\| \pr_{\sQ}  e^{-s\ad(E)}e^{x\ad(q_0)} e^{a\ad(X_+)} e^{\alpha\ad(J_2)}J_1 \big\|
\end{equation}
and we see under which conditions it vanishes. 
%
\begin{equation}
	e^{a\ad(X_+)}J_1=J_1+a(q_2+s_1),
\end{equation}
and then
\begin{equation}
	\begin{aligned}[]
		X&= e^{x\ad(q_0)}\big( J_1+a(q_2+s_1) \big)\\
		&=J_1\big( \cos(x)-a\sin(x) \big)+q_2\big( \sin(x)+a\cos(x) \big)+as_1.
	\end{aligned}
\end{equation}
With $E=q_0+q_1$, we have
\begin{equation}
	\begin{aligned}[]
		e^{s\ad(E)}J_1&=\big( -\frac{ 1 }{2}s^2+1 \big)J_1+sq_2+\frac{ 1 }{2}s^2s_1\\
		e^{s\ad(E)}q_2&=-sJ_1+q_2+ss_1\\
		e^{s\ad(E)}s_1&=-\frac{ 1 }{2}s^2J_1+sq_2+(\frac{ 1 }{2}s^2+1)s_1.
	\end{aligned}
\end{equation}
Thus
\begin{equation}
	\pr_{\sQ} e^{s\ad(E)}X=\Big( \big( \cos(x)-a\sin(x)+a \big)s+(\sin(x)+a\cos(x)) \Big)q_2.
\end{equation}
Its norm vanishes for the value of $s$ given by
\begin{equation}
	s^+=\frac{ a\cos(x)+\sin(x) }{ \big( \sin(x)-1 \big)a-\cos(x) }.
\end{equation}
The same computation with $E=q_0-q_1$ provides the value
\begin{equation}
	s^-=\frac{ a\cos(x)+\sin(x) }{ \big( \sin(x)+1 \big)a-\cos(x) }.
\end{equation}

For small enough $a$, the signs of $s^+$ and $s^-$ are both given the sign of $-\tan(x)$ that can be either positive or negative. Thus there is an open set of points in $AdS_2$ which intersect the singularity in every direction and an open set of points which escape the singularity.

As a side note, the singularity $\hS_2$ described here is not given by the closed orbits of $AN$ or $A\bar N$. Indeed, we show that
\begin{equation}
	\| \pr_{\sQ}\Ad( e^{a\iota(X_+)})J_1 \|^2=\| \pr_{\sQ}(J_1+aq_2+as_1) \|=-4a^2\neq 0.
\end{equation}
Thus the points of $N$ are not part of the singularity.


\section{Dirac operator on \texorpdfstring{$AdS_2$}{AdS2}}
%-------------------------------------

Why to compute Dirac operator on anti de Sitter spaces ? Let $M=AdS_2$ and $R=AN$ acts on $M$. Let $\mO$ be an open orbit of $R\times M\to M$. In the specific case of $AdS_2$, we have $R=\mO=R\cdot\mfo$. In larger dimensions, there is a $\SO(1,n)$ which causes that the orbit is not exactly the acting group. It is
\[ 
  \mO=\frac{ R }{ R\cap \SO(1,n) }.
\]

\subsection{Clifford algebra and spin group}
%/////////////////////////////////////


As definition, we retain
\begin{equation}
\begin{split}
  AdS_2&\equiv t^2+u^2-x^2=1\\
    &=\frac{ \SO(2,1) }{ \SO(1,1) }.
\end{split}
\end{equation}
Let $V=\eR^{1,1}$ and $e_0$, $e_1$ an orthonormal basis. We pose 
\begin{align*}
  f_0=\frac{ 1 }{2}(e_0+e_1)\quad g_0=\frac{ 1 }{2}(e_0-e_1)
\end{align*}
and we define $\tilde\rho$ by
\begin{subequations}
\begin{align}
  \tilde\rho(f_0)\alpha&=f_0\wedge\alpha\\
    \tilde\rho(g_0)\alpha&=-i(g_0)\alpha
\end{align}
\end{subequations}
where $\alpha\in\Lambda W$, $W$ being the space spanned by $f_0$. More explicitly we have :
\begin{subequations}
\begin{align}
  \tilde\rho (f_0)1&=f_0&\tilde\rho (f_0)f_0&=0\\
\tilde\rho (g_0)1&=0&\tilde\rho (g_0)f_0&=-\eta(f_0,g_0).
\end{align}
\end{subequations}
As element of $\Lambda W$, $f_0$ stands for $\eta(f_0,.)$. If we choose the basis
\[ 
  1=
\begin{pmatrix}
1\\0
\end{pmatrix},
\quad
f_0=
\begin{pmatrix}
0\\1
\end{pmatrix},
\]
 the matrices of $\tilde\rho$ are given by
\[ 
  \tilde\rho(e_0)=
\begin{pmatrix}
0&-1/2\\
1&0
\end{pmatrix},
\quad
\tilde\rho(e_1)=
\begin{pmatrix}
0&1/2\\
1&0
\end{pmatrix}.
\]
Up to a change of basis,
\[ 
  \gamma_0=
\begin{pmatrix}
0&1\\1&0
\end{pmatrix},
\quad
\gamma_1=
\begin{pmatrix}
0&-1\\1&0
\end{pmatrix}
\quad
\gamma_0\gamma_1=
\begin{pmatrix}
1&0\\0&-1
\end{pmatrix},
\]
and a general element of $Cl_{(1,1)}$ reads
\[ 
  x\gamma_0+y\gamma_1+u\eR+v\gamma_0\gamma_1=
\begin{pmatrix}
u+v&x-y\\
x+y&u-v
\end{pmatrix}.
\]
With the change of basis $e_1\to ie_1$, we write it under a more simple form :
\begin{equation}
Cl_{(1,1)}\leadsto 
\begin{pmatrix}
\alpha&\beta\\
\bar\beta&\bar\alpha
\end{pmatrix}
\end{equation}
with $\alpha,\beta\in\eC$. In particular, an element of $V$, i.e. a combination of $\gamma_0$ and $\gamma_1$ is
\begin{equation}
V\leadsto
\begin{pmatrix}
0&\xi\\
\overline{\xi}&0
\end{pmatrix}.
\end{equation}
Let
\[ 
  1=
\begin{pmatrix}
1&0\\0&1
\end{pmatrix},
\quad
a=
\begin{pmatrix}
i&0\\0&-i
\end{pmatrix},
\quad b=
\begin{pmatrix}
0&1\\1&0
\end{pmatrix},
\quad
c=
\begin{pmatrix}
0&i\\-i&0
\end{pmatrix}
\]
Let us now determine $\alpha$, the extension of $-\id|_V$ into an automorphism and $\tau$, the extension of $\id|_V$ into an anti-automorphism. We have $\alpha(b)=-b$, $\alpha(c)=-c$, $\tau(b)=b$ and $\tau(c)=c$. We find the others by virtue of relations $bc=-a$ and $b^2=1$. Finally
\begin{equation}
\begin{aligned}
  \alpha(1)&=1&\alpha(a)&=a\\
\alpha(b)&=-b&\alpha(c)&=-c
\end{aligned}
\end{equation}
and
\begin{equation}
\begin{aligned}
  \tau(1)&=1&\tau(a)&=-a\\
\tau(b)&=b&\tau(c)&=c.
\end{aligned}
\end{equation}

The condition for $s\in Cl_{(1,1)}$ to belongs to $\Gamma_{(1,1)}$ is that $\alpha(s)v s^{-1}\in V$ for all $v\in V$. If we consider $s=
\begin{pmatrix}
\alpha&\beta\\\bar\beta&\bar\alpha
\end{pmatrix}$,
we have
\[ 
  \alpha(s)=
\begin{pmatrix}
\alpha&-\beta\\-\bar\beta&\bar\alpha
\end{pmatrix},
\text{ and }
s^{-1}=\frac{1}{ | \alpha |^2-| \beta |^2 }
\begin{pmatrix}
\bar\alpha&-\beta\\-\bar\beta&\alpha
\end{pmatrix}.
\]
If we impose $\alpha(s)v s^{-1}$ to be of the form $\begin{pmatrix}
0&\eta\\\bar\eta&0
\end{pmatrix}$ for all $v$ of the form $\begin{pmatrix}
0&\xi\\\bar\xi&0
\end{pmatrix}$, we find $\Reel(\bar\alpha\beta\bar\xi)=0$ and the $\bar\alpha\beta=0$. So generators of $\Gamma_{(1,1)}$ are
\begin{equation}
\Gamma_{(1,1)}\leadsto
\begin{pmatrix}
\alpha&0\\0&\bar\alpha
\end{pmatrix},\quad
\begin{pmatrix}
0&\beta\\\bar\beta&0
\end{pmatrix}.
\end{equation}
Elements of $Spin_{(1,1)}$ are elements of $\Gamma_{(1,1)}^+$ such that $\tau(s)=s^{-1}$. So
\begin{equation}
Spin_{(1,1)}\leadsto
\begin{pmatrix}
\alpha&0\\0&\bar\alpha
\end{pmatrix},\text{ such that }| \alpha |^2=1.
\end{equation}
We recognize $Spin_{(1,1)}=U(1)$.

\begin{probleme}
This is wrong: in fact $\Spin(1,1)\neq U(1)$.
\end{probleme}

\subsection{Relation between \texorpdfstring{$SU(1,1)$}{SU(1,1)} and \texorpdfstring{$\SO(2,1)$}{SO(2,1)}}
%//////////////////////////////////

A general matrix of $SU(1,1)$ is
\[ 
  U=\begin{pmatrix}
\alpha&\beta\\\bar\beta&\bar\alpha
\end{pmatrix},
\quad
U^{-1}=\begin{pmatrix}
\bar\alpha&-\beta\\-\bar\beta&\alpha
\end{pmatrix}
\]
with $| \alpha |^2 - | \beta |^2=1$. They are matrices which fulfil $\det U=1$ and $U^+g=gU^{-1}$.  If we denote by $V$ the space of matrices of the form $(r,z)=\begin{pmatrix}
r&\bar z\\z&r
\end{pmatrix}$ with $r\in\eR$ and $z\in\eC$, we have a bijection $\psi\colon \eR^{1,1}\to V$ given by
\[ 
  \begin{pmatrix}
u\\t\\x
\end{pmatrix}\mapsto
\begin{pmatrix}
x&t-iu\\ t+iu&x
\end{pmatrix}.  
\]
It becomes an isometry if we pose $\| (r,z) \|=z\bar z-r^2=-\det(r,z)$. The group $SU(1,1)$ has an isometric action on $V$ given by
\[ 
  Uv=UvU^{\dag}.
\]
We immediately remark that $Uv=(-U)v$. We define
\begin{equation}
\begin{aligned}
 T\colon SU(1,1)&\to \SO(2,1) \\ 
T(U)\begin{pmatrix}
u\\t\\x
\end{pmatrix}&= \psi^{-1}\big( U\psi\begin{pmatrix}
u\\t\\x
\end{pmatrix}U^{\dag} \big). 
\end{aligned}
\end{equation}

Now we want to know when $T(U)=T(\tilde U)$. Using the fact that $U^{-1}=gU^{\dag}g$ in the condition $UvU^{\dag}=\tilde Uv\tilde U^{\dag}$, we find
\[ 
  VvV^{\dag}=v
\]
with $V=\tilde U^{-1}U$. Then imposing
\[ 
  \begin{pmatrix}
r&\bar z\\z&r
\end{pmatrix}=
\begin{pmatrix}
\alpha&\beta\\\bar\beta&\bar\alpha
\end{pmatrix}
\begin{pmatrix}
r&\bar z\\z&r
\end{pmatrix}
\begin{pmatrix}
\bar \alpha&\beta\\\bar\beta&\alpha
\end{pmatrix},
\]
we find $T(U)=T(\tilde U)\Leftrightarrow \tilde U=\pm U$. We have
\[ 
  T\begin{pmatrix}
i\\&-i
\end{pmatrix}
=
\begin{pmatrix}
-1\\&-1\\&&1
\end{pmatrix}.
\]
The map $T\colon SU(1,1)\to \SO(2,1)$ is a double covering.

We are now going to explicitly compute the map $T$. First :
\[ 
 \begin{split}
\begin{pmatrix}
\alpha&\beta\\
 \bar\beta&\bar \alpha
\end{pmatrix}
\begin{pmatrix}
r&\bar z\\
z&r
\end{pmatrix}
&
\begin{pmatrix}
\bar\alpha&\beta\\
\bar\beta&\alpha
\end{pmatrix}=\\
&\begin{pmatrix}
\bar\alpha(\alpha r+\beta z)+\bar\beta(\alpha\bar z+\beta r)& \beta(\alpha r+\beta z)+\alpha(\alpha\bar z+\beta r)\\
\bar\alpha(\bar\beta+\bar\alpha z)+\bar\beta(\bar\beta\bar z+\bar\alpha r)&\beta(\bar\beta r+\bar\alpha z)+\alpha(\bar\beta\bar z+\bar\alpha r)
\end{pmatrix}.
\end{split} 
\]
When we pose $z=0$ and $r=1$, i.e., when we look at $\begin{pmatrix}
0\\0\\1
\end{pmatrix}$, we find
\[ 
  \begin{pmatrix}
\bar\alpha\alpha+\bar\beta\beta&2\beta\alpha\\
2\bar\alpha\bar\beta& \beta\bar\beta+\alpha\bar\alpha
\end{pmatrix}
\]
which corresponds to $x=\bar\alpha\alpha$, $t-iu=2\alpha\beta$ and $t+iu=2\bar\alpha\bar\beta$. We conclude that
\[ 
  T\begin{pmatrix}
\alpha&\beta\\\bar\beta&\bar\alpha
\end{pmatrix}
=
\begin{pmatrix}
.&.&i(\alpha\beta-\bar\alpha\bar\beta)\\
.&.&\alpha\beta+\bar\alpha\bar\beta\\
.&.&\alpha\bar\alpha+\beta\bar\beta
\end{pmatrix}.
\]
Similar computations lead to
\begin{equation}
T\begin{pmatrix}
\alpha&\beta\\
\bar\beta&\bar\alpha
\end{pmatrix}
=
\begin{pmatrix}
\frac{ \bar\alpha^2+\alpha^2-\beta^2-\bar\beta^2 }{2} &     \frac{ i }{2}(\alpha^2-\bar\alpha^2+\beta^2-\bar\beta^2) &  i(\alpha\beta-\bar\alpha\bar\beta)\\
\frac{ i }{2}(\beta^2-\bar\beta^2+\bar\alpha^2-\alpha^2) &  \frac{ 1 }{2}(\alpha^2+\bar\alpha^2+\beta^2+\bar\beta^2) &  \alpha\beta+\bar\alpha\bar\beta\\
i(\bar\alpha\beta-\bar\beta\alpha)          &   \bar\alpha\beta+\bar\beta\alpha             &   \alpha\bar\alpha+\beta\bar\beta
\end{pmatrix}
\end{equation}

\subsection{Spin structure on \texorpdfstring{$AdS_2$}{AdS2}}
%/////////////////////////////////////////

We are going to build elements of the following spin structure:
\[
\xymatrix{ \Spin(1,1) \ar@{~>}[r]& SU{(1,1)} \ar[rr]^{\displaystyle\varphi} \ar[rd]_{\displaystyle\pi} && \SO(AdS_2) \ar[ld]^{\displaystyle p}&\SO(2,1) \ar@{~>}[l]  \\& & AdS_2 }
\]
First let $\{ e_t,e_u,e_x \}$ be a basis of $\eR^{1,1}$ with $e_t\in AdS_2$, $e_t\cdot e_t=e_u\cdot e_u=-e_x\cdot e_x=-1$. We suppose that $e_u$ and $e_x$ span tangent space at $e_t$. Let $T$ be a representation of $\SO(1,1)$ on $\eR^{2,1}$  which leaves $e_t$ unchanged: $T(A)e_t=e_t$ for all $A\in \SO(1,1)$. To each element $B\in \SO(AdS_2)$, one can associate an element of $B'\in \SO(AdS_2)$ such that $B$ has the form
\begin{equation} \label{eq_blaseAdS}
  B=\{ B'e_u,B'e_x \}_{B'e_t}.
\end{equation}
We define
\[ 
  p(B)=B'e_t.
\]
Now the action of $A\in \SO(1,1)$ on $B\in \SO(AdS_2)$ is defined, if $B$ has the form \eqref{eq_blaseAdS}, by
\begin{equation}
  B\cdot A=\{ T(A)B'e_u,T(A)B'e_x \}_{B'e_t}.
\end{equation}
The map $\varphi\colon SU(1,1)\to \SO(AdS_2)$ is given by
\begin{equation}
\big( \varphi(U) \big)'=(T\circ S)(U),
\end{equation}
and the projection $\pi\colon SU(1,1)\to AdS_2$, 
  $\pi=p\circ\varphi$.

The group $\Spin(1,1)$ must act on $SU(1,1)$; we define
\begin{equation}
U\cdot s=\varphi^{-1}\big( \varphi(U)\cdot \chi(s) \big).
\end{equation}
We have $\pi(U\cdot s)=\pi(U)$ because
\[ 
  \pi(U\cdot s)=p\big( \varphi(U)\cdot\chi(s) \big),
        =\big[ \varphi(U)\cdot \chi(s) \big]'e_t
        =\varphi(U)\circ T(\chi(s))e_t
        =\varphi(U)'e_t
        =\pi(U).
\]
We have used the fact that  $\chi(s)\in \SO(1,1)$ and that, therefore, $(T\circ\chi)(s)e_t=e_t$.


\subsection{Spinor bundle and connection}
%///////////////////////////////////////

We define $S=\Lambda W$ where $W$ is the (one dimensional) space spanned by $f_0$ and we define 
\begin{equation} \label{eq_mSSUrho}
  \mS= SU(1,1)\times_{\rho}S
\end{equation}
where $\rho\colon Spin_{(1,1)}\times\Lambda W\to \Lambda W$ is the representation of $Spin_{(1,1)}$ on $SU(1,1)$ given by
\begin{equation}
  \rho(s,\alpha)=\tilde\rho(s)\alpha.
\end{equation}
Recall that $\alpha$ is either a scalar either a multiple of $f_0$. The equivalence relation which arises in equation \eqref{eq_mSSUrho} is
\begin{equation}
  (U,\alpha)\sim(U\cdot s,\rho(s^{-1})\alpha).
\end{equation}
The projection is
\[ 
  \pi_{\mS}[(U,\alpha)]=\pi(U).
\]

For the connection on $\SO(AdS_2)$, we want that horizontal vector are tangent vectors to curves formed by parallel transport. In other word, a path 
\[
B(s)=\{ B'(s)e_u,B'(s)e_x \}_{B'(s)e_t}
\]
 has horizontal tangent vector if $B'(s)e_i$ ($i=u,x$) is a parallel transport of $B'(0)e_i$ along the curve $B'(s)e_e$ on $AdS_2$. Here, $B'(s)$ denotes the matrix of $\SO(2,1)$ associated with the basis $B(s)$ : the prime doesn't denotes a derivation. Let us define the $so(1,1)$ valued connection $1$-form which corresponds to this intuition. We consider $b_i(s)$ the parallel transported along the curve $B'(s)e_t$ of $B'(0)e_i$, and $A(s)$, the matrix of $\SO(1,1)$ such that $A(s)B'(s)e_i=b_i(s)$ ($i=u,x$). The definition is
\[ 
  \omega(\dot B)=\Dsdd{ A(s) }{s}{0}.
\]


\begin{proposition}
It is a connection $1$-form.
\end{proposition}

\begin{proof}
First we consider a fundamental vector field
\[ 
  X^*_B=\Dsdd{ B\cdot e^{-tX} }{t}{0}=\Dsdd{ \{ T( e^{-tX})B'e_u,T( e^{-tX})B'e_x \}_{B'e_t} }{t}{0}.
\]
The path in $AdS_2$ on which this path in $\SO(AdS_2)$ is build is constant: it is $B'e_t$. So the parallel transport is constant and the path $A(s)$ is given by
\[ 
  A(s)T( e^{-tX})B'e_u=B'e_u
\]
and $\omega(X^*_B)=X$.

It remains to be proved that for all $B\in \SO(AdS_2)$, $g\in \SO(1,1)$ and $X\in T_B\SO(AdS_2)$,
\begin{equation}
\omega\big( (dR_g)_BX \big)=\Ad(g^{-1})\omega_B(X).
\end{equation}
We give $X$ by the path 
\[ 
  X(s)=\{ B'(s)e_u,B'(s)e_x \}_{B'(s)e_t}.
\]
 The differential $dR_g$ gives rise to the new path
\[ 
  (dR_gX)(s)=\{ gB'(s)e_u,gB'(s)e_x \}_{B'(s)e_t}.
\]
Let $b_i(s)$ be the parallel transport of $B'(0)e_i$ ($i=u,x$) along the path $B'(s)e_t$. We have to compute $\omega_B(X)$ with $A(s)$ defined by $A(s)B'(s)e_i=b_i$. The parallel transport of $gB'(0)e_i$ is given by $gb_i$. Therefore $\omega(dR_gX)$ is given by the path $A^g(s)$ which satisfies $A^g(s)gB'(s)e_i=gA(s)B'(s)e_i$. So
\[ 
  A^g(s)=gA(s)g^{-1}
\]
and
\[ 
  \Dsdd{ A^g(s) }{s}{0}=\Ad(g)\omega(X).
\]

\end{proof}

\subsection{Clifford algebra \texorpdfstring{$(1,1)$}{(1,1)}}
%-------------------------------------

We consider the left invariant vector fields
\begin{subequations}
\begin{align}
  \tilde{e}_J(r_0)&=\Dsdd{ r_0 e^{-sJ} }{s}{0}=-r_0J\\
\tilde{e}_L(r_0)&=\Dsdd{ r_0 e^{-sL} }{s}{0}=-r_0L.
\end{align}
\end{subequations}
More precisely, we consider the vectors given by action of these matrices on the ``base point'' $\begin{pmatrix}
0\\1\\0
\end{pmatrix}$. Hence
\begin{align}
\tilde{e}_J(r_0)=-r_0\begin{pmatrix}
0\\0\\1
\end{pmatrix},\quad
\tilde{e}_L(r_0)=-r_0\begin{pmatrix}
1\\0\\0
\end{pmatrix}
\end{align}
and 
\[ 
  g=\begin{pmatrix}
1\\&-1
\end{pmatrix}.
\]
Remark that this metric is constant (it does not depend on $r_0$) because $r_0$ is an isometry. For this reason, we now turn our attention to Clifford algebra and spin group for $V=\eR^{1,1}$. Following matrices fulfill relation \eqref{3101r3}
\[ 
  \gamma_J=\begin{pmatrix}
0&1\\1&0
\end{pmatrix},\quad
\gamma_L=\begin{pmatrix}
0&-1\\1&0
\end{pmatrix}. 
\]
The complete Clifford algebra has the following matrices too :
\[ 
  1=\begin{pmatrix}
1&0\\0&1
\end{pmatrix},\quad
\gamma_{JL}=\begin{pmatrix}
-1\\&1
\end{pmatrix}.
\]
The Clifford algebra is nothing else than $GL(2,\eR)$, the set of all real $2\times 2$ matrices. From definitions, one can check that
\[ 
 \begin{aligned} 
\alpha(J)&=-J   &\tau(J)&=J\\
\alpha(L)&=-L   &\tau(L)&=L\\
\alpha(JL)&=JL  &\tau(JL)&=-JL\\
\alpha(1)&=1    &\tau(1)&=1
\end{aligned}
\]
Inverse and $\alpha$ of general element in $\Cl(1,1)$ are given by
\[ 
  \begin{pmatrix}
p&q\\r&s
\end{pmatrix}^{-1}=
\frac{1}{ ps-qr }\begin{pmatrix}
s&-q\\-r&p
\end{pmatrix},\quad
\alpha\begin{pmatrix}
p&q\\r&s
\end{pmatrix}=
\begin{pmatrix}
p&-q\\-r&s
\end{pmatrix}.
\]
A general element in $\eR^{1,1}$ is $\begin{pmatrix}
0&\alpha\\\beta&0
\end{pmatrix}$ with $\alpha,\beta\in\eR$, so the condition to belongs to $\Gamma(1,1)$ is that 
\[ 
  \frac{1}{ ps-qr }\begin{pmatrix}
p&-q\\-r&s
\end{pmatrix}
\begin{pmatrix}
0&\alpha\\\beta&0
\end{pmatrix}
\begin{pmatrix}
s&-q\\-r&p
\end{pmatrix}
\]
belongs to $\eR^{1,1}$ for all $\alpha$ and $\beta$. It requires, among others, that $qs\beta-rp\alpha=0$ for all $\alpha$ and $\beta$. Hence $qs=rp=0$, but the alternatives $p=r=0$ and $q=s=0$ are ruled out because we want the determinant $ps-qr$ to be non zero. Therefore, $\Gamma(1,1)$ is generated by
\[ 
  \Gamma(1,1)\leadsto
\begin{pmatrix}
p&0\\0&s
\end{pmatrix},\begin{pmatrix}
0&q\\r&0
\end{pmatrix}.
\]
The latter belongs to $\eR^{1,1}$, so
\[ 
  \Gamma^+(1,1)\leadsto\begin{pmatrix}
z+c&0\\0&z-c
\end{pmatrix}=z\mtu+c\gamma_{JL}.
\]
From 
\[ 
  \tau(z\mtu+c\gamma_{JL})=z\mtu-c\gamma_{JL},
\]
elements in $\Spin(1,1)$ are subject to the relation
\[ 
  \begin{pmatrix}
s&0\\0&p
\end{pmatrix}=
\frac{1}{ ps }\begin{pmatrix}
s&0\\0&p
\end{pmatrix}.
\]
As consequence, we find
\begin{equation}
\Spin(1,1)=\eR_0\leadsto
\begin{pmatrix}
1/p\\&p
\end{pmatrix}.
\end{equation}
 If we put (see decomposition \eqref{eq:expo_ANK})
\[ 
  A=\begin{pmatrix}
 e^{a}\\& e^{-a}
\end{pmatrix},
\]
we have $\Spin(1,1)=A\times\eZ_2$. Let us check that $\Spin(1,1)$ is a double covering of $\SO_0(1,1)$. We know that 
\[ 
  \SO(1,1)=\begin{pmatrix}
\cosh\xi&\sinh\xi\\\sinh\xi&\cosh\xi
\end{pmatrix}\times\eZ_2
\]
while $\SO_0(1,1)$ is 
\[ 
  \SO_0(1,1)=\begin{pmatrix}
\cosh\xi&\sinh\xi\\\sinh\xi&\cosh\xi
\end{pmatrix}=\eR.
\]
This structure of $\SO(1,1)$ comes from the fact (true for all $\SO(1,n)$) that $| \Lambda^0_0 |\geq1$ when $\Lambda$ is a Lorentz transformation. So $\mtu$ and $-\mtu$ cannot belong to the same connected component. Note that $\cosh\xi\geq 1$.
We see intuitively how to cover two times $\eR$ with $\eR_0$. Let us see how the map $\chi$ does that. From definition, $\chi(x)y=\alpha(x)yx^{-1}$, so it is easy to see that 
\[ 
  \chi(1)=\chi(-1)=\id|_{\eR^{1,1}}
\]

\subsection{Parallel transport}
%------------------------------

We have a connection on the frame bundle of $AdS_2$ and we wan to lift the vectors $\tilde{e}_J$ and $\tilde{e}_L$, i.e. we consider a point 
\[ 
  \xi_0=(r_0,v_1,v_2)\in \SO(AdS_2)
\]
where $v_1$ and $v_2$ form an orthonormal (in the sense of $g$) basis of $T_{r_0}AdS_2$. Then we have to find a path $s\to\xi(s)$ in $\SO(AdS_2)$ such that $\xi(0)=\xi_0$, $\omega(\xi'(0))=0$ and $dp\xi'(0)=\tilde{e}_a$. The latter condition allows us to compute $r(s)$ in the expression
\[ 
  \xi(s)=(r(s),v_1(s),v_2(s)),
\]
namely, $r(s)$ is the path of $\tilde{e}_a$. The condition to be horizontal imposes that vectors $v_i(s)$ are parallel transport of $v_i$ along $\tilde{e}_a$. So we have to compute the different $T_a(\tilde{e}_b)(s)$ which is the parallel transported of $\tilde{e}_b$ along the path of $\tilde{e}_a$ at a distance $s$; this is an element of $T_{\tilde{e}_a(s)}AdS_2$. It will be decomposed in the basis 
\[ 
 \begin{split}
  \tilde{e}_J\big( \tilde{e}_a(s) \big)&=-r_0 e^{-s a}J\\
\tilde{e}_L\big( \tilde{e}_a(s) \big)&=-r_0 e^{-sJ}L.
\end{split} 
\]
where we imply the action on the base point $\begin{pmatrix}
0\\1\\0
\end{pmatrix}$. For notational simplicity, from now we write $a(s)$ instead of $\tilde{e}_a(s)$. Various products are easy to compute; for example
\[ 
  \tilde{e}_J(J(s))\cdot\tilde{e}_L(J(s))
        =r_0 e^{-sJ}J\cdot r_0 e^{-sJ}L\\
        =J\cdot L\\
        =\begin{pmatrix}
0\\0\\1
\end{pmatrix}\cdot\begin{pmatrix}
1\\0\\0
\end{pmatrix}\\
        =0
\]
because $r_0 e^{-sJ}$ is an isometry.
In general :
\[ 
  \tilde{e}_a\big( c(s) \big)\cdot \tilde{e}_b\big( c(s) \big)=a\cdot b
\]
Now we pose in general
\[ 
  T_a(\tilde{e}_b)(s)=\alpha(s)\tilde{e}_b\big( a(s) \big)+\beta(s)\tilde{e}_L\big( a(s) \big),
\]
and we want to find the (real valued) functions $\alpha$ and $\beta$. Parallel transport fulfils two conditions: the norm and the angle with the path are constant. This leads us to two conditions :
\begin{subequations}
\begin{align}
  T_a\big( \tilde{e}_b(s) \big)\cdot T_a\big( \tilde{e}_b(s) \big)&=b\cdot b\\
 T_a\big( \tilde{e}_b(s) \big)\cdot \tilde{e}_a\big( a(s) \big)&=b\cdot a.
\end{align}
\end{subequations}
These equations extends to
\begin{subequations}
\begin{align}
  \beta(s)^2-\alpha(s)^2&=b\cdot b\\
\alpha(s)J\cdot a+\beta(s)L\cdot a&=b\cdot a.
\end{align}
\end{subequations}
There are four cases to be considered following that $a=J,L$ and $b=J,L$. The result is that
\begin{equation}
T_a\big(\tilde{e}_b \big)=\tilde{e}_b,
\end{equation}
in other terms, the vectors $\tilde{e}_J$ and $\tilde{e}_L$ are not only parallel vector fields, but each is parallel along the path of the other.

\subsection{Covariant derivative}
%--------------------------------

We will give the horizontal lift of $\tilde{e}_a$ at point
\[ 
  \xi(0)=\{ B_1^b\tilde{e}_b,B_2^c\tilde{e}_c \}_{r_0e_t}
\]
under the form of the path
\[ 
  \xi(s)=\{ B_1^b\tilde{e}_b\big( a(s) \big),B_2^c\tilde{e}_c\big( a(s) \big) \}_{\tilde{e}_a(s)}.
\]
We create a connection on the spinor bundle from the connexion via the formula
\[ 
  \widehat{\nabla_X^E\psi}(\xi)=\overline{ X }_{\xi}(\hat \psi).
\]
In our case, we take $\psi\colon M\to \mS$, or $\hat{\psi}\colon SU(1,1)\to \Lambda W$ such that
\[ 
  \hat{\psi}(U\cdot g)=\rho(g^{-1})\hat{\psi}(U).
\]
Since $\tilde\omega=\varphi^*\omega$, we have $\tilde\omega(X)=\omega(d\varphi X)$ and
\[ 
  \overline{ e }_a{}_{\xi_0}=\varphi^{-1}\big( \tilde{e}_a(s),\ldots \big).
\]
Therefore
\begin{equation}  \label{eq_whidpsinabla}
\widehat{\nabla_a\psi}(\xi_0)=\Dsdd{ (\hat{\psi}\circ\varphi^{-1})\{ B_i^c\tilde{e}_c\big( a(s) \big) \}_{\tilde{e}_a(s)} }{s}{0}
\end{equation}
where $\varphi$ is defined by
\[ 
  \varphi(U)=\{ U\tilde{e}_J,U\tilde{e}_L \}_{Ur_0e_t}.
\]
We have to find
\begin{equation}  \label{eq_varpBic}
  \varphi^{-1}\{ B_i^c\tilde{e}_c\big( a(s) \big) \}_{\tilde{e}_a}.
\end{equation}
Before to write down the inverse of $\varphi$, let us perform some computations.
\[ 
  J=\begin{pmatrix}
&0\\
0&0&1\\
&1
\end{pmatrix},\quad
L=\begin{pmatrix}
0&1&1\\
-1\\
1
\end{pmatrix},
\]
and as far as we only wants to compute derivatives, we can write the exponentials as
\begin{align} 
 e^{sJ}&=\mtu+sJ=\begin{pmatrix}
1\\
&1&s\\
&s&1
\end{pmatrix}\\
 e^{sL}&=\mtu+sL=\begin{pmatrix}
1&s&s\\
-s&1&0\\
s&0&1
\end{pmatrix}.
\end{align}
The path are given by
\begin{equation}
\tilde{e}_a(s)=r_0 e^{-sa}\begin{pmatrix}
0\\1\\0
\end{pmatrix},
\end{equation}
in particular
 \begin{align}
\tilde{e}_J(s)&=r_0\begin{pmatrix}
0\\1\\-s
\end{pmatrix},
&\tilde{e}_L(s)&=r_0\begin{pmatrix}
-s\\1\\0
\end{pmatrix}.
\end{align} 
For the various $\tilde{e}_b\big( a(s) \big)$, we have
\begin{equation}
\tilde{e}_b\big( a(s) \big)=\Dsdd{ a(s) e^{-tb} }{t}{0}\begin{pmatrix}
0\\1\\0
\end{pmatrix}\\
    =\Dsdd{ r_0 e^{-sa} e^{-tb} }{t}{0}\begin{pmatrix}
0\\1\\0
\end{pmatrix}\\
    =-r_0 e^{-sa}b\begin{pmatrix}
0\\1\\0
\end{pmatrix}.
\end{equation}
Results are
\begin{subequations}
\begin{align}
  \tilde{e}_J\big( J(s) \big)&=-r_0\begin{pmatrix}
0\\-s\\1
\end{pmatrix}
&
\tilde{e}_J\big( L(s) \big)&=-r_0\begin{pmatrix}
-s\\0\\1
\end{pmatrix}\\
\tilde{e}_L\big( J(s) \big)&=-r_0\begin{pmatrix}
1\\0\\0
\end{pmatrix}
&
\tilde{e}_L\big( L(s) \big)&=-r_0\begin{pmatrix}
1\\s\\-s
\end{pmatrix}.
\end{align}
\end{subequations}
We finally have to know that
\[ 
  B^c\tilde{e}_c\big( J(s) \big)=-r_0\begin{pmatrix}
B^L\\-sB^J\\B^J
\end{pmatrix},
\quad
B^c\tilde{e}_c\big( L(s) \big)=-r_0\begin{pmatrix}
-sB^J+B^L\\sB^L\\B^J-sB^L
\end{pmatrix}.
\]

Following equation \eqref{eq_varpBic}, in order to write down $\widehat{\nabla_a\psi}$, we have to find $U(s)\in SU(1,1)$ such that
\begin{enumerate}
\item $Ur_0e_t=\tilde{e}_a(s)$,
\item $U\tilde{e}_J=B^c_1\tilde{e}_c\big( a(s) \big)$,
\item $U\tilde{e}_L=B^c_2\tilde{e}_c\big( a(s) \big)$.
\end{enumerate}
If $\overline{ f }$ and $\overline{ g }$ are vectors, solutions in $U$ of equation $Ur_0\overline{ f }=r_0\overline{ g }$ are $U=\AD(r_0)B$ where $B$ fulfils $B\overline{ f }=\overline{ g }$. In the case of $a=J$, the three conditions successively give
\begin{subequations}
\begin{align}
U&=\AD(r_0)\begin{pmatrix}
.&0&.\\
.&1&.\\
.&-s&.
\end{pmatrix}\\
U&=\AD(r_0)\begin{pmatrix}
.&.&B_1^L\\
.&.&-sB_1^J\\
.&.&B_1^J
\end{pmatrix}\\
U&=\AD(r_0)\begin{pmatrix}
-B^L_2&.&.\\
sB_2^J&.&.\\
B_2^J&.&.
\end{pmatrix}.
\end{align}
\end{subequations}
Putting all together in equation \eqref{eq_whidpsinabla} we find
\begin{equation} \label{eq_nabJmoi}
\begin{split}
  \widehat{\nabla_J\psi}(\xi_0)&=\frac{ d }{ ds }\hat{\psi}\AD(r_0)
\begin{pmatrix}
-B_2^L  &   0   &   B_1^L\\
sB_2^J  &   1   &   -sB_1^J\\
B_2^J   &   -s  &   B_1^J
\end{pmatrix}\\
    &=d\hat{\psi}\AD(r_0)\begin{pmatrix}
0&0&0\\
B_2^J&0&-B_1^J\\
0&-1&0
\end{pmatrix}.
\end{split}
\end{equation}
The same with $L$ instead of $J$ leads to
\begin{equation} \label{eq_nabLmoi}
\widehat{\nabla_L \psi}(\xi_0)=d\hat{\psi}\AD(r_0)\begin{pmatrix}
-B_2^J&-1&B_1^J\\
B_2^L&0&B_1^L\\
-B_2^L&0&-B_1^L
\end{pmatrix}.
\end{equation}

However it should be shocking to get $3\times 3$ matrices in $SU(1,1)$ : we had abused between $\SO(2,1)$ and $SU(1,1)$.

\subsection{Another way to write a section (wrong way to do)}
%------------------------------------------------------------

The equivariant function $\hat{\psi}\colon SU(1,1)\to \Lambda W$ fulfills 
\[ 
  \hat{\psi}(U\cdot g)=\rho(g^{-1})\hat{\psi}(U)
\]
for all $g\in\Spin(1,1)$; in particular with $g=-\mtu$,
\begin{equation}
 \hat{\psi}(-U)=-\hat{\psi}(U).
\end{equation}
This gives the idea that it is not impossible to define $\hat{\psi}$ from its projection on $\SO(2,1)$ : we want to get $\tilde{\psi}\colon \SO(2,1)\to \Lambda W$ and define 
\[ 
  \hat{\psi}(U)=\tilde{\psi}\big( T(U) \big).
\]
More precisely, we parametrize $SU(1,1)$ by $\alpha$ and $\beta$ such that $| \alpha |^2-| \beta |^2=1$. Then we divide $SU(1,1)$ into two parts: $\alpha=x+iy$ is green when $x>0$ and when $x=0$, $y<0$; $\alpha$ is red when $x<0$ and when $x=0$, $y>0$. When $\alpha=0$, we classify following $\beta$ in the same way. The result is that $U$ is green if and only if $-U$ is red. For a map $\tilde{\psi}\colon \SO(2,1)\to \Lambda W$, we define
\begin{equation}
\hat{\psi}(U)=
\begin{cases}
\tilde{\psi}\big( T(U) \big)&\text{if $U$ is green}\\
-\tilde{\psi}\big(T(U)\big)&\text{if $U$ is red}
\end{cases}
\end{equation}
We define $T^{-1}\colon \SO(2,1)\to SU(1,1)$ as follows: $T^{-1}(A)$ is the green element of $SU(1,1)$ whose image by $T$ is $A$. In any cases we have
\[ 
\hat{\psi}\circ T^{-1}=\tilde{\psi}.  
\]
The meaning of equations \eqref{eq_nabJmoi} and \eqref{eq_nabLmoi} is that $\AD(r_0)$ is a matrix whose inverse image by $T$ should be given to $\hat{\psi}$; the difficulty is to know which of the two. When $U_0$ is green,
\[ 
 \begin{split}
\widehat{\nabla_a\psi}(U_0)&=\Dsdd{ (\hat{\psi}\circ T^{-1})  \AD(r_0)\Big( \cdots \Big)   }{s}{0} \\
        &=\Dsdd{ \tilde{\psi} \AD(r_0)\Big( \cdots \Big)}{s}{0},
\end{split} 
\]
while when $U_0$ is red,
\[ 
  \widehat{\nabla_a\psi}(U_0)=-\Dsdd{ \tilde{\psi}\AD(r_0)\Big( \cdots \Big) }{s}{0}.
\]
These two show that
\begin{equation}
\widetilde{\nabla_a\psi}\big( T(U_0) \big)=\Dsdd{   \tilde{\psi}\AD(r_0)\Big( \cdots \Big)    }{s}{0}
\end{equation}
All this is only proved in the interior of the green and red regions so that the path $U(s)$ keeps on only one region.

\subsection{Once again}  \label{pg_DiracADsdeux}
%-------------------------

We see $AdS_2$ as\footnote{Here, $G=SL(2,\eR)$} $\mO=\Ad(G)H$ and we consider a base point $o=\Ad(k_0)H$ with $G=SL(2,\eR)=ANK$. Let the principal bundle
\[ 
\xymatrix{%
   A \ar@{~>}[r]^{R}        &   G\ar[d]^{\pi}\\
    &   \mO
}
\]
with $A$ acting on $G$ by $(a,g)\mapsto ga$ and the projection
\begin{equation}
\pi(rk_0a)=\Ad(rk_0a)H.
\end{equation}
where $r\in R$ and $a\in A$. More precisely, the principal bundle we look at is
\begin{equation}
\xymatrix{%
   A \ar@{~>}[r]^{R}        &   \mU_G\ar[d]^{\pi}\\
    &   \mU
}
\end{equation}
where $\mU_G=Rk_0A$ and $\mU=\pi(\mU_G)=\Ad(Rk_0A)H=\Ad(Rk_0)H=\Ad(R)o$. The $\mU_G$ is so defined in order to be the $\pi^{-1}$ of an orbit $\mU=\Ad(R)o$.

We have a manifold isomorphism $R\simeq\mU$ given by
\[ 
  \phi\colon r\to \Ad(r)o.
\]
How to see a left invariant vector field on $R$ \emph{via} this identification ? 
\[ 
  d\phi\tilde X_r=d\phi\Dsdd{ r e^{tX} }{t}{0}
        =\Dsdd{ \Ad(r)\Ad( e^{tX})o }{t}{0}.
\]
This leads us to consider the following field for $X\in\sR$. We define $\xi_X(rk_0a)\in T_{rk_0a}\mU_G$,
\begin{equation}
  \xi_X(rk_0a)=\Dsdd{ r e^{tX}k_0a }{t}{0}.
\end{equation}
Let's see the projection :
\[ 
\begin{split}
d\pi_{rk_0a}\xi_X(rk_0a)&=\Dsdd{ \pi(r e^{tX}k_0a) }{t}{0}\\
        &=\Dsdd{ \Ad(r e^{tX}k_0a)H }{t}{0}\\
        &=\Dsdd{ \Ad(r e^{tX})o }{t}{0}.
\end{split} 
\]
This gives us the idea to define $X^{\sharp}\in T_{\Ad(rk_0a)H}\mU=T_{\pi(rk_0a)}\mU$ by
\begin{equation}
X^{\sharp}_{rk_0a}=\Dsdd{ \Ad(re^{tX})o }{t}{0},
\end{equation}
which is a good definition because $\pi(rk_0a)=\pi(r'k_0a')$ only when $r=r'$. We put the following connection on $\mU_G$ :
\begin{equation}
\alpha_{rk_0a}(\Sigma)=\left[     \big( dL_{rk_0a}^{-1} \big)_{rk_0a}\Sigma    \right]_{\sA}.
\end{equation}
We hope $\xi_X$ to be the horizontal lift\footnote{We will see in proposition \ref{prop_horliftXdiz} that it is not the case, but for the moment, we hope it.} of $X^{\sharp}$; by construction $d\pi\xi_X=X^{\sharp}$. We have
\[ 
 \begin{split}
\alpha_{rk_0a}(\xi_X)&=\left[ dL_{(rk_0a)^{-1}}\xi_X(rk_0a) \right]_{\sA}\\
        &=\Dsdd{ a^{-1}k_0^{-1}r^{-1}r e^{tX}k_0a }{t}{0}^{\sA}\\
        &=\Dsdd{ a^{-1}\AD(k_0^{-1}) e^{tX}a}{t}{0}^{\sA}\\
        &=\left[ \Ad(a^{-1}k_0^{-1})X \right]_{\sA}.
\end{split} 
\]
One can, by brute force computation\footnote{Or by remarking that $\sA$ is abelian.}, show that the difference $\Ad(ak_0)X-\Ad(k_0)X$ is skew-diagonal when
\[ 
  X=\begin{pmatrix}
a'&n\\0&-a'
\end{pmatrix},
\quad k_0=\begin{pmatrix}
\cos a&\sin a\\\sin a&\cos a
\end{pmatrix},
\quad
a=\begin{pmatrix}
a&0\\0&1/p
\end{pmatrix}.
\]
So $\Ad(a)$ does not change the $\sA$-component of $\Ad(k_0)X$. We conclude that
\begin{equation}
  \alpha(\xi_X)=\left[ \Ad(k_0^{-1})X \right]_{\sA}.
\end{equation}
When $X\in\sR$, we consider $\tilde X_g=(dL_g)_eX$;
\begin{equation}                         \label{eq_defXtilde}
  \tilde X_{rk_0a}=dL_{rk_0a}X,
\end{equation}
in particular, $\tilde X_r=\Dsdd{ r e^{tX} }{t}{0}$.
We denote by $\tau$ the action
\begin{equation}
\begin{aligned}
 \tau_g\colon\mO&\to \mO \\ 
\tau_g\Ad(r)H&= \Ad(gr)H 
\end{aligned}
\end{equation}
In particular
\[ 
 \begin{split}
d\pi_gdL_g Y&=\Dsdd{ \pi(g e^{tY}) }{t}{0}\\
        &=\Dsdd{ \ad(g e^{tY}H) }{t}{0}\\
        &=\Dsdd{ \tau_g\Ad( e^{tY})H }{t}{0}\\
        &=(d\tau_g)_Hd\pi_e Y,
\end{split} 
\]
thus
\begin{equation}
d\pi\circ dL = d\tau\circ d\pi.
\end{equation}
With definition \eqref{eq_defXtilde}, we have $\alpha(\tilde X)=X_{\sA}$ because
 \[ 
\alpha\big( dL_{rk_0a}X \big)=\left[ dL_{(rk_0a)^{-1}}dL_{rk_0a}X \right]_{\sA}
        =X_{\sA}.
\]
We are now able to find some horizontal lift. 
\begin{proposition}  \label{prop_horliftXdiz}
The horizontal lift of $X^{\sharp}$ is
\[
  \overline{ X^{\sharp} }=\xi_X-\widetilde{  [\Ad(k_0^{-1})X]_{\sA}  }.
\]

\end{proposition}

\begin{proof}
First, we have
\[ 
 \begin{split}
d\pi\overline{ X^{\sharp} }|_{rk_0a}&=d\pi\xi_X-d\pi(dL_{rk_0a})_e[\Ad(k_0^{-1})X]_{\sA}\\
                &=\Dsdd{ \Ad(r e^{tX}o) }{t}{0}-d\tau\,d\pi[\Ad(k_0^{-1})X]_{\sA}.
\end{split} 
\]
The first term is $X^{\sharp}$ while the second is zero because if $A\in\sA$, 
\[ 
 \begin{split}
d\pi A&=\Dsdd{ \pi( e^{tA}) }{t}{0}\\
        &=\Dsdd{ \Ad( e^{tA})H }{t}{0}\\
        &=0.
\end{split} 
\]
On the other hand,
\[ 
\alpha(\overline{ X^{\sharp} })=\alpha(\xi_X)-[\Ad(k_0)^{-1}X]_{\sA}=0.
\]
\end{proof}

Now we prove that the function $\overline{ X^{\sharp} }\cdot\hat{\psi}$ is equivariant, and therefore that the definition
\[ 
  \widehat{\nabla_{X^{\sharp}}\psi}=\overline{ X^{\sharp} }\cdot\hat{\psi}
\]
works. Using equivariance of $\hat{\psi}$, 
\begin{equation}
\begin{aligned}
  \overline{ X^{\sharp} }\cdot \hat{\psi}(ga_1)&=\Dsdd{ \hat{\psi}(\xi_X(t) }{t}{0}-\hat{\psi}\big( dL_{ga_1}[\Ad(k_0^{-1})X]_{\sA} \big)\\
        &=\Dsdd{ \hat{\psi}\big( r e^{tX}k_0aa_1 \big) }{t}{0}-\Dsdd{ \hat{\psi}\big( ga_1 e^{t[\Ad(k_0^{-1})X]_{\sA}} \big) }{t}{0}\\
        &=\Dsdd{ \rho(a_1)\hat{\psi}\big( r e^{tX}k_0a \big) }{t}{0}-\Dsdd{ \rho(a_1)\hat{\psi}\big( g e^{t[\Ad(k_0^{-1})X]_{\sA}} \big) }{t}{0},\\
        &=\rho(a_1)\hat{\psi}(\xi_X)-\Dsdd{ \rho(a_1)\hat{\psi}\big( \widetilde{[\Ad(k_0^{-1})X]_{\sA} }|_g \big) }{t}{0}\\
        &=\rho(a_1)\hat{\psi}(\xi_X)-\rho(a_1)\widetilde{ [\Ad(k_0^{-1})X]_{\sA}  }\hat{\psi}(g)\\
        &=\rho(a_1)(\overline{ X^{\sharp} }\cdot\hat{\psi})(g).
\end{aligned}
\end{equation}
for the third line, we used the fact that $\sA$ is abelian
\subsubsection{Clifford algebra for \texorpdfstring{$AdS_2$}{AdS2}}
%----------------------------------------------------------------

Our basis of $\sA\oplus\sN$ is
\[ 
  H=\begin{pmatrix}
1&0\\0&-1
\end{pmatrix}\quad\text{and}\quad
E=\begin{pmatrix}
0&1\\0&0
\end{pmatrix}
\]
and we choose
\[ 
  o=\Ad(k_0)H=\cos(2k_0)H+\sin(2k_0)(E+F).
\]
Since (at first order in $t$) $\Ad( e^{tH})o=\cos(2k_0)\mtu+\sin(2k_0)\big( E+2tE+F-2tF \big)$,
\[ 
  H^{\sharp}_{rk_0a}=2\sin(2k_0)\Ad(r)(E-F),
\]
and
\[ 
  E^{\sharp}_{rk_0a}=\Ad(r)\big( -2\cos(2k_0)E+\sin(2k_0)H \big).
\]
We have to compute the metric matrix for this basis; we know from equation \eqref{eq_KillAdinvariant}, the Killing form is $\Ad$-invariant and $(\mathfrak{sl}(2,\eR),B)\simeq(\eR^3,\eta_{21})$. So the $\Ad(r)$ disappears in the computation of $B(X^{\sharp},Y^{\sharp})$. We get
\[
\begin{split}
B(H^{\sharp},H^{\sharp})&=4\sin^2(2k_0)B(E-F,E-F)\\
        &=-32\sin^2(2k_0)\\
B(E^{\sharp},E^{\sharp})&=\sin^2(2k_0)B(H,H)\\
        &=8\sin^2(2k_0)\\
B(E^{\sharp},H^{\sharp})&=-4\sin(2k_0)\cos(2k_0)B(E,E-F)+2\sin^2(2k_0)B(H,E-F)\\
        &=16\sin^2(2k_0)\cos(2k_0).
\end{split}
\]
So the metric is in the basis $\{ H^{\sharp},E^{\sharp} \}$
\begin{equation}
g=
\begin{pmatrix}
-32\sin^2(2k_0) & 16\sin(2k_0)\cos(2k_0)\\
16\sin(2k_0)\cos(2k_0) & 8\sin^2(2k_0)
\end{pmatrix}.
\end{equation}
When we consider the orbit of $E+F$, we choose $o=E+F$, i.e. $\cos(2k_0)=0$, $\sin(2k_0)=1$ so that
\begin{equation}
H^{\sharp}_{rk_0a}=2\Ad(r)(E-F),\quad E^{\sharp}_{rk_0a}=\Ad(r)H,
\end{equation}
and
\[ 
  g=\begin{pmatrix}
-32&0\\0&8
\end{pmatrix};
\]
in the case of the orbit of $-(E+F)$, we get the same. The negative vector is $H^{\sharp}$ and the positive one is $E^{\sharp}$.

\subsubsection{Identification \texorpdfstring{$\sQ\leftrightarrow\Lambda W$}{QW}}
%////////////////////////////////////////////////////////////////////////////////

We want a linear bijection $\phi\colon \sQ\to \Lambda W$ such that
\[ 
  \rho(s)\phi(X)=\phi\big( \rho(s)X \big)
\]
where the left hand side action of $\Spin$ is the usual on $\Lambda W$ while the right hand side one remains to be defined. The implementation of this is easy: we can take any bijection between $\sQ$ and $\Lambda W$ and define
\begin{equation}
  \rho(s)X=\phi^{-1}\big( \rho(s)\phi(X) \big).
\end{equation}

Spinors on $AdS_2$ are given by equivariant functions $\hat{\psi}\colon \mU_G\to \Lambda W$ which are now replaced by $\tilde{\psi}\colon R\to \sQ\simeq\Lambda W$ by
\[ 
  \hat{\psi}(rk_0a)=\rho(a^{-1})\tilde{\psi}(r).
\]
So the set of sections of the spinor bundle over $\mU$ is
\[ 
  \Gamma_{\mU}\simeq  C^{\infty}(R,\Lambda W).
\]

\subsubsection{Covariant derivative}
%////////////////////////////////

The aim is now to compute 
\[ 
\begin{split}
   \widetilde{\nabla_{X^{\sharp}}\psi}(r)&=\widehat{\nabla_{X^{\sharp}}\psi}(rk_0)\\
        &=\overline{ X^{\sharp} }\cdot \hat{\psi}|_{rk_0}\\
        &=\big( \xi_X-\widetilde{ [\Ad(k_0^{-1})X]_{\sA}  } \big)\cdot\hat{\psi}|_{rk_0}\\
        &=\Dsdd{ \tilde{\psi}(r e^{tX}) }{t}{0}-\Dsdd{ \rho\big(  e^{t[\Ad(k_0^{-1})X]_{\sA}} \big) }{t}{0}\tilde{\psi}(r)\\
        &=\tilde X_r\tilde{\psi}(r)-d\rho_e\big( [\Ad(k_0^{-1})X]_{\sA} \big)\tilde{\psi}(r).
\end{split}  
\]
Our final formula for the covariant derivative is
\begin{equation}
  \widetilde{\nabla_{X^{\sharp}}\psi}(r)=\tilde X_r\tilde{\psi}-d\rho\big( [\Ad(k_0^{-1})X]_{\sA} \big)\tilde{\psi}.
\end{equation}
The Dirac operator will be a linear combination of vectors of the form
\[ 
  \tilde X+d\rho\big( [\Ad(k_0^{-1})X]_{\sA}  \big).  
\]
Notice that $\tilde X$ is left invariant and the second term is even independent of the point, so the whole is left invariant.


