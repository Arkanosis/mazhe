% This is part of (almost) Everything I know in mathematics and physics
% Copyright (c) 2013
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    A \defe{Lie algebra}{Lie algebra} is a vector space \( \lG\) on \( \eK(=\eR,\eC)\) endowed with a bilinear operation \( (x,y)\mapsto [x,y]\) from \( \lG\times\lG\) with the properties
    \begin{enumerate}
        \item
            \( [x,y]=-[y,x]\)
        \item
            \( \big[ x,[y,z] \big]+\big[ y,[z,x] \big]+\big[ z,[x,y] \big]=0\).
    \end{enumerate}
    The second condition is the \defe{Jacobi identity}{Jacobi!identity}.
\end{definition}

\section{Adjoint group}\label{sec:adj_gp}
%--------------------------

Let $\lA$ be a \emph{real} Lie algebra. We denote by $GL(\lA)$\nomenclature[G]{$GL(\lA)$}{The group of nonsingular endomorphism of $\lA$} the group of all the nonsingular endomorphism of $\lA$ : the linear and nondegenerate operators on $\lA$ as vector space. An element $\sigma\in\GL(\lA)$ does not specially fulfils somethings like $\sigma[X,Y]=[\sigma X,\sigma Y]$. The Lie algebra $\gl(\lA)$\nomenclature[G]{$\protect\gl(\lA)$}{space of endomorphism with usual bracket} is the vector space of the endomorphism (without non degeneracy condition) endowed with the usual bracket $(\ad A)B=[A,B]=A\circ B-B\circ A$. The map $X\to\ad X$ is a homomorphism from $\lA$ to the subalgebra $\ad(\lA)$ of $\gl(\lA)$.

The group $\Int(\lA)$\nomenclature[G]{$\Int(\lA)$}{Adjoint group of $\lA$} is the analytic Lie subgroup of $\GL(\lA)$ whose Lie algebra is $\ad(\lA)$ by theorem \ref{tho:gp_alg}. This is the \defe{adjoint group}{adjoint!group}\index{group!adjoint} of $\lA$.

\begin{proposition}
The group $\Aut(\lA)$\nomenclature[G]{$\Aut\lA$}{Group of automorphism of $\lA$} of all the automorphism of $\lA$ is a closed subgroup of $\GL(\lA)$.
\end{proposition}

\begin{proof}
The property which distinguish the elements in $\Aut(\lA)$ from the ``commons'' elements of $\GL(\lA)$ is the preserving of structure: $\varphi[A,B]=[\varphi A,\varphi B]$. These are equalities, and we know that a subset of a manifold which is given by some equalities is closed.
\end{proof}

Now, theorem \ref{tho:diff_sur_ferme} provides us an unique analytic structure on $\Aut(\lA)$ in which it is a topological Lie subgroup of $\GL(\lA)$. From now we only consider this structure. We denote by $\partial(\lA)$\nomenclature[G]{$\partial\lA$}{The Lie algebra of $\Aut(\lA)$} the Lie algebra of $\Aut(\lA)$ : this is the set of the endomorphism $D$ of $\lA$ such that $\forall t\in\eR$, $e^{tD}\in\Aut(\lA)$. By differencing the equality
\begin{equation}\label{eq:exp_der}
  e^{tD}[X,Y]=[e^{tD}X,e^{tD}Y]
\end{equation}
with respect to $t$, we see\footnote{As usual, if we consider a basis of $\lA$ as vector space, the expression in the right hand side of \[[e^{tD}X,e^{tD}Y]=\ad(e^{tD}X)e^{tD}X\] can be seen as a product matrix times vector, so that Leibnitz works.} that $D$ is a \defe{derivation}{derivation!of a Lie algebra} of $\lA$ :
\begin{equation}
  D[X,Y]=[DX,Y]+[X,DY]
\end{equation}
for any $X$, $Y\in\lA$. Conversely, consider $D$, any derivation of $\lA$; by induction, 
\begin{equation}
   D^k[X,Y]=\sum_{i+j=k}\frac{k!}{i!j!}[D^iX,D^jY]
\end{equation}
where by convention, $D^0$ is the identity in $\lA$. This relation shows that $D$ fulfils condition \eqref{eq:exp_der}, so that any derivation of $\lA$ lies in $\partial(\lA)$. Then
\[
  \partial(\lA)=\{\text{derivations of $\lA$}\}.
\]
The Jacobi identities show that 
\[
\ad(\lA)\subset\partial(\lA).    \label{pg:ad_subset_der}
\]
From this, we deduce :
\begin{equation}\label{eq:int_sub_aut}
  \Int(\lA)\subset\Aut(\lA).
\end{equation}
(cf. error \ref{err:Intt_Aut}) Indeed the group $\Int(\lA)$ being connected, it is generated\footnote{See proposition \ref{PropUssGpGenere}} by any neighbourhood of $e$; note that $\Aut(\lA)$ has not specially this property. We take a neighbourhood of $e$ in $\Int(\lA)$ under the form  $\exp V$  where $V$ is a sufficiently small neighbourhood of $0$ in $\ad(\lA)$ to be a neighbourhood of $0$ in $\partial(\lA)$ on which $\exp$ is a diffeomorphism. In this case, $\exp V\subset\Aut(\lA)$ and then $\Int(\lA)\subset\Aut(\lA)$.

Elements of $\ad(\lA)$ are the \defe{inner derivations}{derivation!inner} while the ones of $\Int(\lA)$ are the \defe{inner automorphism.}{inner!automorphism} 

Let $\mO$ be an open subset of $\Aut(\lA)$; for a certain open subset $U$ of $\GL(\lA)$, $\mO=U\cap\Aut(\lA)$. Then
\begin{equation}
  \iota^{-1}(\mO)=\mO\cap\Int(\lA)
           =U\cap\Aut(\lA)\cap\Int(\lA)
       =U\cap\Int(\lA).
\end{equation}
The subset $U\cap\Int(\lA)$ is open in $\Int(\lA)$ for the topology because $\Int(\lA)$ is a Lie\quext{Is it true ??} subgroup of $\GL(\lA)$ and thus has at least the induced topology. This proves that the inclusion map $\dpt{\iota}{\Int(\lA)}{\Aut(\lA)}$ is continuous.

The lemma \ref{lem:var_cont_diff} and the consequence below makes $\Int(\lA)$ a Lie subgroup of $\Aut(\lA)$. Indeed $\Int(\lA)$ and $\Aut(\lA)$ are both submanifolds of $\GL(\lA)$ which satisfy \eqref{eq:int_sub_aut}. By definition, $\Aut(\lA)$ has the induced topology from $\GL(\lA)$. Then $\Int(\lA)$ is a submanifold of $\Aut(\lA)$. This is also a subgroup and a topological group ($\Int(\lA)$ is not a topological subgroup of $\Aut(\lA)$, cf remark \ref{rem:sub_Lie}). Then $\Int(\lA)$ is a Lie subgroup of $\Aut(\lA)$.


Schematically, links between $\Int\lG$, $\ad\lG$, $\Aut\lG$ and $\partial\lG$ are
\begin{subequations}\label{eq:schem_ad_int}
\begin{align}
  \Int\lG&\longleftarrow\ad\lG\\
  \Aut\lG&\longrightarrow\partial\lG.
\end{align}
\end{subequations}
Remark that the sense of the arrows is important. By definition $\partial\lG$ is the Lie algebra of $\Aut\lG$, then there exist some algebras $\lG$ and $\lG'$ with $\Aut\lG\neq\Aut\lG'$ but with $\partial\lG=\partial\lG'$, because the equality of two Lie algebras doesn't implies the equality of the groups. The case of $\Int\lG$ and $\ad\lG$ is very different: the group is defined from the algebra, so that $\ad\lG=\ad\lG'$ implies $\Int\lG=\Int\lG'$ and $\Int\lG=\Int\lG'$ if and only if $\ad\lG=\ad\lG'$.


\begin{proposition}
 The group $\Int(\lA)$ is a normal subgroup of $\Aut(\lA)$.
\end{proposition}

\begin{proof}
Let us consider a $s\in\Aut(\lA)$. The map $\dpt{\sigma_s}{\Aut(\lA)}{\Aut(\lA)}$, $\sigma_s(g)=sgs^{-1}$ is an automorphism of $\Aut(\lA)$. Indeed, consider $g$, $h\in\AutA$; direct computations show that $\sigma_s(gh)=\sigma_s(g)\sigma_s(h)$ and $[\sigma_s(g),\sigma_s(h)]=\sigma_s([g,h])$. From this, $(d\sigma_s)_e$ is an automorphism of $\partial(\lA)$, the Lie algebra of $\AutA$. For any $D\in\partial(\lA)$ we have
\begin{equation}\label{eq:ad_s_2}
 (d\sigma_s)_eD=\Dsdd{ sD(t)s^{-1} }{t}{0}
             =sDs^{-1}. 
\end{equation}
Since $s$ is an automorphism of $\lA$ and $\ad(\lA)$, a subalgebra of $\gl(\lA)$,
\begin{equation}\label{eq:ad_s_1}
  s\ad Xs^{-1}=\ad(sX)
\end{equation}
for any $X\in\lA$, $s\in\Aut(\lA)$. Since $\ad(\lA)\subset\partial(\lA)$, we can write \eqref{eq:ad_s_2} with $D=\ad X$ and put it in \eqref{eq:ad_s_1} : 
\[
   (d\sigma)_e\ad X=s\ad Xs^{-1}=\ad(s\cdot X).
\]
We know from general theory of linear operators on vector spaces that if $A,B$ are endomorphism of a vector space and if $A^{-1}$ exists, then $Ae^BA^{-1}=e^{ABA^{-1}}$. We write it with $A=s$ and $B=\ad X$ : 
\[
  \sigma_s\cdot e^{\ad X}=se^{\ad X}s^{-1}=e^{s\ad Xs^{-1}}=e^{\ad(s\cdot X)},
\]
sot that 
\begin{equation}\label{eq:sigma_aut_s}
  \sigma_s\cdot e^{\ad X}=e^{\ad(s X)}.
\end{equation}

Ont the other hand, we know that $\IntA$ is connected, so it is generated by elements of the form $e^{\ad X}$ for $X\in\lA$. Then $\IntA$ is a normal subgroup of $\AutA$; the automorphism $s$ of $\lA$ induces the isomorphism $g\to sgs^{-1}$ in $\IntA$ because of equation \eqref{eq:sigma_aut_s}.
\end{proof}

More generally, if $s$ is an isomorphism from a Lie algebra $\lA$ to a Lie algebra $\lB$, then the map $g\to sgs^{-1}$ is an isomorphism between $\AutA$ and $\AutB$ which sends $\IntA$ to $\IntB$. Indeed, consider an isomorphism $\dpt{s}{\lA}{\lB}$ and $g\in\AutA$. If $g\in\IntA$, we have to see that $sgs^{-1}\in\IntB$. By definition, $\IntA$ is the analytic subgroup of $\GL(\lA)$ which has $\ad(\lA)$ as Lie algebra. We have $g=e^{\ad A}$, then $sgs^{-1}=e^{\ad(sA)}$ which lies well in $\IntB$.

\begin{lemma}       \label{LemadhomomadXadYadXY}
    The adjoint map is an homomorphism \( \ad\colon \lG\to \GL(\lG)\). In other terms for every \( X,Y\in\lG\) we have
    \begin{equation}
        \big[ \ad(X),\ad(Y) \big]=\ad\big( [X,Y] \big)
    \end{equation}
    as operators on \( \lG\). In particular the algebra acts on itself and \( \lG\) carries a representation of each of its subalgebra.
\end{lemma}

\begin{proof}
    Using the fact that \( \ad(X)\) is a derivation and Jacobi, for \( Z\in\lG\) we have
    \begin{subequations}
        \begin{align}
            \big[ \ad(X),\ad(Y) \big]Z&=\ad(X)\ad(Y)Z-\ad(Y)\ad(X)Z\\
            &=\big[ [X,Y],Z \big]+\big[ Y,[X,Z] \big]-\big[ [Y,X],Z \big]-\big[ X,[Y,Z] \big]\\
            &=\ad\big( [X,Y] \big)Z.
        \end{align}
    \end{subequations}
\end{proof}

\section{Adjoint representation}
%////////////////////////////////////

Let $G$ be a Lie group and $g\in G$; one can consider the map $\dpt{I}{G\times G}{G}$ given by $I(g)h=g hg^{-1}$. Seen as $\dpt{I(g)}{G}{G}$, this is an analytic automorphism of $G$. We define :\nomenclature[D]{$\Ad$}{Adjoint representation}
\[
    \Ad(g)=dI(g)_e.
\]
Using equation $\varphi(\exp X)=\exp d\varphi_e(X)$ with $\varphi=I(g)$,
\begin{equation}\label{eq:sigma_X_sigma}
  g e^{X}g^{-1}=\exp[ \Ad(g)X ]
\end{equation}
for every $g\in G$ and $X\in\lG$. The map $g\to\Ad(g)$ is a homomorphism from $G$ to $\GL(\lG)$. This homomorphism is called the \defe{adjoint representation}{adjoint!representation!Lie group on its Lie algebra}\index{representation!adjoint} of $G$.

\begin{proposition}
The adjoint representation is analytic.
\end{proposition}

\begin{proof}
We have to prove that for any $X\in\lG$ and for any linear map $\dpt{\omega}{\lG}{\eR}$, the function $\omega(\Ad(g)X)$ is analytic at $g=e$. Indeed if we take as $\omega$ , the projection to the $i$th component and $X$ as the $j$th basis vector ($\lG$ seen as a vector space), and if we see the product $\Ad(g)X$ as a product matrix times vector, $(\Ad(g)X)_i$ is just $\Ad(g)_{ij}$. Then our supposition is the analyticity of $g\to\Ad(g)_{ij}$ at $g=e$. \quext{L'analicité de $\Ad$, elle vient par prolongement analytique depuis juste un point ?}

Now we prove it. Consider $f\in\Cinf(G)$, analytic at $g=e$ and such that $Yf=\omega(Y)$ for any $Y\in\lG$. Using equation \eqref{eq:sigma_X_sigma}, 
\begin{equation}
  \omega(\Ad(g)X)=(\Ad(g)X)f
                      =\Dsdd{ f(e^{t\Ad(g)X}) }{t}{0}
                      =\Dsdd{ f(g e^{tX}g^{-1}) }{t}{0},
\end{equation}
which is well analytic at $g=e$.
\end{proof}


\begin{proposition}
Let $G$ be a connected Lie group and $H$, an analytic subgroup of $G$. Then $H$ is a normal subgroup\index{normal!subgroup} of $G$ if and only if $\lH$ is an ideal in $\lG$.
\end{proposition}

\begin{proof}
We consider $X$, $Y\in\lG$. Formula $\exp tX\exp tY\exp-tY=\exp( tY+t^2[X,Y]+o(t^3) )$ and equation \eqref{eq:sigma_X_sigma} give
\[
   \exp\Big( \Ad(e^{tX})tY \Big)=\exp\Big(  tY+t^2[X,Y]+o(t^3)  \Big).
\]
Since it is true for any $X$, $Y\in\lG$, $\Ad(e^{tX})tY=tY+t^2[X,Y]$; thus
\begin{equation}
  \Ad(e^{tX})=\mtu+t[X,Y]+o(t^2).
\end{equation}
Since we know that $\dpt{d\Ad_e}{\lG}{\gl(\lG)}$ is a homomorphism ($\Ad$ is seen as a map $\dpt{\Ad}{G}{\GL(\lG)}$), taking the derivative of the last equation with respect to $t$ gives
\begin{equation}
  d\Ad_e(X)=\ad X.
\end{equation}
Then $\Ad(e^X)=e^{\ad X}$. Since is connected, an element of $G$ can be written as $\exp X$ for a certain $X\in\lG$\footnote{Because $G$ is generated by any neighbourhood of $e$ and there exists such a neighbourhood of $e$ which is diffeomorphic to a subset of $\lG$ by $\exp$.}. The purpose is to prove that $g\exp Xg^{-1}=\exp(\Ad(g)X)$ remains in $H$ for any $g\in G$ if and only if $\lH$ is an ideal in $\lG$. In other words, we want $\Ad(g)X\in\lH$ if and only if $\lH$ is an ideal. We can write $g=e^Y$ for a certain $Y\in\lG$. Thus
\[
  \Ad(g)X=\Ad(e^Y)X=e^{\ad Y}X.
\]
Using the expansion 
\begin{equation}
e^{\ad Y}=\sum_k\us{k!}(\ad Y)^k,
\end{equation}
we have the thesis.

\end{proof}

\begin{lemma}
Let $G$ be a connected Lie group with Lie algebra $\lG$. If $\dpt{\varphi}{G}{X}$ is an analytic homomorphism ($X$ is a Lie group with Lie algebra $\lX$), then

\begin{enumerate}
\item The kernel $\varphi^{-1}(e)$ is a topological Lie subgroup of $G$; his algebra is the kernel of $d\varphi_e$.
\item The image $\varphi(G)$ is a Lie subgroup of $X$ whose Lie algebra is $d\varphi(\lG)\subset\lX$.
\item The quotient group $G/\varphi^{-1}(e)$ with his canonical analytic structure is a Lie group. The map $g\varphi^{-1}(e)\mapsto\varphi(g)$ is an analytic isomorphism $G/\varphi^{-1}(e)\to\varphi(G)$. In particular the map $\dpt{\varphi}{G}{\varphi(G)}$ is analytic.
\end{enumerate}
\label{lem:vp_G_X}
\end{lemma}


\begin{proof}
\subdem{First item} We know that a subgroup $H$ closed in $G$ admits an unique analytic structure such that $H$ becomes a topological Lie subgroup of $G$. This is the case of $\varphi^{-1}(e)$. We know that $Z\in\lG$ belongs to the Lie algebra of $\varphi^{-1}(e)$ if and only if $\varphi(\exp tZ)=e$ for any $t\in\eR$. But $\varphi(\exp tZ)=\exp(td\varphi(Z))=e$ if and only if $d\varphi(Z)=0$.

\subdem{Second item}
Consider $X_1$, the analytic subgroup of $X$ whose Lie algebra is $d\varphi(\lG)$. The group $\varphi(G)$ is generated by the elements of the form $\varphi(\exp Z)$ for $Z\in\lG$. The group $X_1$ is generated by the $\exp(d\varphi Z)$. Because of lemma \ref{lemsur5d}, these two are the same. Then $\varphi(G)=X_1$ and their Lie algebras are the same.

\subdem{Third item}
We consider $H$, a closed normal subgroup of $G$; this is a topological subgroup and the quotient $G/H$ has an unique analytic structure such that the map $G\times G/H\to G/H$, $(g,[x])\to [gx]$ is analytic. We consider a decomposition $\lG=\lH\oplus\lM$ and we looks at the restriction $\dpt{\psi}{\lM}{G}$ of the exponential. Then there exists a neighbourhood $U$ of $0$ in $\lM$ which is homomorphically send by  $\psi$ into an open neighbourhood of $e$ in $G$ and such that $\dpt{\pi}{G}{G/H}$ sends homomorphically $\psi(U)$ to a neighbourhood  of $p_0\in G/H$ (cf. lemma \ref{lem:vois_U}).

We consider $\UU$, the interior of $U$ and $B=\psi(\UU)$. The following diagram is commutative :
\begin{equation}
 \xymatrix{
    G\times G/H  \ar[rr]^{\displaystyle\Phi}\ar[dr]_{\displaystyle \pi\times I} &&  G/H\\
     &     G/H\times G/H\ar[ur] _{\displaystyle\alpha}
  }
\end{equation}
with $\Phi(g,[x])=[g^{-1} x]$, $(\pi\times I)(g,[x])=([g],[x])$ and $\alpha([g],[x])=[g^{-1} x]$. Indeed, 
\[
   \alpha\circ(\pi\times I)(g,[x])=\alpha([g],[x])=[g^{-1} x].
\]
In order to see that $\alpha$ is well defined, remark that if $[h]=[g]$ and $[y]=[x]$ $[g^{-1} x]=[h^{-1} y]$ because $H$ is a normal subgroup of $G$.

Now, we consider $g_0,x_0\in G$ and the restriction of $(\pi\times I)$ to $(g_0B)\times(G/H)$. Since $\pi$ is homeomorphic on $\psi(U)$ and $B=\psi(\UU)$, on $g_0B$, $\pi$ is a diffeomorphism (because the multiplication is diffeomorphic as well)

\begin{probleme}\label{prob:diffeo_2}
    Why is the \( \pi\) a diffeomorphism ? I understand why it is qn homeomorphism, but no more.
\end{probleme}

This diffeomorphism maps to a neighbourhood $N$ of $([g_0],[x_0])$ in $G/H\times G/H$. From the commutativity, we know that $\alpha=\Phi\circ(\pi\times I)^{-1}$, so that $\alpha$ is analytic. Consequently, $G/H$ is a Lie group. On $N$, $\alpha$ is analytic, then $\alpha(N)$ is analytic.

All this is for a closed normal subgroup $H$ of $G$. Now we consider $H=\varphi^{-1}(e)$ and $\lH$, the Lie algebra of $H$. From the first item, we know that the Lie algebra of $H$ is the kernel of $d\varphi$ : $\lH=d\varphi^{-1}(0)$ which is an ideal in $\lG$.

From the second point, the Lie algebra of $G/H$ is $d\pi(\lG)$ which is isomorphic to $\lG/\lH$; the bijection is $\gamma(d\pi(X))=[X]\in\lG/\lH$. In order to prove the injectivity, let us consider $\gamma(A)=\gamma(B)$; $A=d\pi(X)$, $B=d\pi(Y)$. The condition is $[X]=[Y]$; thus it is clear that $d\pi(X)=d\pi(Y)$

Let us consider on the other hand the map $Z+\lH\to d\varphi(Z)$ for $Z\in\lG$\footnote{Note that $\lG$ and $\lH$ are not groups; by $[X]$, we mean $[X]=\{ X+h\tq h\in\lH \}$.}. In other words, the map is $[Z]\to d\varphi(Z)$. This is an isomorphism $\lG/\lH\to d\varphi(\lG)$, which gives a local isomorphism between $G/H$ and $\varphi(G)$. This local isomorphism is $[g]\to\varphi(g)$ for $g$ in a certain neighbourhood of $e$ in $G$.

Since $[g]\to\varphi(g)$ has a differential which is an isomorphism, this is analytic at $e$. Then it is analytic everywhere.

\end{proof}


\begin{corollary}
If $G$ is a connected Lie group and if $Z$ is the center of $G$, then
\begin{enumerate}
\item $\Ad_G$ is an analytic homomorphism from $G$ to $\Int(G)$, with kernel $Z$,
\item the map $[g]\to\Ad_G(g)$ is an analytic isomorphism from $G/Z$ to $\Int(\lG)$ (the class $[g]$ is taken with respect to $Z$).
\end{enumerate}
\label{cor:Ad_homom}
\end{corollary}
    

\begin{proof}
\subdem{First item}
A connected Lie group is generated by a neighbourhood of identity, and any element of a suitable such neighbourhood can be written as the exponential of an element in the Lie algebra. So $\Int(\lG)$ is generated by elements of the form $\exp(\ad X)=\Ad(\exp X)$; this shows that $\Int(\lG)\subset\Ad(G)$. In order to find the kernel, we have to  see $\Ad_G^{-1}(e)$ by the formula 
\[ 
   e^{\Ad(g)X}=g e^Xg^{-1}.
\]
We have to find the $g\in G$ such that $\forall X\in\lG$, $\Ad_G(g)X=X$. We taking the exponential of the two sides and using \eqref{eq:sigma_X_sigma},
\begin{equation}
  g e^Xg^{-1}=e^X.
\end{equation}
Then $g$ must commute with any $e^X\in G$ : in other words, $g$ is in the kernel of $G$.

\subdem{Second item}
This is contained in lemma \ref{lem:vp_G_X}. Indeed $G$ is connected and we had just proved that $\dpt{\Ad_G}{G}{\Int(\lG)}$ with kernel $Z$; the third item of lemma \ref{lem:vp_G_X} makes $G/Z$ a Lie group and the map $[g]\to\Ad_G(g)$ an analytic isomorphism from $G/Z$ to $\Ad_G(G)=\Int(\lG)$.
\end{proof}


\begin{lemma}
Let $G_1$ and $G_2$ be two locally isomorphic connected Lie groups with trivial center (i.e. $\lG_1=\lG_2=\lG$ and $Z(G_i)=\{ e \}$). In this case, we have $G_1=G_2=\Int(\lG)$ where $\Int\lG$ stands for the group of internal automorphism of $\lG$.
\end{lemma}

\begin{proof}
We denote by $G_0$ the group $\Int\lG$. The adjoint actions $\Ad_i\colon G_i\to G_0$ are both surjective because of corollary \ref{cor:Ad_homom}. Let us give an alternative proof for injectivity. Let $Z_i=\ker(\Ad_i)=\{ g\in G_i\tq\Ad(g)X=X,\,\forall X\in\lG \}$. Since $G_i$ is connected, it is generated by any neighbourhood of the identity in the sense of proposition \ref{PropUssGpGenere}; let $V_0$ be such a neighbourhood. Taking eventually a subset we can suppose that $V_0$ is a normal coordinate system. So we have
\[ 
  g\exp_{G_i}(X)g^{-1}=\exp_{g_i}(X)
\]
for every $X\in V_0$. Using proposition \ref{PropUssGpGenere} we deduce that $gxg^{-1}=x$ for every $x\in G_i$, thus $g\in Z(G_i)$. That proves that $\ker(\Ad_i)\subset Z(G_i)$. The assumption of triviality of $Z(G_i)$ concludes injectivity of $\Ad_i$.
\end{proof}

\begin{corollary}
Let $\lG$ be a real Lie algebra with center $\{0\}$. Then the center of $\Int(\lG)$ is only composed of the identity.
\end{corollary}

\begin{proof}
We note $G'=\Int(\lG)$ and $Z$ his center; $\ad$ is the adjoint representation of $\lG$ and $\Ad'$, $\ad'$, the ones of $G'$ and $\ad(\lG)$ respectively. We consider the map $\dpt{\theta}{G'/Z}{\Int(\ad(\lG))}$, $\theta([g])=\Ad'(g)$. By the second item of the corollary \ref{cor:Ad_homom}, $[g]\to\Ad_{G'}(g)$ is an analytic homomorphism from $G'$ to $\Int(\lG')$ where $\lG'$ is the Lie algebra of $G'$; this is $\ad(\lG)$. So $\dpt{\theta}{G'/Z}{\Int(\lG')}$ is isomorphic.

Now we consider the map $\dpt{s}{\lG}{\ad(\lG)}$, $s(X)=\ad(X)$; this is an isomorphism. We also consider $\dpt{S}{G'}{ \GL(\ad(\lG))}$, $S(g)=s\circ g\circ s^{-1}$. The Lie algebra of $S(G')$ is $\ad(\lG')=\ad\big(\ad(\lG)\big)$. Then $S(G')$ is the subset of $\GL(\ad\lG)$ whose Lie algebra is $\ad\big(\ad\lG\big)$, i.e. exactly $\Int(\ad\lG)$. So $S$ is an isomorphism $\dpt{S}{G'}{\Int(\ad\lG)}$. From all this,
\begin{equation}
   S(e^{\ad X})=s\circ e^{\ad X}\circ s^{-1}
               =e^{\ad'(\ad X)}
           =\Ad'(e^{\ad X}).        
\end{equation}
With this equality, $\dpt{S^{-1}\circ\theta}{G'/Z}{G'}$ is an isomorphism which sends $[g]$ on $g$ for any $g\in Z$. Then $Z$ can't contains anything else than the identity.
\end{proof}

If we relax the assumptions of the trivial center, we have a counter-example with $\lG=\eR^3$ and the commutations relation
\[
   [X_1,X_2]=X_3,\quad [X_1,X_3]=[X_2,X_3]=0.
\]
The group $\Int(\lG)$ is abelian; then his center is the whole group, although $\lG$ is not abelian.

Note that two groups which have the same Lie algebra are not necessarily isomorphic. For example the sphere $S^2$ and $\eR^2$ both have $\eR^2$ as Lie algebra. But two groups with same Lie algebra are locally the same. More precisely, we have the following lemma.

\begin{lemma}
If $G$ is a Lie group and $H$, a topological subgroup of $G$ with the same Lie algebra ($\lH=\lG$), then there exists a common neighbourhood $A$ of $e$ of $G$ and $G$ on which the products in $G$ and $H$ are the same.
\end{lemma}

\begin{proof}
The exponential is a diffeomorphism between $U\subset\lG$ and $V\subset G$ and between $U'\subset\lH$ and $W\subset H$ (obvious notations). We consider an open $\mO\subset\lH$ such that $\mO\subset U\subset U'$. The exponential is diffeomorphic from $\mO$ to a certain open $A$ in $G$ and $H$. Since $H$ is a subgroup of $G$, the product $e^Xe^Y$ of elements in $A$ is the same for $H$ and $G$. (cf error \ref{err:gp_meme_alg})
\end{proof}

Under the same assumptions, we can say that $H$ contains at least the whole $G_0$ because it is generated by any neighbourhood of the identity. Since $H$ is a subgroup, the products keep in $H$.

For a semisimple Lie group, the Lie algebras $\partial(\lG)$ and $\ad(\lG)$ are the same. Then $\Int(\lG)$ contains at least the identity component of $\Aut(\lG)$. Since $\Int(\lG)$ is connected, for a semisimple group, it is the identity component of $\Aut(\lG)$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Killing form}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

The \defe{Killing form}{Killing!form} of $\mG$ is the symmetric bilinear form :
\begin{equation}
             B(X,Y)=Tr(\ad X\circ \ad Y).
\end{equation}
It is \defe{invariant}{invariant!form} in the sense of
\begin{equation}                        \label{eq:Killing_invariant}
     B\big((\ad S)X,Y\big)=-B\big(X,(\ad S)Y\big),
\end{equation}
$\forall X$, $Y$, $S\in\mG$.

\begin{proposition} \label{PropAutomInvarB}
If $\dpt{\varphi}{\mG}{\mG}$ is an automorphism of $\mG$, then 
\[
   B(\varphi(X),\varphi(Y))=B(X,Y).
\]
\label{prop:auto_2}
\end{proposition}
 
\begin{proof}
The fact that $\varphi$ is an automorphism of $\mG$ is written as $\varphi\circ\ad X=\ad(\varphi(X))\circ\varphi$, or
\[
  \ad(\varphi(X))=\varphi\circ\ad X\circ\varphi^{-1}.
\]
Then
\begin{equation}
\begin{split}
\tr(\ad(\varphi(X))\circ\ad(\varphi(Y)))&=\tr(\varphi\circ\ad X\circ\varphi^{-1}\circ\varphi\ad Y\circ\varphi^{-1})\\
                                &=\tr(\ad X\circ\ad Y).
\end{split}
\end{equation}
\end{proof}


\begin{remark}
The Killing $2$-form is a map $\dpt{B}{\mG\times\mG}{\eR}$. When we say that it is preserved by a map $\dpt{f}{G}{G}$, we mean that it is preserved by $df$ : $B(df\cdot,df\cdot)=B(\cdot,\cdot)$.
\end{remark}

An other important property of the Killing form is its bi-invariance.

\begin{theorem}
The Killing form is bi-invariant\index{Killing!form!bi-invariance} on $G$.
\label{tho:bi_invariance}
\end{theorem}

\begin{remark}
The Killing form is \emph{a priori} only defined on $\mG=T_eG$. For $A$, $B\in T_gG$, one naturally defines
\begin{equation}
  B_g(A,B)=B(dL_{g^{-1}}A,dL_{g^{-1}}B).
\end{equation}
This assures the left invariance of $B$. Now we prove the right invariance.
\end{remark}


\begin{proof}[Proof of theorem \ref{tho:bi_invariance}]
Because of the left invariance,
\[
  B(dR_gX,dR_gY)=B(dL_{g^{-1}}dR_gX,dL_{g^{-1}}dR_gY)=B(\Ad_{g^{-1}}X,\Ad_{g^{-1}}Y).
\]
But $\Ad_{g^{-1}}=d(\AD_{g^{-1}})$ and $\AD_{g^{-1}}$ is an automorphism of $G$. Thus by lemma \ref{lem:auto_1} and proposition \ref{prop:auto_2},
\begin{equation}                    \label{eq_KillAdinvariant}
B\big(\Ad(g^{-1})X,\Ad(g^{-1})Y\big)=B(X,Y).
\end{equation}

\end{proof}

\begin{lemma}  
Let $\lG$ be a Lie algebra and $\lI$ an ideal in $\lG$. Let $\dpt{B}{\lG\times\lG}{\eR}$ be the Killing form on $\lG$ and $\dpt{B'}{\lI\times\lI}{\eR}$, the one of $\lI$. Then $B'=B|_{\lI\times\lI}$, i.e. the Killing form on $\lG$ descent to the ideal $\lI$.
\label{lem:Killing_descent_ideal}
\end{lemma}

\begin{proof}
If $W$ is a subspace of a (finite dimensional) vector space $V$ and $\dpt{\phi}{V}{W}$ and endomorphism, then $\tr\phi=\tr(\phi|_W$). Indeed, if $\{X_1,\ldots,X_n\}$ is a basis of $V$ such that $\{X_1,\ldots,X_r\}$ is a basis of $W$, the matrix element $\phi_{kk}$ is zero for $k>r$. Then 
\[
  \tr\phi=\sum_{i=1}^{n}\phi_{ii}=\sum_{i=1}^r\phi_{ii}=\tr(\phi|_W).
\]

Now consider $X$, $Y\in\lI$; $(\ad X\circ\ad Y)$ is an endomorphism of $\lG$ which sends $\lG$ to $\lI$ (because $\lI$ is an ideal). Then
\[
B'(X,Y)=\tr\big( (\ad X\circ\ad Y)|_{\lI} \big)=\tr(\ad X\circ\ad Y)=B(X,Y).
\]
\end{proof}


We are not going to (not completely) prove an useful formula for some matrix algebras: $B(X,Y)=2n\tr(XY)$ (proposition \ref{PropKillingTraceDeuxn}). We follow \cite{Sagle}. We consider a simple subalgebra $\lG$ of $\gl(V)$ for a certain vector space $V$ and a nondegenerate $\ad$-invariant symmetric $2$-form $f$. Then there exists a $S\in\GL(\lG)$ such that
\begin{subequations}
\begin{align}
  f(X,Y)&=B(SX,Y) \label{eq:S_un}  \\ 
  B(SX,Y)&=B(X,SY).  \label{eq:S_deux}
\end{align}
\end{subequations}
If we consider a basis of $\lG$, we can write $f(X,Y)$ (and the Killing) in a matricial form\footnote{We systematically use the sum convention on the repeated subscript.} as
\[
  f(X,Y)=f_{ij}X^iY^j,\qquad B(X,Y)=B_{ij}X^iY^j.
\]
Since $B$ is nondegenerate, we can define the matrix $(B^{ij})$ by $B^{ij}B_{jk}=\delta^i_k$. It is easy to see that the searched endomorphism of $\lG$ is given by $S^k_l=f_{kj}B^{jl}$.
 
Using the invariance \eqref{eq:Killing_invariant} of the Killing form and \eqref{eq:S_deux}, we find
\[
   B\big( (\ad X\circ S)Y,Z  \big)=-B\big( (S\circ\ad X)Z,Y  \big)
\]
for any $X$, $Y$, $Z\in\lG$. Now using \eqref{eq:S_un}, 
\begin{equation}
 f\big(  (S^{-1}\circ\ad X\circ S)Y,Z  \big)=-f\big((\ad X) Z,Y\big)
                                           =f\big( (\ad Z) X,Y \big)
                       =f\big( Z, (\ad X)Y \big).
\end{equation}
Since $f$ is nondegenerate, we find $\ad X\circ S=S\circ\ad X$. It follows from Schurs'lemma that $S=\lambda I$. Note that $f(X,Y)=\lambda B(X,Y)$; this proves a certain unicity of the Killing form relatively to his invariance properties.

Now we consider $f(X,Y)=\tr(XY)$. This is symmetric because of the cyclic invariance of the trace and this is $ad$-invariant because of the formula $\tr([a,b]c)=\tr(a[b,c])$ which holds for any matrices $a,b,c$.

The newt step is to show that $f$ is nondegenerate; we define
\[
  \lG\hperp=\{X\in\lG\tq f(X,Y)=0\,\forall Y\in\lG   \}.
\]
The simplicity of $\lG$ ($\lG$ has no proper ideals) makes $\lG$ equal to $0$ or $\lG$. Indeed consider $Z\in\lG\hperp$. For any $X$, $Y\in\lG$, we have
\[
0=f(Z,[X,Y])=f([Z,X],Y).
\]
Then $[Z,X]\in\lG\hperp$ and $\lG\hperp$ is an ideal. We will see that the reality is $\lG\hperp=0$ (cf. error \ref{err:f_dege}). Let us suppose $\lG\hperp=\lG$ and consider the lemma \ref{lem:M_nil} with $A=B=\lG$. We define
\[
   M=\{ X\in\lG\tq [X,\lG]\subset\lG \}=\lG.
\]
If $X\in M$ satisfies $\tr(XY)=0$ for any $Y\in M$, then $X$ is nilpotent. Here, $X\in M$ is not a true condition because $M=\lG$. Since $\lG\hperp=\lG$, the trace condition is also trivial. Then $\lG$ is made up with nilpotent endomorphisms of $V$. Then lemma \ref{lem:pre_Engel} makes all the $X\in\lG$ ad-nilpotent, so that $\lG$ is nilpotent. (cf. remark \ref{rem:gl_V_nil})

By the third item of proposition \ref{prop:nil_homom_nil}, $\mZ(\lG)\neq 0$ which contradicts the simplicity of $\lG$. Then $\lG\hperp=0$ and $f$ is nondegenerate. Finally,
\begin{equation}
  B(X,Y)=\lambda\tr(X,Y)
\end{equation}
for a certain real number $\lambda$. With a certain amount of work (in \cite{Sagle,SamelsonNotesLieAlg} for example), one can determine the exact value of $\lambda$ when $\lG$ is the Lie algebra of $n\times n$ matrices with vanishing trace.

\begin{proposition} \label{PropKillingTraceDeuxn}
If $\lG$ is the Lie algebra of $n\times n$ matrices with vanishing trace, then
\[
   B(X,Y)=2n\tr(XY).
\]
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Solvable and nilpotent algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

If $\lG$ is a Lie algebra, the \defe{derived Lie algebra}{derived!Lie algebra}\index{Lie!algebra!derived} is 
\[
   \dD\lG=\Span\{[X,Y]\tq X,Y\in\lG\}.
\]
We naturally define $\dD^0\lG=\lG$ and $\dD^n\lG=\dD(\dD^{n-1}\lG)$ this is the \defe{derived series}{derived!series}. Each $\dD^n\lG$ is an ideal in~ $\lG$. We also define the \defe{central decreasing sequence}{central!decreasing sequence} by $\lA^0=\lA$, $\lA^{p+1}=[\lA,\lA^p]$.

\begin{definition}
The Lie algebra $\lG$ is \defe{solvable}{solvable!Lie algebra}\index{Lie!algebra!solvable} if there exists a $n\geq 0$ such that $\dD^n\lG=\{0\}$. A Lie group is solvable when its Lie algebra is\index{Lie!group!solvable}\index{solvable!Lie group}.  

    The Lie algebra  \( \lG\) is \defe{nilpotent}{nilpotent!Lie algebra} if \( \lG^n=0\) for some~\( n\). We say that \( \lG\) is \( \ad\)-nilpotent if \( \ad(X)\) is a nilpotent endomorphism of \( \lG\) for each \( X\in\lG\).
\end{definition}

Do not confuse \emph{nilpotent} and \emph{solvable} algebras. A nilpotent algebra is always solvable, while the algebra spanned by $\{ A,B \}$ with the relation $[A,B]=B$ is solvable but not nilpotent.

If $\lG\neq\{0\}$ is a solvable Lie algebra and if $n$ is the smallest natural such that $\dD^n\lG=\{0\}$, then $\dD^{n-1}\lG$ is a non zero abelian ideal in $\lG$. We conclude that a solvable Lie algebra is never semisimple (because the center of a semisimple Lie algebra is zero).

A Lie algebra is said to fulfil the \defe{chain condition}{chain!condition} if for every ideal $\lH\neq\{0\}$ in $\lG$, there exists an ideal $\lH_1$ in $\lH$ with codimension $1$.

\begin{lemma}
A Lie algebra is solvable if and only if it fulfils the chain condition.
\end{lemma}

\begin{proof}
\subdem{Necessary condition}
The Lie algebra $\lG$ is solvable (then $\dD\lG\neq\lG$) and $\lH$ is an ideal in $\lG$. We consider $\lH_1$, a subspace of codimension $1$ in $\lH$ which contains $\dD\lH$. It is clear that $\lH_1$ is an ideal in $\lH$ because $[H_1,H]\in\dD\lH\subset\lH_1$.
\subdem{Sufficient condition}
We have a sequence
\begin{equation}\label{eq:solvable_chaine}
   \{0\}=\lG_n\subset\lG_{n-1}\subset\ldots\subset\lG_0=\lG
\end{equation}
where $\lG_r$ is an ideal of codimension $1$ in $\lG_{r-1}$. Let $A$ be the unique vector in $\lG_{r-1}$ which don't belong to $\lG_r$.  When we write $[X,Y]$ with $X$, $Y\in\lG_{r-1}$, at least one of $X$ or $Y$ is not $A$ (else, it is zero) then at least one of the two is in $\lG_r$. But $\lG_r$ is an ideal; then $[X,Y]\in\lG_r$. Thus $\dD(\lG_{r-1})\subset\lG_r$ and
\[
\dD^n\lG=\dD^{n-1}\dD\lG\subset\dD^{n-1}\lG_1\subset\ldots\subset\lG_n=0.
\]
\end{proof}

\begin{theorem}[Lie theorem]\label{tho:Lie_Vu}\index{Lie!theorem}
    Consider $\lG$, a real (resp. complex) solvable Lie algebra and a real (resp. complex) vector space $V\neq\{0\}$. If $\dpt{\pi}{\lG}{\gl(V)}$ is a homomorphism, then there exists a non zero vector in $V$ which is eigenvector of all the elements of $\pi(\lG)$.
\end{theorem}

\begin{probleme}
    It is strange to be stated for real and complex Lie algebras. Following \cite{SamelsonNotesLieAlg}, this is only true for complex Lie algebras while there exists other versions for reals ones.
\end{probleme}

\begin{proof}
Let us do it by induction on the dimension of $\lG$. We begin with $\dim\lG=1$. In this case, $\pi$ is just a map $\dpt{\pi}{\lG}{\gl(V)}$ such that $\pi(aX)=a\pi(X)$. We have to find an eigenvector for the homomorphism $\dpt{\pi(X)}{V}{V}$. Such a vector exists  from the Jordan decomposition \ref{tho:jordan}. Indeed, if there are no eigenvectors, there are no spaces $V_i$ and the decomposition $V=\sum V_i$ can't be true.

Now we consider a general solvable Lie algebra $\lG$ and we suppose that the theorem is true for any solvable Lie algebra with dimension less that $\dim\lG$. Since $\lG$ is solvable, there exists an ideal $\lH$ of codimension $1$ in $\lG$; then there exists a $e_0\neq 0\in V$ which is eigenvector of all the $\pi(H)$ with $H\in\lH$. So we have $\dpt{\lambda}{\lH}{\eR}$ naturally defined by
\[
  \pi(H)e_0=\lambda(H)e_0.
\]
Now we consider $X\in\lG\setminus\lH$ and $e_{-1}=0$, $e_p=\pi(X)^pe_0$ for $p=1,2,\ldots$ We will show that $\pi(H)e_p=\lambda(H)e_p\mod(e_0,\ldots,e_{p-1})$ for all $H\in\lH$ and $p\geq 0$. It is clear for $p=0$. Let us suppose that it is true for $p$. Then
\begin{equation}
\begin{split}
  \pi(H)e_{p+1}&=\pi(H)\pi(X)e_p\\
               &=\pi([H,X])e_p+\pi(X)\pi(H)e_p\\
           &=\lambda([H,X])e_p+\pi(X)\lambda(H)e_p\\
                      &\quad\mod(e_0,\ldots,e_{p-1},\pi(X)e_0,\ldots,\pi(X)e_{p-1}).
\end{split}
\end{equation}
But we can put $\pi([H,X])$ and $\pi(X)e_i$ into the modulus. Thus we have
\[
  \pi(H)e_{p+1}=\lambda(H)e_{p+1}\mod(e_0,\ldots,e_p).
\]

Now we consider the subspace of $V$ given by $W=\Span\{e_p\}_{p=1,\ldots}$. The algebra $\pi(\lH)$ leaves $W$ invariant and our induction hypothesis works on $(\pi(\lH),W)$; then one can find in $W$ a common eigenvector for all the $\pi(H)$. This vector is the one we were looking for.
\end{proof}

\begin{corollary}
Let $\lG$ be a solvable Lie group and $\pi$ a representation of $\lG$ on a finite dimensional vector space $V$. Then there exists a basis $\{e_1,\ldots,e_n\}$ of $V$ in which all the endomorphism $\pi(X)$, $X\in\lG$ are upper triangular matrices.
\label{cor:de_Lie_Vu}
\end{corollary}

\begin{proof}
Consider $e_1\neq 0\in V$, a common eigenvector of all the $\pi(X)$, $X\in\lG$. We consider $E_1=\Span\{e_1\}$. The representation $\pi$ induces a representation $\pi_1$ of $\lG$ on the space $V/E_1$. If $V/E_1\neq\{0\}$, we have a $e_2\in V$ such that $(e_2+E_1)\in V/E_1$ is an eigenvector of all the $\pi_(X)$.

In this manner, we build a basis $\{e_1,\ldots,e_n\}$ of $V$ such that $\pi(X)e_i=0\mod(e_1,\ldots,e_i)$ for all $X\in\lG$. In this basis, $\pi(X)$ has zeros under the diagonal.
\end{proof}

\begin{theorem}
Let $V$ be a real or complex vector space and $\lG$, a subalgebra of $\gl(V)$ made up with nilpotent elements. Then

\begin{enumerate}
\item $\lG$ is nilpotent;
\item $\exists v\neq 0$ in $V$ such that $\forall Z\in\lG$, $Zv=0$;
\item There exists a basis of $V$ in which the elements of $\lG$ are matrices with only zeros under the diagonal.
\end{enumerate}
\label{tho:trois_nil}
\end{theorem}

\begin{proof}
\subdem{First item} We consider a $Z\in\lG$ and we have to see that $\ad_{\lG}Z$ is a nilpotent endomorphism of $\lG$. Be careful on a point: an element $X$ of $\lG$ is nilpotent as endomorphism of $V$ while we want to prove that $\ad X$ is nilpotent as endomorphism of $\lG$. We denote by $L_Z$ and $R_Z$, the left and right multiplication; since we are in a matrix algebra, the bracket is given by the commutator: $\ad Z=L_Z-R_Z$. We have
\begin{equation}
(\ad Z)^p(X)=\sum_{i=0}^p(-1)^p \binom{p}{i}  Z^{p-i}XZ^i
\end{equation}
There exists a $k\in\eN$ such that $Z^k=0$. For this $k$, $(\ad Z)^{2k+1}$ is a sum of terms of the form $Z^{p-i}XZ^i$ : either $p-i$ either $i$ is always bigger than $k$. But $\ad_{\lG}Z$ is the restriction of $\ad Z$ (which is defined on $\gl(V)$) to $\lG$. Then $\lG$ is nilpotent.

\subdem{Second item} Let $r=\dim\lG$. If $r=1$, we have only one $Z\in\lG$ and $Z^k=0$ for a certain (minimal) $k\in\eN$. We take $v$ such that $w=Z^{k-1}v\neq 0$ (this exists because $k$ is the minimal natural with $Z^k=0$). Then $Zw=0$.

Now we suppose that the claim is valid for any algebra with dimension less than $r$. Let $\lH$ be a strict subalgebra of $\lG$ with maximal dimension. If $H\in\lH$, $\ad_{\lG}H$ is a nilpotent endomorphism of $\lG$ which sends $\lH$ onto itself. Thus $\ad_{\lG}H$ induces a nilpotent endomorphism $H^*$ on the vector space $\lG/\lH$. We consider the set $\mA=\{H^*\tq H\in\lH\}$; this is a subalgebra of $\gl(\lG/\lH)$ made up with nilpotent elements which has dimension strictly less than $r$.

The induction assumption gives us a non zero $u\in \lG/\lH$ which is sent to $0$ by all $\mA$, i.e. $(\ad_{\lG}H)u=0$ in $\lG/\lH$. In other words, $u\in\lG\setminus\lH$ is such that $(\ad_{\lG}H)u\in\lH$.

The space $\lH+\eK X$ (here, $\eK$ denotes $\eR$ or $\eC$) of $\lG$ is a subalgebra of $\lG$. Indeed, with obvious notations,
\begin{equation}\label{eq:H_k_X}
[H+kX,H'+k'X]=[H,H']+\ad H(k'X)-\ad H'(kX)+kk'[X,X].
\end{equation}
The first term lies in $\lH$ because it is a subalgebra; the second and third therms belongs to $\lH$ by definition of $X$. The last term is zero. Since $\lH$ is maximal, $\lH+\eK X=\lG$. Then \eqref{eq:H_k_X} shows that $\lH$ is also an ideal. Now we consider
\[
  W=\{e\in V\tq\forall H\in\lH, He=0\}.
\]
Since $\dim\lH< r$, $W\neq\{0\}$ from our induction assumption. Furthermore, for $e\in W$, $HXe=[H,X]e+XHe=0$. Then $X\cdot W\subset W$. The restriction of $X$ to $W$ is nilpotent. Then there exists a $v\in W$ such that $Xv=0$. For him $Hv=0$ because $v\in W$ and $Xv=0$ by definition of $X$. Then $Gv=0$ for any $G\in\lH+\eK X=\lG$.

\subdem{Third item} Let $e_1$ be a non zero vector in $V$ such that $Ze_1=0$ for any $Z\in\lG$ (the existence comes from the second item). We consider $E_1=\Span e_1$. Any $Z\in\lG$ induces a nilpotent endomorphism $Z^*$ on the vector space $V/E_1$. If $V/E_1\neq\{0\}$, we take a $e_2\in V\setminus E_1$ such that $e_2+E_1\in V/E_1$ fulfils $Z^*(e_2+E_1)=0$ for all $Z\in\lG$. By going on so, we have $Ze_1=0$, $Ze_i=0\mod(e_1,\ldots,e_{i-1})$. In this basis, the matrix of $Z$ has zeros on and under the diagonal.
\end{proof}

\begin{corollary}
Let us consider $V$, a finite dimensional vector space on $\eK$ and $\lG$, a subalgebra of $\gl(V)$ made up with nilpotent elements. Then if $s\geq\dim V$ and $X_i\in\lG$, we have $X_1X_2\ldots X_s=0$.
\label{cor:nil_XXX}
\end{corollary}

\begin{proof}
We write the $X_i$'s in a basis where they have zeros on and under the diagonal. It is rather easy to see that each product push the non zero elements into the upper right corner.
\end{proof}

\begin{corollary}
A nilpotent algebra is solvable.
\end{corollary}

\begin{proof}
The algebra $\ad_{\lG}(\lG)$ is a subalgebra of $\gl(\lG)$ made up with nilpotent endomorphisms of $\lG$. The product of $s$ (see notations of previous corollary) such endomorphism is zero. In particular $\lG$ is solvable.
\end{proof}

We recall the definition of the central decreasing sequence: $\lA^0=\lA$, $\lA^{p+1}=[\lA,\lA^p]$.

\begin{corollary}
A Lie algebra $\lA$ is nilpotent if and only if $\lA^m=\{0\}$ for $m\geq\dim\lA$.
\label{cor:nil_Gn}
\end{corollary}

\begin{proof}
The direct sense is easy: we use corollary \ref{cor:nil_XXX} with $\lG=\ad(\lA)$ ($\dim\lG=\dim\lA$). Since $\lG$ is nilpotent, for any $X_i\in\lG$ we have $X_1\ldots X_s$, so that $\lA^m=0$. The inverse sense is trivial.

\end{proof}

\begin{corollary}
A nilpotent Lie algebra $\lA\neq\{0\}$ has a non zero center
\end{corollary}

\begin{proof}
If $m$ is the smallest natural such that $\lA^m=0$, $\lA^{m-1}$ is in the center.
\end{proof}


\begin{lemma}
If $\lI$ and $\lJ$ are ideals in $\lG$, then we have a canonical isomorphism $\dpt{\psi}{(\lI+\lJ)/\lJ}{\lI/(\lI\cap\lJ)}$ given by
\[
  \psi([x])=\cloi
\]
if $x=i+j$ with $i\in\lI$ and $j\in\lJ$. Here classes with respect to $\lJ$ are denoted by $[.]$ and the one with respect to $(\lI\cap\lJ)$ by a bar.
\label{lem:pre_trois_resoluble}
\end{lemma}

\begin{proof}
We first have to see that $\psi$ is well defined. If $x'=i+j+j'$, $\psi([x])=\cloi$ because $j+j'\in\lJ$. If $x=i'+j'$ (an other decomposition for $x=i+j$), $\cloi=\cloj$, $j'-j=i-i'\in\lJ\cap\lI$. Then $\cloi=\overline{i'+j'-j}=\cloip$.

Now it is easy to see that $\psi$ is a homomorphism.
\end{proof}

\begin{proposition}
Let $\lG$ and $\lG'$ be Lie algebras. 

\begin{enumerate}
\item If $\lG$ is solvable then any subalgebra is solvable and if $\dpt{\phi}{\lG}{\lG'}$ is a Lie algebra homomorphism, then $\phi(\lG)$ is solvable in $\lG'$.

\item  If $\lI$ is a solvable ideal in $\lG$ such that $\lG/\lI$ is solvable, then $\lG$ is solvable.
\item If $\lI$ and $\lJ$ are solvable ideals in $\lG$, then $\lI+\lJ$ is also a solvable ideal in $\lG$.
\end{enumerate}
\label{prop:trois_resoluble}
\end{proposition}

\begin{proof}
\subdem{First item}
If $\lH$ is a subalgebra of $\lG$, then $\dD^k\lH\subset\dD^k\lG$, so that $\lH$ is solvable. Now consider $\lH=\phi(\lG)\subset\lG'$. This is a subalgebra of $\lG'$ because $[h,h']=[\phi(g),\phi(g')]=\phi([g,g'])\in\lH$. It is clear that $\dD(\phi(\lG))\subset\phi(\dD(\lG))$ and
\begin{equation}
\dD^2(\phi(\lG))=\dD\big( \dD\phi(\lG) \big)
                \subset\dD(\phi\dD(\lG))  
        \subset\phi\dD\dD(\lG) 
        =\phi(\dD^2(\lG)).
\end{equation}
Repeating this argument, $\dD^k(\lH)\subset\phi(\dD^k\lG)$. So $\lH$ is also solvable. Note that $\phi([g,g'])=[\phi(g),\phi(g')]\subset\dD(\pi(\lG))$. Then 
\begin{equation}
  \dD^k\pi(\lG)=\pi(\dD^k\lG).
\end{equation}

\subdem{Second item}
Let $n$ be the smallest integer such that $\dD^n(\lG/\lI)=0$; we look at the canonical homomorphism $\dpt{\pi}{\lG}{\lG/\lI}$. This satisfies $\dD^n(\pi(\lG))=\pi(\dD^n\lG)=0$. Then $\dD^n(\lG)\subset\lI$. If $\dD^m\lI=0$, then $\dD^{m+n}\lG=0$.

\subdem{Third item}
The space $\lI/(\lI\cap\lJ)$ is the image of $\lI$ by a homomorphism, then it is solvable and $(\lI+\lJ)/\lJ$ is also solvable. The second item makes $\lI+\lJ$ solvable.
\end{proof}

Now we consider $\lG$, any Lie algebra and $\lS$ a maximum solvable ideal i.e. it is included in none other solvable ideal. Let us consider $\lI$, an other solvable ideal in $\lG$. Then $\lI+\lS$ is a solvable ideal; since $\lS$ is maximal, $\lI+\lS=\lS$. Thus there exists an unique maximal solvable ideal which we call the \defe{radical}{radical!of a Lie algebra} of $\lG$. It will be often denoted by $\Rad\lG$. If $\beta$ is a symmetric bilinear form, his \defe{radical}{radical!of a quadratic form} is the set
\begin{equation}
  S=\{x\in\lG\tq\beta(x,y)=0\;\forall y\in\lG\}.
\end{equation}
The form $\beta$ is nondegenerate if and only if $S=\{0\}$. 

\begin{proposition}
Let $\lG$ and $\lG'$ be Lie algebras. 

\begin{enumerate}
\item If $\lG$ is nilpotent, then his subalgebras are nilpotent and if $\dpt{\phi}{\lG}{\lG'}$ is a Lie algebra homomorphism, then $\phi(\lG)$ is nilpotent.

\item If $\lG/\mZ(\lG)$ is nilpotent, then $\lG$ is nilpotent. For recall,
\[
   \mZ(\lG)=\{z\in\lG\tq [x,z]=0\;\forall x\in\lG\}.
\]

\item If $\lG$ is nilpotent, then $\mZ(\lG)\neq 0$.
\end{enumerate}
\label{prop:nil_homom_nil}
\end{proposition}

\begin{proof}
The proof of the first item is the same as the one of \ref{prop:trois_resoluble}. Now if $(\lG/\mZ(\lG))^n=0$, then $\lG^n/\mZ(\lG)=0$; thus $\lG^n\subset\mZ(\lG)$, so that $\lG^{n+1}=[\lG,\mZ(\lG)]=0$. Finally, if $n$ is the smallest natural such that $\lG^n=0$, then $[\lG^{n-1},\lG]=0$ and $\lG^{n-1}\subset\mZ(\lG)$.
\end{proof}

The condition to be nilpotent can be reformulated by $\exists n\in\eN$ such that $\forall X_i$, $Y\in\lG$,
\[
   (\ad X_1\circ\ldots\circ\ad X_n)Y=0,
\]
in particular for any $X\in\lG$, there exists a $n\in\eN$ such that $(\ad X)^n=0$. An element for which such a $n$ exists is \defe{ad-nilpotent}{ad-nilpotent@$\ad$-nilpotent}. If $\lG$ is nilpotent, then all his elements are ad-nilpotent.

Some results without proof :

\begin{lemma}
If $X\in\gl(V)$ is a nilpotent endomorphism, then $\ad X$ is nilpotent.
\label{lem:pre_Engel}
\end{lemma}

\begin{remark}
The inverse implication is not true, as the unit matrix shows.
\end{remark}

\begin{theorem}[Engel]
\index{theorem!Engel}\index{Engel theorem}
A Lie algebra is nilpotent if and only if all his elements are ad-nilpotent.
\label{tho:Engel}
\end{theorem}

For a proof see \cite{SamelsonNotesLieAlg}.

\begin{remark}
The combination of these two last results makes that if $\lG\subset\gl(V)$ is made up with nilpotent endomorphisms of $V$, then $\lG$ is nilpotent as Lie algebra.
\label{rem:gl_V_nil}
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Flags and nilpotent Lie algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Here we give a ``flag description'' of some previous results. In particular the chain \eqref{eq:solvable_chaine}. If $V$ is a vector space of dimension $n<\infty$, a \defe{flag}{flag} in $V$ is a chain of subspaces $0=V_0\subset V_1\subset\ldots\subset V_{n-1}\subset V_n=\lG$ with $\dim V_k=k$. If $x\in\End{V}$ fulfils $x(V_i)\subset V_i$, then we say that $x$ \defe{stabilise}{stabiliser of a flag} the flag.

\begin{theorem}
If $\lG$ is a subalgebra of $\gl(V)$ in which the elements are nilpotent endomorphisms and if $V\neq 0$, then there exists a $v\in V$, $v\neq 0$ such that $\lG v=0$.
\end{theorem}

\begin{proof}
This is the second item of theorem \ref{tho:trois_nil}.
\end{proof}

\begin{corollary}
Under the same assumptions, there exists a flag $(V_i)$ stable under $\lG$ such that $\lG V_i\subset V_{i-1}$. In other words, there exists a basis of $V$ in which the matrices of $\lG$ are nilpotent; this basis is the one given by the flag.
\end{corollary}

\begin{proof}
Let $v\neq 0$ such that $\lG v=0$ which exists by the theorem and $V_1=\Span v$. We consider $W=V/V_1$; the action of $\lG$ on $W$ is also made up with nilpotent endomorphisms. Then we go on with $V_1$ and $W_1=W/V_2$,\dots
\end{proof}


\begin{lemma}
If $\lG$ is nilpotent and if $\lI$ is an non trivial ideal in $\lG$, then $\lI\cap\mZ(\lG)\neq 0$.
\end{lemma}

\begin{proof}
Since $\lI$ is an ideal, $\lG$ acts on $\lI$ with the adjoint representation. The restriction of an element $\ad X$ for $X\in\lG$ to $\lI$ is in fact a nilpotent element in $\gl(\lI)$. Then we have a $I\in\lI$ such that $\lG I=0$. Thus $I\in\lI\cap\mZ(\lG)$.
\end{proof}

\begin{theorem}
Let $\lG$ be a solvable Lie subalgebra of $\gl(V)$. If $V\neq 0$, then $V$ posses a common eigenvector for all the endomorphisms of $\lG$.
\label{tho:sol_ss_dem}
\end{theorem}

\begin{proof}
This is exactly the Lie theorem \ref{tho:Lie_Vu}
\end{proof}

\begin{corollary}[Lie theorem]\index{Lie!theorem}\index{theorem!Lie}
Let $\lG$ be a solvable subalgebra of $\gl(V)$. Then $\lG$ stabilize a flag of $V$.
\label{tho:Lie_Vd}
\end{corollary}

\begin{proof}

This corollary is the corollary given in \ref{cor:de_Lie_Vu}.

We consider $v_1$ the vector given by theorem \ref{tho:sol_ss_dem}. Since it is eigenvector of all $\lG$, $\Span v_1$ is stabilised by $\lG$. Next we consider $v_2$ in the complementary which is also a common eigenvector,\ldots
\end{proof}

\begin{corollary}
If $\lG$ is a solvable Lie algebra, then there exists a chain of ideals in $\lG$
\[
  0=\lG_0\subset\lG_1\subset\ldots\subset\lG_n=\lG
\]
with $\dim\lG_k=k$.
\end{corollary}

\begin{proof}
If $\dpt{\phi}{\lG}{\gl(V)}$ is a finite-dimensional representation of $\lG$, then $\phi(\lG)$ is solvable by proposition \ref{prop:nil_homom_nil}. Then $\phi(\lG)$ stabilises a flag of $V$. Now we take as $\phi$ the adjoint representation of $\lG$. A stable flag is the chain of ideals; indeed if $\lG_i$ is a part of the flag, then $\forall H\in\lG$ $\ad H\lG_i\subset\lG_i$ because the flag is invariant.
\end{proof}


\begin{corollary}
If $\lG$ is solvable then $X\in\dD\lG$ implies that $\ad_{\lG}X$ is nilpotent. In particular $\dD\lG$ is nilpotent.
\end{corollary}

\begin{proof}
We consider the ideals chain of previous corollary and an adapted basis: $\{X_1,\ldots,X_n\}$ is such that $\{X_1,\ldots,X_i\}$ spans $\lG_i$. In such a basis the matrices of $\ad(\lG)$ are upper triangular and it is easy to see that in this case, the matrices of $[\ad\lG,\ad\lG]$ are \emph{strictly} upper triangular: they have zeros on the diagonal. But $[\ad\lG,\ad\lG]=\ad_{\lG}[\lG,\lG]$. Then for $X\in\ad_{\lG}\dD\lG$, $\ad_{\lG}X$ is nilpotent. \emph{A fortiori}, $\ad_{\dD\lG}X$ is nilpotent and by the Engels'theorem \ref{tho:Engel}, $\dD\lG$ is nilpotent.
\end{proof}

The following lemma is computationally useful because it says that if $X$ is a nilpotent element of a Lie algebra, then $g\cdot X$ is also nilpotent with (at most) the same order.

\begin{lemma}
  The following formula
\begin{equation}
\ad(g\cdot X)^nY=g\cdot \ad(X)^n(g^{-1}\cdot Y)
\end{equation}
holds for all $g\in G$ and $X$,$Y\in\lG$,
\label{lem:nil_Ad}
\end{lemma}

The proof is a simple induction on $n$.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Semisimple Lie algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

A useful reference to go trough semisimple Lie algebras is \cite{Wisser}. Very few proofs, but the statements of all the useful results with explanations.

\begin{definition}
    A Lie algebra is \defe{semisimple}{semisimple!Lie algebra} if it has no proper abelian invariant Lie subalgebra. A Lie algebra is \defe{simple}{simple!Lie algebra} if it is not abelian and has no proper Lie subalgebra.
\end{definition}

In that definition, we say that a Lie subalgebra \( \lH\) is \defe{invariant}{invariant!Lie subalgebra} if \( \ad(\lG)\lH\subset\lH\).

There are a lot of equivalent characterisations. Here are some that are going to be proved (or not) in the next few pages. A Lie algebra is semisimple if an only if one of the following conditions is respected.
\begin{enumerate}
    \item
        The Killing form is nondegenerate.
    \item
        The radical of \( \lG\) is zero (theorem \ref{ThoRadicalEquivSS}).
    \item
        There are no abelian proper invariant subalgebra.
\end{enumerate}

\begin{probleme}
    I think that in the following I took the degenerateness of Killing as definition.
\end{probleme}

The Killing form is a convenient way to define a Riemannian metric on a semisimple\footnote{In this case, $B$ is nondegenerate.} Lie group.

\begin{corollary}
An automorphism of a semisimple Lie group is an isometry for the Killing metric. Stated in other words,
    \begin{equation}\label{eq:Aut_Iso}
        \Aut(G)\subset\Iso G.
    \end{equation}
\end{corollary}

\begin{proof}
    By lemma \ref{lem:auto_1}, if $f$ is an automorphism of $G$, $df$ is an automorphism of $\mG$. Now, by proposition \ref{prop:auto_2}, $f$ is an isometry of $G$.
\end{proof}

\begin{proposition}
Let $\lG$ be a semisimple Lie algebra, $\lA$ an ideal in $\lG$, and $\lA^{\perp}=\{X\in\lG\tq B(X,A)=0\forall A\in\lA\}$.
Then
\begin{enumerate}
\item $\lA^{\perp}$ is an ideal,
\item $\lG=\lA\oplus\lA^{\perp}$,
\item $\lA$ is semisimple,
\end{enumerate}
\label{prop:a_aperp}
\end{proposition}

\begin{proof}
\subdem{First item}
We have to show that for any $X\in\lG$ and $P\in\lA^{\perp}$, $[X,P]\in\lA^{\perp}$, or $\forall\, Y\in\lA$, $B(Y,[X,P])=0$. From invariance of $B$,
\[
  B(Y,[X,P])=B(P,[Y,X])=0.
\]
\subdem{Second item}
Since $B$ is nondegenerate, $\dim\lA+\dim\lA^{\perp}=\dim\lG$. Let us consider $Z\in\lG$ and $X$, $Y\in\lA\cap\lA^{\perp}$. We have $B(Z,[X,Y])=B([Z,X],Y)=0$. Then $[X,Y]=0$ because $B(Z,[X,Y])=0$ for any $Z$ and $B$ is nondegenerate. Thus $\lA\cap\lA^{\perp}$ is abelian. It is also an ideal because $\lA$ and $\lA^{\perp}$ are.

Now we consider $\lB$, a complementary of $\lA\cap\lA^{\perp}$ in $\lG$, $Z\in\lG$ and $T\in\lA\cap\lA^{\perp}$. The endomorphism $E=\ad T\circ\ad Z$ sends $\lA\cap\lA^{\perp}$ to $\{0\}$. Indeed consider $A\in\lA\cap\lA^{\perp}$; $(\ad Z) A\in\lA\cap\lA^{\perp}$ because it is an ideal, and then $(\ad T\circ\ad Z)A=0$ because it is abelian.

The endomorphism $E$ also sends $\lB$ to $\lA\cap\lA^{\perp}$ (it may not be surjective); then $\tr(\ad T\circ\ad Z)=0$ and $\lA\cap\lA^{\perp}=\{0\}$. Since $B$ is nondegenerate, $\dim\lA+\dim\lA^{\perp}=\dim\lG$. Then $\lA\oplus\lA^{\perp}=\lG$ is well a direct sum.
\subdem{Third item}
From lemma \ref{lem:Killing_descent_ideal}, the Killing form of $\lG$ descent to the ideal $\lA$; then it is also nondegenerate and $\lA$ is also semisimple.
 \end{proof}

\begin{corollary}
A semisimple Lie algebra has center $\{0\}$.
\label{cor:ss_no_centre}
\end{corollary}

\begin{proof}
If $Z\in\ker\lG$, $\ad Z=0$. So $B(Z,X)=0$ for any $X\in\lG$. Since $B$ is nondegenerate, it implies $Z=0$.
\end{proof}


\begin{corollary}
If $\lG$ is a semisimple Lie algebra, it can be written as a direct sum
\[
   \lG=\lG_1\oplus\ldots\oplus\lG_r
\]
where the $\lG_i$ are simples ideals in $\lG$. Moreover each simple ideal in $\lG$ is a direct sum of some of them.
\label{cor:decomp_ideal}
\end{corollary}

\begin{proof}
If $\lG$ is simple, the statement is trivial. If it is not, we consider $\lA$, an ideal in $\lG$.
Proposition \ref{prop:a_aperp} makes $\lG=\lA\oplus\lA^{\perp}$. Since $\lA$ and $\lA^{\perp}$ are semisimple, we can once again brake them in the same way. We do it until we are left with simple algebras.

For the second part, consider $\lB$ a simple ideal in $\lG$ which is not a sum of $\lG_i$. Then $[\lG_i,\lB]\subset\lG_i\cap\lB=\{0\}$. Then $\lB$ is in the center of $\lG$. This contradict corollary \ref{cor:ss_no_centre}.
\end{proof}

\begin{proposition}
If $\lG$ is semisimple then 
\[
   \ad(\lG)=\partial(\lG),
\]
i.e. any derivation is an inner automorphism :
\label{prop:ss_derr_int}
\end{proposition}

\begin{proof}
We saw at page \pageref{pg:ad_subset_der} that $\ad(\lG)\subset\partial(\lG)$ holds without assumptions of (semi)simplicity. Now we consider $D$, a derivation: $\forall X\in\lG$,
\[
   \ad(DX)=[D,\ad X].
\]
Then $\ad(\lG)$ is an ideal in $\partial(\lG)$ because the commutator of $\ad X$ with any element of $\partial(\lG)$ still belongs to $\ad(\lG)$. Let us denote by $\lA$ the orthogonal complement of $\ad(\lG)$ in $\partial(\lG)$ (for the Killing metric). The algebra $\ad(\lG)$ is semisimple because of it isomorphic to $\lG$. Since the Killing form on $\ad(\lG)$ is nondegenerate, $\lA\cap\ad(\lG)=\{0\}$. Finally $D\in\lA$ implies $[D,\ad X]\in\lA\cap\ad(\lG)=\{0\}$. Then $\ad(DX)=0$ for any $X\in\lG$, so that $D=0$. This shows that $\lA=\{0\}$, so that $\ad(\lG)=\partial(\lG)$.
\end{proof}


If $V$ is a finite dimensional space, a subspace $W$ in $V$ is \defe{invariant}{invariant!vector subspace} under a subset $G\subset\Hom(V,V)$ if $sW\subset W$ for any $s\in G$. The space $V$ is \defe{irreducible}{irreducible!vector space} when $V$ and $\{0\}$ are the only two invariant subspaces. The set $G$ is \defe{semisimple}{semisimple} if any invariant subspace has an invariant complement. In this case, the vector space split into $V=\sum_iV_i$ with $V_i$ invariant and irreducible.

\begin{theorem}[Jordan decomposition]\index{Jordan decomposition}\index{decomposition!Jordan}
Any element $A\in\Hom(V,V)$ is decomposable in one and only one way as $A=S+N$ with $S$ semisimple and $N$ nilpotent and $NS=SN$. Furthermore, $S$ and $N$ are polynomials in $A$. More precisely :

If $V$ is a complex vector space and $A\in\Hom(V,V)$ with $\lambda_1,\ldots,\lambda_r$ his eigenvalues, we pose 
\[
V_i=\{ v\in V\tq (A-\lambda_i\mtu)^kv=0 \textrm{ for large enough $k$}\}.
\]
Then

\begin{enumerate}
\item $V=\sum_{i=1}^rV_i$,
\item each $V_i$ is invariant under $A$,
\item the semisimple part of $A$ is given by
\[
   S(\sum_{i=1}^rv_i)=\sum_{i=1}^r\lambda_iv_i,
\]
for $v_i\in V_i$,

\item the characteristic polynomial of $A$ is
\[
  \det(\lambda\mtu-A)=(\lambda-\lambda_1)^{d_1}\ldots(\lambda-\lambda_r)^{d_r}
\]
where $d_i=\dim V_i$ ($1\leq i\leq r$).
\end{enumerate}
\label{tho:jordan}
\end{theorem}

\subsection{Jordan decomposition}\index{Jordan decomposition}\index{decomposition!Jordan}
%--------------------------------

If $V$ is a finite dimensional vector space, we say that an element of $\End{V}$ is \defe{semisimple}{semisimple!endomorphism}\label{pg:def_semisimple} when it is diagonalisable. We know that two commuting semisimple endomorphism are simultaneously diagonalisable. So the sums and differences of semisimple elements still are semisimple. 

\label{pg:E_ij}Let $E_{kl}$ be the $(n+2)\times(n+2)$ matrix with a $1$ at position $(k,l)$ and $0$ anywhere else: $(E_{kl})_{ij}=\delta_{ki}\delta_{lj}$. An easy computation show that \nomenclature{$E_{ij}$}{Matrix full of zero's and $1$ at position $ij$}
\begin{equation}        \label{EqFormMulEmtr}       %\label{EqJsqnmunmtu}       Ce second label est certainement une erreur.
    E_{kl}E_{ab}=\delta_{la}E_{kb},
\end{equation}
and
\begin{equation}\label{comm_de_E}
    [E_{kl},E_{rs}]=\delta_{lr}E_{ks}-\delta_{sk}E_{rl}.
\end{equation}

Now we give a great theorem without proof.

\begin{theorem}[Jordan decomposition]
Let $V$ be a finite dimensional vector space and $x\in\End{V}$. 

\begin{enumerate}
\item There exists one and only one choice of $x_s,x_n\in\End(V)$ such that $x=x_s+x_n$, $x_s$ is semisimple, $n_n$ is nilpotent and $[x_s,x_n]=0$.

\item There exists polynomials $p$ and $q$ without independent term such that $x_s=p(x)$, $x_n=q(x)$; in particular if $y\in\End{V}$ commutes with $x$, then it commutes with $x_s$ and $x_n$.

\item If $A\subset B\subset V$ are subspaces of $V$ and if $x(B)\subset A$, then $x_s(B)\subset A$ and $x_n(B)\subset A$.
\end{enumerate}
\label{prop:Jordan_decomp}
\end{theorem}

A proof can be found in \cite{}<++>

As an example consider the adjoint representation of $\gl(V)$. As seen in lemma \ref{lem:pre_Engel}, if $x\in\gl(V)$ is nilpotent, then $\ad x$ is also nilpotent.

\begin{lemma}
If $x\in\gl(V)$ is semisimple, then $\ad x$ is also semisimple.
\end{lemma}

\begin{proof}
We choose a basis $\{v_1,\cdots,v_n\}$ of $V$ in which $x$ is diagonal with eigenvalues $a_1,\ldots,a_n$. For $\gl(V)$, we consider the basis $\{E_{ij}\}$ in which $E_{ij}$ is the matrix with a $1$ at position $(i,j)$ and zero anywhere else. This satisfies $[E_{kl},E_{rs}]=\delta_{lr}E_{ks}-\delta_{sk}E_{rl}$. We easily check that $E_{kl}(v_i)=\delta_{li}v_k$. Since we are in a matrix algebra, the adjoint action is the commutator: $(\ad x)E_{ij}=[x,E_{ij}]$; as we know that $x=a_kE_{kk}$, 
\begin{equation}
 (\ad x)E_{ij}=a_k[E_{kk},E_{ij}]=(a_i-a_j)E_{ij}
\end{equation}
which proves that $\ad x$ has a diagonal matrix in the basis $\{E_{ij}\}$ of $\gl(V)$. Furthermore, we have an explicit expression for his matrix: the eigenvalues are $(a_i-a_j)$.

\end{proof}

\begin{lemma}
Let $x\in\End{V}$ with his Jordan decomposition $x=x_s+x_n$. Then the Jordan decomposition of $\ad x$ is
\begin{equation}\label{eq:ad_x_xs_xn}
   \ad x=\ad x_s+\ad x_n.
\end{equation}
\label{lem:Jordan_ad}
\end{lemma}

\begin{proof}
We already know that $\ad x_s$ is semisimple and $\ad x_n$ is nilpotent. They commute because $[\ad x_s,\ad x_n]=\ad[x_s,x_n]=0$. Then the unicity part of Jordan theorem \ref{prop:Jordan_decomp} makes \eqref{eq:ad_x_xs_xn} the Jordan decomposition of $\ad x$.
\end{proof}

\subsection{Cartan criterion}
%----------------------------

Let us recall a result: $\dD\lG=\lG^1$, $[\dD\lG,\dD\lG]\subset\lG^2$; then $\dD^k\lG\subset\lG^k$. Thus if $\lG$ is nilpotent, it is solvable. On the other hand, by the Engel theorem \ref{tho:Engel}, $\dD\lG$ is nilpotent if and only if all the $\ad_{\dD\lG}x$ are nilpotent for $x\in\dD\lG$.

\begin{lemma}
Let $A\subset B$ be two subspace of $\gl(V)$ with $\dim V<\infty$. We pose 
\[
   M=\{x\in\gl(V)\tq [x,B]\subset A\},
\]
and we suppose that $x\in M$ verify $\tr(x\circ y)=0$ for all $y\in M$. Then $x$ is nilpotent.
\label{lem:M_nil}
\end{lemma}

\begin{proof}
We use the Jordan decomposition $x=x_s+x_n$ and a basis in which $x_s$ takes the form $diag(a_1,\ldots,a_m)$; let $\{v_1,\ldots,v_m\}$ be this basis. We denotes by $E$ the vector space on $\eQ$ spanned by $\{a_1,\ldots,a_m\}$. We want to prove that $x_s=0$, i.e. $E=0$. Since $E$ has finite dimension, it is equivalent to prove that its dual is zero. In other words, we have to see that any linear map $\dpt{f}{E}{\eQ}$ is zero.

We consider $y\in\gl(V)$, an element whose matrix is $diag(f(a_1),\ldots,f(a_m))$ and $(E_{ij})$, the usual basis of $\gl(V)$. We know that 
\begin{subequations}
\begin{align}
  (\ad x_s)E_{ij}&=(a_i-a_j)E_{ij},\\
  (\ad y)E_{ij}&=(f(a_i)-f(a_j))E_{ij}.
\end{align}
\end{subequations}
It is always possible to find a polynomial $r$ on $\eR$ without constant term such that $r(a_i-a_j)=f(a_i)-f(a_j)$. Note that this is well defined because of the linearity of $f$ : if $a_i-a_j=a_k-a_l$, then $f(a_i)-f(a_j)=f(a_k)-f(a_l)$. Since $\ad x_s$ is diagonal, $r(\ad x_s)$ is the matrix with $r(\ad x_s)_{ii}$ on the diagonal and zero anywhere else. Then $r(\ad x_s)=\ad y$. By lemma \ref{lem:Jordan_ad}, $\ad x_s$ is the semisimple part of $\ad x$, then $\ad y$ is  a polynomial without constant term with respect to $\ad x$ (second point of theorem \ref{prop:Jordan_decomp}).

Since $(\ad y)B\subset A$, $y\in M$ and $\tr(xy)=0$. It is easy to convince ourself that the $s_n$ part of $x$ will not contribute to the trace because $x_n$ is strictly upper triangular and $y$ is diagonal. From the explicit forms of $x_s$ and $y$,
\[
  \tr(xy)=\sum_ia_if(a_i)=0.
\]
This is a $\eQ$-linear combination of element of $E$ : we have to see it as $a_i$ being a basis vector and $f(a_i)$ a coefficient, so that we can apply $f$ on both sides to find $0=\sum_if(a_i)^2$. Then for all $i$, $f(a_i)=0$, so that $f=0$ because  the $a_i$ spans $E$.
\end{proof}


\begin{theorem}[Cartan criterion]
Let $\lG$ be a subalgebra of $\gl(V)$. We suppose that $\tr(xy)=0$ $\forall x\in\dD\lG, y\in\lG$. Then $\lG$ is solvable.
\end{theorem}

\begin{proof}
It is sufficient to prove that $\dD\lG$ is nilpotent indeed if we write $\dD^k\lG\subset\lG^k$ with $\dD\lG$ instead of $\lG$, $\dD^{k+1}\lG\subset(\dD\lG)^k$. If $\dD\lG$ is nilpotent, $(\dD\lG)^n=0$ and $\dD^{n+1}\lG=0$ so that $\lG$ is solvable. 

Let us consider $x\in\dD\lG$. We have to prove that it is ad-nilpotent (see the Engel theorem \ref{tho:Engel}). Let $A=\dD\lG$, $B=\lG$ and $M=\{x\in\gl(V)\tq [x\lG]\subset\dD\lG\}$. By definition of $\dD\lG$, $\lG\subset M$. The lemma \ref{lem:M_nil} will conclude that $x\in\dD\lG$ is nilpotent if $\tr(xy)=0$ for any $y\in M$. Here we just have this equality for $y\in\lG$. 

A typical generator of $\dD\lG$ is $[x,y]$ with $x$, $y\in\lG$. Take a $z\in M$; by the formula $\tr([x,y]z)=\tr(x[y,z])$, the trace that we have to check is
\begin{equation}
  \tr([x,y]z)=\tr(x[y,z])
             =\tr([y,z]x).
\end{equation}
But with $z\in M$, $[y,z]\in\dD\lG$, then $\tr([x,y]z)=\tr([y,z]x)=0$. Thus we are in the situation of the lemma.
\end{proof}


\begin{corollary}\label{cor:ad_g_sol}
A Lie algebra $\lG$ for which $\tr(\ad x\circ\ad y)=0$ for all $x\in\dD\lG$, $y\in\lG$ is solvable.
\end{corollary}

\begin{proof}
We consider $\lH=\ad\lG$; this is a subalgebra of $\gl(V)$ such that $a\in\dD\lH$ and $b\in\lH$ imply $\tr(ab)=0$. In order to see it, remark that $a\in\dD\lH$ can be written as $a=[\ad x,\ad y]=\ad[x,y]$ for certain $x$, $y\in\lG$. Then $\tr(ab)=\tr(\ad[x,y]\ad z)$ with $x$, $y$, $z\in\lG$; this is zero from the hypothesis. Then $\lH=\ad\lG$ is solvable.

It is also known that $\ker(\ad)=\mZ(\lG)$ is also solvable. Now we consider $\lM$ a complementary of $\mZ(\lG)$ in $\lG$ : $\lG=\mZ\oplus\lM$. The Lie algebra $\ad(\lM)$ is solvable and the homomorphism $\dpt{\phi}{\ad\lM}{\lM}$ defined by $\phi(\ad x)=x$ is well defined. From the first item of the proposition \ref{prop:trois_resoluble}, $\lM$ is solvable. With obvious notations, an element of $\dD\lM$ can be written as $[m,m']$ (because $\mZ(\lG)$ don't contribute to $\dD\lG$). Then $\dD\lG=\dD\lM$, so that $\lG$ is as much solvable than $\lM$.
\end{proof}


\begin{lemma}
The radical of a Lie algebra is non zero if and only if it has at least non zero abelian ideal.
\label{lem:ss_ideal}
\end{lemma}

\begin{proof}
The radical of $\lG$ is its unique maximal solvable ideal. An eventually non empty abelian ideal should be in the radical.

Let us now consider that the radical is non zero, and consider the derived series of $\Rad\lG$. Since $\Rad\lG$ is solvable, we can consider $n$, the minimal integer such that $\dD^n\Rad\lG=0$. Then $\dD^{n-1}\Rad\lG$ is a non zero abelian ideal.
\end{proof}


\begin{theorem}     \label{ThoRadicalEquivSS}
A Lie algebra is semisimple if and only if its radical is zero.
\end{theorem}

\begin{proof}
\subdem{Direct sense}
We suppose $\Rad\lG=0$ and we consider $S$, the radical of the Killing form :
\[
   S=\{X\in\lG\tq B(X,Y)=0\,\forall Y\in\lG\}.
\]
By definition, for any $X\in S$ and $Y\in\lG$, $\tr(\ad X\circ\ad Y)=0$. The Cartan criterion makes $\ad S$ solvable and the corollary \ref{cor:ad_g_sol} makes $S$ solvable.

Now, the $ad$-invariance of the Killing form turns $S$ into an ideal, so that $S\subset\Rad(\lG)$ because any solvable ideal is contained in $\Rad\lG$. From the assumptions, $\Rad S=0$, then $S\subset\Rad\lG=0$. This shows that the Killing form is nondegenerate.

\subdem{Inverse sense}
We suppose $S=0$ and we will show that any abelian ideal of $\lG$ is in $S$. In this case, if $A$ is a solvable ideal with $\dD^nA=0$, then $\dD^{n-1}A$ is an abelian ideal, so that $\dD^{n-1}A=0$. By induction, $A=0$.

Let $I$ be an abelian ideal of $\lG$, $X\in I$ and $Y\in\lG$. Then $\ad X\circ\ad Y$ is nilpotent because for $Z\in\lG$,
\begin{equation}
  (\ad X\ad Y\ad X\ad Y)Z=(\ad X\ad Y)\underbrace{( [X,[Y,Z]] )}_{=X_1\in I}
                         =(\ad X)\underbrace{[Y,X_1]}_{=X_2\in I}
             =(\ad X)X_2
             =0.
\end{equation}
Then $0=\tr(\ad X\ad Y)=B(X,Y)$ and $X\in S$, so that $I\subset S=0$.

\end{proof}

\subsection{More about radical}
%-------------------------------

If $\lG$ is a Lie algebra whose radical is $\lR$, we say that a subalgebra $\lS$ of $\lG$ is a \defe{Levi subalgebra}{levi subalgebra} if $\lG=\lR\oplus\lS$.

Any Lie algebra posses a Levi subalgebra\quext{Reference needed.}.

\begin{lemma}
If $\lA$ is an ideal in a Lie algebra $\lG$, then 
\[
  \Rad\lA=(\Rad\lR)\cap\lA.
\]
\label{lem:rad_ideal}
\end{lemma}

Before to begin the proof, let us recall that lemma \ref{lem:pre_trois_resoluble} gives us an isomorphism $\dpt{\psi}{(\lA+\lB)/\lA}{\lB/(\lA\cap\lB)}$ when $\lA$ and $\lB$ are ideals in $\lG$.

\begin{proof}[Proof of the lemma]
If $\lR$ is the radical of $\lG$, then the radical of $\lG/\lR$ is zero, so that $\lR/\lR$ is semisimple. Let $\lA$ be an ideal in $\lG$, then $(\lA+\lR)/\lR$ is an ideal in the semisimple Lie algebra $\lG\lR$, so that it is also semisimple. From the isomorphism, $\lA/(\lA\cap\lR)$ is also semisimple and $\lA\cap\lR$ must contains the radical of $\lA$. Indeed if a solvable ideal of $\lA$ where not in $\lA\cap\lR$, then this should give rise to a non zero solvable ideal in $\lA/(\lA\cap\lR)$ although the latter is semisimple. Then $\lA\cap\lR=\Rad\lA$.
\end{proof}

\begin{proposition}
If $A$ is a compact group of automorphisms of the Lie algebra $\lG$, then there exists a Levi subalgebra of $\lG$ which is invariant under $A$.
\end{proposition}

\begin{proof}
Let $\lR$ be the radical of $\lG$; we will split our proof into two cases following $[\lR,\lR]=0$ or not.
\subdem{The radical is abelian}
In this first case we consider an induction with respect to the dimension of $\lG$. We consider $\olG=\lG/\lRlR$ and $\olR=\lR/\lRlR$ : these are algebras with one less dimension that $\lG$ and $\lR$. We denote by $\dpt{\pi}{\lG}{\olG}$ the natural projection.

We begin to prove that $\olR$ is the radical of $\olG$. It is clear from the Lie algebra structure on a quotient that $\olR$ is an ideal because $\lR$ is. It is also clear that $\olR$ is solvable. We just have to see that $\olR$ is maximal in $\olG$. For this, suppose that $\olR\cup\oX$ is a solvable ideal in $\olG$. Then it is easy to see that $\lR\cup X$ is an ideal in $\lG$. Taking commutators in $\olR\cup\oX$, we always finish in $\overline{0}\in\olG$, i.e. in $\lRlR$. Taking again some commutators, we finish on $0\in\lG$ because $\lR$ is solvable. This contradict the maximality of $\lR$.

Since $A$ is made up of automorphisms, it leaves $\lR$ invariant, so that it also acts on $\olG$ as an automorphism group: $a\oX=\overline{aX}$ for $a\in A$ and $X\in\lG$. From the induction assumption, we can find a Levi subalgebra $\olS$ in $\olG$ : $\olS\oplus\olR=\olG$. In this case, the radical of $\pi^{-1}(\olS)$ is $\lRlR$. Indeed in the one hand, $\olR\cap\olS=0$, so that $\pi^{-1}(\olR\cap\olS)=\lRlR$. In the other hand $\pi^{-1}(\olR\cap\olS)=\pi^{-1}(\olR)\cap\pi^{-1}(\olS)=\lR\cap\pi^{-1}(\olS)$. The lemma \ref{lem:rad_ideal} conclude that $\Rad\pi^{-1}(\olS)=\lRlR$.

Now $A$ is a compact group of automorphism which leaves invariant $\pi^{-1}(\olS)$, so we have a Levi subalgebra $\lS$ of $\pi^{-1}(\olS)$ invariant under $A$. We will see that this is in fact a Levi subalgebra of the whole $\lG$, i.e. we have to prove that $\lS\oplus\lR=\lG$. From the definition of $\lS$, 
\[
   \lS\oplus\lRlR=\pi^{-1}(\olS),
\]
and by definition of $\olS$,
\[
   \olS\oplus\frac{\lR}{\lRlR}=\olG.
\]
Then
\begin{equation}
  \lG=\pi^{-1}(\olS)\oplus\lR+\lRlR
     =\lS\oplus\lRlR\oplus\lR+\lRlR
     =\lS\oplus\lR.
\end{equation}

We can now pass to the second case: $\lRlR=0$.
\subdem{The radical is not abelian}
Let $\lS_0$ and $\lS$ be Levi subalgebras of $\lG$. For $X\in\lS_0$, we write
\[
   X=f(X)+X_{\lS}
\]
with respect to the decomposition $\lG=\lS\oplus\lR$. This defines a linear map $\dpt{f}{\lS_0}{\lR}$. For any $X$, $Y\in\lS_0$, $[X_{\lS},X_{\lS}]=[X,Y]-[X,f(y)]-[f(X),Y]$ because $\lR$ is abelian. Since\quext{C'est pas clair pourquoi on a \c a.}, $[X_{\lS},X_{\lS}]=[X,Y]_{\lS}$,
\begin{equation}\label{eq:f_presque_isom}
f([X,Y])=[X,f(Y)]-[f(X),Y].
\end{equation}
Now let us consider a map $\dpt{f}{\lS_0}{\lR}$ which satisfy this equation. Then the map $X\to X-f(X)$ is an isomorphism between $\lS_0$ and his image which is a Levi subalgebra of $\lG$. Indeed
\begin{equation}
\begin{split}
[X,Y]&\to[X,Y]-f([X,Y])\\
     &=[X,Y]-[X,f(Y)]-[f(X),Y]\\
     &=[X-f(X),Y-f(Y)].
\end{split}
\end{equation}
Now we consider $V$, the space of all the linear maps $\lS_0\to\lR$ which fulfil the condition \eqref{eq:f_presque_isom}. We have a bijection between $V$ and the Levi subalgebras of $\lG$ : for any Levi subalgebra we associate the map $f\in V$ given by $X=f(X)+X_{\lS}$.

So our proof can be reduced to find a fixed point of $V$ under the action of $A$. In order to do that, we will see that $A$ is a group of \emph{affine} transformations on $V$. Consider a $\alpha\in A$ and $f_0$, $f_0^{\alpha}$, $f^{\alpha}$ be the elements of $V$ corresponding to $\lS_0$, $\lS$ and $\alpha(\lS)$. We take a $X\in\lS_0$ and we denote by $\oalpha(X)$ the $\lS_0$-component of $\alpha(X)$ with respect to the decomposition $\lG=\lR\oplus\lS_0$ :
\[
   \alpha(X)=\oalpha(X)+\beta(X).
\]
This also defines $\dpt{\beta}{\lG}{\lR}$ and $-\beta(X)$ is the $\lR$-component of $\oalpha(X)$ with respect to $\lG=\lR\oplus\alpha(\lS_0)$. Since $f_0^{\alpha}$ just correspond to this decomposition, $f_0^{\alpha}(\oalpha(X))=-\beta(X)$, so that 
\begin{equation}
\begin{split}
\oalpha(X)&=f_0^{\alpha}(\oalpha(X))+\alpha(X)\\
          &=f_0^{\alpha}(\oalpha(X))+\alpha(f(X))-\alpha(f(X))+\alpha(X).
\end{split}
\end{equation}
Since $X-f(X)\in\lS$, $\alpha(X)-\alpha(f(X))\in\alpha(\lS)$, then $f_0^{\alpha}(\oalpha(X))+\alpha(X)$ is the $\lR$-component of $\oalpha(X)$ with respect to $\lG=\lR\oplus\alpha(\lS)$. Then 
\[
   f_0^{\alpha}(\oalpha(X))+\alpha(f(X))=f^{\alpha}(\oalpha(X))=f^{\alpha}(\oalpha(X)).
\]
Since $X$ was taken arbitrary, $f^{\alpha}=f^{\alpha}_0+\alpha\circ f\circ\oalpha^{-1}$. Then the map $V\to V$, $f\to f^{\alpha}$ is an affine transformation with translation equals to $f_0^{\alpha}$ and linear part being $f\to\alpha\circ f\circ\oalpha$.

A general result shows that a compact group of affine transformations on a vector space has a fixed point.

\end{proof}

\subsection{Compact Lie algebra}\label{pg:compact_Lie}
%-------------------------------

We consider $\lG$, a real Lie algebra and $\lH$, a subalgebra of $\lG$. Let $K^*$ be the analytic subgroup of $\Int(\lG)$ which corresponds to the subalgebra $\ad_{\lG}(\lH)$ of $\ad_{\lG}(\lG)$. 

\begin{definition}
We say that $\lH$ is \defe{compactly embedded}{compactly embedded} in $\lG$ if $K^*$ is compact. A Lie algebra is \defe{compact}{compact!Lie algebra}\index{Lie!algebra!compact} when it is compactly embedded in itself.
\end{definition}

The analytic subgroup of $\Int(\lG)$ which corresponds to $\ad_{\lG}(\lG)$, by definition, is $\Int(\lG)$. Then the compactness of $\lG$ is the one of $\Int(\lG)$.

\begin{remark}
The compactness notion on a Lie group is defined from the topological structure of the Lie group seen as a manifold. It is all but trivial that the compactness on a Lie group is related to the compactness on its Lie algebra; the proposition \ref{prop:alg_grp_compact} will however make the two notions related in the natural way.
\end{remark}

\begin{remark}
The topology on $K^*$ is not necessary the same as the induced one from $\Int(\lG)$ and $\Int(\lG)$ has also not necessary the induced topology from $\GL(\lG)$. However the next proposition will show that the compactness notion is well the one induced from $\GL(\lG)$.
\end{remark}

\begin{proposition}
We consider $\tK$, the same set and group as $K^*$, but with the induced topology from $\GL(\lG)$. Then $\tK$ is compact if and only if $K^*$ is compact.
\end{proposition}

Note however that $K^*$ and $\tK$ are not automatically the same as manifold.

\begin{proof}
\subdem{$K^*$ compact implies $\tK$ compact}
The identity map $\dpt{\iota}{K^*}{\GL(\lG)}$ is analytic, and then is continuous because $\Int(\lG)$ is by definition an analytic subgroup of $\GL(\lG)$ and $K^*$ an analytic subgroup of $\Int(\lG)$. If we have a covering of $\tK$ with open set $\mO_i\cap\tK$ of $\tK$ ($\mO_i$ is open in $\GL(\lG)$), the continuity of $\iota$ make the finite subcovering of $K^*$ good for $\tK$.
\subdem{$\tK$ compact implies $K^*$ compact}
If $\tK$ is compact, then it is closed in $\GL(\lG)$. As set, $K^*$ is closed in $\GL(\lG)$ and by definition it is connected. Then by the theorem \ref{tho:H_ferme}, $K^*$ is a topological subgroup of $\GL(\lG)$. Consequently, $K^*$ and $\tK$ are homeomorphic and they have same topology.
\end{proof}

A lemma without proof\quext{J'ai m\^eme pas trouv\'e d'\'enonc\'e de ce th\'eor\`eme.}.
\begin{lemma}
If $G$ is a compact group in $\GL(n,\eR)$, then there exists a $G$-invariant quadratic form on $\eR^n$.
\end{lemma}

\begin{proposition}     \label{ProplGcompactKillNeg}
Let $\lG$ be a real Lie algebra.

\begin{enumerate}
\item If $\lG$ is semisimple, then $\lG$ is compact if and only if  the Killing form is strictly negative definite.
\item If it is compact then it is a direct sum
\begin{equation}
   \lG=\mZ\oplus [\lG,\lG]
\end{equation}
where $\mZ$ is the center of $\lG$ and the ideal $[\lG,\lG]$ is compact and semisimple.
\end{enumerate}
\label{prop:compact_Killing}
\end{proposition}

\begin{proof}
\subdem{If the Killing form is nondegenerate}
We consider $\lG$, a Lie algebra whose Killing form is strictly negative definite. Up to some dilatations (and a sign), this is the euclidian metric. Then $O(B)$, the group of linear transformations which leave $B$ unchanged is compact in the topology of $\GL(\lG)$ : this is almost the rotations. From equation \eqref{eq:Aut_Iso}, $\Aut(\lG)\subset O(B)$. With this, $\Aut(\lG)$ is closed in a compact, then it is compact. Then $\Int(\lG)$ is closed in $\Aut(\lG)$ --here is the assumption of semi-simplicity-- and $\Int(\lG)$ is compact.
\subdem{If $\lG$ is compact}
Since $\lG$ is compact, $\Int(\lG)$ is compact in the topology of $\Aut(\lG)$; then there exists an $\Int(\lG)$-invariant quadratic form $Q$. In a suitable basis $\{X_1,\ldots,X_n\}$ of $\lG$, we can write this form as
\[
   Q(X)=\sum x_i^2
\]
for $X=\sum x_iX_i$. In this basis the elements of $\Int(\lG)$ are orthogonal matrices and the matrices of $\ad(\lG)$ are skew-symmetric matrices (the Lie algebra of orthogonal matrices). Let us consider a $X\in\lG$ and denote by $a_{ij}(X)$ the matrix of $\ad(X)$. We have
\begin{equation}
\begin{split}
  B(X,X)=\tr(\ad X\circ\ad X)
        =\sum_i\sum_ja_{ij}(X)a_{ji}(X)
    =-\sum_{ij}a_{ij}(X)^2\leq 0.
\end{split}
\end{equation}
Then the Killing form is negative definite\footnote{Here we use ``negative definite''\ and ``\emph{strictly} negative definite''; in some literature, the terminology is slightly different and one says ``\emph{semi} negative definite''\ and ``negative definite''.}. On the other hand, $B(X,X)=0$ implies $\ad(X)=0$ and $X\in\mZ(\lG)$. Thus $\lG^{\perp}\subset\mZ$. If $\lG$ is semisimple, this center is zero; this conclude the first item of the proposition.

Now $\mZ$ is an ideal and corollary \ref{cor:decomp_ideal} decomposes $\lG$ as
\begin{equation}
  \lG=\mZ\oplus\lG'.
\end{equation}
Let us suppose that the restriction of $B$ to $\lG'\times\lG'$ is actually the Killing form on $\lG'$ (we will prove it below). Then the Killing form on $\lG'$ is strictly negative definite; then $\lG'$ is compact.

Now we prove that the Killing form on $\lG$ descent to the Killing form on~$\lG'$. Remark that $\mZ$ is invariant under all the automorphism. Indeed consider $Z\in\mZ$, i.e.  $[X,Z]=0$. If $\sigma$ is an automorphism, 
\[
   [X,\sigma Z]=\sigma[\sigma^{-1} X,Z]=0.
\]
Here the difference between $\Int(\lG)$ and $\Aut(\lG)$ is the fact that $\Int(\lG)$ is compact; then we can construct a $\Int(\lG)$-invariant quadratic form $Q$, but not a $\Aut(\lG)$-invariant one. We consider an orthogonal complement (with respect to $Q$) $\lG'$ of $\mZ$:
\begin{equation}
   \lG=\lG'\oplus_{\perp}\mZ.
\end{equation}
The algebra $\lG'$ is also invariant because for any $Z\in\mZ$, 
\[
Q(Z,\sigma X)=Q(\sigma^{-1}(Z),X)=0.
\]
It is also clear that $\mZ$ is invariant under $\ad\lG$ because $(\ad X)Z=0$. Finally $\lG'$ is invariant as well under $\ad(\lG)$. Indeed $a\in\ad(\lG)$ can be written as $a=a'(0)$ for a path $a(t)\in\Int(\lG)$. We identify $\lG$ and his tangent space (as vector spaces),
\[
  aX=\Dsdd{ a(t)X }{t}{0}.
\]
If $X\in\lG'$, $a(t)X\in\lG'$ for any $t$ because $\lG'$ is invariant under $\Int(\lG)$\footnote{As physical interpretation, if something is invariant under a group of transformations, it is invariant under the infinitesimal transformations as well.}. Thus $a(t)X$ is a path in $\lG'$ and his derivative is a vector in $\lG'$. 

All this make $\lG'$ an ideal in $\lG$; then the Killing form descent by lemma \ref{lem:Killing_descent_ideal}. Now if $X\in\lG$, we have
\begin{equation}
  B(X,X)=\tr(\ad X\circ\ad X)
        =\sum_{ij}a_{ij}(X)a_{ji}(X)
    =-\sum_{ij}a_{ij}(X)^2;
\end{equation}
then $B(X,X)\leq 0$ and the equality holds if and only if $\ad X=0$ i.e. if and only if $X\in\mZ$. Thus $B$ is strictly negative definite on $\lG'$.

Up to now we have proved that $\lG'$ is semisimple (because $B$ is nondegenerate) and compact (because $B$ is strictly negative definite).

It remains to be proved that $\lG'=[\lG,\lG]=\dD(\lG)$. From corollary \ref{cor:decomp_ideal}, $\dD\lG$ has a complementary $\lA$ which is also an ideal: $\lG=\dD\lG+\lA$. Then $[\lG,\lA]\subset\dD\lG$ and $[\lG,\lA]\subset\lA\cap\dD\lG:\{0\}$. Then $\lA\subset\mZ$, so that
\begin{align}\label{eq:G_Z_B}
   \lG=\mZ+\dD\lG&&\text{(non direct sum)}.
\end{align}
Now we have to prove that the sum is actually direct. The ideal $\mZ$ has a complementary ideal $\lB$ : $\lG=\mZ\oplus\lB$ and
\[
   \dD\lG=[\lG,\lG]\subset\underbrace{[\lG,\mZ]}_{=0}+[\lG,\lB]\subset\lB.
\]
Then $\dD\lG\subset\lB$ which implies that $\dD\lG\cap\mZ=\{0\}$ because the sum $\lG=\mZ\oplus\lB$ is direct. Then the sum \eqref{eq:G_Z_B} is direct.

\end{proof}

\begin{proposition}
A real Lie algebra $\lG$ is compact if and only if one can find a compact Lie group $G$ which Lie algebra is isomorphic to $\lG$.
\label{prop:alg_grp_compact}
\end{proposition}

\begin{proof}
\subdem{Direct sense} Since $\lG$ is compact, $\lG=\mZ\oplus\dD\lG$ with $\dD\lG=\lG'$ compact and semisimple; in particular, the center of $\lG'$ is $\{0\}$. Since $\mZ$ is compact and abelian, it is isomorphic to the torus $S^1\times\ldots\times S^1$. Since $\lG'$ is compact, $\Int(\lG')$ is compact, but the Lie algebra if $\Int(\lG')$ is --by definition--  $\ad(\lG')$. The center of a semisimple Lie algebra is zero; then $\ad X'=0$ implies $X=0$ (for $X\in\lG'$). Then $\ad$ is an isomorphism between $\lG'$ and $\ad\lG'$.

All this shows that --up to isomorphism-- $\mZ$ and $[\lG,\lG]$ are Lie algebras of compact groups. We know from lemma \ref{lemLeibnitz} that the Lie algebra of $G\times H$ is $\lG\oplus\lH$. Thus, here, $\lG$ is the Lie algebra of the compact group $S^1\times\ldots\times S^1\times\Int(\lG)$.
\subdem{Reverse sense}
We consider a compact group $G$ and we have to see the its Lie algebra $\lG$ is compact. If $G$ is connected, $\Ad_G$ is an analytic homomorphism from $G$ to $\Int(\lG)$. If $G$ is not connected, the Lie algebra of $G$ is $T_eG_0$ ($G_0$ is the identity component of $G$) where $G_0$ is connected and compact because closed in a compact.
\end{proof}
 
\begin{proposition}
Let $\lG$ be a real Lie algebra and $\mZ$, the center of $\lG$. We consider $\lK$, a compactly embedded in $\lG$. If $\lK\cap\mZ=\{0\}$ then the Killing form of $\lG$ is strictly negative definite on $\lK$.
\label{prop:K_Z_Killing}
\end{proposition}

\begin{proof}
Let $B$ be the Killing form on $\lG$ and $K$ the analytic subgroup of $\Int(\lG)$ whose Lie algebra is $\ad_{\lG}(\lK)$. By assumption, $K$ is a compact Lie subgroup of $\GL(\lG)$. Then there exists a quadratic form on $\lG$ invariant under $K$, and a basis in which the endomorphisms $\ad_{\lG}(T)$ for $T\in\lK$ are skew-symmetric because the matrices of $K$ are orthogonal. If the matrix of $\ad T$ is $(a_{ij})$, then
\begin{equation}
   B(T,T)=\sum_{ij}a_{ij}(T)a_{ji}(T)
         =-\sum_{ij}a_{ij}^2(T)\leq 0,
\end{equation}
and the equality hold only if $\ad T=0$ i.e. if $T\in\mZ$. From the assumptions, $\lK\cap\mZ=\{0\}$; then $B(T,T)=0$ if and only if $T=0$. 
\end{proof}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Cartan subalgebras in complex Lie algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecCartaninComplex}
About Cartan algebra, one can read \cite{Dragan,Berndt,Hochschild,SamelsonNotesLieAlg}.

In this section $\lG$ will always denotes a complex finite dimensional Lie algebra. 

\begin{definition}\label{PgDefCentralisateur}
    When \( \lH\) is a subalgebra of \( \lG\), the \defe{centralizer}{centralizer} of \( \lH\) is the set\nomenclature[G]{$\mZ(\lH)$}{the centralizer of \( \lH\)}
    \begin{equation}
        \mZ(\lH)=\{x\in\lG\tq [x,\lH]\subset\lH\}.
    \end{equation}
    More generally if $\lG$ is a Lie algebra and if $\lA$, $\lB$ are two subset of $\lG$, the centraliser of $\lA$ in $\lB$ is
    \begin{equation}
        \mZ_{\lB}(\lA)=\{X\in\lB\tq [X,\lA]=0\}.
    \end{equation}
    If $\lA$ is a subalgebra of $\lG$, its \defe{normalizer}{normalizer} is 
    \begin{equation}
        \lN_{\lA}=\{X\in\lG\tq [X,\lA]\subset\lA\}.
    \end{equation}
    One can check that $\lA$ is an ideal in $\lN_{\lA}$.
\end{definition}

\begin{definition}
A subalgebra $\lH$ of a Lie algebra $\lG$ is a \defe{Cartan subalgebra}{Cartan!subalgebra} if it is nilpotent and if it is its own centralizer: $[x,\lH]\subset\lH$ implies $x\in\lH$.
\end{definition}

Our first task is to show that every Lie algebra has a Cartan algebra.

\begin{lemma}[Primary decomposition theorem]
    Let \( V\) be a complex vector space and \( A\colon V\to V\) be linear map. Then we have the direct sum decomposition
    \begin{equation}        \label{EqPrimDecomTho}
        V=\bigoplus_{\lambda\in\eC}V_{\lambda}(A)
    \end{equation}
    where \( V_{\lambda}(A)=\{ v\tq (A-\lambda\mtu)^nv=0\text{ for some \( n\in\eN\)} \} \)
\end{lemma}
This is the result that restricts ourself to \emph{complex} Lie algebras when proving that Cartan subalgebras exist. Notice that the sum in \eqref{EqPrimDecomTho} is reduced to the eigenvalues of \( A\) since \( \lG_{\lambda}(A)=0\) when \( \lambda\) is not an eigenvalue. Indeed if \( \big( A-\lambda\mtu \big)^nY=0\) then \( (A-\lambda\mtu)^{n-1}Y\) is an eigenvector for \( A\) with eigenvalue \( \lambda\).

For any \( \lambda\in\eC\) and \( X\in\lG\) we consider the space
\begin{equation}
    \lG_{\lambda}(X)=\{ Y\in\lG\tq \big( \ad(X)-\lambda\mtu \big)^nY=0\text{ for some \( n\)} \}.
\end{equation}
The primary decomposition theorem implies the decomposition
\begin{equation}        \label{EqDecomplGpRimDecombijk}
    \lG=\bigoplus_{\lambda}\lG_{\lambda}(X)
\end{equation}
for each \( X\in\lG\). 

\begin{lemma}
    For each \( X\in\lG\) and \( \lambda,\mu\in\eC\) we have
    \begin{equation}
        \big[ \lG_{\lambda}(X),\lG_{\mu}(X) \big]\subset\lG_{\lambda+\mu}(X).
    \end{equation}
\end{lemma}

\begin{proof}
    Let \( X_{\lambda}\in\lG_{\lambda}(X)\) and \( X_{\mu}\in\lG_{\mu}(X)\). Using the fact that \( \ad(X)\) is a derivation we have
    \begin{equation}
        \ad(X)[X_{\lambda},X_{\mu}]-(\lambda+\mu)[X_{\lambda},X_{\mu}]=\Big[ \big( \ad(X)-\mu\mtu \big)X_{\lambda},X_{\mu} \Big]+\Big[ X_{\lambda},\big( \ad(X)+\mu\mtu \big)X_{\mu} \Big]
    \end{equation}
    and by induction\footnote{this is made more explicitly in the proof of theorem \ref{TholGCartalphaplusbeta}.} we find
    \begin{equation}
        \big( \ad Z-(\lambda+\mu)\mtu\big)^n[X_{\lambda},X_{\mu}]=\sum_{i=0}^{\infty}\binom{n}{i}[ (\ad Z-\lambda I)^iX_{\lambda} ,(\ad Z-\mu I)^{n-i}X\mu]
    \end{equation}
    which vanishes when \( n\) is large enough.
\end{proof}

We say that \( X\) is \defe{regular}{regular} if \( \dim\lG_0(X)\) is the smallest with respect to the others \( \dim\lG_0(Y)\).

The following proposition shows that every complex Lie algebra has a Cartan Lie subalgebra.
\begin{proposition}
    If $X$ is regular in \( \lG\) then the subalgebra \( \lG_0(X)\) is Cartan.
\end{proposition}

\begin{proof}
    Since \( X\in\lG_0(X)\) we have \( \ad(X)\lG_{\lambda}(X)\subset\lG_{\lambda}(X)\). Thus we see \( \ad(X)\) as a linear operator on \( \lG_{\lambda}(X)\). The operator \( \ad(X)|_{\lG_{\lambda}(X)}\) is nonsingular\footnote{it means that \( \ad(Y)\) is invertible.} when \( \lambda\neq 0\). Indeed all the eigenvalues of \( \ad(X)\) on \( \lG_{\lambda}(X)\) are equal to \( \lambda\) because  
    \begin{equation}
        \big( \ad(X)-\mu\mtu \big)Y=0
    \end{equation}
    implies \( Y\in\lG_{\mu}(X)\). If \( Y\in\lG_{\lambda}(X)\) it only occurs when \( \mu=\lambda\) since the sum \eqref{EqPrimDecomTho} is direct.

    For each eigenvalue \( \lambda\) we have a neighborhood \( \mU_{\lambda}\) of $X$ in \( \lG_0(X)\) such that for all \( Y\in\mU_{\lambda}\), \( \ad(Y)\) is nonsingular on \( \lG_{\lambda}(X)\). We consider \( \mU=\bigcap_{\lambda}\mU_{\lambda}\) which is a non empty open set since the intersection is taken over the eigenvalues of \( \ad(X)\) that are in finite numbers.

    Let us prove that the restriction to \( \lG_0(X)\) of the linear operator \( \ad(Y)\) is nilpotent for each \( Y\in\mU\). First we have 
    \begin{equation}        \label{EqLgzsubsetlzY}
        \lG_0(Y)\subseteq\lG_0(X)
    \end{equation}
    because by construction \( \ad(Y)\) cannot be nilpotent on the other spaces \( \lG_{\lambda}(X)\). But by hypothesis the element \( X\) is regular, thus the inclusion \eqref{EqLgzsubsetlzY} cannot be strict. Thus \( \lG_0(X)\subset\lG_0(Y)\) which means that \( \ad(Y)\) is nilpotent on \( \lG_0(X)\).

    Now the fact for \( \ad(Y)\) to be nilpotent means the vanishing of a polynomial determined by the coefficients of the matrix of \( \ad(Y)\). Since this polynomial vanishes on the open set \( \mU\), it vanishes identically, so that \( \ad(Y)\) is nilpotent on \( \lG_0(X)\). It results that \( \lG_0(X)\) is a \( \ad\)-nilpotent algebra and the Engel's theorem \ref{tho:Engel} concludes that \( \lG_0(X)\) is nilpotent.

    We still have to prove that \( \lG_0(X)\) is its own centralizer. Since \( \lG_0(X)\) is a subalgebra we have the inclusion
    \begin{equation}
        \lG_0(X)\subseteq\mZ\big( \lG_0(X) \big).
    \end{equation}
    Let \( Z\in\mZ\big( \lG_0(X) \big)\). For each \( Y\in\lG_0(X)\) we have \( [Z,Y]\in\lG_0(X)\). In particular with \( Y=X\) we have \( \ad(X)Z\in\lG_0(X)\). Thus
    \begin{equation}
        \ad(X)^nZ=\ad(X)^{n-1}\underbrace{\ad(X)Z}_{\in\lG_0(X)}
    \end{equation}
    and there exists a \( n\) such that \( \ad(X)^{n-1}\ad(X)Z=0\).
\end{proof}

If \( \lG\) is a Lie algebra, the group of \defe{inner automorphism}{inner!automorphism} is the subgroup of \( \Aut(\lG)\) generated by the elements of the form \(  e^{\ad(X)}\) with \( X\in\lG\). This definition is motivated in the context of matrix groups by the fact that when \( g= e^{Y}\in G\) and \( X\in\lG\) we have
\begin{equation}
    gXg^{-1}= e^{\ad(Y)}X.
\end{equation}
\begin{example}
    If 
    \begin{equation}
        \begin{aligned}[]
            g=\begin{pmatrix}
                \cos(t)    &   \sin(t)    &   0    \\
                -\sin(t)    &   \cos(t)    &   0    \\
                0    &   0    &   1
            \end{pmatrix},&&X=\begin{pmatrix}
                0    &   a    &   b    \\
                -a    &   0    &   0    \\
                -b    &   0    &   0
            \end{pmatrix},
        \end{aligned}
    \end{equation}
    then one checks that \( g= e^{Y}\) with
    \begin{equation}
        Y=\begin{pmatrix}
              0  &  t     &   0    \\
            -t    &   0    &   0    \\
            0    &   0    &   0
        \end{pmatrix}
    \end{equation}
    and 
    \begin{equation}
        gXg^{-1}= e^{\ad(Y)}X=\begin{pmatrix}
            0    &   a    &   b\cos(t)    \\
            -a    &   0    &   -b\sin(t)    \\
            -b\cos(t)    &   b\sin(t)    &   0
        \end{pmatrix}.
    \end{equation}
\end{example}

\begin{theorem}
    The group of inner automorphisms of \( \lG\) acts transitively on the set of Cartan subalgebras. 
\end{theorem}

For a proof, see \cite{SerreSSAlgebres}. In particular they have all the same dimension and the definition of the \defe{rank}{rank!of a complex Lie algebra} as the dimension of its Cartan algebra make sense. In \cite{SerreSSAlgebres} we have a more abstract definition of the rank, see page III-2.

\begin{proposition}     \label{PropCartanLzXtjs}
    If \( \lH\) is a Cartan subalgebra of the complex Lie algebra \( \lG\), there exists a regular element \( X\in\lG\) such that \( \lH=\lG_0(X)\).
\end{proposition}

For a proof, see \cite{SerreSSAlgebres}.

\begin{proposition}\label{prop:Cartan_max_nil}
A Cartan subalgebra is a maximal nilpotent subalgebra.
\end{proposition}

\begin{proof}
Let $\lH$ be a Cartan subalgebra of $\lG$ and $\lN$, a nilpotent algebra which contains $\lH$. Let $\{X_1,\ldots,X_n\}$ be a basis of $\lG$ chosen in such a way that the $p$ first vectors form a basis of $\lH$ while the $r$ first, a basis of $\lN$ ($r>p$ of course). As notational convention, the subscript $i,j$ are related to $\lH$ and $u,t$ to $\lN\ominus\lH$.

Let us first suppose $\dim\lN=\dim\lH+1$ and let $X_u$ be the basis vector of $\lN$ which is not in $\lH$. Since $\lH$ is Cartan, we can find $X_i\in\lH$ such that $Y=[X_u,X_i]\notin\lH$. Then $Y$ has a $X_u$-component and this contradict the fact that $\ad X_i$ is nilpotent.

The next case is $\lN=\lH\oplus X_u\oplus X_t$. In this case we can find a $X_i\in\lH$ such that $Y=[X_u,X_i]\notin\lH$. The fact to be nilpotent makes that $Y$ has no $X_u$-component, so that it has a $X_t$-component. Now it is clear that for any $X_j\in\lH$, $[Y,X_j]$ still has no $X_u$-component (because $(\ad X_i\circ\ad X_j)$ has to be nilpotent), but has also no $X_t$-component. Then for any $X\in\lH$, $[Y,X]\in\lH$ with $Y\notin\lH$. There is a contradiction.

Now the step to the general case is easy: if $\dim\lN=\dim\lH+m$, we  consider $X_1,\ldots,X_m\in\lH$ and $A=(\ad X_1\circ\ad X_m)X_u$. This is not in $\lH$ although $[A,X]\in\lH$ for any $X\in\lH$.
\end{proof}


\begin{proposition}
    If \( \lG\) is a semisimple Lie algebra, a subalgebra \( \lH\) is Cartan if and only if the two following conditions are satisfied:
    \begin{enumerate}
        \item
            \( \lH\) is a maximal abelian subalgebra
        \item
            the endomorphism \( \ad(H)\) is diagonalizable for every \( H\in\lH\).
    \end{enumerate}
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Root spaces in semisimple complex Lie algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecRootcomplexss}
In this section we particularize ourself to complex semisimple Lie algebras. A very good reference about complex semisimple algebras including the reconstruction \emph{via} the Cartan matrix and Chevalley-Weyl basis is \cite{SerreSSAlgebres}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Introduction and notations}
%---------------------------------------------------------------------------------------------------------------------------

Real and complex Lia algebras deserve quite different treatment with root space. We review here the main steps in both cases, emphasising the differences. We restrict ourself to semisimple Lie algebras. See \cite{Wisser}.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Complex Lie algebras}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

If \( \lG\) is a complex semisimple Lie algebra, we choose a Cartan subalgebra \( \lH\) and the root spaces are given by
\begin{equation}
    \lG_{\alpha}=\{ X\in\lG\tq [H,X]=\alpha(H)X \forall H\in\lH \}.
\end{equation}
The dimension of \( \lH\) is the rank of \( \lG\). Then the root space decomposition reads
\begin{equation}
    \lG=\lH\oplus\bigoplus_{\alpha\in\Phi}\lG_{\alpha}
\end{equation}
where \( \Phi\) is the set of roots.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Real Lie algebras}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

If \( \lG\) is a real semisimple Lie algebra we consider a Cartan involution and the Cartan decomposition \( \lG=\lK\oplus\lP\). Then we choose a maximally abelian subalgebra \( \lA\) in \( \lP\) and we define
\begin{equation}
    \lG_{\lambda}=\{ X\in\lG\tq [J,X]=\alpha(J)X \forall J\in\lA \}.
\end{equation}
The rank of \( \lG\) is the dimension of \( \lA\). The root space decomposition then reads
\begin{equation}
    \lG=\lG_0\oplus\bigoplus_{\lambda\in\Sigma}\lG_{\lambda}
\end{equation}
where \( \Sigma\) is the set of \( \lambda\in\lA^*\) such that \( \lambda\neq 0\) and \( \lG_{\lambda}\neq 0\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Notations}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\label{SubsecNotationRootsDel}

We summarize the notations that will be used later. Let \( \lH\) be a Cartan algebra in the complex semisimple Lie algebra \( \lG\). An element \( \alpha\in\lH^*\) is a root if the space
\begin{equation}
    \lG_{\alpha}=\{ X\in\lG\tq \ad(H)X=\alpha(H)x,\forall H\in\lH \}
\end{equation}
is non empty.

\begin{enumerate}
    \item
        \( \Phi\) is the set of all the roots. We consider an ordering notion on \( \Phi\) and \( \Phi^+=\Pi\) is the set of positive roots.
    \item
        An element in \( \Phi^+\) is simple if it cannot be written as the sum of two positive roots.
    \item
        \( \Delta\) is the set of simple roots\footnote{The symbol \( \Delta\) has not a fixed signification in the literature. As example, in \cite{Cornwell} the symbol \( \Delta\) is the set of roots while in \cite{SternLieAlgebra} it denotes the set of simple roots.}. The simple roots are denoted by \( \{ \alpha_1,\ldots,\alpha_l \}\).
\end{enumerate}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Root spaces}
%---------------------------------------------------------------------------------------------------------------------------

We are considering a complex semisimple Lie algebra \( \lG\) with a Cartan subalgebra \( \lH\).

\begin{definition}      \label{DefRootSpace}
    For each \( \alpha\in\lH^*\) we define
    \begin{equation}            \label{eq:lG_alpha_nss}
        \lG_{\alpha}=\{  x\in\lG\tq\forall h\in\lH, \big(\ad h-\alpha(h)\big)^nx=0\text{ for some $n\in\eN$}    \}.
    \end{equation}
    If \( \lG_{\alpha}\) is not reduced to \( 0\), we say that \( \alpha\) is a \defe{root}{root} and \( \lG_{\alpha}\) is a \defe{root space}{root!space}.
\end{definition}
Corollary \ref{CorCoolWrlGbalpha} will provide an easier formula for the root spaces when the algebra \( \lG\) is complex and semisimple.
\begin{theorem}     \label{TholGCartalphaplusbeta}
Let $\lG$ be a complex Lie algebra with Cartan subalgebra $\lH$. If $\alpha,\beta\in\lG^*$ then
\begin{enumerate}
    \item   \label{ItemTholGCartalphaplusbetai}
    $[\lG_{\alpha},\lG_{\beta}]=\lG_{\alpha+\beta}$,
\item $\lG_0=\lH$.
\end{enumerate}
\label{prop:deux_racine}
\end{theorem}

\begin{proof}
For $z\in\lH$ and $x$, $y\in\lG$ we have
\begin{equation}
\big( \ad z-(\alpha+\beta)(z) \big)[x,y]=[ (\ad z-\alpha(z))x,y ]+[ x,(\ad z-\beta(z))y ].
\end{equation}
Now suppose that for some $n$,
\begin{equation}
\big( \ad z-(\alpha+\beta)(z) \big)^n[x,y]=\sum_k \binom{k}{n} \binom{k}{n} 
\left[
(\ad z-\alpha(z))^k(x),(\ad z-\beta(z))^{n-k}(y)
\right].
\end{equation}
If we apply $(\ad z-(\beta+\alpha)(z))^{n}$ to this equality, we find
\begin{equation}
\begin{split}
(\ad z-(\beta&+\alpha)(z))^{n+1}[x,y]\\
&=\sum_{k=1}^n \binom{k}{n} 
\Big(
\big[  (\ad z-\alpha(z))(\ad z-\alpha(z))^k(x),  (\ad z -\beta(z))^{n-k}(y)  \big]\\
&\quad+  \big[  (\ad z-\alpha(z))(\ad z-\alpha(z))^k(x),  (\ad z -\beta(z))^{n-k+1}(y)  \big] 
\Big)\\
&=\sum_{k=1}^{n+1}\binom{k}{n+1}
\big[
(\ad z-\alpha(z))^k(x), (\ad z-\beta(z))^{n+1-k}(y)
\big].
\end{split}
\end{equation}
This formula shows that $[\lG_{\alpha},\lG_{\beta}]\subset\lG_{\alpha+\beta}$. Indeed let $x\in\lG_{\alpha}$, $y\in\lG_{\beta}$ and $n$ be large enough,
\begin{equation}
    \left( \ad z-(\alpha+\beta)(z) \right)^n[x,y]=0.
\end{equation}

    Now we turn our attention to the second part. Let us apply the Lie theorem \ref{tho:Lie_Vu} to the action of \( \lG\) on the quotient \( \lG_0/\lH\). There exists \( [X_0]\in\lG_0/\lH\) such that \( h[X_0]=\lambda(h)[X_0]\) where the bracket stand for the class. Since \( \lH\) is nilpotent on \( \lG_0\) we have \( \lambda=0\) identically. Looking outside the class, the existence of a non vanishing \( [X_0]\in\lG/\lH\) such that \( h[X_0]=0\) means that there exists \( X_0\in\lG_0\setminus\lH\) such that \( [h,X_0]\in\lH\) for every \( h\in\lH\). This contradicts the fact that \( \lH\) is its own centralizer.
\end{proof}

\begin{proposition}
    The complex Lie algebra decomposes into the root spaces as
    \begin{equation}
        \lG=\bigoplus_{\alpha\in\lH^*}\lG_{\alpha}.
    \end{equation}
\end{proposition}

\begin{proof}
    Let \( H\in\lH\). We consider the primary decomposition \eqref{EqDecomplGpRimDecombijk} with respect to the operator \( \ad(H)\):
    \begin{equation}
        \lG=\bigoplus_{\lambda}\lG_{\lambda}(H).
    \end{equation}
    If \( H'\in\lH\) the operator \( \ad(H')\) acts the space \( \lG_{\lambda}(H)\) because \( H'\in\lG_0(H)\) so that
    \begin{equation}
        [H',\lG_{\lambda}(H)]\subset\lG_{\lambda}(H).
    \end{equation}
    Thus we can write the primary decomposition of \( \lG_{\lambda}(H)\) with respect to the operator \( \ad(H')\) knowing that
    \begin{equation}
        \big( \lG_{\lambda}(H) \big)_{\mu}(H')=\{ X\in\lG_{\lambda}(H)\tq\big( \ad(H')-\mu \big)^nX=0 \}=\lG_{\lambda}(H)\cap\lG_{\mu}(H').
    \end{equation}
    What we get is the decomposition
    \begin{equation}
        \lG=\bigoplus_{\lambda}\bigoplus_{\mu}\lG_{\lambda}(H)\cap\lG_{\mu}(H').
    \end{equation}
    We continue the decomposition with \( H'',H''',\ldots\) until each \( \ad(H)\) with \( H\in\lH\) has only one eigenvalue on each of the summand of the decomposition
    \begin{equation}
        \lG=\bigoplus_{\lambda_1,\ldots,\lambda_l}\lG_{\lambda_1}(H_1)\cap\ldots\cap\lG_{\lambda_l}(H_l).
    \end{equation}
    For each \( l\)-uple \( (\lambda_1,\ldots,\lambda_l)\), the eigenvalue of \( H_i\) on \( \lG_{\lambda_1}\cap\ldots\cap\lG_{\lambda_l}\) is \( \lambda_i\). Thus we can see \( \lambda\) as a \( 1\)-form on \( \lH\) and write
    \begin{equation}        \label{EqdirectumlGRoots}
        \lG=\bigoplus_{\lambda}\lG_{\lambda}
    \end{equation}
    with
    \begin{equation}
        \lG_{\lambda}=\{ X\in\lG\tq\big( \ad(H)-\lambda(H) \big)^nX=0 \}. 
    \end{equation}
\end{proof}

\begin{corollary}\label{cor:Bxy_zero}
    If $X_{\alpha}\in\lG_{\alpha}$ and $X_{\beta}\in\lG_{\beta}$ with $\alpha+\beta\neq 0$, then $B(X_{\alpha},X_{\beta})=0$.
\end{corollary}

\begin{proof}
    From the second point of proposition \ref{prop:deux_racine}, we have $\dpt{\ad X_{\alpha}\circ\ad X_{\beta}}{\lG_{\mu}}{\lG_{\mu+\alpha+\beta}}$. If $\alpha+\beta\neq 0$, the fact that the sum \eqref{EqdirectumlGRoots} is direct makes the trace of $\ad X_{\alpha}\circ\ad X_{\beta}$ zero.
\end{proof}


Since \( \lG\) is semisimple, the restriction of the Killing form on \( \lH\) is nondegenerate\footnote{Because the Killing form is zero on each space \( \lG_{\alpha}\) with \( \alpha\neq 0\).}. Thus we can introduce, for each linear function $\phi\colon \lH\to \eC$, the unique element $t_{\phi}\in\lH$ such that 
\begin{equation}
    \phi(h)=B(t_{\phi},h) 
\end{equation}
for every $h\in\lH$. \nomenclature[G]{\( t_{\alpha}\)}{a basis of \( \lH\)} This element is nothing else that the dual \( \phi^*\) with respect to the Killing form. Indeed
\begin{equation}
    t_{\phi}^*(h)=B(t_{\phi},h)=\phi(h),
\end{equation}
so that \( t_{\phi}^*=\phi\). Incidentally, this proves that when \( \phi\) runs over a basis of \( \lH^*\), the vector \( t_{\phi}\) runs over a basis of \( \lH\). The space $\lH^*$ is endowed with an inner product defined by\nomenclature{$(\alpha,\beta)$}{Inner product on the dual $\lH^*$ of a Cartan algebra}\nomenclature[G]{\( (\alpha,\beta)\)}{inner product on the dual \( \lH^*\).}
\begin{equation}        \label{EqDefInnprHestrar}
    (\alpha,\beta) = B(t_{\alpha},t_{\beta})=\beta(t_{\alpha})=\alpha(t_{\beta}).
\end{equation}

\begin{lemma}       \label{LemXYBXYtalpha}\label{Propoxalphaymoinaalpha}
    If \( X\in\lG_{\alpha}\) and \( Y\in\lG_{-\alpha}\), then
    \begin{equation}
        [X,Y]=B(X,Y)t_{\alpha}.
    \end{equation}
\end{lemma}

\begin{proof}
    By theorem \ref{TholGCartalphaplusbeta}\ref{ItemTholGCartalphaplusbetai}, $[X,Y]\in\lG_0=\lH$. Now we consider $h\in\lH$ and the invariance formula \eqref{eq:Killing_invariant}. We find:
    \begin{equation}
        B\big( h,[X,Y] \big)=-B\big( [X,h],Y \big)=\alpha(h)B(X,Y)=B(h,t_{\alpha})B(X,Y)=B\big(h,B(X,Y)t_{\alpha}\big).
    \end{equation}
    The lemma is proven since it is true for any $h\in\lH$ and $B$ is nondegenerate on $\lH$. 
\end{proof}

The elements \( t_{\alpha}\) allow to introduce an inner product on \( \lH^*\) and hence on the roots by defining
\begin{equation}
    (\alpha,\beta)=B(t_{\alpha},t_{\beta}).
\end{equation}

\begin{lemma}       \label{Leminnerabequaaggb}
    If \( \alpha\) and \( \beta\) are roots we have the formula
    \begin{equation}
        (\alpha,\beta)=\sum_{\gamma\in\Phi}(\dim\lG_{\gamma})(\alpha,\gamma)(\beta,\gamma).
    \end{equation}
\end{lemma}

\begin{proof}
    We consider for \( \lG\) a basis in which all the elements are part of one of the root spaces and we look at the endomorphism \( \ad(t_{\alpha})\) of \( \lG\). This is diagonal and has zeros on the entries corresponding to \( \lH\). The other entries on the diagonal are of the form \( \gamma(t_{\alpha})\). Thus
    \begin{equation}
        B(t_{\alpha},t_{\beta})=\sum_{\gamma\in\Phi}(\dim\lG_{\gamma})\gamma(t_{\alpha})\gamma(t_{\beta}).
    \end{equation}
    Thus we have \( (\alpha,\beta)=B(t_{\alpha},t_{\beta})=\sum_{\gamma\in\Phi}(\dim\lG_{\gamma})(\alpha,\gamma)(\beta,\gamma)\).
\end{proof}

\begin{proposition}     \label{PropScalrooTsQ}
    Let \( \alpha\) and \( \beta\) be roots. We have
    \begin{enumerate}
        \item
            \( (\alpha,\beta)\in\eQ\),
        \item
            \( (\alpha,\alpha)\geq 0\).
    \end{enumerate}
\end{proposition}
The proof comes from \cite{Cornwell} page 826.

\begin{proof}
    Let \( \alpha,\beta\in\Phi\) and consider the space
    \begin{equation}
        V=\bigoplus_{m\in\eZ}\lG_{\beta+m\alpha}.
    \end{equation}
    If \( X_{\alpha}\in\lG_{\alpha}\) and \( X_{-\alpha}\in\lG_{-\alpha}\) with \( [X_{\alpha},X_{-\alpha}]=t_{\alpha}\) we have, for all \( v\in V\), 
    \begin{subequations}
        \begin{align}
            [X_{\alpha},v]&\in V\\
            [X_{-\alpha},v]&\in V\\
            [t_{\alpha},v]&\in V.
        \end{align}
    \end{subequations}
    Thus we can consider the restrictions to \( V\) of the operators \( \ad(X_{\alpha})\), \( \ad(X_{-\alpha})\) and \( \ad(t_{\alpha})\). Since \( \ad\) is an homomorphism we have, as operator on \( V\), 
    \begin{equation}
        \ad(t_{\alpha})=\big[ \ad(X_{\alpha}),\ad(X_{-\alpha}) \big],
    \end{equation}
    and then \( \tr\big( \ad(t_{\alpha})|_V \big)=0\). 

    Let us compute that trace on the basis \( \{ v_k^{(i)} \}\) where \( v_k^{(i)}\in\lG_{\beta+k\alpha}\). Since
    \begin{equation}
        \ad(t_{\alpha})v_k^{(i)}=(\beta+k\alpha)(t_{\alpha})v_k^{(i)}
    \end{equation}
    we have
    \begin{subequations}
        \begin{align}
            0&=\tr\big( \ad(t_{\alpha})|_V \big)\\
            &=\sum_{k\in\eZ}\dim\lG_{\beta+k\alpha}(\beta+k\alpha)(t_{\alpha})\\
            &=\sum_{k\in\eZ}\dim_{\beta+k\alpha}\big( (\alpha,\beta)+(\alpha,\alpha) \big)
        \end{align}
    \end{subequations}
    and
    \begin{equation}        \label{EqunderbAabmaaB}
        \underbrace{\left( \sum_{k\in\eZ}\dim\lG_{\beta+k\alpha} \right)}_{A\in\eN}(\alpha,\beta)=-(\alpha,\alpha)\underbrace{\left( \sum_{k\in\eZ}k\dim\lG_{\beta+k\alpha} \right)}_{B\in\eZ}.
    \end{equation}
    If \( (\alpha,\alpha)=0\) then we have \( (\beta,\alpha)=0\) for every \( \beta\in\Phi\), hence \( B(t_{\alpha},t_{\beta})=0\) which contradicts non degeneracy of the Killing form. We conclude that \( (\alpha,\alpha)\neq 0\). By the formula of lemma \ref{Leminnerabequaaggb} we get
    \begin{equation}
        (\alpha,\alpha)=\sum_{\beta\in\Phi}\dim\lG_{\beta}(\alpha,\beta)^2.
    \end{equation}
    Replacing in that formula the value of \( (\alpha,\beta)\) taken from formula \eqref{EqunderbAabmaaB} we found
    \begin{equation}
        (\alpha,\alpha)=\sum_{\beta\in\Phi}\dim\lG_{\beta}\frac{ B^2 }{ A^2 }(\alpha,\alpha)^2
    \end{equation}
    and then \( (\alpha,\alpha)\in\eQ^+\). The fact that \( (\alpha,\beta)\) is rational follows.

    Notice that the sign of \( B\) is not guaranteed because it's not sure because we do not know whether there are more positive or negative terms in the sum of the right hand side of \eqref{EqunderbAabmaaB}.
\end{proof}

\begin{proposition}
    Let \( \alpha\) be a root of the complex semisimple Lie algebra \( \lG\). Then 
    \begin{enumerate}
        \item
            \( \dim\lG_{\alpha}=1\), 
        \item
            the only integer multiple of \( \alpha\) to be roots are \( \pm\alpha\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Let \( X_{\alpha}\in\lG_{\alpha}\) and consider the vector space
    \begin{equation}
        V=\eC t_{\alpha}\oplus\eC X_{\alpha}\oplus\bigoplus_{m<0}\lG_{m\alpha}.
    \end{equation}
    Let \( y\in\lG_{-\alpha}\) be chosen in such a way that \( [X_{\alpha},y]=t_{\alpha}\); by lemma \ref{LemXYBXYtalpha} this is only a matter of normalization. The space \( V\) is invariant under \( \ad(X_{\alpha})\) and \( \ad(y)\). Indeed
    \begin{enumerate}
        \item
            \( \ad(X_{\alpha})t_{\alpha}=-\alpha(t_{\alpha})X_{\alpha}\in\eC X_{\alpha}\);
        \item
            \( \ad(X_{\alpha})X_{\alpha}=0\);
        \item
            \( \ad(X_{\alpha})\lG_{m\alpha}\subset\lG_{(m+1)\alpha}\); if \( m<-1\), \( (m+1)<0\), while if \( m=-1\) we know that the commutator \( [X_{\alpha},\lG_{-\alpha}]\) is included in \( \eC t_{\alpha}\in V\);
        \item
            \( \ad(y)t_{\alpha}\in\lG_{-\alpha}\)
        \item
            \( \ad(y)X_{\alpha}=-t_{\alpha}\) by definition;
        \item
            \( \ad(y)\lG_{m\alpha}\subset\lG_{(m-1)\alpha}\).
    \end{enumerate}
    Since \( \ad\colon \lG\to \GL(\lG)\) is an homomorphism (lemma \ref{LemadhomomadXadYadXY}) we have
    \begin{equation}
        \big[ \ad(X_{\alpha}),\ad(y) \big]=\ad(t_{\alpha})
    \end{equation}
    and then \( \tr\big( \ad(t_{\alpha}) \big)=0\) because the trace of a commutator is zero\footnote{From the cyclic invariance of the trace.}. Since \( V\) is an invariant subspace, the trace of \( \ad(t_{\alpha})\) restricted to \( V\) is also vanishing. Let us compute that trace on the basis \( \{ X_{\alpha},t_{\alpha},X^i_{m\alpha} \}_{m<0}\) where \( i\) takes as many values as the dimension of \( \lG_{m\alpha}\).

    We have \( \ad(t_{\alpha})X_{-\alpha}=-\alpha(t_{\alpha})X_{-\alpha}\), \( \ad(t_{\alpha})t_{\alpha}=0\) and \( \ad(t_{\alpha})X^i_{m\alpha}=m\alpha(t_{\alpha})X_{m\alpha}\), thus the trace is
    \begin{equation}    \label{Eqzequalmulsumnotinfty}
        0=\alpha(t_{\alpha})\Big( -1+\sum_{m=1}^{\infty}m\dim\lG_{m_{\alpha}} \Big).
    \end{equation}
    Notice that the sum is in fact finite since the dimension of \( \lG\) is finite. We know that \( \alpha(t_{\alpha})=B(t_{\alpha},t_{\alpha})\neq 0\), so that equation \eqref{Eqzequalmulsumnotinfty} is only possible with \( \dim\lG_{\alpha}=1\) and \( \dim\lG_{m\alpha}=0\) for \( m\neq 1\).
\end{proof}

A very similar proof can be found in \cite{Cornwell}, page 827. 


\begin{corollary}       \label{CorCoolWrlGbalpha}
    In the case of semisimple complex Lie algebra,
    
    \begin{enumerate}
        \item
            the root spaces are given by
            \begin{equation}        \label{EqExpWeightSemiSimple}
                \lG_{\alpha}=\{ X\in\lG\tq\forall h\in\lH, [h,X]=\alpha(h)X \};
            \end{equation}
        \item
            for every $ x_{\alpha}\in\lG_{\alpha}$, and for every $h\in\lH$, we have 
            \begin{equation}
                [h, x_{\alpha}]=\alpha(h) x_{\alpha}.
            \end{equation}
    \end{enumerate}
\end{corollary}

\begin{proof}
    Let \( X\in\lG_{\alpha}\), we have
    \begin{equation}
        \big( \ad(h)-\alpha(h) \big)^nX=0,
    \end{equation}
    so
    \begin{equation}    \label{EqadhalphahnnmuvX}
        \big( \ad(h)-\alpha(h) \big) \underbrace{\big( \ad(h)-\alpha(h) \big)^{n-1} X}_v=0.
    \end{equation}
    In particular the vector \( v= \big( \ad(h)-\alpha(h) \big)^{n-1} X\) belongs to \( \lG_{\alpha}\). Since the latter space has dimension one, the vector \( v\) is a multiple of \( X\) and consequently equation \eqref{EqadhalphahnnmuvX} shows that
    \begin{equation}
        \big( \ad(h)-\alpha(h) \big)v=\big( \ad(h)-\alpha(h) \big)X=0.
    \end{equation}

    The second point is only an other way to write the same.
\end{proof}

\begin{lemma}       \label{LemHzesialphaHz}
    If \( H\) is an element of \( \lH\) with \( \alpha(H)=0\) for every root, then \( H=0\)
\end{lemma}

\begin{proof}
    Consider the decompositions (not unique) \( H=\sum_{\alpha\in\Phi}a_{\alpha} t_{\alpha}\) and \( H'=\sum_{\beta\in\phi}a'_{\beta}t_{\beta}\). Then 
    \begin{subequations}
        \begin{align}
            B(H,H')&=\sum_{\alpha,\beta}a_{\alpha}a_{\beta}'B(t_{\alpha},t_{\beta})\\
            &=\sum_{\alpha,\beta}a'_{\beta}\beta(a_{\alpha},t_{\alpha})\\
            &=\sum_{\beta}a'_{\beta}\beta(H)\\
            &=0.
        \end{align}
    \end{subequations}
    Such an element is thus Killing-orthogonal to the whole space \( \lH\) but we already know the \( \lH\) is orthogonal to each space \( \lG_{\alpha}\) (\( \alpha\neq 0\)). By non degeneracy of the Killing form we must have \( H=0\).
\end{proof}

\begin{proposition}
    The set of roots of a complex semisimple Lie algebra spans the dual space \( \lH^*\).
\end{proposition}

\begin{proof}
    Consider a basis \( \{ H_i \}\) of \( \lH\) with \( \{ H_0,\ldots,H_m \}=\Span(\Phi)\) and \( \{ H_{m+1},\ldots,H_r \}\) be outside of \( \Span\Phi\). A root reads \( \alpha=\sum_{k=0}^ma_kH_k^*\).
    Thus \( \alpha(H_{m+1})=0\), which implies that \( H_{m+1}=0\) by lemma \ref{LemHzesialphaHz}.
\end{proof}

\begin{corollary}
    A Cartan algebra \( \lH\) of a complex semisimple Lie algebra is abelian.
\end{corollary}

\begin{proof}
    Let \( H',H''\in\lH\) and consider \( H=[H',H'']\), a root \( \alpha\) and \( X_{\alpha}\in\lG_{\alpha}\). On the one hand we have
    \begin{equation}
        \big[ [H',H''],X_{\alpha} \big]=-\alpha(H')[X_{\alpha},H']+\alpha(H')[X_{\alpha},H'']=0
    \end{equation}
    and on the other hand we have \( \big[ [H',H''],X_{\alpha} \big]=[H,X_{\alpha}]=\alpha(H)X_{\alpha}\). We deduce that \( \alpha(H)=0\) for every root and then that \( H=0\) by lemma \ref{LemHzesialphaHz}.
\end{proof}

We denote by \( \Phi\) the set of roots. These are the elements \( \lambda\in\lH^*\) such that \( \lG_{\lambda}\) is non trivial. We suppose to have chosen a positivity notion on \( \lH^*\), so that we can speak of \( \Phi^+\), the set of \defe{positive roots}{positive root}.

A positive root is \defe{simple}{simple!root} is it cannot be written as the sum of two positive roots.

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Generators}
%---------------------------------------------------------------------------------------------------------------------------

We are going to build the Chevalley basis of the complex semisimple Lie algebra \( \lG\). That will essentially be a choice of a basis vector in each of the root spaces. We are following the notations summarized in point \ref{SubsecNotationRootsDel}.


Now, for each root $\alpha$, we pick $e_{\alpha}\in\lG_{\alpha}$. We will see that, up to renormalization, we can set the in nice commutation relations.

\begin{lemma}       \label{LemBalpahbetaef}
    If \( \alpha\) and \( \beta\) are roots such that \( \alpha+\beta\neq 0\), then 
    \begin{equation}
        B(e_{\alpha},e_{\beta})=0.
    \end{equation}
    If \( f_{\alpha}\in\lG_{-\alpha}\) we also have \( B(e_{\alpha},f_{\alpha})\neq 0\).
\end{lemma}

\begin{proof}
    By definition \( B(e_{\alpha},e_{\beta})=\tr\big( \ad(e_{\alpha})\circ\ad(e_{\beta}) \big)\). If we apply \( \ad(e_{\alpha})\circ\ad(e_{\beta})\) to an element of \( e_{\gamma}\) (including \( \lG_0=\lH\)), we get an element of \( \lG_{\alpha+\beta+\gamma}\). Thus the trace defining the Killing form is zero and \( B(e_{\alpha},e_{\beta})=0\) when \( \alpha+\beta=0\). 

    Since the Killing form is nondegenerate, we conclude that \( B(e_{\alpha},e_{-\alpha})\neq 0 \).
\end{proof}

\begin{corollary}       \label{CorrExistInverseRoot}
    Let \( \lG\) be a semisimple complex Lie algebra and \( \lH\) be a Cartan subalgebra of \( \lG\). Let \( \alpha\) be a root of \( \lG\) and \( \lH_{\alpha}=[\lG_{\alpha},\lG_{-\alpha}]\). There exist an unique \( H_{\alpha}\in\lH_{\alpha}\) such that \( \alpha(H_{\alpha})=2\).
\end{corollary}

\begin{proof}
    We have \( [e_{\alpha},f_{\alpha}]=B(e_{\alpha},f_{\alpha})t_{\alpha}\) and the lemma \ref{LemBalpahbetaef} shows that the Killing form is non zero. Multiplying by a suitable number provides the result.
\end{proof}
The element \( H_{\alpha}\in\lH\) defined in this lemma is the \defe{inverse root}{inverse!root} of \( \alpha\).

\begin{lemma}       \label{Lemalphaakbetaknimport}
    Let \( \{ \beta_1,\ldots,\beta_l \}\) be a choice of elements in \( \lH^*\) such that the set \( \{ t_{\beta_1},\ldots,t_{\beta_l} \}\) is a basis of \( \lH\). Thus the roots can be decomposed as
    \begin{equation}
        \alpha=\sum_{k=1}^la_k\beta_k
    \end{equation}
    with \( a_k\in\eQ\).
\end{lemma}

\begin{proof}

    Let \( \alpha=\sum_{k=1}^la_k\beta_k\). We know that the vectors \( t_{\beta_i}\) form a basis of \( \lH\), so we have the decomposition \( t_{\alpha}=\sum_ka_kt_{\beta_k}\). Indeed
    \begin{equation}
        B\big( h,\sum_k a_kt_{\beta_k} \big)=\sum_ka_k B(h,t_{\beta_k})=\sum_ka_k\beta_k(h)=\alpha(h).
    \end{equation}
    For each \( k=1,2,\ldots,l\) we have
    \begin{equation}
        (\alpha_k, \alpha) =\sum_{j=1}^la_k(\alpha_k, \alpha_j).
    \end{equation}
    This is a system of linear equations for the \( l\) variables \( a_k\). Since the coefficients \( (\alpha_k,\alpha)\) and \( (\alpha_k,\alpha_j)\) are rational by proposition \ref{PropScalrooTsQ}, the solutions are rational too.
\end{proof}

\begin{remark}
    The lemma \ref{Lemalphaakbetaknimport} deals with a quite general basis of \( \lH\). We will see in the proposition \ref{ThoposrootnjajnZ} that in the case of the basis of simple roots, the coefficients \( a_k\) are integers, either all positive or all negative.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Subalgebra \texorpdfstring{$ \gsl(2)_i$}{SL2R} }
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecCopiedeSLdansGi}

For each nonzero root \( \alpha\in\lH^*\), we choose \( e_{\alpha}\in\lG_{\alpha}\) and \( f_{\alpha}\in\lG_{-\alpha}\) in such a way to have
\begin{equation}
    B(e_{\alpha},f_{\alpha})=\frac{ 2 }{ B(t_{\alpha},t_{\alpha}) },
\end{equation}
and then we pose
\begin{equation}
    h_{\alpha}=\frac{ 2 }{ B(t_{\alpha},t_{\alpha}) }t_{\alpha}.
\end{equation}
Notice that these choices are possible because the Killing form is non degenerated on \( \lH\). 



% This proposition about gsl(2,R) was at position 198631779.
The following comes from page 82 of \cite{SternLieAlgebra}
\begin{proposition}
    For each root, the set $\{ e_{\alpha},f_{\alpha},h_{\alpha} \}$ generates an algebra isomorphic to $\gsl(2,\eR)$, i.e. they satisfy
    \begin{subequations}
        \begin{align}
            [h_{\alpha},e_{\alpha}]&=2e_{\alpha}\\
            [h_{\alpha},f_{\alpha}]&=-2f_{\alpha}\\
            [e_{\alpha},f_{\alpha}]&=h_{\alpha}\\
        \end{align}
    \end{subequations}
\end{proposition}

\begin{proof}
    Since \( \alpha(t_{\alpha})=B(t_{\alpha},t_{\alpha})\) we have \( \alpha(h_{\alpha})=2\). Now the computations are quite direct. The first is
    \begin{equation}
        [h_{\alpha},e_{\alpha}]=\alpha(h_{\alpha})e_{\alpha}=2e_{\alpha}.
    \end{equation}
    For the second,
    \begin{equation}
        [h_{\alpha},f_{\alpha}]=-\alpha(h_{\alpha})f_{\alpha}=-2f_{\alpha}.
    \end{equation}
    For the third, we know that \( [e_{\alpha},f_{\alpha}]\in\lH\); thus \( B\big( X,[e_{\alpha},f_{\alpha}] \big)=0\) whenever \( X\in\lG_{\lambda}\) with \( \lambda\neq 0\). Let \( h\in\lH\). Using the invariance of the Killing form,
    \begin{equation}
        B\big( h,[e_{\alpha},f_{\alpha}] \big)=B\big( [h,e_{\alpha}],f_{\alpha} \big)=\alpha(h)B(e_{\alpha},f_{\alpha})=B(t_{\alpha},t_{\alpha})B(e_{\alpha},f_{\alpha})=B\big( B(e_{\alpha},f_{\alpha})t_{\alpha},h \big).
    \end{equation}
    Thus
    \begin{equation}
        [e_{\alpha},f_{\alpha}]=B(e_{\alpha},f_{\alpha})f_{\alpha}=h_{\alpha}.
    \end{equation}
\end{proof}
Remark that we used the non degeneracy of the Killing form in a crucial way. The copy of \( \gsl(2,\eR)\) formed by \( \{ e_{\alpha},f_{\alpha},h_{\alpha} \}\) is denoted by $\gsl(2,\eR)_{\alpha}$. 


\begin{proposition}
    In the universal enveloping algebra,
    \begin{equation}        \label{Eqhjfikplusun}
        [h_j,f_i^{k+1}]=-(k+1)\alpha_i(h_j)f_i^{k+1}
    \end{equation}
    as generalisation of the previous one.
\end{proposition}

\begin{proof}
    We use an induction over $k$. Since $\ad(h_j)$ is a derivation in $\mU(\lG)$, the induction hypothesis and the definition relation $[h,f_i]=-\alpha_i(h)f_i$ with $h=h_i$, we have
    \begin{equation}
        \begin{split}
            \ad(h_j)f^{k+1}_i   &=\big( \ad(h_j)f_i^k \big)f_i+f_i^k\ad(h_j)f_i.\\
                        &=-k\alpha+i(h_j)f_i^kf_i-\alpha_i(h_j)f^{k+1}_i\\
                        &=-(k+1)\alpha_i(h_j)f_i^{k+1}.
        \end{split}
    \end{equation}
\end{proof}

Now the Lie algebra \( \lG\) can be seen as a \( \gsl(2,\eR)\)-module. As an example, for each choice of \( \beta\in\Phi\), the algebra \( \gsl(2)_{\alpha}\) acts on the vector space
\begin{equation}
    V=\bigoplus_{k\in\eZ}\lG_{\beta+k\alpha}.
\end{equation}
The vector space \( \lG\) carries thus several representations of \( \gsl(2)\); this fact will be used in a crucial way during the proof of proposition \ref{Proppqasbabaa}.


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Chevalley basis}
%---------------------------------------------------------------------------------------------------------------------------

The Chevalley basis corresponds to an other choice of normalization of the element \( e_{\alpha}\), \( h_{\alpha}\). If we set
\begin{subequations}
    \begin{numcases}{}
        H_{\alpha}=K_{\alpha}t_{\alpha}\\
        E_{\alpha}=N_{\alpha}e_{\alpha}
    \end{numcases}
\end{subequations}
with
\begin{equation}
    \begin{aligned}[]
        K_{\alpha}&=\frac{ 2 }{ (\alpha,\alpha) }\\
        N_{\alpha}&=\sqrt{\frac{ 2 }{ B(e_{\alpha},e_{-\alpha})(\alpha,\alpha) }},
    \end{aligned}
\end{equation}
then we have the \defe{Chevalley relations}{Chevalley!basis}:
\begin{subequations}        \label{EqsChevalleuRels}
    \begin{align}
        [E_{\alpha},E_{-\alpha}]&=H_{\alpha}\\
        [H_{\alpha},E_{\beta}]&=\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }E_{\beta}\\
        [H_{\alpha},H_{\beta}]&=0.
    \end{align}
\end{subequations}
The last relation is nothing else than the fact that the Cartan subalgebra \( \lH\) is abelian. Notice that we don't give relations between \( E_{\alpha}\) and \( E_{\beta}\). Of course \( [E_{\alpha},E_{\beta}]\sim E_{\alpha+\beta}\) but the spaces \( \lG_{\alpha}\) and \( \lG_{\beta}\) being Killing orthogonal, the Killing does not provides a natural relative normalisation between \( E_{\alpha}\) and \( E_{\beta}\).

If \( \{ \alpha_i \}_{i=1,\ldots,l}\) is the set of simple roots, we consider the notation \( X_i^+=E_{\alpha_i}\), \( X^-_i=E_{-\alpha_i}\), \( H_i=H_{\alpha_i}\) and we introduce the \defe{Cartan matrix}{Cartan!matrix}
\begin{equation}
    A_{ij}=\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }.
\end{equation}
Reduced to the simple roots the relations \eqref{EqsChevalleuRels} become
\begin{equation}        \label{EqChevalleySimple}
    \begin{aligned}[]
        [X^+_i,X^-_j]&=\delta_{ij}H_i\\
        [H_i,X^{\pm}_j]&=\pm A_{ij}X^{\pm}_j\\
        [H_i,H_j]&=0.
    \end{aligned}
\end{equation}
The first relation comes from the fact that \( \alpha_i-\alpha_j\) is not a root when \( \alpha_i\) and \( \alpha_j\) are simple roots. 

\begin{remark}
    The idea behind the Chevalley relations  is that the algebra \( \lG\) is generated by the elements \( X^{\pm}_i\), \( H_i\) and the commutation relations \eqref{EqChevalleySimple}. Even if these elements do not form a basis (while the elements \( E_{\alpha}\), \( H_{\alpha}\) do), one can define a function on \( \lG\) by giving its values on \( X^{\pm}_i\) and \( H_i\) providing one has a canonical way to extend it on commutators.

    The definition \ref{EqDefCobrackStandard} of standard cobracket works in this way.
\end{remark}

\begin{remark}
    Notice that these relations do not give the value of
    \begin{equation}
        [E_{\alpha},E_{\beta}]=N_{\alpha,\beta}E_{\alpha+\beta}
    \end{equation}
    when \( \alpha+\beta\) is a root.
\end{remark}

\begin{probleme}
    It has to be possible to compute \( N_{\alpha,\beta}\), but I do not know how. The answer is given in equation \eqref{EqChevalleyBasis} but I don't know where I got them. Maybe there are some hints in \cite{Cornwell} (Il faut ajouter Cornwell à la bibliographie et enlever le problème \ref{ProbAvecCorwell}).
\end{probleme}

\begin{probleme}
    It seems that \( A_{ij}\) is the larger integer \( k\) such that \( \alpha_i+k\alpha_j\) is a root. This is the justification of the other Serre's relations that read
    \begin{equation}
        \ad^{1-A_{ij}}(X^{\pm}_i)X^{\pm}_j=0.
    \end{equation}
    That relation has to be written with the Chevalley's ones.
\end{probleme}

One can choose the coefficients in a more scientific way\cite{SerreSSAlgebres}. Let \( \alpha\) be a positive root, let \( H_{\alpha}\) be the inverse root of \( \alpha\) and \( e_{\alpha}\in\lG_{\alpha}\). We have
\begin{equation}
    [e_{\alpha},e_{\beta}]=\begin{cases}
        N_{\alpha,\beta}e_{\alpha+\beta}    &   \text{if \( \alpha+\beta\) is a root}\\
        0    &    \text{if \( \alpha+\beta\) is not a root}.
    \end{cases}
\end{equation}
We are going to find a multiple \( E_{\alpha}\) of \( e_{\alpha}\) in such a way to have in the same time
\begin{subequations}
    \begin{numcases}{}
        [E_{\alpha},E_{-\alpha}]=H_{\alpha}\\
        N_{\alpha,\beta}=-N_{-\alpha,-\beta}.
    \end{numcases}
\end{subequations}

Let \( \sigma\) be an involutive automorphism of \( \lG\) such that \( \sigma|_{\lH}:-\id\). First we have \( \sigma(\lG_{\alpha})=\lG_{-\alpha}\) because
\begin{equation}
    [h,\sigma(e_{\alpha})]=\sigma[\sigma(h),e_{\alpha}]=-\sigma\alpha(h)e_{\alpha}=-\alpha(h)\sigma(e_{\alpha})
\end{equation}
for every \( h\in\lH\) and \( e_{\alpha}\in\lG_{\alpha}\). From corollary \ref{CorrExistInverseRoot} there exist a number \( a_{\alpha}\) such that
\begin{equation}
    [e_{\alpha},\sigma(e_{\alpha})]=a_{\alpha} H_{\alpha}.
\end{equation}
We pose 
\begin{subequations}
    \begin{numcases}{}
        E_{\alpha}=\frac{1}{ \sqrt{-a_{\alpha}} }e_{\alpha}\\
        E_{-\alpha}=-\sigma(E_{\alpha}).
    \end{numcases}
\end{subequations}
With that choice we immediately have \( [E_{\alpha},E_{-\alpha}]=H_{\alpha}\). We also have \( N_{\alpha,\beta}=-N_{-\alpha,-\beta}\); in order to see it, consider
\begin{equation}
    [\sigma E_{\alpha},\sigma E_{\beta}]=\sigma[E_{\alpha},E_{\beta}]=N_{\alpha,\beta}\sigma(E_{\alpha+\beta})=-N_{\alpha,\beta}E_{-\alpha-\beta}.
\end{equation}
But the same is also equal to
\begin{equation}
    [-E_{-\alpha},-E_{-\beta}]=[E_{-\alpha},E_{-\beta}]=N_{-\alpha,-\beta}E_{-\alpha,-\beta}.
\end{equation}

\begin{proposition}
    With these choices we have
    \begin{equation}
        N_{\alpha,\beta}=\pm(p+1)
    \end{equation}
    where \( p\) is the largest integer \( j\) such that \( \beta-j\alpha\) is a root.
\end{proposition}

\begin{probleme}
    I don't know a proof of that, but \cite{SerreSSAlgebres} gives a reference.
\end{probleme}

From proposition \ref{Propoxalphaymoinaalpha} we know that \( t_{\alpha}\in\lH_{\alpha}\), so that \( H_{\alpha}\) is a multiple of \( H_{\alpha}\). The proportionality factor is easy to fix since
\begin{equation}        \label{EqRealsHalpatalphaNorsmd}
    \begin{aligned}[]
        \alpha(H_{\alpha})&=2 &\text{definition of $H_{\alpha}$}\\
        \alpha(t_{\alpha})&=(\alpha,\alpha) &\text{definition \eqref{EqDefInnprHestrar}}.
    \end{aligned}
\end{equation}
Thus \( H_{\alpha}=\frac{ 2 }{ (\alpha,\alpha) }t_{\alpha}\) and 
\begin{equation}
    [H_{\alpha},E_{\beta}]=\beta(H_{\alpha})E_{\beta}=\frac{ 2 }{ (\alpha,\alpha) }\beta(t_{\alpha})=\frac{ 2(\alpha,\beta) }{ (\alpha,\beta) }
\end{equation}
again by the definition \eqref{EqDefInnprHestrar}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Coefficients in the Cartan matrix}
%---------------------------------------------------------------------------------------------------------------------------

In this section we search to give the form of the coefficients in the Cartan matrix. We will show that the values of \( (\alpha,\beta)\) are quite restricted.

\begin{remark}
    The notations are not standard. Here the symbol \( \Delta\) denotes the set of \emph{simple} roots while the set of all roots is denoted by \( \Phi\). In the book \cite{Cornwell}, the symbol \( \Delta\) is the set of all roots. This makes quite a difference !
\end{remark}

\begin{definition}
    If \( \alpha\) and \( \beta\) are roots of the complex semisimple Lie algebra \( \lG\), then the \defe{\( \alpha\)-string}{string!of roots} containing \( \beta\) is the set of roots of the form \( \alpha+k\beta\) with \( k\in\eZ\).
\end{definition}

Among other things, the following proposition shows that a string has no gap.
\begin{proposition}     \label{Proppqasbabaa}
    Let \( \alpha,\beta\in\Phi\). Then there exits integers \( p,q\) such that \( \{ \beta+k\alpha \}_{-p\leq k\leq q} \) is the \( \alpha\)-string containing \( \beta\). The numbers \( p\) and \( q\) satisfy
    \begin{equation}        \label{Eq2qbaaapmq}
        p-q=\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }
    \end{equation}
    and the form
    \begin{equation}
        \beta-\frac{ 2(\beta,\alpha) }{ (\alpha,\alpha) }\alpha
    \end{equation}
    is a nonzero root.
\end{proposition}

\begin{proof}
    We consider the vector space
    \begin{equation}
        V=\bigoplus_{k\in\eZ}\lG_{\beta+k\alpha}
    \end{equation}
    and the Lie algebra \( \gsl(2)_{\alpha}=\langle e_{\alpha},f_{\alpha},h_{\alpha}\rangle\) defined in subsection \ref{SubSecCopiedeSLdansGi}. The latter acts on \( V\). Simple computation using the fact that \( \beta(h_{\alpha})=2(\alpha,\beta)/(\alpha,\alpha)\) shows that
    \begin{equation}
        [\frac{ 1 }{2}h_{\alpha},e_{\beta+k\alpha}]=\left( \frac{ (\alpha,\beta) }{ (\alpha,\alpha) }+k \right)e_{\beta+k\alpha}.
    \end{equation}
    Thus the matrix of \( \ad(\frac{ 1 }{2}h_{\alpha})\) is diagonal and has no multiplicity in its eigenvalues. We deduce that the representation if irreducible. From general theory of irreducible representations of \( \gsl(2)\) we know that there exists an half-integer number \( j\) such that the diagonal entries of \( \ad(\frac{ 1 }{2}h_{\alpha})\) take \emph{all} the values from \( -j\) to \( j\) by integer steps. Thus the \( \alpha\)-string containing \( \beta\) has the form \( \{ \beta+k\alpha \}_{-p\leq k\leq q}\) where \( p\) and \( q\) satisfy
    \begin{subequations}
        \begin{numcases}{}
            \frac{ (\alpha,\beta) }{ (\alpha,\alpha) }-p=-j\\
            \frac{ (\alpha,\beta) }{ (\alpha,\alpha) }+q=j.
        \end{numcases}
    \end{subequations}
    Summing we get
    \begin{equation}       
        p-q=\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }.
    \end{equation}
    If \( \lambda\) is an eigenvalue of \( \ad(\frac{ 1 }{2}h_{\alpha})\), then \( -\lambda\) is also an eigenvalue (this is still from the irreducible representation theory of \( \gsl(2)\)). The number \( (\alpha,\beta)/(\alpha,\beta)\) is obviously an eigenvalue (with \( k=0\)), thus the string contains a \( k\) such that
    \begin{equation}
        \frac{ (\alpha,\beta) }{ (\alpha,\alpha) }+k=-\frac{ (\alpha,\beta) }{ (\alpha,\alpha) }.
    \end{equation}
    The solution is \( k=-2(\alpha,\beta)/(\alpha,\alpha)\) and we deduce that
    \begin{equation}
        \beta-2\frac{ (\alpha,\beta) }{ (\alpha,\alpha) }\alpha
    \end{equation}
    is a root of \( \lG\).
\end{proof}

\begin{proposition}
    Let \( \alpha,\beta\) be two roots. Then we have
    \begin{equation}
        \frac{2(\alpha,\beta)}{(\alpha,\alpha)}=0,\pm 1,\pm 2,\pm 3.
    \end{equation}
\end{proposition}

\begin{proof}
    First, equation \eqref{Eq2qbaaapmq} shows that \( \frac{2(\alpha,\beta)}{(\alpha,\alpha)}\) is integer. If \( \alpha=\pm\beta\), the result is \( 2\). If \( \alpha\neq\pm\beta\), the vectors \( t_{\alpha}\) and \( t_{\beta}\) are linearly independent and the Schwarz inequality shows
    \begin{equation}        \label{EqprofmqxqCqrtmai}
        (\alpha,\beta)^2=| B(t_{\alpha},t_{\beta}) |< B(t_{\alpha},t_{\alpha})B(t_{\beta},t_{\beta})=(\alpha,\alpha)(\beta,\beta).
    \end{equation}
    Thus
    \begin{equation}        \label{EqprofmqxqCqrtmaii}
        \left| \frac{2(\alpha,\beta)}{(\alpha,\alpha)} \right| \left| \frac{2(\alpha,\beta)}{(\beta,\beta)} \right| <\frac{ 4| (\alpha,\beta)(\alpha,\beta) | }{ (\alpha,\beta)^2 }=4.
    \end{equation}
    Consequently the number \( | 2(\alpha,\beta)/(\alpha,\alpha) |\) being integer can only take the values \( 0\), \( 1\), \( 2\) and \( 3\). Notice that the inequality in \eqref{EqprofmqxqCqrtmai} and \eqref{EqprofmqxqCqrtmaii} are strict since \( \alpha_i\) is not collinear to \( \alpha_j\).
\end{proof}

\subsection{Simple roots}
%------------------------
As seen before, $\Phi$ admits an order inherited from $\lHeR^*$. A root $\alpha>0$ is \defe{simple}{simple!root} if it cannot be written as a sum of two positive roots.

\begin{theorem}      \label{ThoposrootnjajnZ}
    Let \( \{ \alpha_1,\ldots,\alpha_l \}\) be the set of simple roots. Then every root \( \beta\in\Phi\) can be decomposed into
    \begin{equation}
        \beta=\sum_{i=1}^ln_i\alpha_i
    \end{equation}
    where non vanishing the numbers \( n_i\in\eZ\) are either all positive or all negative.
\end{theorem}

\begin{proof}
    Let \( \beta\) be positive. If it is not simple, the one can decompose it into two positive roots:
    \begin{equation}
        \beta=\gamma+\delta
    \end{equation}
    with \( \gamma,\delta>0\). If \( \gamma\) and/or \( \delta\) are not simple, they can be decomposed further. This process has to be finite, indeed if the process is not finite, the decomposition of at least one positive root has to contains itself (because there are finitely many of them) while it is impossible to have \( \gamma=\gamma+\alpha\) with \( \alpha>0\).
\end{proof}

Two corollaries: a root is either positive or negative (this is part of the definition of positivity) and when a root is positive, its decomposition into simple roots has only positive coefficients.


\begin{lemma}
    If $\alpha-\beta$ are simple roots with $\alpha\neq\beta$, then $\beta-\alpha$ is not a root and $B( h_{\alpha},\hbb)\leq 0$.
\end{lemma}

\begin{proof}
    Define $\gamma=\beta-\alpha\in\Delta$ (and not $\Phi$ because $\alpha\neq\beta$). If $\gamma>0$, the fact that $\beta=\gamma+\alpha$ contradict the simplicity of $\beta$ while if $\gamma<0$, in the same way $\alpha=\beta-\alpha$ contradict the simplicity of $\alpha$.

    Since $\beta-\alpha$ is not a root, $\lbba=0$ and $\lbha\geq 0$ thus formula $2\beta( h_{\alpha})=(\lbba-\lbha)\alpha( h_{\alpha})$ gives
    \begin{equation}
        2B( h_{\alpha},\hbb)=\underbrace{(\lbba-\lbha)}_{\leq 0}B( h_{\alpha}, h_{\alpha}).
    \end{equation}
    Now proposition \ref{prop:lHeR} gives the result.
\end{proof}

\begin{lemma}
    The simple roots are linearly independent.
\end{lemma}

\begin{proof}
    In the definition of a simple root, we need an order notion on $\Delta$ which is then seen as a subset of $\lHeR$. But the roots are real thereon. Then the right notion of ``independence''{} for the simple root is the independence with respect to \emph{real} combinations.

    If one has a combination $c^i\alpha_i=0$ (sum over $i$) with at least one non zero among the $c^i$'s  by putting the negative $c^i$'s at right, one can write
    \[
        a^i\alpha_i=b^j\alpha_j
    \]
    with $a^i,b^j\geq 0$. Let us consider $\gamma=a^i\alpha_i$ and $h_{\gamma}$. For every $h\in\lH$, we have
    \[
        B(h,h\gamma)=\gamma(h)=a^i\alpha_i(h_{\gamma}).
    \]
    but $h_{\gamma}=a^jh_{\alpha}$, then
    \begin{equation}
        B(h_{\gamma},h\bgamma)=a^ia^j\alpha_i(h_{\alpha_j})
                    =a^ia^jB(h_{\alpha_i},h_{\alpha_i}).
    \end{equation}
    Since the $\alpha_i$ are all simple roots, the right hand side is negative, but proposition \ref{prop:lHeR} makes the left hand side positive. Thus $\gamma=0$.
\end{proof}

\begin{theorem}
    If $\{\alpha_1,\ldots,\alpha_r\}$ is the set of all the simple roots, then $\dim\lHeR=r$ and every $\beta\in\Phi$ can be decomposed as
    \[
        \beta=\sum_{i=1}^rn_i\alpha_i
    \]
    where the $n_i$ are integers either all positive either all negative.
\end{theorem}

\begin{proof}
    Let $\beta$ be a non simple positive root. Then it can be decomposed as $\beta=\gamma+\delta$ with $\gamma,\delta>0$.We can also separately decompose $\gamma$ and $\delta$ and continue so until we are left with simple roots. We have to see why the process stops. Since there are only a finite number of positive root, if the process does not stop, then the decomposition of (at least) one of the positive roots $\gamma$ contains $\gamma$ itself. So we have a situation $\gamma=\gamma+\alpha$ for a certain positive $\alpha$. This contradict the notion of order.

    In particular $\Span_{\eN}\{\alpha_i\}=\{\textrm{positive roots}\}$. Thus it is clear that
    \[
        \Span_{\eR}\{\alpha_i\}=\Phi.
    \]
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Weyl group}
%---------------------------------------------------------------------------------------------------------------------------
References about Weyl group: \cite{Knapp_reprez}. See also \cite{Cornwell}, page 530.

If \( \alpha\) is a root of \( \lG\) we define the \defe{symmetry}{symmetry!of a root} of \( \alpha\) as
\begin{equation}
    \begin{aligned}
        s_{\alpha}\colon \lH^*&\to \lH^* \\
        \beta&\mapsto \beta-\beta(H_{\alpha})\alpha
    \end{aligned}
\end{equation}
where \( H_{\alpha}\in\lH\) is the inverse root of \( \alpha\). Since \( \alpha(H_{\alpha})=2\) we have \( s_{\alpha}(\alpha)=-\alpha\). The group generated by the symmetries and the identity is the \defe{Weyl group}{Weyl!group}.

From what is said around equation \eqref{EqRealsHalpatalphaNorsmd} and the definition \( (\alpha,\beta)=\alpha(t_{\beta})\), we have
\begin{equation}
    s_{\alpha}(\beta)=\beta-\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }\alpha.
\end{equation}

We know from proposition \ref{Proppqasbabaa} that \( s_{\alpha}(\beta)\) is a root while there are only finitely many roots; thus the Weyl group is finite since there are only a finite number of maps from a finite set to itself.

The symmetries associated to roots are involutive:
\begin{equation}
    s_{\alpha}^2=\id.
\end{equation}
Indeed
\begin{equation}
    \begin{aligned}[]
        s^2_{\alpha}(\beta)&=s_{\alpha}\big( \beta-\beta(H_{\alpha})\alpha \big)\\
        &=\beta-\beta(H_{\alpha})-\big( \beta-\beta(H_{\alpha})\alpha \big)(H_{\alpha})\alpha\\
        &=\beta
    \end{aligned}
\end{equation}
if we take into account \( \alpha(H_{\alpha})=2\).

Relative to the symmetry \( s_{\alpha_i}\) we have the symmetry \( s_i\) on \( \lH\) defined by
\begin{equation}        \label{EqSymsiReltosalphai}
    s_i(h)=h-\alpha_i(h)H_i
\end{equation}
where \( h\in\lH\) and \( H_i\) is the inverse root of \( \alpha_i\).

\begin{remark}
    The simple roots \( \alpha_i\) {\bf are not} orthogonal.
\end{remark}

Let $\Delta$ be a reduced abstract root system on a real finite dimensional vector space $V$. The group $W$ generated by the $s_{\alpha}:\alpha\in\Delta$ is the \defe{Weyl group}{Weyl!group}\index{group!Weyl}.

\begin{proposition}     \label{PropWeylIsomalphai}
    The elements \( s_{\alpha_i}\) are isometries of \( \lH^*\), i.e.
    \begin{equation}
        \big( s_{\alpha_i}(\alpha),s_{\alpha_i}(\beta) \big)=(\alpha,\beta).
    \end{equation}
\end{proposition}

\begin{proof}
    For the sake of shortness, let us write
    \begin{equation}
        n_{i,\alpha}=\frac{ 2(\alpha_i,\alpha) }{ (\alpha_i,\alpha_i) }.
    \end{equation}
    We have \( t_{s_{\alpha_i}(\alpha)}=t_{\alpha}-n_{i,\alpha}t_{\alpha_i}\). Thus
    \begin{equation}
        B\big( t_{s_{\alpha_i}(\alpha)}, t_{s_{\alpha_i}(\beta)} \big)=B(t_{\alpha}-n_{i,\alpha}t_{\alpha_i},t_{\beta}-n_{i,\beta}t_{\alpha_i})
    \end{equation}
    distributing and taking into account the fact all the relations like \( B(t_{\alpha},t_{\alpha_i})=(\alpha,\alpha_i)\), the right hand side reduces to \( B(t_{\alpha},t_{\beta})=(\alpha,\beta)\).
\end{proof}

When \( \Phi\) is the root system, one can chose many different notions of positivity; each of them bring to different simple systems. It turns out that the action of the Weyl group on a simple system produces the simple system of an other choice of positivity on \( \Phi\).

\begin{lemma}       \label{LemalphajsPhipinjsasbab}
    If \( s_{\alpha_i}\alpha=s_{\alpha_i}\beta\), then \( \alpha=\beta\).
\end{lemma}

\begin{proof}
    The hypothesis \( s_{\alpha_j}(\alpha-\beta)=0\) provides
    \begin{equation}
        0=\alpha-\beta-\frac{ 2(\alpha-\beta,\alpha_j) }{ (\alpha_j,\alpha_j) }\alpha_j
    \end{equation}
    so that \( \alpha=\beta+z\alpha_j\) for some \( z\in\eC\). Thus we have
    \begin{equation}
        s_{\alpha_j}(\alpha)=s_{\alpha_j}(\beta)+zs_{\alpha_j}(\alpha_j)=s_{\alpha_j}(\alpha)-z\alpha_j.
    \end{equation}
    Thus \( z=0\) and \( \alpha=\beta\).
\end{proof}

\begin{proposition}     \label{PropsalphaisurjPhipmaj}
    Let \( \alpha_i\) a simple root. The set \( \Phi^+\setminus\{ \alpha_i \}\) is stable under \( s_{\alpha_i}\), i.e.
    \begin{equation}
        s_{\alpha_i}\big( \Phi^+\setminus\{ \alpha_i \} \big)=\Phi^+\setminus\{ \alpha_i \}.
    \end{equation}
\end{proposition}

\begin{proof}
    Let \( \lambda\in\Phi^+\) be a positive root. By theorem \ref{ThoposrootnjajnZ} we have
    \begin{equation}        \label{Eqllamsumajajajpos}
        \lambda=\sum_ja_j\alpha_j
    \end{equation}
    with \( a_j\geq 0\). Since \( \lambda\neq\alpha_i\) we have \( a_j>0\) for some \( j\neq i\). Indeed the only multiple of \( \alpha_i\) to be a root are \( 0\) and \( \pm\alpha_i\). Since \( \lambda\in\Phi^+\) and \( \lambda\neq \alpha_i\), none of these three solutions are taken into consideration.
    
    Let's apply \( s_{\alpha_i}\) on both sides of \eqref{Eqllamsumajajajpos}:
    \begin{equation}        \label{eqalialphaillamfracalphai}
        \begin{aligned}[]
            s_{\alpha_i}(\lambda)&=s_{\alpha_i}\big( \sum_ja_j\alpha_j \big)\\
            &=\sum_{j\neq i}a_js_{\alpha_i}(\alpha_j)+a_i\underbrace{s_{\alpha_i}(\alpha_i)}_{-\alpha_i}\\
            &=\sum_{j\neq i}a_j\alpha_j-\sum_{j\neq i}a_j\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }\alpha_i-a_i\alpha_i
        \end{aligned}
    \end{equation}
    Since a root is either positive or negative, the coefficients are either \emph{all} positive or \emph{all} negative. Since all the coefficients (apart for the one of \( \alpha_i\)) are the same as the ones of \( \lambda\), the root \eqref{eqalialphaillamfracalphai} is positive.

    We still have to prove that \( s_{\alpha_i}(\lambda)\neq \alpha_i\). Indeed if \( s_{\alpha_i}(\lambda)=\alpha_i\) we have
    \begin{equation}
        \lambda=s_{\alpha_i}s_{\alpha_i}(\lambda)=s_{\alpha_i}(\alpha_i)=-\alpha_i,
    \end{equation}
    which contradicts the positivity of \( \lambda\).

    Up to now we proved that \( s_{\alpha_i}\big( \Phi^+\setminus\{ \alpha_i \} \big)\subset\Phi^+\setminus\{ \alpha_i \}\). If \( \lambda\in\Phi^+\setminus\{ \alpha_i \}\), then
    \begin{equation}
        \sigma=s_{\alpha_i}(\lambda)\in s_{\alpha_i}\big( \Phi^+\setminus\{ \alpha_i \} \big)\subset\Phi^+\setminus\{ \alpha_i \}
    \end{equation}
    and \( s_{\alpha_i}(\sigma)=\lambda\), so that \( \lambda\) is the image by \( s_{\alpha_i}\) of \( \sigma\in\Phi^+\setminus\{ \alpha_i \}\).
\end{proof}

\begin{theorem}      \label{ThosajBijSurpPpsmaj}
    The map \( s_{\alpha_j}\colon \Phi^+\setminus\{ \alpha_j \}\to \Phi^+\setminus\{ \alpha_j \}\) is bijective.
\end{theorem}

\begin{proof}
    Surjectivity is proposition \ref{PropsalphaisurjPhipmaj} while injectivity is lemma \ref{LemalphajsPhipinjsasbab}.
\end{proof}

\begin{lemma}[\cite{Cornwell}, page 533]
    We consider the half sum of the positive roots:
    \begin{equation}
        \delta=\frac{ 1 }{2}\sum_{\alpha\in\Phi^+}\alpha.
    \end{equation}
    We have
    \begin{enumerate}
        \item
            If \( \alpha_j\) is a simple root, \( s_{\alpha_j}\delta=\delta-\alpha_j\).
        \item
            If \( \alpha_j\) is a simple root, \( (\delta,\alpha_j)=\frac{ 1 }{2}(\alpha_j,\alpha_j)\).
    \end{enumerate}    
\end{lemma}

\begin{proof}
    We compute \( s_{\alpha_j}\delta\) dividing the sum into two parts:
    \begin{subequations}
        \begin{align}
            s_{\alpha_j}\delta&=\frac{ 1 }{2}\sum_{\substack{\alpha\in\Phi^+\\\alpha\neq\alpha_j}}s_{\alpha_j}(\alpha)+\frac{ 1 }{2}s_{\alpha_j}(\alpha_j)\\
            &=\frac{ 1 }{2}\sum_{\substack{\alpha\in\Phi^+\\\alpha\neq\alpha_j}}\alpha-\frac{ 1 }{2}\alpha_j.
        \end{align}
    \end{subequations}
    The second inequality is from the fact that \( s_{\alpha_j}\) is bijective on \( \Phi^+\setminus\{ \alpha_j \}\) by theorem \ref{ThosajBijSurpPpsmaj}. Adding a subtracting \( \frac{ \alpha_j }{2}\) we get
    \begin{equation}
        s_{\alpha_j}\delta=\frac{ 1 }{2}\sum_{\alpha\in\Phi^+}\alpha-\frac{ \alpha_j }{2}-\frac{ \alpha_j }{2}=\delta-\alpha_j
    \end{equation}

    Using the proposition \ref{PropWeylIsomalphai}, we have
    \begin{equation}
        (\delta,\alpha_j)=(s_{\alpha_j}\delta,s_{\alpha_j}\alpha_j)=(\delta-\alpha_j,-\alpha_j)=-(\delta,\alpha_j)+(\alpha_j,\alpha_j),
    \end{equation}
    consequently, \( 2(\delta,\alpha_j)=(\alpha_j,\alpha_j)\) and the result follows.
\end{proof}

\subsection{Abstract root system}
%--------------------------------

The material about abstract root system  mainly comes from \cite{Knapp_reprez}. 

\begin{definition}      \label{DefAbsRootSystSerre}
    An \defe{abstract root system}{abstract!root system}\index{root!abstract} in a finite dimensional vector space $V$ endowed with an is a subset $\Phi$ of $V$ such that
    \begin{itemize}
        \item \( \Phi\) is finite and $\Span\Phi=V$,
        \item
            For every \( \alpha\in \Phi\), there is a symmetry \( s_{\alpha}\) of vector \( \alpha\) leaving \( \Phi\) stable.
        \item 
            For every \( \alpha,\beta\in\Phi\), the vector \( s_{\alpha}(\beta)-\beta\) is an integer multiple of \( \alpha\).
    \end{itemize}
\end{definition}
The abstract system is \defe{reduced}{reduced abstract root system} when $\alpha\in\Phi$ implies $2\alpha\notin\Phi$. It is \defe{irreducible}{irreducible!abstract root system} is $\Phi$ doesn't admits non trivial decomposition as $\Phi=\Phi'\cup\Phi''$ with $(\alpha,\beta)=0$ for any $\alpha\in\Phi'$ and $\beta\in\Phi''$. We use the notation $\Phi:=\Phi\cup\{0\}$. 


The following is a consequence of all we did up to now.
\begin{theorem}
    The root system of a complex semisimple Lie algebra is a reduced abstract root system.
\end{theorem}

The \defe{Weyl group}{Weyl group!abstract setting} of \( \Phi\) is the subgroup of \( \GL(V)\) generated by the transformations \( s_{\alpha}\) with \( \alpha\in\Phi\).

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Link with other definitions}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The definition \ref{DefAbsRootSystSerre} is not the ``usual'' one (in \cite{Wisser}, page 14 for example). We show now that we retrieve the usual features of an abstract.

\begin{lemma}
    An abstract root system admits a bilinear positive symmetric non degenerate form which is invariant under its Weyl group.
\end{lemma}

\begin{proof}
    If \( (.,.)_1\) is a bilinear positive non degenerate symmetric form on the vector space \( V\), the form
    \begin{equation}
        (\alpha,\beta)=\sum_{w\in W}(w\alpha,w\beta)_1
    \end{equation}
    is invariant under the Weyl group. This construction is possible since the Weyl group is finite.
\end{proof}

\begin{definition}
    Let \( V\) be a vector space and \( v\in V\) a non vanishing vector. A symmetry of vector \( v\) is an automorphism \( s\colon V\to V\) such that
    \begin{enumerate}
        \item
            \( s(v)=-v\);
        \item
            the set \( H=\{ w\in V\tq \alpha(w)=w \}\) is an hyperplane in \( V\).
    \end{enumerate}
\end{definition}
A symmetry of vector \( v\) induces the decomposition \( V=H\oplus\eR v\). The symmetries are of order \( 2\): \( s^2=\id\).

\begin{lemma}
    let \( v\) be a nonzero vector of \( V\) and \( A\) be a finite part of \( V\) such that \( \Span(A)=V\). Then there exists at most one symmetry of vector \( v\) leaving \( A\) invariant.
\end{lemma}

\begin{proof}
    Let \( s\) and \( s'\) be two such symmetries and consider \( u=ss'\). We immediately have \( u(A)=A\) and \( u(v)=v\). Let us prove that \( u\) induce the identity on the quotient \( V/\eR v\). A general vector in \( V\) can be written (in a non unique way) under the form
    \begin{equation}
        h+h'+v
    \end{equation}
    with \( h\in H\) and \( h'\in H'\). Let \( h=h'_1+\beta v\) be the decomposition of \( h\) in \( H'\oplus \eR v\) and \( h'=h_1+\gamma v\) be the decomposition of \( h'\) with respect to the direct sum \( V=H\oplus\eR v\).  Then we have
    \begin{subequations}
        \begin{align}
            ss'(h+h'+\alpha v)&=ss'\big( (h'_1+\beta v)+h'+\alpha v \big)\\
            &=s\big( (h'_1-\beta v)+h'+\alpha v \big)\\
            &=s(h-2\beta v+h_1+\gamma v+\alpha v)\\
            &=h+2\beta v+h_1-\gamma v+\alpha v\\
            &=h+h'+(\alpha-2\gamma+2\beta)v.
        \end{align}
    \end{subequations}
    Thus at the level of the quotient, $u$ leaves invariant \( h+h'\).

    It is not guaranteed that \( u\) is the identity, but the eigenvalues of \( u\) are \( 1\). For each \( x_i\in A\), there exists \( n_i\in\eN\) such that \( u^{n_i}x_i=x\). If \( n\) is a common multiple of all the \( n_i\) (these are finitely many), we have \( u^n(x)=x\) for every \( x\in A\). Since \( A\) generates \( V\), we have \( u^n=\id\) and then \( u\) is diagonalizable.

    We already mentioned the fact that the eigenvalues of \( u\) are \( 1\). Since \( u\) is diagonalizable, it is the identity and \( s=s'\).
\end{proof}

The invariant form give to \( V\) a structure of euclidian vector space for which the elements of the Weyl group are orthogonal matrices. Thus the symmetries read
\begin{equation}    \label{EqSymparnnusul}
    s_{\alpha}(x)=x-2\frac{ (x,\alpha) }{ (\alpha,\alpha) }\alpha.
\end{equation}
This is the only transformation which makes \( s_{\alpha}(\alpha)=-\alpha\) in the same time as being implemented by an orthogonal matrix. The symmetry \( s_{\alpha}\) is nothing else than the orthogonal symmetry with respect to the hyperplane orthogonal to \( \alpha\).

The expression \eqref{EqSymparnnusul} has the consequence that
\begin{equation}
    s_{\alpha}(\beta)-\beta=-\frac{ (\beta,\alpha) }{ (\alpha,\alpha) }\alpha.
\end{equation}
By the definition of an abstract root system, the latter has to be an integer multiple of \( \alpha\), so
\begin{equation}
    \frac{ 2(\beta,\alpha) }{ (\alpha,\alpha) }\in\eZ.
\end{equation}

\begin{definition}
    Two abstract root systems \( \Phi\) on \( V\) and \( \Phi'\) on \( V'\) are \defe{isomorphic}{isomorphism!of abstract root system} is there exists an isomorphism of vector space \( \psi\colon V\to V'\) such that \( \psi(\Phi)=\Phi'\) and
    \begin{equation}
        2\frac{ (\alpha,\beta) }{ (\alpha,\alpha) }=2\frac{ \big( \psi(\alpha),\psi(\beta) \big) }{ \big( \psi(\alpha),\psi(\alpha) \big) }
    \end{equation}
    for every \( \alpha,\beta\in \Phi\).
\end{definition}


%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Basis of abstract root system}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
The part about basis of abstract root system comes from \cite{SerreSSAlgebres}.

\begin{definition}      \label{DefbasisabsRoot}
    Let \( \Phi\) be an abstract root system. A part \( S\subset \Phi\) is a \defe{basis}{basis!of an abstract root system} of \( \Phi\) if 
    \begin{enumerate}
        \item
            \( S\) is a basis of \( V\) as vector space;
        \item
            every \( \beta\in\Phi\) can be written under the form
            \begin{equation}
                \beta=\sum_{\alpha\in S}m_{\alpha}\alpha
            \end{equation}
            where \( m_{\alpha}\) are all integers of the same sign.
    \end{enumerate}
\end{definition}
The set \( \Delta\) of simple roots of the root system of a complex semisimple Lie algebra is a basis.

We are going to build a basis of an abstract root system. Let \( h\in V^*\) be such that \( \alpha(h)\neq 0\) for every  \( \alpha\in\Phi\) and define
\begin{equation}
    \Phi_h^+=\{ \alpha\in\Phi\tq \alpha(h)>0 \}.
\end{equation}
We have \( \Phi=\Phi^+_h\cup -\Phi_h^+\). We say that an element \( \alpha\in\Phi^+_h\) is \defe{decomposable}{decomposable!in an abstract root system} if there exist \( \beta,\gamma\in\Phi_h^+\) such that \( \alpha=\beta+\gamma\). We write \( S_h\) the set of undecomposable elements in \( \Phi^+_h\).

\begin{lemma}       \label{LemShPhihpCBLSh}
    Any element in \( \Phi^+_h\) is a linear combination with positive coefficients of elements of \( S_h\).
\end{lemma}

\begin{probleme}
    It seems to me that Serre's book\cite{SerreSSAlgebres} has a misprint here. At page V-11 he writes :
    \begin{quote}
        Tout élément de \( R^+_t\) est combinaison linéaire, à coefficients entiers \( \geq 0\) des éléments de S.
    \end{quote}
    Shouldn't he have written \( S_t\).
\end{probleme}

\begin{proof}
    Let \( I\) be the set of \( \alpha\in\Phi^+_h\) that cannot be written under such a decomposition. We choose \( \alpha\in I\) such that \( \alpha(h)\) is minimal. If \( \alpha\) is undecomposable, then \( \alpha\in S_h\) and the condition \( \alpha\in I\) is contradicted. Thus \( \alpha\) is decomposable. Let \( \beta,\gamma\in\Phi^+_h\) be such that \( \alpha=\beta+\gamma\). Since \( \alpha(h)\) is minimal,
    \begin{equation}
        \begin{aligned}[]
            \beta(h)&\leq \alpha(h)\\
            \gamma(h)&\leq \alpha(h).
        \end{aligned}
    \end{equation}
    Thus we have \( \beta(h)=\alpha(h)-\gamma(h)<0\) which contradicts \( \beta\in\Phi^+\). We conclude that \( I\) is empty.
\end{proof}

\begin{lemma}       \label{LemShabShablesz}
    If \( \alpha,\beta\in S_h\), then \( (\alpha,\beta)\leq 0\).
\end{lemma}

\begin{proof}
    If \( (\alpha,\beta)\geq 0\), then proposition \ref{PropPropAbstrRootviiiikl}\ref{enubv} shows that \( \gamma=\alpha-\beta\) is a root. There are two possibilities: \( \gamma\in\pm\Phi^+_h\). If \( \gamma\in\Phi^+_h\), then \( \alpha=\gamma+\beta\) is decomposable; contradiction. If \( \gamma\in -\Phi^+_h\), then \( \beta=\alpha-\gamma\) is decomposable; contradiction.
\end{proof}

\begin{lemma}[Lemme 4 page V-12]        \label{LemIndepAhVstar}
    Let \( h\in V^*\) and \( A\subset V\) be a subset satisfying
    \begin{enumerate}
        \item
            \( \alpha(h)>0\) for every \( \alpha\in A\);
        \item
            \( (\alpha,\beta)\leq 0\) for every \( \alpha,\beta\in A\).
    \end{enumerate}
    Then the elements in \( A\) are linearly independent.
\end{lemma}

\begin{proof}
    Let us consider a vanishing linear combination of elements in \( A\):
    \begin{equation}        \label{EqNullCombinsumAmAuAd}
        \sum_{\alpha\in A}m_{\alpha}\alpha=0.
    \end{equation}
    We can sort the terms following that \( m_{\alpha}\) is positive or negative and cut the sum in two parts:
    \begin{equation}
        \sum_{\beta\in A_1}y_{\beta}\beta=\sum_{\gamma\in A_2}z_{\gamma}\gamma
    \end{equation}
    with \( y_{\beta},z_{\gamma}\geq 0\) and where \( A_1\) and \( A_2\) are disjoint subsets of \( A\). Let us consider \( \lambda=\sum_{\beta\in A_1}y_{\beta}\beta\) and compute
    \begin{equation}        \label{EqllamllamnprofAunAdeuxsom}
        (\lambda,\lambda)=\sum_{\substack{\beta\in A_1\\\gamma\in A_2}}y_{\beta}z_{\gamma}(\beta,\gamma).
    \end{equation}
    By hypothesis \( (\beta,\gamma)\) is lower than zero and by construction the product \( y_{\beta},z_{\gamma}\) is positive. Thus the right hand side of equation \eqref{EqllamllamnprofAunAdeuxsom} is negative. We conclude that \( \lambda=0\). Thus
    \begin{equation}
        0=\lambda(h)=\sum_{\beta\in A_1}y_{\beta}\beta(h).
    \end{equation}
    Since all the terms in the sum are larger than zero we have \( y_{\beta}=0\). In the same way we get \( z_{\gamma}=0\). The vanishing linear combination \eqref{EqNullCombinsumAmAuAd} is then trivial and the elements of \( A\) are linearly independent.
\end{proof}

\begin{proposition}\label{PropSestShsi}
    The elements of \( S_h\) form a basis of \( \Phi\) in the sense of definition \ref{DefbasisabsRoot}. Conversely, if \( S\) is a basis of \( \Phi\) and if \( h\in V^*\) is such that \( \alpha(h)>0\) for every \( \alpha\in S\),we have \( S=S_h\).
\end{proposition}

\begin{proof}
    The set \( S_h\) satisfies the conditions of lemma \ref{LemIndepAhVstar} since by definition \( \alpha(h)>0\) for every \( \alpha\in S_h\) and by lemma \ref{LemShabShablesz} the inner products are all negative. Thus \( S_h\) is a free set. It is generating by lemma \ref{LemShPhihpCBLSh}. Again by lemma \ref{LemShPhihpCBLSh}, every element in \( \Phi\) can be written as sum of elements of \( S_h\) with all coefficients of the same sign. Here we use the fact that \( v\) is positive if and only if \( -v\) is negative and that every vector is either positive or negative.

    For the second part, let \( S\) be a basis and \( h\in V^*\) such that \( \alpha(h)>0\) for all \( \alpha\in S\). Let
    \begin{equation}
        \Phi^+=\{ \sum_{\alpha\in S}m_{\alpha}\alpha \text{ with \( m_{\alpha}\in\eN\)} \}.
    \end{equation}
    We have \( \Phi^+\subset\Phi_h^+\) and \( -\Phi^+\subset -\Phi_h^+\). Since \( \Phi=\Phi^+\cup-\Phi^+\) we also have \( \Phi^+=\Phi_h^+\). Since elements of \( S\) are indecomposable in \( \Phi^+\), they are indecomposable in \( \Phi^+_h\) and we have \( S\subset S_h\).

    The sets \( S\) and \( S_h\) have the same number of elements because they both are basis of \( V\), thus \( S=S_h\).
\end{proof}

\begin{lemma}\label{LemwShShpahwahp}
    If \( h\) and \( h'\) are elements of \( V^*\) related by \( \alpha(h)=(w\alpha)h'\), then \( w(S_h)=S_{h'}\) (if these space can be defined).
\end{lemma}

\begin{proof}
    Let \( \alpha\in S_h\). The element \( w(\alpha)\) belongs to \( \Phi_{h'}^+\) because
    \begin{equation}
        w(\alpha)h'=\alpha(h)>0
    \end{equation}
    because \( \alpha\in\Phi_h^+\). We still have to check that \( w(\alpha)\) is undecomposable in \( \Phi_{h'}^+\). If \( w(\alpha)=\beta+\gamma\) with \( \beta,\gamma\in\Phi_{h'}^+\), we have \( \alpha=w^{-1}\beta+w^{-1}\gamma\). From the link between \( h\) and \( h'\) we have
    \begin{equation}
        (w^{-1}\beta)(h)=(ww^{-1}\beta)h'=\beta(h')>0.
    \end{equation}
    Thus \( w^{-1}\beta\in \Phi_h^+\) which is a contradiction because we supposed that \( \alpha\) is undecomposable.
\end{proof}

\begin{lemma}\label{Lemswwsbwemucirc}
    If \( \alpha,\beta\in\Phi\) and if \( w\in W_S\), then \( s_{w(\beta)}=w\circ s_{\beta}\circ w^{-1}\).
\end{lemma}

\begin{proof}
    Using the fact that the symmetries are isometries of the inner product,
    \begin{equation}
        s_{w(\beta)}(\alpha)=\alpha-\frac{ \big( w(\beta),\alpha \big) }{ \big( w(\beta),w(\beta) \big) }w(\beta)=\alpha-\frac{ (\beta),w^{-1}\alpha }{ (\beta,\beta) }w\beta.
    \end{equation}
    Applying that to \( w(\alpha)\) instead of \( \alpha\) and applying \( w^{-1}\), we have
    \begin{subequations}
        \begin{align}
            w^{-1}s_{w(\beta)}\big(w(\alpha)\big)&=w^{-1}\left( w\alpha-\frac{ (\beta,w^{-1}w\alpha) }{ (\beta,\beta) }w\beta \right)\\
            &=\alpha-\frac{ (\beta,\alpha) }{ (\beta,\beta) }w^{-1}w\beta\\
            &=s_{\beta}(\alpha).
        \end{align}
    \end{subequations}
\end{proof}

The following theorem is from \cite{SerreSSAlgebres}, page V-16.
\begin{theorem}     \label{ThoWeylGenere}
    Let \( W\) be the Weyl group of the abstract root system \( \Phi\). Let \( S\) a basis of \( \Phi\) and \( W_S\) the subgroup of \( W\) generated by \( s_{\alpha}\) with \(\alpha\in S\). Then
    \begin{enumerate}
        \item   \label{ItemThoWeylGenerei}
            for every \( h\in V^*\), there exists \( w\in W_S\) such that \( (w\alpha)(h)\geq 0\) for every \( \alpha\in S\).
        \item   \label{ItemThoWeylGenereii}
            If \( S'\) is a basis of \( \Phi\), the there exists a \( w\in W_S\) such that \( w(S')=S\).
        \item\label{ItemThoWeylGenereiii}
            For every \( \beta\in\Phi\) there exists \( w\in W_S\) such that \( w(\beta)\in S\).
        \item\label{ItemThoWeylGenereiv}
            The group \( W\) is generated by the symmetries \( s_{\alpha}\) with \( \alpha\in S\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    For item \ref{ItemThoWeylGenerei}, consider \( h\in V^*\) and \( \delta=\frac{ 1 }{2}\sum_{\gamma\in S}\gamma\). Let \( w\in W_S\) be such that \( w(\delta)h\) is the largest possible\footnote{We can consider that \( w\) because \( W\) is finite.}. If \( \alpha\in S\) we have
    \begin{equation}
        w(\delta)h\geq ws_{\alpha}(\delta)h=w(\delta)h-w(\alpha)h,
    \end{equation}
    so that \( w(\alpha)h\geq 0\) for every \( \alpha\in S\). This proves our first assertion.

    We pass to point \ref{ItemThoWeylGenereii}. Let \( h'\in V^*\) be such that \( \alpha'(h')> 0\) for every \( \alpha'\in S'\). By the first item there exists \( w\in W_S\) such that
    \begin{equation}
        (w\alpha)(h')\geq 0
    \end{equation}
    for every \( \alpha\in S\). In fact we even have \( w\alpha h'>0\) for every \( \alpha\in S\). Indeed \( w\alpha\) can be decomposed as \( \sum_{\alpha'\in S'}m_{\alpha'}\alpha'\) where all the \( m_{\alpha'}\) have the same sign. In this case
    \begin{equation}        \label{Eqwapsummapaphp}
        (w\alpha)h'=\sum_{\alpha'}m_{\alpha'}\alpha'(h')\neq 0
    \end{equation}
    because each of \( \alpha'(h')\) is strictly positive while all the terms of the sum have the same sign. This means, by the way, that \( S'=S_{h'}\) following the proposition \ref{PropSestShsi}.
    
    We define \( h\in V^*\) by the relation
    \begin{equation}
        \alpha(h)=(w\alpha)(h').
    \end{equation}
    By what we said in equation \eqref{Eqwapsummapaphp} we have \( \alpha(h)>0\) for every \( \alpha\in S\), so that we have \( S=S_h\). Finally by lemma \ref{LemwShShpahwahp}, \( w(S_h)=S_{h'}\).

    We prove now the point \ref{ItemThoWeylGenereiii}. For \( \gamma\in\Phi\) we consider the hyperplane
    \begin{equation}
        L_{\gamma}=\{ h\in V^*\tq \gamma(h)=0 \}.
    \end{equation}
    Consider a particular \( \beta\in\Phi\) the hyperplanes \( L_{\gamma}\) with \( \gamma\neq\pm\beta\) do not coincide with \( L_{\beta}\) and there is only finitely many of them, so there exists a \( h_0\in L_{\beta}\) such that \( h_0\) do not belong to any \( L_{\gamma}\) for any \( \gamma\neq \pm\beta\).

    In particular we have \( \beta(h_0)=0\) and \( \gamma(h_0)\neq 0\) for every \( \gamma\in\Phi\), \( \gamma\neq\pm\beta\). If we choose \( \epsilon\) small enough, there exists \( h\) near from \( h_0\) such that
    \begin{subequations}
        \begin{numcases}{}
            \beta(h)=\epsilon>0\\
            | \gamma(h) |>\epsilon&\text{if \( \gamma\neq \pm\beta\).}
        \end{numcases}
    \end{subequations}
    Let \( S_{h}\) be the basis associated with this \( h\). We have \( \beta\in S_h\). Indeed first \( \beta(h)=\epsilon>0\) and if \( \beta=\gamma+\rho\), we would have
    \begin{equation}
        \gamma(h)=\beta(h)-\rho(h)=\epsilon-\rho(h)<0,
    \end{equation}
    so that \( \beta\) is undecomposable in \( \Phi_h^+\). Now from point \ref{ItemThoWeylGenereii} there exists \( w\in W_S\) such that \( w(S_h)=S\). In particular \( w(\beta)\in S\).

    We turn our attention to the item \ref{ItemThoWeylGenereiv}. We are going to prove that \( W=W_S\). Since \( W\) is generated by the symmetries \( s_{\beta}\) (\( \beta\in\Phi\)), it is sufficient to prove that \( W_S\) generates the symmetries \( s_{\beta}\).

    Let \( \beta\in\Phi\) and consider the element \( w\in W_S\) such that \( \alpha=w(\beta)\in S\). From lemma \ref{Lemswwsbwemucirc} we have
    \begin{equation}
        s_{\alpha}=s_{w(\beta)}=w\circ s_{\beta}\circ w^{-1},
    \end{equation}
    so that
    \begin{equation}
        s_{\beta}=w^{-1}\circ s_{\alpha}\circ w\in W_S.
    \end{equation}
\end{proof}

What this theorem says in the case of complex semisimple Lie algebras is that if \( \{ \alpha_1,\ldots,\alpha_l \}\) is the set of simple roots, the symmetries \( s_{\alpha_i}\) generate the Weyl group. Now, since any root can be mapped on a simple one using the Weyl group, any root can be recovered from a simple one acting with the Weyl group that is generated by the simple ones.

Thus one can determine all the roots from the data of the simple ones by computing \( s_{\alpha_i}\alpha_j\) and then acting again with the \( s_{\alpha_i}\) on the results and again and again. This is the fundamental reason from which the root system can be recovered for the Cartan matrix.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Properties}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The main properties of an abstract root system are given in the  following proposition.
\begin{proposition}     \label{PropPropAbstrRootviiiikl}
If $\Phi$ is an abstract root system in a vector space $V$, one has the following properties:

\begin{enumerate}
\item\label{enubi} If $\alpha\in\Phi$ then $-\alpha\in\Phi$.

\item\label{enubii} If $\alpha\in\Phi$, the multiples of $\alpha$ which could also be in $\Phi$ are either $\pm\alpha$, or $\pm\alpha$ and $\pm 2\alpha$ or $\pm\alpha$ and $\pm\frac{1}{2}\alpha$.

\item\label{enubiii} If $\alpha\beta\in\Phi$ then $\frZ{\alpha}{\beta}$ can take the nonzero values $\pm 1$, $\pm 2$, $\pm 3$ or $\pm 4$. The case $\pm 4$ can only arise if $\beta=\pm 2\alpha$.

\item\label{enubiv} If $\alpha,\beta\in\Phi$ are not proportional each other and if $|\alpha|\leq|\beta|$, then $\frZ{\beta}{\alpha}$ equals $0$ or $\pm 1$.

\item\label{enubv} If $\alpha,\beta\in\Phi$ and $(\alpha,\beta)>0$, then $\alpha-\beta\in\Phi$ and if $(\alpha,\beta)<0$, the $\alpha+\beta\in\Phi$.

\item\label{enubvi} If $\alpha,\beta\in\Phi$ and neither $\alpha+\beta$ neither $\alpha-\beta$ belongs to $\Phi$, then $(\alpha,\beta)=0$.

\item\label{enubvii} If $\alpha\in\Phi$ and $\beta\in\Phi$, the $n\in\eZ$ such that $\beta+n\alpha\in\Phi$ fulfils $-p\leq n\leq q$ for certain $p,q\geq 0$. Moreover there are no gap,
\[
   p-q=\frZ{\alpha}{\beta},
\]
and there are at most four roots in the set $\{\beta+n\alpha\}_{-p\leq n\leq q}$.

\item\label{enubviii} If $\Phi$ is reduced, 

\begin{enumerate}
\item\label{enubviiia} If $\alpha\in\Phi$, the only multiples of $\alpha$ to lies in $\Phi$ are $\pm\alpha$,
\item\label{enubviiib} If $\alpha\in\Phi$ and $\beta\in\Phi$, then $\frZ{\alpha}{\beta}$can be equal to $0$, $\pm 1$, $\pm 2$ or $\pm 3$.
\end{enumerate}
\end{enumerate} \label{prop:Cartan_matr}
\end{proposition}
The proof will not use the fact that $\Phi$ spans $V$.

\begin{proof}
\ref{enubi} $s_{\alpha}\alpha=-\alpha$.

\ref{enubii} If $\beta=c\alpha$ with $|c|<1$, then 
\[
\frZ{\alpha}{\beta}=2c
\]
must belongs to $\eZ$, then $c=0,\pm\frac{1}{2}$. If $|c|>1$, we use exactly the same with $\alpha=\us{c}\beta$, so that $\us{c}=0;\pm\frac{1}{2}$. Now if $2\alpha$ is a root, it is clear that $\frac{1}{2}\alpha$ can't be.

If $\Phi$ is reduced, the fact that $\frac{1}{2}\alpha\in\Phi$ implies that $\alpha\notin\Phi$, so that $\pm\frac{1}{2}\alpha$ is excluded if $\alpha\in\Phi$, under the same assumption, $2\alpha$ is also excluded. This proves \ref{enubviiia}.

\ref{enubiii} The Schwartz inequality $|(\alpha,\beta)|\leq|\alpha||\beta|$ gives
\[
\left|   \frZ{\alpha}{\beta}\frZ{\beta}{\alpha}     \right|\leq 4.
\]
The equality only holds for $\beta=c\alpha$. In this case, we just saw that $\frZ{\alpha}{\beta}=2c$ with $c=2$ at most. If the equality is strict, then $\frZ{\alpha}{\beta}$ and $\frZ{\beta}{\alpha}$ are two integers whose product is $\leq 3$. The possibilities are $0$, $\pm 1$, $\pm 2$, $\pm 3$.

\ref{enubiv} If $|\alpha|\leq|\beta|$, then the following integer inequality holds:
\[
\left|\frZ{\alpha}{\beta}   \right|\leq\left|\frZ{\beta}{\alpha}   \right|.
\]
Since the product of the two  is $\leq 3$, the smallest is $0$ or $1$.

\ref{enubv}
If $\beta=c\alpha$, then $c=\pm\frac{1}{2},\pm 2,\pm 1$. All the cases are easy. If $(\alpha,\beta)>0$, then $c>0$ and $\alpha-\beta= \alpha-\frac{1}{2}\alpha=\frac{1}{2}\alpha$ or $\alpha-\beta=\alpha-2\alpha=-\alpha$.

Then we can suppose that $\alpha$ and $\beta$ are not proportional each other. We consider $\alpha,\beta\in\Phi$ and $(\alpha,\beta)>0$ (the other case is proved in much the same way). We just saw in \ref{enubiv} that $\frZ{\beta}{\alpha}$ could be equals to $0$ or $\pm 1$, then the fact that $(\alpha,\beta)>0$ imposes $\frZ{\beta}{\alpha}=1$, so that $s_{\beta}(\alpha)=\alpha-\beta$.

If $|\beta|\leq|\alpha|$, we use
\begin{equation}
s_{\alpha}(\beta)=\beta-\frZ{\beta}{\alpha}\alpha
               =\beta-\alpha,
\end{equation}

\ref{enubvi} is an immediate consequence of the previous point.

\ref{enubvii} Let $-p$ and $q$ be the smallest and the largest values of $n$ such that $\beta+n\alpha \in\Phi$. They exist because $\Phi$ is a finite set. Suppose that there is a gap between $r$ and $s$ ($r<s-1$), i.e. $\beta+r\alpha\in\Phi$, $\beta+s\alpha\in\Phi$, but $\beta+(r+1)\alpha,\beta+(s-1)\alpha\notin\Phi$.

By the point \ref{enubv}, $(\beta+r\alpha,\alpha)\geq 0$ and $(\beta+s\alpha,\alpha)\leq 0$. Making the difference between these two inequalities,
\[
   (r-s)|\alpha|^2\geq 0,
\]
then $r\geq s$, which contradict the definition of $r$ and $s$. So there is no gap. Now let us compute
\begin{equation}
\begin{split}
   s_{\alpha}(\beta+n\alpha)&=\beta+n\alpha-\frZ{\alpha}{\beta+n\alpha}\alpha\\
                          &=\beta+n\alpha-\left(    \frZ{\alpha}{\beta}+2n    \right)\alpha\\
              &=\beta-n\alpha-\frZ{\alpha}{\beta}\alpha\in\Phi.
\end{split}
\end{equation}
Then for any $n$ in $-p\leq n\leq q$, 
\[
   -q\leq n+\frZ{\alpha}{\beta}\leq p.
\]
With $n=q$, the second inequality gives $\frZ{\alpha}{\beta}\leq p-q$ while the first one with $n=-p$ gives  $p-q\leq\frZ{\alpha}{\beta}$.

The last point is to check the length of the string of root. We can suppose $q=0$ (i.e to look the string of $\beta-q\alpha$ instead of the one of $\alpha$; of course this is the same), then the length is $p+1$ and
\[
   p=\frZ{\alpha}{\beta}.
\]
If $\alpha$ and $\beta$ are not proportional, the point \ref{enubiii} makes it equals at most to $3$. If they are proportional, then the possibilities are $\alpha=\pm\beta,\pm\frac{1}{2}\beta,\pm 2\beta$. The string $\beta+n\alpha$ with $\alpha=\beta$ is at most $\{\beta,2\beta\}$, if $\alpha=\frac{1}{2}\beta$, this is just $\{\beta\}$ and if $\alpha=2\beta$, this is $\{\beta,-\beta\}$.

The proof is complete.
\end{proof}

When we have the Cartan matrix \( A \) of a semisimple complex Lie algebra, the first point is to find the norm of the roots by finding the diagonal matrix \( D\). We have \( (\alpha_i,\alpha_i)=D_{ii}\). For the other products we write
\begin{equation}
    A_{ij}=\frac{ 2(\alpha_i,\alpha_j) }{ D_{ii} },
\end{equation}
thus
\begin{equation}
    (\alpha_i,\alpha_j)=\frac{ D_{ii}A_{ij} }{ 2 }.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Abstract Cartan matrix}
%---------------------------------------------------------------------------------------------------------------------------

The following proposition summarize the properties of the of the Cartan matrix.
\begin{definition}      \label{DeabstrCartanmatr}
    A matrix \( (A_{ij})_{1\leq i,j\leq l}\) satisfying the following conditions is an \defe{abstract Cartan matrix}{Cartan!matrix!abstract}\index{abstract!Cartan matrix}
    \begin{enumerate}
        \item
            \( A_{ij}\in\eZ\),
        \item
            \( A_{ii}=2\),
        \item   \label{ItempoprCartaniii}
            \( A_{ij}\leq 0\) if \( i\neq j\),
        \item
            \( A_{ij}=0\) if and only if \( A_{ji}=0\),
        \item\label{ItempoprCartanv}
            there exists a diagonal matrix \( D\) with positive coefficients such that \( DAD^{-1}\) is symmetric and positive defined.
    \end{enumerate}
\end{definition}
The classification of abstract Cartan matrix will be performed in subsection \ref{SubsecDynkindiam}. The data of an abstract Cartan matrix defines an abstract root system. For a proof, see \cite{CartanRootProject}.

\begin{proposition}
    The Cartan matrix of a semisimple complex Lie algebra is an abstract Cartan matrix.
\end{proposition}

\begin{proof}
    The first two points are already done. For the point \ref{ItempoprCartaniii}, note that the sign of \( (\alpha,\beta)\) is not sure when \( \alpha\) is any root. However here we are speaking of simple roots. Let us consider the root
    \begin{equation}
        \lambda=\alpha_i-\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }\alpha_i
    \end{equation}
    Since it is a root, proposition \ref{ThoposrootnjajnZ} says that the coefficients in the decomposition in simple roots have to be all integer and of the same sign. Thus the combination \( (\alpha_i,\alpha_j)/(\alpha_i,\alpha_i)\) has to be negative.

    The point \ref{ItempoprCartanv} is also non trivial. Consider the diagonal matrix \( D=\diag\big( (\alpha_i,\alpha_i) \big)_{i=1,\ldots,l}\). We have
    \begin{subequations}
        \begin{align}
            (DAD^{-1})_{ij}&=\sum_{kl}D_{ik}A_{kl}(D^{-1})_{lj}\\
            &=\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i)^{1/2}(\alpha_j,\alpha_j)^{1/2} }.
        \end{align}
    \end{subequations}
    This is a symmetric matrix. In order to proof that this is positive defined, we are going to provide a matrix \( B\) such that \( DAD^{-1}=BB^t\). Let \( \{ \lambda_i \}\) be an orthonormal basis of \( \lH^*\) and consider the matrix \( b\) given by the decomposition of the simple roots in this basis:
    \begin{equation}
        \alpha_i=\sum_j b_{ij}\lambda_j.
    \end{equation}
    In particular we have \( (\alpha_i,\alpha_j)=\sum_kb_{ik}b_{jk}\). Then we consider the matrix
    \begin{equation}
        B_{ij}=\frac{ b_{ij} }{ (\alpha_i,\alpha_i)^{1/2} }
    \end{equation}
    which is non degenerate since the \( \alpha_i\) are simple and thus a re linearly independent. Small computation shows that 
    \begin{subequations}
        \begin{align}
            (BB^t)_{ij}&=\sum_k\frac{ b_{ik} }{ (\alpha_i,\alpha_i)^{1/2} }\frac{ b_{jk} }{ (\alpha_j,\alpha_j)^{1/2} }\\
            &=\frac{ (\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i)^{1/2}(\alpha_j,\alpha_j)^{1/2} }\\
            &=(DAD^{-1})_{ij}.
        \end{align}
    \end{subequations}
    But \( BB^t\) is positive defined, then \( DAD^{-1}\) is.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dynkin diagrams}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubsecDynkindiam}

The sources for Dynkin diagrams is \cite{SternLieAlgebra,Wisser}.

We are going to associate to each abstract Cartan matrix, a diagram that will uniquely correspond to an abstract root system. In other words what we are going to do is to classify the matrix satisfying the conditions of definition \ref{DeabstrCartanmatr}.

If \( A\) is an abstract Cartan matrix we build the \defe{Dynkin diagram}{Dynkin diagram} of \( A\) with the following rules.
\begin{enumerate}
    \item
        We put \( l\) vertices (one for each root)
    \item
        The vertex \( i\) and \( j\) are joined with \( A_{ij}A_{ji}\) lines.
\end{enumerate}
A step by step construction is available in \cite{Wisser}.

In the following we are considering an abstract Cartan matrix \( A\) and its associated abstract root system \( \{ \alpha_i \}\).

\begin{lemma}   \label{LesmabsCartDynk}
    A abstract Cartan matrix with its abstract root system and its Dynkin diagram have the following properties.
    \begin{enumerate}
        \item\label{ItemLesmabsCartDynki}
            If one remove the \( i\)th line an column of an abstract Cartan matrix, one still has an abstract Cartan matrix. In other words, each subdiagram of a Dynkin diagrm is a Dynkin diagram.
        \item\label{ItemLesmabsCartDynkii}
            Two vertices are linked by \emph{at most} three lines.
        \item\label{ItemLesmabsCartDynkiii}
            Each Dynkin diagram has more vertices than linked pairs.
        \item\label{ItemLesmabsCartDynkiv}
            A Dynkin diagram has no loop.
        \item\label{ItemLesmabsCartDynkv}
            A vertex in a Dynkin diagram has at most three lines attached (including multiplicities). Note: this is a generalization of point \ref{ItemLesmabsCartDynkii}.
        \item\label{ItemLesmabsCartDynkvi}
            Two root linked by a simple edge have equal \defe{weight}{weight!in a Dynkin diagram}, that is \( (\alpha_i,\alpha_i)=(\alpha_j,\alpha_j)\).
        \item\label{ItemLesmabsCartDynkvii}
            If the two roots \( \alpha_i\), \( \alpha_i\) are connected by a simple edge, we can collapse them, removing the connecting edge and conserving all the other edges.

    \end{enumerate}
\end{lemma}

\begin{proof}
    For point \ref{ItemLesmabsCartDynkii} we have
    \begin{equation}
        A_{ij}A_{ji}=4\frac{ (\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }\frac{ (\alpha_j,\alpha_i) }{ (\alpha_j,\alpha_j) }<4
    \end{equation}
    by Cauchy-Schwarz inequality. We insist on the fact that the inequality is strict since \( \alpha_i\) and \( \alpha_j\) are not collinear: they are simple roots.

    For point \ref{ItemLesmabsCartDynkiii} consider the form
    \begin{equation}
        \gamma=\sum_{i=1}^l\alpha_i(\alpha_i,\alpha_i)^{1/2}.
    \end{equation}
    Since the simple roots are linearly independent, this sum is nonzero and we have \( 0<(\gamma,\gamma)\). We have
    \begin{equation}
        \begin{aligned}[]
            0<(\gamma,\gamma)&=\sum_{ij}\frac{ (\alpha_i,\alpha_j) }{ \sqrt{(\alpha_i,\alpha_i)(\alpha_j,\alpha_j)} }\\
            &=2\sum_{i<j} \frac{ (\alpha_i,\alpha_j) }{ \sqrt{(\alpha_i,\alpha_i)(\alpha_j,\alpha_j)} }+\text{number of nodes}\\
            &=-\sum_{i<j}(A_{ij}A_{ji})^{1/2}+\text{number of nodes}.
        \end{aligned}
    \end{equation}
    Since for each linked pair \( (i,j)\) we have a term \( A_{ij}A_{ji}\geq 1\), we have \( -\sum_{i<j}(A_{ij}A_{ji})^{1/2}\leq \text{number of pairs}\) and the positivity of the sum shows that
    \begin{equation}
        \text{number of nodes}>\sum_{ij}A_{ij}A_{ji}\geq\text{number of pairs}.
    \end{equation}

    For item \ref{ItemLesmabsCartDynkiv}, suppose that a loop is given by the roots \( \alpha_1,\ldots,\alpha_n\). Since any sub-Dynkin diagram is a Dynkin diagram (from point \ref{ItemLesmabsCartDynki}), we can consider only the loop. This is a diagram with \( n\) vertices and \( n\) pairs, which contradicts point \ref{ItemLesmabsCartDynkiii}.

    We pass to item \ref{ItemLesmabsCartDynkv}. Let \( \alpha_0\) be a root linked to \( n\) simple lines, \( m\) double lines and \( p\) triple lines. For notational convenience, we write \( v_i=\alpha_i/(\alpha_i,\alpha_i)\), \( \{ v_i \}_{1\leq i\leq n}\) is the set of ``simply'' linked roots to \( \alpha_0\), \( \{ v'_i \}_{1\leq i\leq m}\) the set of ``doubly'' linked and \( \{ v''_i \}_{1\leq i\leq p}\) the set of ``triply'' ones. Consider the vector
    \begin{equation}
        \gamma=v_0+\sum_{i=1}^nf_iv_i+\sum_{i=1}^mg_iv'_i+\sum_{i=1}^ph_iv''_i
    \end{equation}
    where \( f_i\), \( g_i\) and \( h_i\) are constant to be determined. In order to compute the norm of \( \gamma\), notice that since there are no loops, no lines join \( v_i\), \( v'_i\) and \( v''_i\) together, so we have \( (v_i,v'_j)=(v_i,v''_j)=(v'_i,v''_j)=0\) and from the number of lines, \( (v_0,v_i)=-1/2\), \( (v_0,v'_i)=-1/\sqrt{2}\) and \( (v_0,v''_i)=-\sqrt{3}/2\). Thus we have
    \begin{equation}
        (\gamma,\gamma)=1+\sum_{i=1}^m(f_i^2-f_i)+\sum_{i=1}^m(g_i^2-\sqrt{2}g_i)+\sum_{i=1}^p(h_i^2-\sqrt{3}h_i).
    \end{equation}
    The minimum is realised with \( f_i=1/2\), \( g_i=\sqrt{2}/2\) and \( h_i=\sqrt{3}/2\) and for these values we have
    \begin{equation}
        (\gamma,\gamma)=1-\frac{ n+2m+3p }{ 4 }.
    \end{equation}
    Since the inner product has to be positive we must have \( n+2m+3p<4\), the is the number of lines issued from \( \alpha_0\) has to be lower or equal to \( 3\).
   
    In order to proof \ref{ItemLesmabsCartDynkvi}, remark that if \( \alpha_i\) and \( \alpha_j\) are connected by a simple edge, then \( A_{ij}A_{ji}=1\), which is only possible with \( A_{ij}=A_{ji}=-1\). In particular we have \( 2(\alpha_i,\alpha_j)/(\alpha_i,\alpha_i)=2(\alpha_j,\alpha_i)/(\alpha_j,\alpha_j)\), which proves that \( (\alpha_i,\alpha_i)=(\alpha_j,\alpha_j)\).

    Proof of item \ref{ItemLesmabsCartDynkvii}. Since the two roots have same weight, the item \ref{ItemLesmabsCartDynkvi} says that up to permutation the Cartan matrix has a block \( 2\times 2\) looking like
    \begin{equation}
        \begin{pmatrix}
            2    &   -1    \\ 
            -1    &   2    
        \end{pmatrix}.
    \end{equation}
    The proposed move consist to replace that block with the \( 1\times 1\) matrix \( (2)\). As an example,
    \begin{equation}
        \begin{pmatrix}
             2   &   -1    &   0    &   0    \\
             -1   &   2    &   -1    &   -1    \\
             0   &   -1    &   2    &   0    \\ 
             0   &   -1    &   0    &   2     
         \end{pmatrix}\mapsto
         \begin{pmatrix}
             2   &   -1    &   -1    \\
             -1   &   2    &   0    \\
             -1   &   0    &   2
         \end{pmatrix}.
    \end{equation}
    It is clear that the obtained matrix is still an abstract Cartan matrix.
\end{proof}

From these properties we can deduce much constrains on the Dynkin diagrams. First, the only diagram containing a triple edge is
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@3{-}[r]        &   \alpha_2
       }
\end{equation}

Let pass to the diagrams with only simple and double edges. If there is a double, there cannot be a triple point: the following is impossible
\begin{equation}
    \xymatrix{%
         &                          &           &               &       \alpha_5\\
        \alpha_1 \ar@2{-}[r]   &    \alpha_2 \ar@{-}[r] & \alpha_3\ar@{-}[r]&  \alpha_4 \ar@{-}[ru]\ar@{-}[rd]\\
        &&&&\alpha_6
       }
\end{equation}
since collapsing the roots \( \alpha_2\), \( \alpha_3\) and \( \alpha_4\) should create a point with four edges. Thus a diagram with a double edge is only possible inside a straight chain. Let us study the diagram
\begin{equation}        \label{EqdiaguduuuDy}
    \xymatrix{%
    \alpha_1 \ar@1{-}[r]&\alpha_2  \ar@2{-}[r]   &    \alpha_3 \ar@{-}[r] & \alpha_4\ar@{-}[r]&  \alpha_5
       }
\end{equation}
Once again we denote \( v_i=\alpha_i/| \alpha_i |\) and we consider the (non vanishing) vector
\begin{equation}
    \gamma=v_1+bv_2+cv_3+dv_4+ev_5
\end{equation}
whose norm is given by
\begin{equation}
    (\gamma,\gamma)=1+b^2+c^2+d^2+e^2-b-\sqrt{2}bc=cd=de.
\end{equation}
Equating all the partial derivative to zero provides the point 
\begin{equation}
    \begin{aligned}[]
        b&=2&c&=\frac{ 3 }{ \sqrt{2} }&d&=\sqrt{2}&e&=\frac{1}{ \sqrt{2} }.
    \end{aligned}
\end{equation}
One check that with these values \( (\gamma,\gamma)=0\) which is impossible. The diagram \eqref{EqdiaguduuuDy} is thus impossible. By the collapsing principle, all the diagrams of the form
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@1{-}[r]&\alpha_2  \ar@2{-}[r]   &    \alpha_3 \ar@{-}[r] & \alpha_4\ar@{-}[r]& \ldots \ar@{-}[r]&  \alpha_l
       }
\end{equation}
are impossible. The only possible diagrams with double edge are thus
\begin{subequations}
    \begin{align}
    \xymatrix{%
    \alpha_1 \ar@1{-}[r]&\alpha_2  \ar@2{-}[r]   &    \alpha_3 \ar@{-}[r] & \alpha_4
       }\\
    \xymatrix{%
    \alpha_1 \ar@2{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_l
    }    \label{subEqDynkdspds}\\
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@2{-}[r]&\alpha_l.
    }    \label{subEqdunksspd}
    \end{align}
\end{subequations}
The diagrams \eqref{subEqDynkdspds} and \eqref{subEqdunksspd} are the same. They however do not completely determine the abstract Cartan matrix because the diagram \eqref{subEqdunksspd} induces an asymmetry between \( \alpha_1\) and \( \alpha_2\). The so written Dynkin diagram cannot distinguish between the matrices
\begin{equation}
    \begin{aligned}[]
        \begin{pmatrix}
            2    &   -2    &   0    \\
            -1    &   2    &   -1    \\
            0    &   -1    &   2
        \end{pmatrix}&&
        \text{and}&&
        \begin{pmatrix}
            2    &   -1    &   0    \\
            -2    &   2    &   -1    \\
            0    &   -1    &   2
        \end{pmatrix}&
    \end{aligned}
\end{equation}
Thus we split the diagram \eqref{subEqdunksspd} into
\begin{subequations}        \label{suBeqdfynkabGP}
    \begin{align}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@2{->}[r]&\alpha_l.
    }   \\
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@2{<-}[r]&\alpha_l.
    }    
    \end{align}
\end{subequations}
In which the arrow points to the biggest root. The first one means that \( | \alpha_1 |=\ldots=| \alpha_{l-1} |=1\), \( \alpha_{l}=2\) while the second diagram means \( | \alpha_1 |=\ldots=| \alpha_{l-2} |=| \alpha_l |=1\), \( \alpha_{l-1}=2\).

We'll have to come back on this point later in subsection \ref{subsecRecbyhanfd}. Notice that this is the only diagram on which that problem occurs.

We are left to study the diagrams with only single edge. The following diagram is the simplest possible one:
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l}.
       }
\end{equation}
We have to know under what conditions one can have a triple point. We already know that there can be only one triple point.

If a diagram has a triple point, then one of the branch is of length \( 1\). Indeed if not we would have the following diagram:
\begin{equation}
    \xymatrix{%
    &                          &                       &       \alpha_2\ar@{-}[r]&\alpha_5\\
        \alpha_7 \ar@{-}[r]   &    \alpha_4 \ar@{-}[r] & \alpha_1 \ar@{-}[ru]\ar@{-}[rd]\\
        &&&\alpha_6\ar@{-}[r]&\alpha_6
       }
\end{equation}
Looking at the vector \( \gamma=3v_1+2(v_2+v_3+v_4)+v_5+v_6+v_7\) provides \( (\gamma,\gamma)=-3\) which is impossible. Thus the diagrams with a branch are straight chains with one unique triple point which has a branch of length one. The question is: where can happen that branch ? The diagram
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]&\alpha_4  \ar@1{-}[r]\ar@{-}[d]&\alpha_5  \ar@1{-}[r]&\alpha_6  \ar@1{-}[r]&\alpha_7\\
    &&&\alpha_8
       }
\end{equation}
cannot happen since the corresponding vector \( v_1+2v_2+3v_3+4v_4+3v_5+2v_6+v_7+2v_8\) has norm zero. Thus on a triple point, one branch has one branch of length \( 1\) and at least one other to be of length \( 1\) or \( 2\). It turns out that all the diagrams of the form
\begin{equation}
    \xymatrix{%
    &                          &                       &       \alpha_{l-1}\\
    \alpha_1 \ar@{-}[r]   &    \ldots \ar@{-}[r] & \alpha_{l-2} \ar@{-}[ru]\ar@{-}[rd]\\
        &&&\alpha_l
       }
\end{equation}
are possible. We are thus left with diagrams with a triple point with a branch of length \( 1\) and a branch of length \( 2\) :
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_5  \ar@1{-}[r]&\ldots  \ar@1{-}[r]&\alpha_l \\
    &&\alpha_4
       }
\end{equation}
The diagram with a branch of length \( 5\)
\begin{equation}
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_5  \ar@1{-}[r]   &\alpha_6  \ar@1{-}[r]   &\alpha_7  \ar@1{-}[r]    &\alpha_8 \ar@1{-}[r]&\alpha_9  \\
    &&\alpha_4
       }
\end{equation}
does not exist. We achieve the proof of that fact using for example this code for \href{http://www.sagemath.org}{sage}:
\begin{verbatim}
----------------------------------------------------------------------
| Sage Version 4.7.1, Release Date: 2011-08-11                       |
| Type notebook() for the GUI, and license() for information.        |
----------------------------------------------------------------------
sage: a=[var('a'+str(i-1)) for i in range(1,11)]
sage: l=9
sage: a[1]=1
sage: squares = sum( [a[i]**2 for i in range(1,l+1)] )     # The sum goes to l
sage: lines = sum(  [a[i]*a[i+1] for i in range(1,l-1)  ]   )+a[3]*a[9]  # The sum goes up to l-2
sage: f=symbolic_expression(squares - lines)
sage: X = solve( [f.diff(a[i])==0 for i in range(2,l+1)],[ a[i] for i in range(2,l+1)  ]  )   
sage: print X[0]
[a2 == 2, a3 == 3, a4 == (5/2), a5 == 2, a6 == (3/2), a7 == 1, a8 == (1/2), a9 == (3/2)]
sage: f(   *tuple(  [  X[0][i].rhs() for i in range(0,l-1)  ]  )   )
0
\end{verbatim}
This proves that the vector \( v_1+2v_2+3a_3+\frac{ 5 }{2}v_4+2v_5+\frac{ 3 }{2}v_6+v_7+\frac{ 1 }{2}v_8+\frac{ 3 }{2}v_9\) has vanishing norm, which is impossible.

\begin{probleme}
    This code raises a deprecation warning that I'm not able to solve.
\end{probleme}

We are finally left with the diagrams with one triple point with one branch of length \( 1\), one branch of length \( 2\) and the third branch with length \( 1\), \( 2\), \( 3\) or \( 4\) :
\begin{subequations}
    \begin{align}
        &\xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_4   \\
        &&\alpha_5
           }\\
        &\xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_4  \ar@1{-}[r]& \alpha_5\\
        &&\alpha_6
        }\\
        &\xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_4  \ar@1{-}[r]&\alpha_5  \ar@1{-}[r]& \alpha_6\\
        &&\alpha_7
        }\\
        &\xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3  \ar@1{-}[r]\ar@{-}[d]&\alpha_4  \ar@1{-}[r]&\alpha_5  \ar@1{-}[r]& \alpha_6  \ar@1{-}[r]&\alpha_7\\
        &&\alpha_8
        }
    \end{align}
\end{subequations}

In order to list all the possible complex semisimple Lie algebra, we have to check each of the left Dynkin diagrams if they give rise to an abstract Cartan matrix.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Example of reconstruction by hand}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecRecbyhanfd}

We turn now our attention on the difference between the two diagrams \eqref{suBeqdfynkabGP}. The Cartan matrix of the diagram $
        \xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2 \ar@2{->}[r]&\alpha_3
        }   $ is given by
        \begin{equation}
        A=\begin{pmatrix}
            2    &   -1    &   0    \\
            -1    &   2    &   -2    \\
            0    &   -1    &   2
       \end{pmatrix}.
        \end{equation}
The diagonal matrix \( D\) of definition \ref{DeabstrCartanmatr} is
\begin{equation}
    D=\begin{pmatrix}
        1    &       &       \\
            &   1    &       \\
            &       &   2
    \end{pmatrix}
\end{equation}
and the length of the roots are \( \| \alpha_1 \|=\| \alpha_2 \|=1\) and \( | \alpha_3 |=2\). Let us compute the angles between the roots. In order to compute \( (\alpha_1,\alpha_2)\) we look at \( A_{12}\):
\begin{equation}
    A_{12}=-1=2\frac{ (\alpha_1,\alpha_2) }{ (\alpha_1,\alpha_1) },
\end{equation}
and the same computation with \( A_{23}\) provides
\begin{subequations}
    \begin{align}
        (\alpha_1,\alpha_2)&=-\frac{ 1 }{2}\\
        (\alpha_2,\alpha_3)&=-1
    \end{align}
\end{subequations}
We compute all the roots using the theorem \ref{ThoWeylGenere} which basically says that acting with the ``simple'' Weyl group \( W_S\) on the simple roots generates all the roots. On the first strike we have
\begin{equation}
    \begin{aligned}[]
        s_1(\alpha_2)&=\alpha_2+\alpha_1&s_2(\alpha_1)&=\alpha_1+\alpha_2&s_3(\alpha_1)&=\alpha_1\\
        s_1(\alpha_3)&=\alpha\alpha_3 &s_2(\alpha_3)&=\alpha_3+2\alpha_2&s_3(\alpha_2)&=\alpha_2+\alpha_3.
    \end{aligned}
\end{equation}
We discovered the roots \( \alpha_2+\alpha_1\), \( \alpha_3+2\alpha_2\) and \( \alpha_2+\alpha_3\). Acting again on these roots by \( s_{\alpha_1}\), \( s_{\alpha_2}\) and \( s_{\alpha_3}\) the only new results are
\begin{equation}
    \begin{aligned}[]
        s_1(\alpha_3+\alpha_2)&=\alpha_1+\alpha_2+\alpha_3\\
        s_1(\alpha_3+2\alpha_2)&=2\alpha_1+2\alpha_2+\alpha_3.
    \end{aligned}
\end{equation}
Acting again we find only one new root: 
\begin{equation}
    s_{\alpha_2}(\alpha_1+\alpha_2+\alpha_3)=\alpha_1+2\alpha_2+\alpha_3. 
\end{equation}
We check that acting once again with the three simple roots on this last one does not brings new roots. Thus we have \( 9\) positive roots. Adding the negative ones, we are left with \( 18\) root spaces of dimension one. The Cartan algebra has dimension \( 3\), so the algebra we are looking at has dimension \( 21\).

Now take a look at the similar Dynkin diagram and its Cartan matrix:
\begin{subequations}
    \begin{align}
        \xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2 \ar@2{<-}[r]&\alpha_3
        }   &
        &A&=\begin{pmatrix}
            2    &   -1    &   0    \\
            -1    &   2    &   -1    \\
            0    &   -2    &   2
       \end{pmatrix}
    \end{align}
\end{subequations}
The inner products are
\begin{equation}
    \begin{aligned}[]
        | \alpha_1 |=|\alpha_3|=1, | \alpha_2 |=2 \\
        (\alpha_1,\alpha_2)=-1/\sqrt{2},(\alpha_2,\alpha_3)=-1
    \end{aligned}
\end{equation}
and the roots are
\begin{subequations}
    \begin{align}
        \alpha_1\\
        \alpha_2\\
        \alpha_3\\
        \alpha_1+\alpha_2\\
        \alpha_2+\alpha_3\\
        \alpha_2+2\alpha_3\\
        \alpha_1+\alpha_2+\alpha_3\\
        \alpha_1+\alpha_2+2\alpha_3\\
        \alpha_1+2\alpha_2+2\alpha_3.
    \end{align}
\end{subequations}
We see that the inner products are already not the same. Notice that the roots are really different: it is not simply a renaming \( \alpha_2\leftrightarrow \alpha_3\).

Thus the two Dynkin diagrams \eqref{subEqdunksspd} are describing two different Lie algebras.


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Reconstruction}
%---------------------------------------------------------------------------------------------------------------------------

The construction theorem is the following.
\begin{theorem}
    Let \( R\) be an abstract root system in a complex vector space \( V^*\) and \( \{ \alpha_1,\ldots,\alpha_n \}\) be a basis of \( R\). We denote by \( H_i\in V\) the \defe{inverse root}{inverse!root}\index{root!inverse} of \( \alpha_i\)(i.e. \( \alpha(H_{\alpha})=2\)). We define the Cartan matrix
    \begin{equation}
        A_{ij}=\alpha_j(H_i).
    \end{equation}
    Let \( \lG\) be the Lie algebra defined by the \( 3n\) generators \( X_i,Y_i,H_i\) and the relations
    \begin{subequations}
        \begin{align}
            [H_i,H_j]&=0\\
            [X_i,Y_j]&=\delta_{ij}H_i\\
            [H_i,X_j]&=A_{ij}X_j\\
            [H_i,Y_j]&=-A_{ij}Y_j
        \end{align}
    \end{subequations}
    and, for \( i\neq j\),
    \begin{subequations}
        \begin{align}
            \ad(X_i)^{-A_{ij}+1}(X_j)&=0        \label{EqSerrea}\\
            \ad(Y_i)^{-A_{ij}+1}(Y_j)&=0.
        \end{align}
    \end{subequations}
    Then \( \lG\) is a semisimple Lie algebra in which a Cartan subalgebra is generated by \( H_1,\ldots,H_n \) and its root system is \( R\).
\end{theorem}
A complete proof can be found in \cite{SerreSSAlgebres} at page VI-19. We are going to give some ideas.

We consider \( \lG\), the Lie algebra generated by the elements \( H_i\), \( X_i\) and \( Y_i\). We denote by \( \lH\) the abelian Lie algebra generated by the elements \( H_i\).
\begin{lemma}       \label{LemadXiNilpotent}
    The endomorphism \( \ad(X_i)\) and \( \ad(Y_i)\) are nilpotent.
\end{lemma}

\begin{proof}
    Let \( V_i\) the subspace of \( \lG\) of elements \( z\) such that \( \ad(X_i)^kz=0\) for some \( k\in\eN\). The space \( V_i\) is a Lie subalgebra of \( \lG\) because
    \begin{equation}
        \ad(X_i)[z,z']=-[z,\ad(X_i)z']+[z',\ad(X_i)z].
    \end{equation}
    Acting with \( \ad(X_i)^n\) we get terms of the form \( [\ad(X_i)^kz,\ad(X_i)^lz']\) with \( k+l=n\). If \( n\) is large enough, all the terms vanish.

    From the relation \eqref{EqSerrea} we see that \( X_j\in V_i\) for every \( j\). Since \( [X_i,H_j]\) is proportional to \( X_i\) we also have \( H_j\in V_i\) and then \( Y_j\in V_i\) because \( [X_i,Y_j]=\delta_{ij}H_i\in V_i\). Thus the Lie algebra \( V_i\) contains all the Chevalley generators and then \( V_i=\lG\).
\end{proof}

For \( \lambda\in\lH^*\) we define
\begin{equation}
    \lG_{\lambda}=\{ z\in\lG\tq\ad(h)z=\lambda(h)z\forall h\in\lH \}.
\end{equation}

Then one prove that \( \dim\lG_{\alpha_i}=1\) and \( \dim\lG_{m\alpha_i}=0\) if \( m\neq \pm 1,0\). This corresponds to the fact that we have a reduced root system, which is always the case in complex semisimple Lie algebras\footnote{However, at this point we have not proved yet that \( \lG\) is semisimple and has that root system.}. We denote by \( \Phi\) the subset of \( \lambda\in\lH^*\) such that \( \lG_{\lambda}\neq 0\).

It turns out that we have the direct sum decomposition
\begin{equation}
    \lG=\lH\oplus\bigoplus_{\alpha\in\Phi}\lG_{\alpha}.
\end{equation}

One of the key ingredients in this building is the following lemma.
\begin{lemma}
    If \( \lambda\) and \( \mu\) are related by an element of the Weyl group, then \( \dim\lG_{\lambda}=\lG_{\mu}\).
\end{lemma}


\begin{proof}
    Lemma \ref{LemadXiNilpotent} allows us to introduce the automorphism
    \begin{equation}
        \theta_i= e^{\ad(X_i)} e^{-\ad(Y_i)} e^{\ad(X_i)}
    \end{equation}
    of \( \lG\). We see that the restriction of \( \theta_i\) to \( \lH\) is the symmetry associated to \( \alpha_i\) (see \eqref{EqSymsiReltosalphai}). Indeed the first exponential reduces to
    \begin{equation}
        e^{\ad(X_i)}H_k=H_k-A_{ki}X_i
    \end{equation}
    where \( A_{ki}=\alpha_i(H_k)\). The second exponential gives
    \begin{equation}
        \begin{aligned}[]
            e^{\ad(-Y_i)}(H_k-A_{ki}X_i)&=H_k-A_{ki}X_i+(-A_{ki}Y_i-A_{ki}H_i)+\frac{ 1 }{2}(2A_{ki}Y_i)\\
            &=H_k-A_{ki}H_i-A_{ki}X_i.
        \end{aligned}
    \end{equation}
    Notice the simplification of \( A_{ki}Y_i\). The third exponential then provides the result (after some simplifications):
    \begin{equation}
        e^{\ad(X_i)}(H_k-A_{ki}H_i-A_{ki}X_i)=H_k-A_{ki}H_i=H_k-\alpha_i(H_k)H_i.
    \end{equation}
    We proved that \( \theta_i(H_k)=s_I(H_k)\).  We deduce that \( \theta_ie_{\alpha}\in\lG_{s_{\alpha_i}(\alpha)}\) whenever \( e_{\alpha}\in\lG_{\alpha}\). Since \( \theta_i\) is an automorphism of \( \lG\) we have
    \begin{equation}
        [H_k,\theta_ie_{\alpha}]=\theta_i[\theta_i^{-1}H_k,e_{\alpha}].
    \end{equation}
    Since \( \theta_i\) reduces to the involutive automorphism \( s_i\) on \( \lH\) we have \( \theta_i^{-1}H_k=\theta_iH_k=s_i(H_k)\). Then we have
    \begin{equation}
        [H_k,\theta_ie_{\alpha}]=\theta_i[s_i(H_k),e_{\alpha}]=\theta_i\alpha\big( s_i(H_k) \big)e_{\alpha}.
    \end{equation}
    The eigenvalue of \( \theta_ie_{\alpha}\) for \( \ad(H_k)\) is thus \( \alpha\big( s_i(H_k) \big)\). Using the definition and \( A_{ki}=\alpha_i(H_k)\) we have
    \begin{equation}
        \begin{aligned}[]
            \alpha\big( s_i(H_k) \big)&=\alpha(H_k)-\alpha_i(H_k)\alpha(H_i)\\
            &=\big( \alpha-\alpha(H_i)\alpha_i \big)H_k\\
            &=s_{\alpha_i}(\alpha)H_k.
        \end{aligned}
    \end{equation}
    At the end we got
    \begin{equation}
        [H_k,\theta_ie_{\alpha}]=s_{\alpha_i}(H_k)\theta_ie_{\alpha}
    \end{equation}
    and then \( \theta_ie_{\alpha}\in\lG_{s_{\alpha_i}(\alpha)}\). Thus the automorphism \( \theta_i\) transforms \( \lG_{\lambda}\) into \( \lG_{\mu}\) when \( \mu=s_i(\lambda)\) and
    \begin{equation}
        \dim\lG_{\lambda}=\dim\lG_{s_i(\lambda)}.
    \end{equation}
\end{proof}
From here we prove that \( \dim\lG_{\alpha}=1\) for every root \( \alpha\)\footnote{\cite{SerreSSAlgebres} page VI-23. Be careful: this is not the statement of page VI-2.}. 

Now if \( \alpha+\beta=\gamma+\mu\), the elements \( [E_{\alpha},E_{\beta}]\) and \( [E_{\gamma},E_{\mu}]\) are proportional since they belong to the one-dimensional space \( \lG_{\alpha+\beta}\).


\begin{remark}      \label{RemChevDefmapCommXH}
    A linear map \( \phi\colon \lG\to V\) from \( \lG\) to a vector space \( V\) can be defined on the generators \( X_i\), \( Y_i\) and \( H_i\) among with a formula giving \( \phi([X,Y])\) in terms of \( \phi(X)\) and \( \phi(Y)\).
\end{remark}

\begin{probleme}
    This remark could be made more precise. I'm thinking to the proposition \ref{PropStandardBialgStruct} giving the standard bialgebra structure on a Lie algebra.
\end{probleme}

The classification of complex semisimple Lie algebras is the following:
\begin{subequations}
    \begin{align}
        A_l&&\gsl(l+1,\eC)&&\dim=l(l+2)&&l=1,2,\ldots&&
        \xymatrix{%
        \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\ldots  \ar@1{-}[r]&\alpha_l  
           }\\
        B_l&&\go(2l+1,\eC)&&\dim=l(2l+1)&&l=2,3,\ldots&&
    \xymatrix{%
    \alpha_1 \ar@2{->}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@{-}[r]&\alpha_l.
    }   \\
    C_l&&\gsp(l,\eC)&&\dim=l(2l+1)&&l=3,4,\ldots&&
    \xymatrix{%
    \alpha_1 \ar@{->}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-1}\ar@2{->}[r]&\alpha_l.
    }   \\
    D_l&&\go(2l,\eC)&&\dim=l(2l-1)&&l=4,5,\ldots&&
    \xymatrix{%
    &                       &                       &                          &\alpha_{l-1}\\
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \ldots \ar@{-}[r] & \alpha_{l-2}\ar@{-}[dr]\ar[ur]\\
    &               &                               &                              &\alpha_l
    }   \\
    E_6&&&&\dim=78&&l=7,\ldots&&
    \xymatrix{%
    &                   &   \alpha_{6}\\
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]   & \alpha_3 \ar@{-}[r]\ar@{-}[u] & \alpha_4\ar@{-}[r] &\alpha_5
    }   \\
    E_7&&&&\dim=133&&l=7,\ldots&&
    \xymatrix{%
    &                  & &   \alpha_{7}\\
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3\ar@{-}[r]   & \alpha_4 \ar@{-}[r]\ar@{-}[u] & \alpha_5\ar@{-}[r] &\alpha_6
    }   \\
    E_8&&&&\dim=248&&l=8,\ldots&&
    \xymatrix{%
    &                &  & &   \alpha_{8}\\
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@1{-}[r]&\alpha_3\ar@{-}[r]&\alpha_4\ar@{-}[r]  & \alpha_5 \ar@{-}[r]\ar@{-}[u] & \alpha_6\ar@{-}[r] &\alpha_7
    }   \\
    F_4&&&&\dim=52&&l=4,\ldots&&
    \xymatrix{%
    \alpha_1 \ar@{-}[r]&\alpha_2  \ar@2{->}[r]   & \alpha_3 \ar@{-}[r] & \alpha_4
    }   \\
    G_2&&&&\dim=14&&l=2,\ldots&&
    \xymatrix{%
    \alpha_1 \ar@3{-}[r]&\alpha_2
    }  
    \end{align}
\end{subequations}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Cartan-Weyl basis}
%---------------------------------------------------------------------------------------------------------------------------

Let us study the eigenvalue equation
\begin{equation}        \label{EqvalpradAprho}
    \ad(A)X=\rho X.
\end{equation}
The number of solutions with $\rho=0$ depends on the choice of $A\in\lG$.

\begin{lemma}
    If $A$ is chosen in such a way that $\ad(A)X=0$ has a maximal number of solutions, then the number of solutions is equal to the rank\index{rank of a Lie algebra} of $\lG$ and the eigenvalue $\alpha=0$ is the only degenerated one in equation \eqref{EqvalpradAprho}.
\end{lemma}

We suppose $A$ to be chosen in order to fulfill the lemma. Thus we have linearly independent vectors $H_i$ ($i=1,\ldots l$) such that
\begin{equation}
    [A,H_i]=0
\end{equation}
where $l$ is the rank of $\lG$. Since $[A,A]=0$, the vector $A$ is a combination $A=\lambda^iH_i$. Since $\ad(A)$ is diagonalisable, one can find vectors $E_{\alpha}$ with
\begin{equation}
    [A,E_{\alpha}]=\alpha E_{\alpha},
\end{equation}
and such that $\{ H_i,E_{\alpha} \}$ is a basis of $\lG$. Using the fact that $\ad(A)$ is a derivation, we find
\begin{equation}
    [A,[H_i,E_{\alpha}]]=\alpha[H_i,E_{\alpha}],
\end{equation}
The eigenvalue $\alpha=0$ being the only one to be degenerated, one concludes that $[H_i,E_{\alpha}]$ is a multiple of $E_{\alpha}$:
\begin{equation}
    [H_i,E_{\alpha}]=\alpha_i E_{\alpha}.
\end{equation}
Replacing $A=\lambda^iH_i$, we have
\begin{equation}
    \alpha E_{\alpha}=[\lambda^iH_i,E_{\alpha}]=\lambda^i\alpha_iE_{\alpha},
\end{equation}
thus $\alpha=\lambda^i\alpha_i$ (with a summation over $i=1,\ldots,l$).

Before to go further, notice that the space spanned by $\{ H_i \}_{i=1,\ldots,l}$ is a maximal abelian subalgebra of $\lG$, so that it is a Cartan subalgebra that we,  naturally denote by $\lH^*$. Thus, what we are doing here is the usual root space construction. In order to stick the notations, let us associate the form $\sigma_{\alpha}\in\lH^*$ defined by $\sigma_{\alpha}(H_i)=\alpha_i$. In that case,
\begin{equation}
    \sigma_{\alpha}(A)=\sigma_{\alpha}(\lambda^iH_i)=\lambda^i\alpha_i=\alpha
\end{equation}
and we have
\begin{equation}
    [A,E_{\alpha}]=\sigma_{\alpha}(A)E_{\alpha}.
\end{equation}
On the other hand, we have $[H_i,E_{\alpha}]=\alpha_iE_{\alpha}=\sigma_{\alpha}(H_i)E_{\alpha}$, so that the eigenvalue $\alpha$ is identified to the root $\alpha$, and we have $E_{\alpha}\in\lG_{\alpha}$.

Let us now express the vectors $t_{\alpha}$ in the basis of the $H_i$. The definition property is $B(t_{\alpha},H_i)=\alpha(H_i)=\alpha_i$. If $t_{\alpha}=(t_{\alpha})^iH_i$, we have
\begin{equation}
    \alpha_i=B(t_{\alpha},H_i)=B_{kl}(t_{\alpha})^k\underbrace{(H_i)^l}_{=\delta^l_i}=B_{ki}(t_{\alpha})^k.
\end{equation}
If $(B^{ij})$ are the matrix elements of $B^{-1}$, we have
\begin{equation}
    (l_{\alpha})^l=\alpha_iB^{il}=\alpha^l
\end{equation}
where $\alpha^l$ is defined by the second equality. Using proposition \ref{Propoxalphaymoinaalpha}, we have
\begin{equation}
    [E_{\alpha},E_{-\alpha}]=B(E_{\alpha},E_{-\alpha})\alpha^lH_l.
\end{equation}
Thus one can renormalise $E_{\alpha}$ in such a way to have
\begin{equation}
    \begin{aligned}[]
        [H_i,H_j]       &=0,\\
        [E_{\alpha},E_{-\alpha}]    &=\alpha^iH_i\\
        [H_i,E_{\alpha}]    &=\alpha_iE_{\alpha}=\alpha(H_i)E_{\alpha}\\
        [E_{\alpha},E_{\beta}]  &=N_{\alpha\beta}E_{\alpha+\beta}
    \end{aligned}
\end{equation}
where the constant $N_{\alpha\beta}$ are still undetermined. A basis $\{ H_i,E_{\alpha} \}$ of $\lG$ which fulfill these requirements is a basis of \defe{Cartan-Weyl}{Cartan-Weyl basis}.

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Cartan matrix}
%---------------------------------------------------------------------------------------------------------------------------

We follow \cite{Wybourne}. We denote by $\Pi$ the system of simple roots of $\lG$. All the positive roots have the form
\begin{equation}
    \sum_{\alpha\in\Pi}k_{\alpha}\alpha
\end{equation}
with $k_{\alpha}\in\eN$.

\begin{theorem}
    Let $\alpha$ and $\beta$ be simple roots Thus
    \begin{enumerate}
        \item
            $\alpha-\beta$ is not a simple root
        \item
            we have
            \begin{equation}        \label{EqabSuraaStrictNEf}
            \frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }=-p
        \end{equation}
        where $p$ is a strictly positive integer.
    \end{enumerate}
\end{theorem}

\begin{proof}[Partial proof]
    We are going to prove that $\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }$ is an integer. Let $\alpha$ and $\gamma$ be non vanishing roots such that $\alpha+\gamma$ is not a root, and define
\begin{equation}
    E'_{\gamma-j\alpha}=\ad(E_{-\alpha})^kE_{\gamma}\in\lG_{\gamma-k\alpha}.
\end{equation}
Since there are a finite number of roots, there exists a minimal positive integer $g$ such that $\ad(E_{-\alpha})^{g+1}E_{\gamma}=0$. We define the constants $\mu_k$ (which depend on $\gamma$ and $\alpha$) by
\begin{equation}
    [E_{\alpha},E'_{\gamma-k\alpha}]=\mu_kE'_{\gamma-(k-1)\alpha}.
\end{equation}
Using the definition of $E'_{\gamma-k\alpha}$ and Jacobi, one founds
\begin{equation}
    \mu_kE'_{\gamma-(k-1)\alpha}=\big[E'_{\alpha},[E_{-\alpha},E'_{\gamma-(k-1)\alpha}]\big]=\alpha^i[H_i,E'_{\gamma-(k-1)\alpha}]+\mu_{k-1}E'_{\gamma-(k-a)\alpha},
\end{equation}
so that $\mu_k=\alpha^i\gamma_i-(k-1)\alpha^i\alpha_i+\mu_{k-1}$, and we have the induction formula
\begin{equation}
    \mu_k=(\alpha,\gamma)-(k-1)(\alpha,\alpha)+\mu_{k-1}
\end{equation}
for $k\geq 2$. If we define $\mu_0=0$, that relation is even true for $k=1$. The sum for $k=1$ to $k=j$ is easy to compute and we get
\begin{equation}
    \mu_j=j(\alpha,\gamma)-\frac{ j(j-1) }{ 2 }(\alpha,\alpha).
\end{equation}
Since $\mu_{g+1}=0$, we have 
\begin{equation}        \label{Eqalphagammapargdeux}
    (\alpha,\gamma)=g(\alpha,\alpha)/2,
\end{equation}
and thus
\begin{equation}
    \mu_j=\frac{ j(g-j+1)(\alpha,\alpha) }{ 2 }.
\end{equation}
Let $\beta$ be any root and look at the string $\beta+j\alpha$. There exists a maximal $j\geq 0$ for which $\beta+j\alpha$ is a root while $\beta+(j+1)\alpha$ is not a root. Now we consider $\gamma=\beta+j\alpha$ with that maximal $j$. Putting $\gamma=\alpha+j\beta$ in \eqref{Eqalphagammapargdeux}, one finds
\begin{equation}
    (\alpha,\beta)=\frac{ (g-2j)(\alpha,\alpha) }{ 2 }, 
\end{equation}
and finally,
\begin{equation}
    \frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }=g-2j,
\end{equation}
which is obviously an integer.


\end{proof}

From the inner product on $\lH^*$, we deduce a notion of \defe{angle}{angle between roots}:
\begin{equation}
    \cos(\theta_{\alpha,\beta})=\frac{ (\alpha,\beta) }{ \sqrt{(\alpha,\alpha)(\beta,\beta)} }.
\end{equation}
The \defe{length}{length of a root} of the root $\alpha$ is the number $\sqrt{(\alpha,\alpha)}$.

\begin{lemma}
    If $\alpha$ and $\beta$ are roots, then
    \begin{equation}
        \frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }\in\eZ,
    \end{equation}
    and
    \begin{equation}
        \beta-\frac{ 2(\alpha,\beta) }{ (\alpha,\alpha) }
    \end{equation}
    is a root too.

    If $\alpha$ and $\beta$ are non vanishing, then the $\alpha$-string which contains $\beta$ contains at most $4$ roots. Finally, the ratio
    \begin{equation}
        \frac{ 2(\alpha,\beta) }{ (\alpha,\beta) }
    \end{equation}
    takes only the values $0$, $\pm 1$, $\pm 2$ or $\pm 3$.
\end{lemma}

Let $\Pi=\{ \alpha_1,\ldots,\alpha_l \}$ be a system of simple roots. The \defe{Cartan matrix}{Cartan!matrix} is the $l\times l$ matrix with entries
\begin{equation}        \label{EqDefMatriceCartan}
    A_{ij}=\frac{ 2(\alpha_i,\alpha_j) }{ (\alpha_i,\alpha_i) }.
\end{equation}
Notice that, in the literacy, one find also the convention $A_{ij}=2(\alpha_i,\alpha_j)/(\alpha_j,\alpha_j)$, as in \cite{rncahn}, for example.

\begin{lemma}       \label{LemRatdjaijdjaji}
    There exist positive rational numbers \( d_i\) such that 
    \begin{equation}        \label{EqdiAijdjAji}
        d_i A_{ij}=d_jA_{ji}
    \end{equation}
    where \( A\) is the Cartan matrix.
\end{lemma}

\begin{proof}
    The numbers are given by
    \begin{equation}
        d_i=\frac{ (\alpha_i,\alpha_i) }{ (\alpha_1,\alpha_1) }.
    \end{equation}
    The relations \eqref{EqdiAijdjAji} are easy to check using the definition \eqref{EqDefMatriceCartan}. The fact that \( d_i\) is a strictly positive rational number comes from \eqref{EqabSuraaStrictNEf}.
\end{proof}

\begin{probleme}
    I think that there is a property saying (something like) that \( A_{ij}\) is the larger integer \( k\) such that \( \alpha_i+k\alpha_j\) is  a root.
\end{probleme}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Other results}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Abstract Cartan matrix}
%---------------------------------------------------------------------------------------------------------------------------

As before if we chose a basis $\{\varphi_1\ldots\varphi_l\}$ of $V$, we can consider a lexicographic ordering\index{lexicographic ordering} on $V$. A root is \defe{simple}{simple!abstract root} when it is positive and can't be written as as sum of two positive roots. As in a non abstract case, abstract simple root also have the following property:

\begin{proposition}
If $\dim V=l$, one has only $l$ simple roots $\alpha_1,\ldots,\alpha_l$; they are linearly independent and if $\beta\in\Phi$ expands into $\beta=\sum c_j\alpha_j$, the $c_j$'s all are integers and the non zero ones all have the same sign.
\end{proposition}

An ordering on $V$ gives a notion of simple roots. The $l\times l$ matrix whose entries are
\[
   A_{ij}=\frZ{\alpha_i}{\alpha_j}
\]
is the \defe{abstract Cartan matrix}{abstract!Cartan matrix}\index{Cartan!abstract matrix} of the abstract root system and the given ordering. 

\begin{theorem}
    The main properties are
    \begin{enumerate}
        \item $A_{ij}\in\eZ$,
        \item $A_{ii}=2$,
        \item if $i\neq j$, then $A_{ij}\leq 0$ and $A_{ij}$ can only take the values $0,-1,-2$ or $-3$,
        \item if $i\neq j$, $A_{ij}A_{ji}<4$ (no sum),
        \item $A_{ij}=0$ is and only if $A_{ji}=0$,
        \item $\det A$ is integer and positive.
    \end{enumerate}
\end{theorem}

    \begin{proof}
    The last point is the only non immediate one. The matrix $A$ is the product of the diagonal matrix with entries $2/|\alpha_i|^2$ and the matrix whose entries are $(\alpha_i,\alpha_j)$. The fact that the latter is positive definite is a general property of linear algebra. If $\{e_i\}$ is a basis of a vector space $V$, the matrix whose entry $ij$ is given by $(e_i,e_i)$ is positive definite. Indeed one can consider an orthonormal basis $\{f_i\}$ and a nondegenerate change of basis $e_i=B_{ik}f_k$. Then $(e_i,e_j)=(BB^t)_{ij}$. It is easy to see that for all $v\in V$, we have $(BB^t)_{ij}v^iv^j=\sum_k(v^iB_{ik})^2>0$.

    The fact that the determinant is integer is simply the fact that this is a polynomial with integer variables.
\end{proof}

If we have an ordering on $V$ we define $\Phi^+$, the set of positive roots. From there, one can consider $\Pi$, the set of simple roots. Any element of $\Phi$ expands to a sum of elements of $\Pi$. Note that the knowledge of $\Pi$ is sufficient to find $\Phi^+$ back because $\alpha>0$ implies $\alpha=\sum c_i\alpha_i$ with $c_i\geq 0$.

We can make this reasoning backward. Let us consider $\Pi=\{\alpha_1,\ldots,\alpha_l\}$ be a basis of $V$ such that any $\alpha\in\Phi$ expands as a sum of $\alpha_i$ with all coefficients of the same sign. Such a $\Pi$ is a \defe{simple system}{simple!system}. From such a $\Pi$, we can build a $\Phi^+$ as the set of elements of the form $\alpha=\sum c_i\alpha_i$ with $c_i\geq 0$.

\begin{proposition}
The so build $\Phi^+$ is the set of positive roots for a certain ordering.
\end{proposition}

\begin{proof}
If we consider on $V$ the lexicographic ordering with respect to the basis $\Pi$, a positive element $\alpha=\sum c_i\alpha_i$ has at least one positive coefficient among the $c_i$. If $\alpha\in\Phi$, we can say (by definition of $\Pi$) that in this case \emph{all} the coefficients are positive, then the positive roots exactly form the set $\Phi^+$.
\end{proof}

From now when we speak about a $\Phi^+$, it will always be with respect to a simple system. The advantage is the fact that there are no more implicit ordering.

\begin{lemma}
Let $\Pi=\{\alpha_1,\ldots,\alpha_l\}$ be a simple system and $\alpha\in\Phi^+$. Then
\[
   s_{\alpha_i}=\begin{cases}
                           -\alpha_i & \text{if $\alpha=\alpha_i$}\\
                    >0       & \text{if $\alpha\neq\alpha_i$}.
                          \end{cases}
\]
\end{lemma}

\begin{proof}
The first case is well know from a long time. For the second, compute
\begin{equation}
\begin{split}
  s_{\alpha_i}( \sum c_j\alpha_j )&=\sum_{j\neq i}c_j\alpha_j+c_i\alpha_i-2c_i\alpha_i
                                         -\sum_{j\neq i}\frac{2c_j}{|\alpha_i|^2}(\alpha_j,\alpha_i)\alpha_i\\
                              &=\sum_{j\neq i}+\left(
        -\sum_{i\neq j}       \frac{2c_j}{|\alpha_i|^2}(\alpha_j,\alpha_i)+c_i
                                    \right)\alpha_i.
\end{split}
\end{equation}
We see that between $\sum c_k\alpha_k$ and $s_{\alpha_i}(\sum c_k\alpha_k)$, there is just the coefficient of $\alpha_i$ which changes. Then if $\alpha\neq \alpha_i$, the positivity is conserved.

\end{proof}

\begin{proposition}
Let $\Pi=\{\alpha_1,\ldots,\alpha_l\}$ be a simple system. Then $W$ is generate by the $s_{\alpha_i}$'s. If $\alpha\in\Phi$, then there exists a $\alpha_i\in\Pi$ and $s\in W$ such that $s\alpha_j=\alpha$.
\end{proposition}

\begin{proof}
We denote by $W'$ the group generate by the $s_{\alpha_i}$'s; the purpose is to show that $W=W'$. We begin to show that if $\alpha>0$, then $\alpha=s\alpha_j$ for certain $s\in W'$ and $\alpha_j\in\Pi$. For this, we write $\alpha=\sum c_j\alpha_j$ and we make an induction with respect to $\niv(\alpha)=\sum c_j$. If $\niv(\alpha)=1$, then $\alpha-\alpha_j$ and $s=\id$ works.  Now we suppose that it works for $\niv<\niv(\alpha)$. We have
\[
   0<(\alpha,\alpha)=\sum c_i(\alpha,\alpha_i).
\]
Since all the $c_i$ are positive, it assures the existence of a $i_0$ such that $(\alpha,\alpha_{i_0)}>0$. Then from the lemma, $\beta=s_{\alpha_{i_0}}(\alpha)>0$ ($\alpha\neq \alpha_{i_0}$ because $\niv(\alpha)>1$). The root $\beta$ can be expanded as
\begin{equation}
\beta=\sum_{j \neq i_0}c_j\alpha_j+\left(  c_{i_0}-\sum_{j\neq i_0}\frac{c_j}{|\alpha_{i_0}|^2}(\alpha,\alpha_{i_0})   \right)\alpha_{i_0}.
\end{equation}
Since $(\alpha,\alpha_{i_0})>0$, it implies $\niv(\beta)<\niv(\alpha)$ and thus $\beta=s'\alpha_j$ for a certain $s'\in W'$. So $\alpha=s_{\alpha_{i_0}}s'\alpha_j$ with $s_{\alpha_{i_0}}s'\in W'$. This conclude the induction. For $\alpha<0$, the same result holds by writing $-\alpha=s\alpha_j$ and $\alpha=ss_{\alpha_j}\alpha_j$.

Now it remains to prove that $W'\subseteq W$. For a $\alpha\in\Phi$, we write $\alpha=s\alpha_j$ with $s\in W'$. Then
\[
   s_{\alpha}=ss_{\alpha_j}s^{-1}\in W'.
\]
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Dynkin diagram}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}
    If $\alpha$ and $\beta$ are simple roots, then the angle $\theta_{\alpha,\beta}$ can only take the values $90\degree$, $120\degree$, $135\degree$ or $150\degree$.
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

In order to draw the \defe{Dynkin diagram}{Dynkin!diagram} of a Lie algebra, one draws a circle for each simple root, and one joins the roots with $1$, $2$ or $3$ lines, following that the value of the angle is $120\degree$, $135\degree$ or $150\degree$. If the roots are orthogonal (angle $90\degree$), they are not connected. If the length of a root is maximal, the circle is left empty. If not, it is filled.

One easily determines the number of lines between two roots by the following proposition.
\begin{proposition}         \label{PropProdNbLignes}
    If $\alpha$ and $\beta$ are two simple roots with $(\alpha,\alpha)\leq(\beta,\beta)$, then
    \begin{equation}
        \frac{ (\alpha,\alpha) }{ (\beta,\beta) }=
    \begin{cases}
        1   &\text{if $\theta_{\alpha,\beta}=120\degree$}\\
        2   &\text{if $\theta_{\alpha,\beta}=135\degree$}\\
        3   &\text{if $\theta_{\alpha,\beta}=150\degree$}.
    \end{cases}
    \end{equation}
\end{proposition}
\begin{proof}
    No proof.
\end{proof}

If $M$ is a weight of a representation, its \defe{Dynkin coefficients}{Dynkin!coefficient} are
\begin{equation}
    M_i=\frac{ 2(M,\alpha_i) }{ (\alpha_i,\alpha_i) },
\end{equation}
and we can compute the Dynkin coefficients from one weight to another by the simple formula
\begin{equation}        \label{EqCofDynMmoisAlpha}
    (M-\alpha_j)_i=M_i-A_{ij}.
\end{equation}
A weight is \defe{dominant}{weight!dominant}\index{dominant weight} if all its Dynkin coefficients are strictly positive.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Strings of roots}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Let $\alpha,\beta$ be two roots with respect to $\lH$ and suppose $\beta\neq 0$. We denote by $\alpha^{\beta}$ the largest integer $m$ such that $\alpha+m\beta$ is a root and by $\alpha_{\beta}$ the one such that $\alpha-m\beta$ is a root. Let $x\in\lG_{\alpha}$; since the Killing form is nondegenerate, there exists a $y\in\lG$ such that $B(x,y)\neq 0$. Using the root space decomposition \eqref{eq:decomp_racine} for $y$ and corollary \ref{cor:Bxy_zero}, $B(x,y)=B(x,y_{-\alpha})$ . Then \label{pg:root_ss}
\[
\forall x\in\lG_{\alpha},\exists y\in\lG_{-\alpha}\textrm{ such that } B(x,y)\neq 0.
\]
In particular if $\alpha$ is a root, $-\alpha$ is also a root and the restriction of $B$ to $\lH\times\lH$ is nondegenerate because $\lH=\lG_0$. So
\[
\forall\mu\in\lH^*,\exists!h_{\mu}\in\lH\textrm{ such that } \forall h\in\lG, B(h,h_{\mu})=\mu(h).
\]
This is a general result about nondegenerate (here we use the semi-simplicity assumption) bilinear forms on a vector space. If $B(x,y)=B_{ij}x^iy^j$ and $a(x)=a_ix^i$, then a vector $v$ such that $B(x,v)=a(x)$ exists, is unique and is given by coordinates $v^k=B^{ki}a_i$ where the matrix $(B^{ij})$ is the inverse of $(B_{ij})$.

We will sometimes use the following notation if $\alpha$ and $\beta$ are roots:
\[
(\alpha,\beta)=B( h_{\alpha},\hbb),\qquad |\alpha|^2=(\alpha,\alpha).
\]



By proposition \ref{tho:six_Cartan}, the roots come by pairs $(\alpha,-\alpha)$. For each of them, we choose $x_{\alpha}\in\lG_{\alpha}$. Our choice of $x_{-\alpha}$ is made as following. From discussion at page \pageref{pg:root_ss} we can find a $x_{-\alpha}\in\lG_{-\alpha}$ such that $B(x_{-\alpha},x_{\alpha})=1$. Note that this choice is unambigous: if we had chosen first $x_{-\alpha}\in\lG_{-\alpha}$, this construction would have given the same $x_{\alpha}$ than our starting point. Note also that $h_{-\alpha}=-h_{\alpha}$. These $ x_{\alpha}$ fulfil $[ x_{\alpha},\xbma]= h_{\alpha}$.

\begin{probleme}
    Here the notation \( \Delta\) does not follow our convention of subsection \ref{SubsecNotationRootsDel}.
\end{probleme}

Let $\Delta$ be the set of non zero roots. We define an antisymmetric map $\dpt{c}{\Delta\times \Delta}{\eC}$ as following. If $\alpha,\beta\in S$ are such that $\alpha+\beta\notin\Delta$, we pose $c(\alpha,\beta)=0$. If $\alpha+\beta\in\Delta$, 
\begin{equation}
[x_{\alpha},x_{\beta}]=c(\alpha,\beta)x_{\alpha+\beta}.
\end{equation}
It is easy to see that $c(\alpha,\beta)=-c(\beta,\alpha)$.

\begin{proposition}
If $\alpha,\beta,\alpha+\beta\in\Delta$, then

\begin{enumerate}
\item 
\[
c(-\alpha,\alpha+\beta)=c(\alpha+\beta,-\beta)=c(-\beta,-\alpha),
\]
\label{enuai}
\item\label{enuaii} If $\alpha,\beta,\gamma,\delta\in\Delta$ and $\alpha+\beta+\gamma+\delta=0$ wile $\delta$ is neither $-\alpha$, nor $-\beta$ nor $-\gamma$, then
\begin{equation}\label{eq:enuaii}
c(\alpha,\beta)c(\gamma,\delta)+c(\beta,\gamma)c(\alpha,\delta)+c(\gamma,\alpha)c(\beta,\delta)=0,
\end{equation}

\item if $\beta\neq\alpha\neq -\beta$, then
\[
c(\alpha,\beta)+c(-\alpha,-\beta)=c(\alpha,-\beta)c(-\alpha,\beta)-B(h_{\alpha},h_{\beta}),
\]
\label{enuaiii}
\item\label{enuaiv} if $\alpha+\beta\neq0$ then
\begin{equation}\label{eq:enuaiv}
2c(\alpha,\beta)c(-\alpha,-\beta)=\lbha(1+\lbba)\alpha(h_{\alpha}).
\end{equation}

\end{enumerate}
\label{prop:enua}
\end{proposition}

\begin{proof}
From our choice of $ x_{\alpha}$, we find that $B(\xbb,\xbmb)=B(\xbma, x_{\alpha})=B(x_{\alpha+\beta},\xbmab)=1$, but
\begin{equation}
\begin{split}
  B\big(c(-\alpha,\alpha+\beta)\xbb,\xbmb\big)&=B\big(\xbma,c(\alpha+\beta,-\beta) x_{\alpha}\big)\\
                        &=B\big(x_{\alpha+\beta},c(-\beta,-\alpha)\xbmamb \big).
\end{split}                            
\end{equation}
This proves \ref{enuai}. In order to prove \ref{enuaii}, suppose that
\begin{equation}\label{eq:amontrer}
c(\alpha,\beta)c(\gamma,\delta)=B\Big(  \big[[ x_{\alpha},\xbb],\xbg\big] ,\xbd  \Big)
\end{equation}
Then the Jacobi identity gives the result:
\begin{equation}
\begin{split}
0&=B\Big( \big[[ x_{\alpha},\xbb],\xbg\big],\xbd \Big)+B\Big(\big[[\xbb,\xbg], x_{\alpha}\big],\xbd\Big)+B\Big(\big[[\xbg, x_{\alpha}],\xbb\big] ,\xbd\Big)\\
&=c(\alpha,\beta)c(\gamma,\delta)+c(\beta,\gamma)c(\alpha,\delta)+c(\gamma,\alpha)c(\beta,\delta),
\end{split}
\end{equation}
Here, we used the hypothesis $-\gamma\neq\delta\neq -\beta$ by supposing that \eqref{eq:amontrer} still hold after permutation of $\alpha,\beta,\gamma$.
Now we show the \eqref{eq:amontrer} is true. The assumptions imply  $\alpha+\beta=-(\gamma+\delta)\neq 0$, then
\begin{equation}
\begin{split}
B\big(\big[[ x_{\alpha},\xbb],\xbg \big],\xbd\big)&=B\big( [ x_{\alpha},\xbb],[\xbg,\xbd]  \big)\\
&=c(\alpha,\beta)c(\gamma,\delta)B(x_{\alpha+\beta},x_{\gamma+\delta})\\
&=c(\alpha,\beta)c(\gamma,\delta).
\end{split}
\end{equation}
Now we turn our attention to \ref{enuaiii}. If $\alpha$ and $\beta$ fulfil the condition $\beta\neq\alpha\neq-\beta$, we can apply \ref{enuaii} on the quadruple $(\alpha,\beta,-\alpha,-\beta)$ to get $c(\alpha,\beta)c(-\alpha,-\beta)=
-B\big(   [ x_{\alpha},\xbb],[\xbma,\xbmb]   \big)$. 
If we replace $\beta$ by $-\beta$ and if we make the difference between the two expressions,
\begin{equation}
\begin{split}
c(\alpha,\beta)c(-\alpha,-\beta)&=-B\big(   [ x_{\alpha},\xbb],[\xbma,\xbmb]    \big)+B\big(   [ x_{\alpha},\xbmb],[\xbma,\xbb]    \big)\\
&=B\big( [ x_{\alpha},[\xbmb,\xbma]],\xbb \big)-B\big(
[\xbma,[ x_{\alpha},\xbmb]],\xbb \big)\\
&=-B\big( [\xbma, x_{\alpha}],[\xbmb,\xbb]  \big) \\
&=-B(h_{\alpha},h_{\beta}).
\end{split}
\end{equation}

In order to prove \ref{enuaiv}, we consider $\alpha+\beta\neq0$ and we pose
\[
d(\alpha,\beta)=c(\alpha,\beta)c(-\alpha,-\beta)-\frac{1}{2}\lbha(1+\lbba)\alpha(h_{\alpha}).
\]
Our aim is to prove that it is zero. We will do it by induction on $\lbha$. First $\lbha=0$ means that $\beta+\alpha=0$, so that $c(\alpha,\beta)=0$ and $d(\alpha,\beta)=0$. Now we suppose that $\lbha>0$ and that \ref{enuaiv} is yet checked for lower cases. Note that $\beta+\alpha\in \Delta$ and $(\beta+\alpha)+\alpha\neq 0$ because $-2\alpha$ is not a root. Then $\beta=2\alpha$ is not possible. From the fact that $(\beta+\alpha)^{\alpha}=\lbha-1$, we conclude $d(\alpha,\beta+\alpha)=0$. Then
\[
c(\alpha,\alpha+\beta)c(-\alpha,-\alpha-\beta)=c(\alpha,-\alpha-\beta)c(-\alpha,\alpha+\beta)-B(h_{\alpha},h_{\alpha+\beta}).
\]
On the other hand, \ref{enuai} and the antisymmetry of $c$ give
\begin{subequations}
\begin{align}
c(-\alpha,\alpha+\beta)=c(-\beta,-\alpha)=-c(-\alpha,-\beta)\\
\intertext{and}
c(\alpha,-\alpha-\beta)=c(\beta,\alpha)=-c(\alpha,\beta)
\end{align}
\end{subequations}
With all this
\begin{equation}
\begin{split}
d(\alpha,\beta+\alpha)&=c(\alpha,\alpha+\beta)c(-\alpha,-\alpha-\beta)-\frac{1}{2}(\alpha+\beta)^{\alpha}(1+(\alpha+\beta)_{\alpha})\alpha(h_{\alpha})\\
&=c(\alpha,\beta)c(-\alpha,-\beta)-k(\alpha,\beta)
\end{split}
\end{equation}
where $k(\alpha,\beta)=B(h_{\alpha},h_{\alpha+\beta})+\frac{1}{2}(\alpha+\beta)^{\alpha}(1+(\alpha+\beta)_{\alpha})\alpha(h_{\alpha})$. But $h_{\alpha+\beta}$ is definied in order to have $B(h,h_{\alpha+\beta})=(\alpha+\beta)(h)$ for any $h\in\lH$. Then using $2\beta(h_{\alpha})=(\lbba-\lbha)\alpha(h_{\alpha})$, we find $k(\alpha,\beta)=\frac{1}{2}\alpha(h_{\alpha})\lbha(1+\lbba)$.

\end{proof}

\begin{proposition}\label{prop:lHeR}\index{real!form!of a vector space}
Let 
\begin{equation}
\lHeR=\sum_{\alpha\in\Delta}\eR h_{\alpha}.
\end{equation}

\begin{enumerate}
\item Any root is real on $\lHeR$,
\item the Killing form is real and strictly positive definite on $\lHeR$,
\item $\lH=\lHeR\oplus i\lHeR$.
\end{enumerate}

\end{proposition}

The last item shows that $\lHeR$ is a real form of $\lH$. Remark also that $\lHeR$ can also be written as
\[
\lHeR=\{h\in\lH\tq \alpha(h)\in\eR\,\forall\alpha\in\Phi \}.
\]
\begin{proof}
Let $\beta\in\Delta$; we looks at $\beta( h_{\alpha})$. From \ref{ite:six_deux} of theorem \ref{tho:six_Cartan}, we know that $\alpha( h_{\alpha})$ is real and positive, and \ref{ite:six_trois} makes $\beta( h_{\alpha})$ real. From the formula $B( h_{\alpha},\hbb)=\sum_{\gamma\in\Delta}\gamma( h_{\alpha})\gamma(\hbb)$, the Killing form is real and positive definite on $\lHeR\times\lHeR$. If $B(h,h)=0$ for a certain $h\in\lHeR$, we find $\alpha(h)=0$ for all $\alpha\in\Delta$. Then any $x=x^{\alpha} x_{\alpha}\in\lG$ commutes with $h$ because
\[
[h,x]=\sum_{\alpha\in\Phi}a^{\alpha}(\ad h) x_{\alpha}=\sum_{\alpha}a^{\alpha}\alpha(h)=0.
\]
So $h$ is in the center of $\lG$ and so $h=0$ be cause $\lG$ is semisimple. Thus the Killing form is strictly positive definite on $\lHeR\times\lHeR$.

Now we are going to show that $\lH=\lHeR\oplus i\lHeR$. If $h\in\lHeR\cap i\lHeR$, it can be written as $h=ih'$ with $h,h'\in\lHeR$. Then
\[
0<B(h,h)=B(ih',ih')=-B(h',h')<0,
\]
so that $h=0$ because $B$ is nondegenerate. This shows that $\lHeR\cap i\lHeR=0$. It is clear that $\sum_{\alpha\in\Delta}\eC h_{\alpha}\subset\lH$; thus it remains to be proved that $\lH\subset\sum_{\alpha\in\Delta}\eC h_{\alpha}$. It it is not, we can build a linear function $\dpt{\lambda}{\lH}{\eC}$ which is not identically zero but which is zero on the subspace $\sum_{\alpha\in\Delta}\eC h_{\alpha}$. Then there exists (only one) $h_{\lambda}\in\lH$ such that $B(h,h_{\lambda})=\lambda(h)$ for every $h\in\lH$. In particular, $\alpha(h_{\lambda})=0$ for every $\alpha\in\Delta$ because $\alpha(h_{\lambda})=B( h_{\alpha},h_{\lambda})=\lambda( h_{\alpha})$. This implies that $h_{\lambda}=0$, so that $\lambda\equiv 0$.

\end{proof}

One interest in the third point of this proposition is that we are now able to see $\Delta$ as a subset of $\lHeR^*$ because the definition of $\alpha\in\Delta$ on $\lHeR$ only is sufficient to define $\alpha$ on the whole $\lH$. 


If $\{e_i\}$ is a basis of a vector space $V$, we say that $x=x^ie_i>y=y^ie_i$ if $x-y=a^ie_i$ and the first non zero $a^i$ is positive. This is the \defe{lexicographic order}{lexicographic ordering} on $V$. It is clear that it doesn't works on a complex vector space (because in this case we should first define $a^i>0$), but we can anyway get an order on $\Delta$ by seeing it as a subset of $\lHeR$.

The following important result is the fact that a complex semisimple Lie algebra is determined by its root system.
\begin{theorem}
Let $\lG$ and $\lG'$ be two semisimple complex Lie algebras; $\lH$ and $\lH'$, Cartan subalgebras. We suppose that we have a bijection $\Phi\to\Phi'$, $\alpha\to\alpha'$ which preserve the root system:

\begin{itemize}
\item $\alpha'+\beta'=0$ if and only if $\alpha+\beta=0$,
\item $\alpha'+\beta'$ is not a root if and only if $\alpha+\beta$ is also not a root,
\item $(\alpha+\beta)'=\alpha'+\beta'$ whenever $\alpha+\beta$ is a root.
\end{itemize}
Then we have a Lie algebra isomorphism $\dpt{\eta}{\lG}{\lG'}$ such that $\eta(\lH)=\lH'$ and $\alpha'\circ\eta|_{\lH}=\alpha$.
\end{theorem}

\begin{proof}
From the assumptions, $\lbha=(\beta')^{\alpha'}$ and $\lbba=(\beta')_{\alpha'}$ and the point \ref{ite:six_deux} of theorem \ref{tho:six_Cartan} makes $\alpha'(h_{\alpha'})=\alpha(h_{\alpha})$. The fourth point of the same theorem then gives 
\begin{equation}\label{eq:beta_h_beta}
\beta'(h_{}\alpha')=\beta(h_{\alpha}).
\end{equation}
Now we choose a maximally linearly independent set $(\alpha_1,\ldots,\alpha_R)$ of roots of $\lG$. Because of theorem \ref{tho:Phi_base}, this is a basis of $\lH^*$. For notational convenience, we put $h_r=h_{\alpha_r}$ and naturally, $h'_r=h_{\alpha'_r}$. It is easy to see that the set of $h_r$ is a basis of $\lH$. Indeed if $a^rh_r=0$ (with sum over $r$), then $B(h,a^r,h_r)=a^r\alpha_r(h)=0$ which implies that $a^r\alpha_r|_{\lH}=0$ but it is impossible because the $\alpha_r$ are free in $\lH^*$.
\begin{equation*}
\begin{split}
\{ \alpha_1,\ldots,\alpha_r \}&\textrm{ is a basis of $\lH^*$},\\
\{ h_1,\ldots,h_r \}&\textrm{ is a basis of $\lH$}.
\end{split}  
\end{equation*}
Then the matrix $(A_{ij})=\alpha_i(h_j)$ has non zero determinant. Since $\alpha'_i(h_j')=\alpha_i(h_j)$, the set $\{\alpha'_1,\ldots,\alpha'_r\}$ is free and $\{h'_1,\ldots,h'_r\}$ is a basis of $\lH'$.
\begin{equation*}
\begin{split}
\{ \alpha'_1,\ldots,\alpha'_r \}&\textrm{ is a basis of ${\lH'}^*$},\\
\{ h'_1,\ldots,h'_r \}&\textrm{ is a basis of $\lH'$}.
\end{split}  
\end{equation*}
Then can define an isomorphism $\dpt{\etalH}{\lH}{\lH'}$ by $\etalH(h_i)=h'_i$. If $x\in\lH$ is decomposed as $x=a^rh_r$, from equation \eqref{eq:beta_h_beta} we have $(\alpha'_i\circ\etalH)(a^rh_r)=a^r\alpha'(h'_r)=\alpha_i(h_r)$. Then
\[
\alpha'_i\circ\etalH=\alpha_i.
\]

Let $\alpha\in\Phi$; we can write $\alpha=c_i\alpha_i$ and $\alpha'=c'_i\alpha'_i$ (with a sum over $i$). We have
\begin{equation}
c_i\alpha_i(h_k)=\alpha(h_j)
=\alpha'(h_j)
=c'_i\alpha_i(h_j).
\end{equation}
As the determinant of $(\alpha_i(h_j))$ is non zero, this implies $c_i=c'_i$, so that 
\begin{equation}
\alpha'\circ\etalH=\alpha
\end{equation}
because $\alpha'\circ\etalH=c'_i(\alpha'_i\circ\etalH)=c_i\alpha_i=\alpha$. Now we ``just''{} have to extend $\etalH$ into a Lie algebra isomorphism $\dpt{\eta}{\lG}{\lG'}$. As before for each $\alpha\in \Delta$ we choose $ x_{\alpha}\in\lG_{\alpha}$ such that $B( x_{\alpha},\xbma)=-1$ and $[\xbma, x_{\alpha}]= h_{\alpha}$. We naturally do the same for $x_{\alpha'}\in\lG'_{\alpha'}$. We also consider the function $c$ as before: $[ x_{\alpha},\xbb]=c(\alpha,\beta)x_{\alpha+\beta}$. Since $\lH=\lG_0$, these $ x_{\alpha}$ form a basis of $\lG\ominus\lH$ and $\eta$ can be defined by the date of $\eta( x_{\alpha})$. We set $\eta( x_{\alpha})=a_{\alpha}x_{\alpha'}$ (without sum).

The condition $\eta\big(  [ x_{\alpha},\xbb]=[\eta( x_{\alpha}),\eta(\xbb)]  \big)$ gives
\begin{subequations}
\begin{align}
\label{eq:ca_caa_a}
c(\alpha,\beta)a_{\alpha+\beta}&=c(\alpha',\beta')\aba\abb&\quad\text{if $\alpha+\beta\neq 0$}\\
\intertext{and}  
\label{eq:ca_caa_b}
\aba\abma &=1                        &\quad\forall\alpha\in\Phi.
\end{align}
\end{subequations}
These two conditions are necessary and also sufficient. Indeed there are three cases of $[x,y]$ to check: $x$, $y\in\lH$, one of these two is out of $\lH$ or $x,y$ are booth out of $\lH$. In the third case, using \eqref{eq:ca_caa_a},
\begin{equation}
\eta([ x_{\alpha},\xbb])=c(\alpha,\beta)a_{\alpha+\beta}x_{\alpha'+\beta'}
            =x(\alpha',\beta')\aba\abb x_{\alpha'+\beta'}
            =\aba\abb[x_{\alpha'},\abbp]
            =[\eta( x_{\alpha}),\eta(\xbb)].
\end{equation}
If $x$, $y\in\lH$, then from theorem \ref{tho:Phi_base}, $\eta([x,y])=0=[\eta(x),\eta(y)]$. Using the fact that $[h, x_{\alpha}]=\alpha(h) x_{\alpha}$, we find the third case:
\begin{equation}
\eta\big(  [\hbb, x_{\alpha}]  \big)=\eta\big(  \alpha( h_{\alpha}) x_{\alpha}  \big)
                =\eta\big(  \alpha'(h_{\beta'}) x_{\alpha}  \big)
                =\aba[h_{\beta'},x_{\alpha'}]
                =[\eta(h_{\beta}),\eta( x_{\alpha})].
\end{equation}
Now we are going to find some $\aba\in\eC$ such that
\begin{itemize}
\item $\aba\abma=1$ for any $\alpha$,
\item $c(\alpha,\beta)a_{\alpha+\beta}=c(\alpha',\beta')\aba\abb$ if $\alpha+\beta\neq 0$.  
\end{itemize}
We consider the lexicagraphic order \index{lexicographic ordering} on $\Phi$: this is the order on $\Phi$ seen as a subset of $\lHeR$ on which we put the lexicagraphic order. For a root $\alpha>0$, we will fix the coefficient $\aba$ by an induction with respect to the order and put $\abma=\aba^{-1}$. Let us consider $\rho>0$ and suppose that $\aba$ is already defined for $-\rho<\alpha<\rho$ in such a manner that $\aba\abma=1$ and $c(\alpha,\beta)\abab=c(\alpha',\beta')\aba\abb$ for every $\alpha,\beta$ such that $\alpha,\beta$ and $\alpha+\beta$ are stricly between $-\rho$ and $\rho$. We have to define $\abr$ in such a way that if $\abmr=\abr^{-1}$, the second condition holds for every $\alpha,\beta$ such that $\alpha,\beta$ and $\alpha+\beta$ are no zero roots between $-\rho$ and $\rho$.

If such a pair $(\alpha,\beta)$ doesn't exist, there are no problem to put $\abr=\abmr=1$. Let us suppose that such a pair exists: $\alpha+\beta=\rho$. Then $\lbha\neq 0$ and the point \ref{enuaiii} of proposition \ref{prop:enua} shows that $c(\alpha,\beta)\neq 0$; in the same way, $(\beta')^{\alpha'}=\lbha\neq 0$ implies $c(\alpha',\beta')\neq 0$. We define
\begin{subequations}\label{eq:def_abr}
\begin{align}
\abr&=c(\alpha,\beta)^{-1} c(\alpha',\beta')\aba\abb,\\
\abmr&=\abr^{-1}.
\end{align}   
\end{subequations}


Since the value of the right hand side of \eqref{eq:enuaiv} doesn't change under $\alpha\to\alpha'$ and $\beta\to\beta'$, it gives  $c(\alpha,\beta)c(-\alpha,-\beta)=c(\alpha',\beta')c(-\alpha',-\beta')$ and thus
\begin{equation}
\begin{split}
c(-\alpha,-\beta)\abmr&=c(-\alpha,-\beta)c(\alpha,\beta)c(\alpha',\beta')^{-1}\abma\abmb\\
&=c(\alpha',\beta')c(-\alpha',-\beta')c(\alpha',\beta')^{-1}\aba\abmb\\
&=c(-\alpha',-\beta')\abma\abmb.
\end{split}
\end{equation}
Thus the definition \eqref{eq:def_abr} fulfils the requirements for the pair $(\alpha,\beta)$. It should be shown whether that works as well with another pair $(\gamma,\delta)$ such that $-\rho\leq\gamma,\delta\leq\rho$ and $\gamma+\delta=\rho$. If this second pair is really different that $(\alpha,\beta)$, then $\delta$ is neither $\alpha$ nor $\beta$; it is allso clear that $\delta$ is not $-\gamma$. Then formula \eqref{eq:enuaii}  works with the quadruple $(-\alpha,-\beta,\gamma,\delta)$:
\begin{equation}\label{eq:c_un}
c(-\alpha,-\beta)c(\gamma,\delta)+c(-\beta,\gamma)c(-\alpha,\delta)+c(\gamma,-\alpha)c(-\beta,\delta)=0.
\end{equation}
If $\alpha<0$, the assumption $\alpha+\beta=\rho$ makes $\beta>\rho$, which is in contradiction with $-\rho\leq\beta\leq\rho$. Then $\alpha,\beta,\gamma,\delta>0$ and moreover, the difference of any two of them is strictly between $-\rho$ and $\rho$. Since $\delta-\alpha=-(\gamma-\beta)$, if $\gamma-\beta$ is a root, $\delta-\alpha$ is also a root and the induction hypothesis gives
\begin{subequations}\label{eq:c_deux_un}
\begin{align}
c(\gamma,-\beta)a_{\gamma-\beta}&=c(\gamma',-\beta')\abg\abmb,  \label{eq:c_deux_un_a}\\
c(-\alpha,\delta)a_{-\alpha+\delta}&=c(-\alpha',\delta')\abma\abd.\label{eq:c_deux_un_b}
\end{align}
\end{subequations}
If we take for the convention $a_{\mu}=1$ whenever $\mu$ is not a root, these relations still hold if $\gamma-\beta$ is not a root. In the same way,
\begin{subequations}\label{eq:c_deux_deux}
\begin{align}
c(\gamma,-\alpha)a_{\gamma-\alpha}&=c(\gamma',-\alpha')\abma\abg,\label{eq:c_deux_deux_a}\\
c(-\beta,\delta)a_{-\beta+\delta}&=c(-\beta',\delta')\abmb\abd.\label{eq:c_deux_deux_b}
\end{align}
\end{subequations}
As $\delta-\alpha=-(\gamma-\beta)$, we have $a_{\delta-\alpha}a_{\gamma-\beta}=1$ and in the same way, $a_{\gamma-\alpha}a_{\delta-\beta}=1$. Taking it into account and multiplicating \eqref{eq:c_deux_un_a} by \eqref{eq:c_deux_un_b} and \eqref{eq:c_deux_deux_a} by \eqref{eq:c_deux_deux_a}, we find:
\begin{subequations}\label{eq:c_deux_trois}
\begin{align}
c(-\beta,\gamma)c(-\alpha,\delta)&=c(-\beta',\gamma')c(-\alpha',\delta')\abma\abmb\abg\abd\label{eq:c_deux_trois_a}\\
c(\gamma,-\alpha)c(-\beta,\delta)&=c(\gamma',-\alpha')c(-\beta',\delta')\abma\abmb\abg\abd.
\end{align}
\end{subequations}
We can use it to rewrite equation \eqref{eq:c_un}. After multiplication by $\aba\abb\abmg\abmd$,
\begin{equation}
c(-\alpha,-\beta)c(\gamma,\delta)\aba\abb\abmg\abmd+c(-\beta',\gamma')c(-\alpha',\delta')+c(\gamma',-\alpha')c(-\beta',\delta')=0.
\end{equation}
But equation \eqref{eq:c_un} is also true for $(\alpha',\beta',\gamma',\delta')$ instead of $(\alpha,\beta,\gamma,\delta)$, so that the last two terms can be replaced by only one term to give
\[
c(-\alpha,-\beta)c(\gamma,\delta)\aba\abb\abmg\abmd-c(-\alpha',-\beta')c(\gamma',\delta')=0.
\]
Since the pair $(\alpha,\beta)$ fulfils $c(-\alpha,-\beta)a_{-\alpha-\beta}=c(-\alpha',-\beta')\abma\abmb$, using $\alpha+\beta=\gamma+\delta$, we find
\[
c(\gamma,\delta)a_{\gamma+\delta}=c(\gamma',\delta')\abg\abd.
\]

\end{proof}


\begin{corollary}
The elements $ x_{\alpha}\in\lG_{\alpha}$ can be chosen in order to satisfy

\begin{itemize}
\item $B( x_{\alpha},\xbma)=1,$
\item $[ x_{\alpha},\xbma]= h_{\alpha}$,
\item $c(\alpha,\beta)=c(-\alpha,-\beta)$.
\end{itemize}

\end{corollary}

These vectors $ x_{\alpha}\in\lG_{\alpha}$ are called \defe{root vectors}{root!vectors}.

\begin{proof}
We consider the isomorphism $\alpha\to\alpha$ from $\Phi$ to $\Phi$; by the theorem this induces an isomorphism $\dpt{\eta}{\lG}{\lG}$ given by some constants $c_{\alpha}$:
\[
\eta( x_{\alpha})=c_{-\alpha}\xbma
\]
without sum on $\alpha$, because of course $\eta( x_{\alpha})\in\lG_{-\alpha}$. We choose $a x_{\alpha}\in\eC$ in such a way that
\begin{subequations}
\begin{align}
a_{\alpha}^2&=-c_{-\alpha}\\
a_{\alpha} a_{-\alpha}&=1,
\end{align} 
\end{subequations} 
and then we put $y_{\alpha}=a_{\alpha} x_{\alpha}$. It is immediate that $B(y_{\alpha},y_{-\alpha})=1$, thus the redefinition $ x_{\alpha}\to y_{\alpha}$ doesn't change the obtained relations. Acting on $y_{\alpha}$, the isomorphism $\eta$ gives
\begin{equation}
\eta(y_{\alpha})=a_{\alpha} c_{-\alpha}\xbma
=-a_{-\alpha}\xbma
=-y_{-\alpha}.
\end{equation}

If $\alpha,\beta,\alpha+\beta\in\Delta$, we naturally define $c'(\alpha,\beta)$ by
\[
[y_{\alpha},y_{\beta}]=c'(\alpha,\beta)y_{\alpha,\beta}.
\]
Using the fact that $\eta$ is a Lie algebra automorphism of $\lG$ we have:
\begin{equation}
-c'(\alpha,\beta)y_{-(\alpha+\beta)}=\eta\big(  c'(\alpha,\beta)y_{\alpha+\beta}   \big)
            =[-y_{-\alpha},-y_{-\beta}]
            =c'(-\alpha,-\beta)y_{-(\alpha+\beta)}.
\end{equation}
\end{proof}

From now we always our $ x_{\alpha}$ in this way.

\begin{remark}
It is also possible to choice the $ x_{\alpha}$ in such a way that 

\begin{itemize}
\item $B( x_{\alpha},\xbma)=-1$,
\item $c(\alpha,\beta)=c(-\alpha,-\beta)$.
\end{itemize}
This is the choice of the reference \cite{Hochschild}.
\end{remark}

Here is a characterization for Cartan subalgebras of semisimple Lie algebras. This is sometimes taken as the \emph{definition} of a Cartan subalgebra in books devoted to semisimple Lie algebras (for example in \cite{Helgason}).

\begin{proposition}     \label{PropCartanMaxAnel}
    A subalgebra $\lH$ of a semisimple Lie algebra $\lG$ is a Cartan subalgebra if and only if

    \begin{itemize}
        \item $\lH$ is maximally abelian in $\lG$,
        \item the endomorphism $\ad h$ is semisimple for every $h\in\lH$.
    \end{itemize}
\end{proposition}

Here, ``semisimple''{} means ``diagonalisable'', cf. definition at page \ref{pg:def_semisimple}.

\begin{proof}
\subdem{Necessary condition} We know from theorem \ref{tho:Phi_base} that $\lH$ is abelian and from proposition \ref{prop:Cartan_max_nil} that it is maximally nilpotent. Then it is maximally abelian. On the other hand, let $h\in\lH$; the endomorphism $\ad h$ is diagonalisable with respect to the decomposition $\lG=\lH\bigoplus_{\alpha\in\Delta}\lH_{\alpha}$.

\subdem{Sufficient condition}
Firstly it is clear that a maximal abelian subalgebra is nilpotent and the $\ad h_i$ are simultaneously diagonalisable for the different $h_i\in\lH$. Let $\{x_1,\ldots,x_n\}$ be a basis of $\lG$ which diagonalise all the $\ad h_i$. In this basis, if $(\ad h)_{ii}=0$ for any $h\in\lH$, then $x_i\in\lH$: if it was not, $\lH\cup\{x_i\}$ would be abelian.

Let $x\in\lG$ such that $(\ad h)x\in\lH$ for every $h\in\lH$. Suppose that $x$ has a $x_i$-component with $x_i\notin\lH$. There is a $h\in\lH$ with $(\ad h)_{ii}\neq 0$. Then $(\ad h)x$ has a $x_i$-component and can't lies in $\lH$.

\end{proof}

This characterization of Cartan subalgebras is used to prove the existence of Cartan subalgebra for any complex semisimple Lie algebra.

\begin{theorem}\label{tho:Phi_base}
The Cartan algebra of a complex semisimple Lie algebra is abelian and the dual is spanned by the roots: \( \Span\Phi=\lH^*\).
\end{theorem}

\begin{proof}
Let $\alpha$ be a non zero root; from the point \ref{prop:trois_poids:deux} of proposition \ref{prop:trois_poids}, there exists a $v\in\lG_{\alpha}$ such that for any $x\in\lH$, $[x,v]=\alpha(x)v$. Since $\dim\lG_{\alpha}=1$ it is in fact true for any $v\in\lG_{\alpha}$. In particular $\forall v\in\lG_{\alpha}$ and $h\in\lH$, we have $[h,x]=\alpha(h)x$.

Let $\lN\subset\lH$ be the set of elements which are annihilated by all the roots:
\begin{equation}
    \lN=\{ H\in\lH\tq\alpha(H)=0\,\forall \alpha\in\Phi \}.
\end{equation}
First remark that 
\begin{equation}\label{eq:GNz}
[\lG_{\alpha},\lN]=0
\end{equation}
because for $x\in\lG_{\alpha}$ and $h\in\lN\subset\lH$, we have $[h,x]=\alpha(h)x=0$. An other property of $\lN$ is
\begin{equation}\label{eq:HHN}
[\lH,\lH]\subset\lN.
\end{equation}
Indeed consider a root $\alpha$ and $x\in\lG_{\alpha}$. We have
\begin{equation}
\begin{split}
-\alpha([h,h'])x&=[x,[h,h']]
=[h,[h',x]]+[h',[x,h]]
=\alpha(h)[h',x]+\alpha(h')[x,h]\\
&=\alpha(h)\alpha(h')-\alpha(h')\alpha(h)
=0.
\end{split}
\end{equation}
If $x\in\lG$ is decomposed as $x=\sum_{\alpha\in\Phi}x_{\alpha}$ and if $n\in\lN$, then
\[
[x,n]=\sum_{\alpha}[x_{\alpha},n]=\sum_{\alpha}\alpha(n)x_{\alpha}=0.
\]
In particular, $\lN$ is an ideal\quext{\c Ca me semble quand m\^eme fort de prouver que c'est le centralisateur pour dire que c'est un id\'eal. D'autant plus que je pourais directement dire que $\lN$ est centralisateur dans un semisimple et donc nulle.}. Moreover, the fact that $\lN\subset\lH$ makes $\lN$ a \emph{nilpotent} ideal in the semisimple Lie algebra $\lG$. Then $\lN=0$. Equation \eqref{eq:GNz} makes $\lH$ abelian while equation \eqref{eq:HHN} says that no element of $\lH$ is anihilated by all the roots. This implies that $\Span\Phi=\lH^*$. To see it more precisely, if $\Phi$ don't span a certain (dual) basis element $e_i^*$ of $\lH^*$, then a basis of $\Span\Phi$ is at most $\{e_j\}_{j\neq i}$. Then it is clear that $\alpha(e_i)=0$ for any root $\alpha$.
\end{proof}

\begin{theorem}\label{tho:six_Cartan}
    If $\alpha,\beta$ are roots of a semisimple Lie algebra $\lG$ with respect to a Cartan subalgebra $\lH$, then

    \begin{enumerate}
        \item if $x_{\alpha}\neq0\in\lG_{\alpha}$ fulfils $[h,x_{\alpha}]=\alpha(h)x_{\alpha}$ for all $h\in\lH$, then $\forall y\in\lG_{-\alpha}$
        \[
        [x_{\alpha},y]=B(x_{\alpha},y)h_{\alpha},
        \] 
        \item\label{ite:six_deux} $\alpha(h_{\alpha})$ is rational and positive. Moreover 
        \[
        \alpha(h_{\alpha})\sum_{\gamma\in\Phi}(\gamma_{\alpha}-\gamma^{\alpha})^2=4,
        \]
        \item $2\beta(h_{\alpha})=(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})$,
        \label{ite:six_trois}
        \item the forms $0,\alpha,-\alpha$ are the only integer multiples of $\alpha$ which are roots,
        \label{ite:six_quatre}
        \item $\dim\lG_{\alpha}$=1,
        \label{ite:six_cinq}
        \item any $k$ which makes $\beta+k\alpha$ a root lie between $-\beta_{\alpha}$ and $\beta^{\alpha}$. In other words, $\beta+k\alpha\in\Phi$ is only true with $-\beta_{\alpha}\leq k\leq\beta^{\alpha}$.
        \label{ite:six_six}
    \end{enumerate}

\end{theorem}

\begin{proof}

The fact that $y\in\lG_{-\alpha}$ and that $x\in\lG_{\alpha}$ make $[x,y]\in\lG_0=\lH$. Now we consider $h\in\lH$ and the invariance formula \eqref{eq:Killing_invariant}. We find:
\begin{equation}
B(h,[x_{\alpha},y])=-B([x_{\alpha},h],y)\\
        =\alpha(h)B(x_{\alpha},y)\\
        =B(h,h_{\alpha})B(x_{\alpha},y)\\
        =B(h,B(x_{\alpha},y)h_{\alpha}).
\end{equation}
Since it is true for any $h\in\lH$ and $B$ is nondegenerate on $\lH$ we find the first point. In order to prove \ref{ite:six_deux}, we consider
\[
U=\bigoplus_{-\beta_{\alpha}\leq m\leq\beta^{\alpha}}\lG_{\beta+m\alpha}.
\]
By definition of $\alpha_{\beta}$ and $\alpha^{\beta}$, each term of the sum is a root space. If $z\in\lG_{\alpha}\oplus\lG_{-\alpha}$, then $U$ is stable under $\ad z$ because the terms in $\ad z U$ are of the form $[z,x_{\beta+m\alpha}]\in\lG_{\beta+m\alpha\pm\alpha}$. Note however that this $\ad z U$ is not \emph{equal} to $U$.

Let $x_{\alpha}\neq 0\in\lG_{\alpha}$. There exists a $y\in\lG_{-\alpha}$ such that $[x_{\alpha},y]=B(x_{\alpha},y)h_{\alpha}$ (here we use semi-simplicity). By fitting the norm of $y$, we can choose it in order to get  $[x_{\alpha},y]=h_{\alpha}$, so that 
\[
\ad h_{\alpha}=[\ad x_{\alpha},\ad y].
\]
Now we look at the restriction of $\ad h_{\alpha}$ to $U$:
\begin{equation}
\tr(\ad h_{\alpha})=\tr(\ad x_{\alpha}\circ\ad y)-\tr(\ad y\circ\ad x_{\alpha})=0.
\end{equation}
Since $h_{\alpha}\in\lH=\lG_0$, we have $\dpt{\ad h_{\alpha}}{U}{U}$, so that the annihilation of the trace of $\ad h_{\alpha}$ can be particularised to 
\[
\tr(\ad h_{\alpha}|_U)=0.
\]
On the other hand, by definition $\ad h_{\alpha}-(\beta+m\alpha)(h_{\alpha})$ is nilpotent on $\lG_{\beta+m\alpha}$. Then it has a vanishing trace:
\[
\sum_m\tr( \ad h_{\alpha}-(\beta+m\alpha)h_{\alpha}  )=0.
\]
But we had yet seen that the term with $\ad h_{\alpha}$ is zero; then
\begin{equation}\label{eq:trace_U}
\sum_{-\beta_{\alpha}\leq m\leq \beta^{\alpha}} (\beta+m\alpha)h_{\alpha}\dim\lG_{\beta+m\alpha}=0.
\end{equation}
If we suppose that $\alpha(h_{\alpha})=0$ this gives $\beta(h_{\alpha})=0$. Since this conclusion is true for any root $\beta$, we find $B(h,h_{\alpha})=0$ for any $h\in\lH$. In other words, $\alpha(h)=0$ for any $h\in\lH$. This contradicts the assumption, so that we conclude $\alpha(h_{\alpha})\neq 0$.


Let $V=\lH+(x_{\alpha})+\sum_{m<0}\lG_{m\alpha}$ where $(x_{\alpha})$ is the one dimensional space spanned by $x_{\alpha}$. On the one hand,  from the definition of $x_{\alpha}$, $\ad x_{\alpha}\lH\subset(x_{\alpha})$ and $\ad x_{\alpha}\lG_{m\alpha}\subset\lG_{(m+1)\alpha}$. On the other hand, $y\in\lG_{-\alpha}$ is defined by the relation $[x_{\alpha},y]=h_{\alpha}$, then  $\ad y\lH\subset\lG_{-\alpha}\subset\sum_{m<0}\lG_{m\alpha}$, $\ad y(x_{\alpha})\subset\lG_0=\lH$ and $\ad y\sum_{m<0}\lG_{m\alpha}=\sum_{m<0}\lG_{(m-1)\alpha}$. All this make $V$ invariant under $\ad x_{\alpha}$ and $\ad y$. 

Since $\ad h_{\alpha}=[\ad x_{\alpha},\ad y]$, the trace of $\ad h_{\alpha}$ is zero so that the invariance of $V$ gives
\[
\tr(\ad h_{\alpha}|_V)=0.
\]
By the definition of $x_{\alpha}$ particularised to $h\to h_{\alpha}$, we have $\tr(\ad h_{\alpha}|_{(x_{\alpha})})=\alpha(h_{\alpha})$. By the definition of $\lG_0$, for any $x\in\lH$ and $v\in\lG_0$, $\ad x$ is nilpotent on $v$. Taking $h_{\alpha}$ as $x$, we see that 
$(\ad h_{\alpha})h$ don't contain ``$h$-component''. Then $\tr(\ad h_{\alpha}|_{\lH})=0$. Finally the operator $(\ad h_{\alpha}-m\alpha(h_{\alpha}))$ is nilpotent on $\lG_{m\alpha}$, so that $\tr(\ad h_{\alpha}|_{\lG_{m\alpha}})=\tr( m\alpha(\alpha)|_{\lG_{m\alpha}} )=m\alpha(h_{\alpha})\dim\lG_{m\alpha}$. All this gives
\begin{equation}
\alpha(h_{\alpha})\left( 1+\sum_{m<0}m\dim\lG_{m\alpha}  \right)=0.
\end{equation}
As we saw that $\alpha(h_{\alpha})\neq 0$, we conclude that $\dim\lG_{m\alpha}=0$ for $m<-1$ and $\dim\lG_{-\alpha}=1$. This proves \ref{ite:six_cinq}. 

This also prove \ref{ite:six_quatre} in the particular case of \emph{integer} multiples. It is rather simple to get relations such that $0_{\alpha}=1$, $0^{\alpha}=1$, $\alpha_{\alpha}=2$, $(-\alpha)_{\alpha}=0$, and it is easy to check \ref{ite:six_trois} in the cases $\beta=-\alpha,0,\alpha$. Now we turn our attention to the case in which $\beta$ is not an integer multiple of $\alpha$. By \ref{ite:six_quatre} applied to $\alpha\to\beta+m\alpha$, we have $\dim\lG_{\beta+m\alpha}=1$ whenever $-\beta_{\alpha}\leq m\leq\beta^{\alpha}$.

From equation \eqref{eq:trace_U}, $\sum_{-\beta_{\alpha}\leq m\leq\beta^{\alpha}}( \beta(h_{\alpha})+m\alpha(h_{\alpha}) )=0$, then
\begin{equation}
(\beta_{\alpha}+\beta^{\alpha}+1)\beta(h_{\alpha})=(\sum_m m)\alpha(h_{\alpha})\\
                        =\left(
                    \frac{\beta_{\alpha}(\beta_{\alpha}+1)}{2}-\frac{(\beta^{\alpha}-1)\lbha}{2}
                        \right)\alpha(h_{\alpha}).
\end{equation}
This gives \ref{ite:six_trois}. Now we consider the formula of theorem \ref{tho:Killing_Cartan} in the case $x=y=h_{\alpha}$ and we use the fact that $B(h,h_{\alpha})=\alpha(h)$ in the case $h=h_{\alpha}$:
\begin{equation}
B(h_{\alpha},h_{\alpha})=\alpha(h_{\alpha})\\
            =\sum_{\gamma\in\Phi}\dim\lG_{\gamma}\gamma(h_{\alpha})^2\\
            =\sum_{\gamma\in\Phi}\gamma(h_{\alpha})^2.
\end{equation}
Since $\beta(h_{\alpha})=\frac{1}{2}(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})$, we find \ref{ite:six_deux}. In order to prove \ref{ite:six_quatre}, we consider $\beta=c\alpha$ for a $c\in\eC$. By \ref{ite:six_trois}, $2c\alpha(h_{\alpha})=(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})$, so that $c$ is an half integer: $c=p/2$ with $p\in\eZ$. If $c$ is non zero, we can interchange $\alpha$ and $\beta$ and see that $\alpha=c^{-1}\beta$ implies $c^{-1}=q/2$ with $q\in\eZ$. It is clear the $pq=4$. But we had already discussed the case of integer multiples of $\alpha$, so that we can suppose that $p$ is odd. The only odd $p$ such that $pq=3$ with $q\in\eZ$ are $p=1,-1$, which are two excluded cases: they are $\alpha=\pm 2\beta$ which lies in the case of integer multiples.

It remains to prove \ref{ite:six_six}. By definition of $\beta^{\alpha}$, the form $\beta+(\beta^{\alpha}+1)\alpha$ is not a root. But it remains possible that $\beta+(\beta^{\alpha}+2)\alpha$ is. We suppose that $k_1,\ldots,k_p$ are the $p$ positive integers such that $\beta+k_i\alpha\in\Phi$. We pose
\[
W=\bigoplus_{i=1}^p\lG_{\beta+k_i\alpha}.
\]
As usual we see that $W$ is stable under $\ad x_{\alpha}$ and $\ad y$ (because $k_i\geq\beta^{\alpha}+2$). The trace of $\ad g_{\alpha}$ on $W$ is zero, thus
\begin{equation}
0=\sum_{i=1}^p(\beta+k_i\alpha)(h_{\alpha}).
\end{equation}
By \ref{ite:six_trois}, we find
\[
p(\beta_{\alpha}-\beta^{\alpha})\alpha(h_{\alpha})=2(k_1+\ldots+k_p)>p(\beta^{\alpha}+1). 
\]
This is not possible because it would gives $-\beta^{\alpha}-\beta_{\alpha}>2$.
\end{proof}

\begin{theorem}
Let $\lG$ be a Lie algebra, $\lH$ a Cartan subalgebra of $\lG$ and $B$ the Killing\index{Killing!form} form of $\lG$. Then for all $x$, $y\in\lH$,
\begin{equation}
B(x,y)=\sum_{\gamma\in\Phi}d_{\gamma}\gamma(x)\gamma(y)
\end{equation}
where $g_{\gamma}=\dim\lG\bgamma$.
\label{tho:Killing_Cartan}
\end{theorem}

\begin{proof}
We are seeing $\lG$ as a $\lH$-module for the adjoint representation. In particular, proposition \ref{prop:trois_poids} makes $\lG$ a direct sum of the $\lH$-submodules $\lG_{\gamma}$. Then
\begin{equation}
B(x,y)=\tr({\ad x}^2)\\
        =\sum_{\gamma\in\Phi}\tr(\ad x|_{\gamma}^2)
\end{equation}
where $\ad x|_{\gamma}$ means the restriction of $\ad x$ to $\lG\bgamma$. It is clear that $\ad x|\bgamma-\gamma(x)$ is nilpotent, then $\ad x|\bgamma^2-\gamma(x)^2$ is also nilpotent because
\[
\ad x|_{\gamma}^2-\gamma(x)^2=(\ad x|\bgamma+\gamma(x))(\ad x|\bgamma-\gamma(x))
\]
and the fact that these two terms commute. The trace of a nilpotent endomorphism is zero, then $\tr(\ad x|_{\gamma}^2-\gamma(x)^2)=0$ or for all $x\in\lG$,
\begin{equation}\label{eq:Bxx}
B(x,x)=\sum_{\gamma\in\Phi}d_{\gamma}\gamma(x)^2.
\end{equation}
on the other hand, we know that a quadratic form determines only one bilinear form. Here the form \eqref{eq:Bxx} gives
\[
B(x,y)=\sum_{\gamma\in\Phi}d_{\gamma}\gamma(x)\gamma(y).
\]
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Weyl: other results}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}
Two immediate properties of the Weyl group are 

\begin{enumerate}
\item $W$ is a finite group of orthogonal transformations of $V$,
\item if $r$ is an orthogonal transformation of $V$, the $s_{r\alpha}=rs_{\alpha} r^{-1}$.
\end{enumerate}
\end{proposition}

\begin{proof}
\subdem{First item} By definition of an abstract root system, $W$ leaves $\Delta$ invariant; since $V$ is spanned by $V$, it implies that $W$ also leaves $V$ invariant. From an easy computation, $(s_{\alpha}\varphi,s_{\alpha}\phi)=(\varphi,\phi)$. Since $\Delta$ is a finite set, there are only a finite number of common permutations of elements of $\Delta$ \emph{a fortiori} $W$ is finite.

\subdem{Second item}
It is easy to see that $s_{r\alpha}(r\varphi)=rs_{\alpha}\varphi$, then $s_{r\alpha}=r\circ s_{\alpha}\circ r^{-1}$.
\end{proof}


We introduce the \defe{root reflexion}{root!reflexion} $\dpt{s_{\alpha}}{\lHeR^*}{\lHeR^*}$ for $\alpha\in\Phi$ and $\varphi\in\lHeR^*$ by
\begin{equation}
s_{\alpha}(\varphi)=\varphi-\frac{2(\varphi,\alpha)}{|\alpha|^2}\alpha.
\end{equation}

\begin{proposition}
If $\alpha\in\Phi$, then $s_{\alpha}$ leaves $\Phi$ invariant.
\end{proposition}

\begin{proof}
If $\alpha$ or $\varphi$ is zero, then it is clear that $s_{\alpha}(\varphi)$ belongs to $\Phi$. Thus we can suppose that $\alpha\in\Delta$ and proof that $s_{\alpha}$ leaves $\Delta$ invariant. For, we use the theorem \ref{tho:six_Cartan} to find
\begin{equation}
  s_{\alpha}\beta=\beta-\frac{2(\beta,\alpha)}{|\alpha|^2}\alpha
               =\beta-(\lbba-\lbha)\alpha.
\end{equation}
If $\lbba-\lbha>0$, we are in a case $\beta-n\alpha$ with $\lbba-\lbha<\lbba$, so that $s_{\alpha}\beta$ is a root. The case $\lbha>\lbba$ is treated in the same way. It just remains to check that  if $\alpha,\beta\in\Delta$, then $s_{\alpha}\beta\neq0$. The problem is to show that the equation (with a given $\alpha$ in $\Delta$)
\begin{equation}\label{eq:beta_frZ_alpha}
   \beta=\frZ{\alpha}{\beta}\alpha
\end{equation}
has no solution in $\Delta$ (the indeterminate is $\beta$). The only nonzero multiples of $\beta$ which are roots are $\pm\beta$, then if we set $\beta=r\alpha$, equation \eqref{eq:beta_frZ_alpha} gives $r=\pm\frac{1}{2}$, which is impossible.

\end{proof}

\begin{proposition}
    The Weyl group permutes simply transitively the simple systems.
\end{proposition}



%---------------------------------------------------------------------------------------------------------------------------
\subsection{Longest element}
%---------------------------------------------------------------------------------------------------------------------------

Let \( w\in W\). The \defe{length}{length!in Weyl group}\nomenclature[G]{\( l(w)\)}{length in the Weyl group} of \( w\) is the smallest \( k\) such that \( w\) can be written as a composition of \( k\) reflexions \( s_{\alpha_i}\). That is the smallest \( k\) such that
\begin{equation}
    w=s_{\alpha_{i_1}}s_{\alpha_{i_2}}\ldots s_{\alpha_{i_k}}.
\end{equation}

\begin{lemma}
    If \( w\) and \( w'\) are elements of the Weyl group,
    \begin{enumerate}
        \item
            \( l(w)=l(w^{-1})\),
        \item
            \( l(w)=0\) if and only if \( w=\id\),
        \item
            \( l(ww')\leq l(w)+l(w')\),
        \item
            \( l(ww')\geq l(w)-l(w')\),
        \item
            \( l(w)-1\leq l(ws_{\alpha_i})\leq l(w)+1\).
    \end{enumerate}
\end{lemma}
Let \( n(w)\) be the number of positive simple roots that are send to a negative root:
\begin{equation}
    n(w)=Card\,\Pi\cap w^{-1}(-\Pi).
\end{equation}

\begin{proposition}
    Let \( \Delta\) be a system of simple roots and \( \Pi\) the associated positive system. The following conditions on an element \( w\) of the Weyl group are equivalent:
    \begin{enumerate}
        \item
            \( w\Pi=\Pi\);
        \item
            \( w\Delta=\Delta\);
        \item
            \( l(w)=0\);
        \item
            \( n(w)=0\);
        \item
            \( w=\id\).
    \end{enumerate}
\end{proposition}

For a proof see page 15 in \cite{HumphreysCoxeter}.

\begin{theorem}
    If \( w\) is an element of the Weyl group,
    \begin{equation}
        l(w)=n(w).
    \end{equation}
\end{theorem}

\begin{proof}
    No proof.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Weyl group and representations}
%---------------------------------------------------------------------------------------------------------------------------

This subsection comes from \cite{Cornwell}.

\begin{theorem}     \label{Thoirrepllamifffmor}
    There exists an irreducible representation of highest weight $\Lambda$ if and only if
    \begin{equation}
        \Lambda_{\alpha}=\frac{ 2(\Lambda,\alpha) }{ (\alpha,\alpha) }\in\eN
    \end{equation}
    for every simple root $\alpha$. Moreover, if $\xi$ is a highest weight vector and if $\alpha$ is a simple root, then
    \begin{equation}
        E_{-\alpha}^k\xi    
                \begin{cases}
                    \neq 0  &\text{if $k\leq\Lambda_{\alpha}$}\\
                    =0  &\text{if $k>\Lambda_{\alpha}$}.    
                \end{cases}
    \end{equation}
\end{theorem}

\begin{proof}
    No proof.
\end{proof}

\begin{theorem}     \label{ThoLOngestlowestrepres}
    If \( \Lambda\) is the highest weight of a representation and if \( w_0\) is the longest element of the Weyl group, then \( w_0\Lambda\) is the lowest weight.
\end{theorem}

\begin{probleme}
    It is still not clear for me how does the proof works. Questions to be answered:
    \begin{enumerate}
        \item
            existence, unicity
        \item
            \( w_0\) is the longest element of the Weyl group
        \item
            if \( \Lambda\) is the highest weight, then \( w_0\Lambda\) is the lowest.
    \end{enumerate}
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Chevalley basis (deprecated)}
%---------------------------------------------------------------------------------------------------------------------------
See \cite{SSLA_Modave2005}.

Let $\Phi$\nomenclature{$\Phi$, $\Phi^+$}{Root system} be the finite set of roots of $\lG$. Then chose a positivity notion on $\lH^*$ and consider $\Phi^+$, the positive subset of $\Phi$. We also take $\Delta$\nomenclature{$\Delta$}{Basis of the roots}, a basis of the roots. An element of $\Phi^+$ is a \defe{simple root}{simple!root}\index{root!simple} if it cannot be written under the form of a sum of two elements of $\Phi^+$. Every positive root is a sum of simple roots. 

Let
\begin{equation}
    \{ \alpha_1,\ldots,\alpha_l \}
\end{equation}
be a basis of $\lH^*$ made of simple roots and
\begin{equation}
    \{ h_1,\ldots,h_l \},
\end{equation}
the dual basis. One can choose the $\alpha_i$ in such a way that $\{ h_1,\ldots,h_l \}$ is orthogonal with respect to the Killing form\quext{Why ?}. One consequence of that is that
\begin{equation}            \label{EqBhihalphaih}
    B(h_i,h)=\alpha_i(h)
\end{equation}
for every $h\in\lH$. Indeed, $h$ can be written, in the basis, as $h=h^jh_j$ where $h^j=B(h_j,h)$. Thus one has
\begin{equation}
    B(h_i,h)=h^i=h^j\delta_{ij}=\alpha_i(h^jh_j)=\alpha_i(h).
\end{equation}
We consider $\{ \alpha_1,\ldots,\alpha_m \}$, the positive roots (the roots $\alpha_1$,\ldots,$\alpha_l$ are some of them). One knows that $\lG_{\alpha_i}$ is one dimensional, so one take $e_i\in\lG_{\alpha_i}$ and $f_i\in\lG_{-\alpha_i}$ as basis of their respective spaces. If we denote by $\lN^+=\Span\{ e_1,\ldots, e_m \}$ and $\lN^-=\Span\{ f_1,\ldots,f_m \}$, we have the decomposition
\begin{equation}
    \lG=\lN^-\oplus\lH\oplus\lN^+.
\end{equation}


% The proposition about SL(2,R) was here. 198631779

It $\{ \alpha_i \}$ are the simple roots, we consider the following new basis for $\lH$:
\begin{equation}
    H_{\alpha_i}=\frac{ 2\alpha_i^* }{ (\alpha_i,\alpha_i) }
\end{equation}
where $\alpha_i^*$ is the dual of $\alpha_i$ with respect to the inner product on \( \lH^*\), this means
\begin{equation}
    \alpha_j(\alpha_i^*)=(\alpha_i,\alpha_j).
\end{equation}
Since \( \lH\) is abelian (proposition \ref{PropCartanMaxAnel}), we have
\begin{equation}
    [H_{\alpha_i},H_{\alpha_j}]=0.
\end{equation}
Each root is a combination of the simple roots. If $\beta=\sum_{i=1}^lk_i\alpha_i$, we generalise the definition of $H_{\alpha_i}$ to
\begin{equation}
    H_{\beta}=\frac{ 2\beta^* }{ (\beta,\beta) }=\sum_i k_i\frac{ (\alpha_i,\alpha_i) }{ (\beta,\beta) }H_{\alpha_i}.
\end{equation}
The element $H_{\beta}$ is the \defe{co-weight}{co-weight} associated with the weight $\beta$.

Using the inner product $(.,.)$, we have the decomposition $\beta=\sum_i(\beta,\alpha_i)\alpha_i$ of the roots. An immediate consequence is that 
\begin{equation}
    \beta(\alpha_i^*)=(\alpha_i,\beta).
\end{equation}
If $\beta$ is any root, we denote by $\beta_i$ the result of $\beta$ on $H_{\alpha_i}$:
\begin{equation}            \label{EqbetaialphaiH}
    \beta_i=\beta(H_{\alpha_i})=\frac{ 2(\alpha_i,\beta) }{ (\alpha_i,\alpha_i) }.
\end{equation}

\begin{theorem}[Chevalley basis]\index{Chevalley!basis}
    For each root $\beta$, one can found an eigenvector $E_{\beta}$ of $\ad(H_{\beta})$ such that
    \begin{equation}            \label{EqChevalleyBasis}
        \begin{aligned}[]
            [H_{\beta},H_{\gamma}]      &=  0\\
            [E_{\beta},E_{-\beta}]      &=  H_{\beta}\\
            [E_{\beta},E_{\gamma}]      &=
                                \begin{cases}
                                    \pm(p+1)E_{\beta+\gamma}    &\text{if $\beta+\gamma$ is a root}\\
                                    0               &\text{otherwise}\\
                                \end{cases}\\
            [H_{\beta},E_{\gamma}]      &=2\frac{ (\beta,\gamma) }{ (\beta,\beta) }E_{\gamma}
        \end{aligned}
    \end{equation}
    where $p$ is the biggest integer $j$ such that $\gamma+j\beta$ is a root. Moreover, if $\alpha_i$ and $\alpha_j$ are simple roots, the latter becomes
    \begin{equation}
        [H_{\alpha_i},E_{\pm\alpha_j}]=\pm A_{ij}E_{\pm\alpha_j}
    \end{equation}
    where $A$ is the Cartan matrix.
\end{theorem}

An important point to notice is that, for each positive root $\alpha$, the algebra generated by $\{ H_{\alpha},E_{\alpha},E_{-\alpha} \}$ is $\gsl(2)$. This is the reason why the representation theory of $\lG$ reduces to the representation theory of $\gsl(2)$.

\section{Real Lie algebras}
%++++++++++++++++++++++++++

\subsection{Real and complex vector spaces}
%//////////////////////////////////////////////

If $V$ is a real vector space, the \defe{complexification}{complexification!of a vector space} of $V$ is the vector space\nomenclature{$V\heC$}{Complexification of $V$}
\[
  V\heC:=V\otimes\beR\eC.
\]
If $\{v_i\}$ is a basis of $V$ on $\eR$, then $\{v_i\otimes 1\}$ is a basis of $V\heC$ on $C$. Then
\[
   \dim_{\eR}V=\dim_{\eC}V\heC.
\]

Let $W$ be a complex vector space. If one restrains the scalars to $\eR$, we find a real vector space denoted by $W\heR$\nomenclature{$W\heR$}{Restriction of a complex vector spaces to $\eR$}. If $\{w_j\}$ is a basis of $W$, then $\{w_j,iw_j\}$ is a basis of $W\heR$ and 
\[
  \dim\beC W=\frac{1}{2}\dim\beR W\heR.
\]
Note that $(V\heC)\heR=V\oplus iV$.

A real vector space $V$ is a \defe{real form}{real!form!of complex vector space} of a complex vector space $W$ if $W\heR=V\oplus iV$. If $V$ is a real form of $W$, the map $\dpt{\varphi}{V\heC}{V\heC}$ given by the identity on $V$ and the multiplication by $-1$ on $iV$ is the \defe{conjugation}{conjugation} of $V\heC$ with respect of the real form $V$.

\subsection{Real and complex Lie algebras}
%/////////////////////////////////////////

For notational convenience, if not otherwise mentioned, $\lG$ will denote a complex Lie algebra and $\lF$ a real one. If $\lF$ is a real Lie algebra and $\lF\heC=\lF\otimes\eC$, its complexification (as vector space), we endow $\lF\heC$ with a Lie algebra structure by defining
\[
  [ (X\otimes a),(Y\otimes b)  ]=[X,Y]\otimes ab.
\]
This is a bilinear extension of the Lie algebra bracket of $\lF$. It is rather easy to see that $[\lF,\lF]\heC=[\lF\heC,\lF\heC]$.

Now we turn our attention to the Killing form. Let $\lF$ be a real Lie algebra with a Killing form $B\blF$. A basis of $\lF$ is also a basis of $\lF\heC$. Then the matrix $B_{ij}=\tr( \ad X_i\circ\ad X_j )$ of the Killing form is the same for $\lF\heC$ than for $\lF$. In conclusion:
\[
   B_{\lF\heC}|_{\lF\times\lF}=B\blF.
\]

Let us study the inverse process: $\lG$ is a complex Lie algebra and $\lG\heR$ is the real Lie algebra obtained from $\lG$ by restriction of the scalars. If $\mB=\{v_j\}$ is a basis of $\lG$, $\mB'=\{v_j,iv_j\}$ is a one of $\lG\heR$. For a certain $X\in\lG$ we denote by $(c_{kl})$ the matrix of $\ad_{\lG}X$. Now we study the matrix of $\ad_{\lG\heR}X$ in the basis $\mB'$ by computing
\begin{equation}
(\ad_{\lG}X)v_i=c_{ik}v_k
               =\big[ \Reel(c_{ik})+i\Imag(c_{ik}) \big]v_k
           =a_{ik}v_k+b_{ik}(iv_k)
\end{equation}
if $a=\Reel c$ and $b=\Imag c$. Then the columns of $\ad_{\lG\heR}$ which correspond to the $v_i\in\mB'$'s are given by
\[
\ad_{\lG\heR}X=\begin{pmatrix}
                 a&\cdot\\
         b&\cdot
               \end{pmatrix}
\]
where the dots denote some entries to be find now:
\begin{equation}
(\ad_{\lG}X)(iv_i)=i\big(  a_{ik}v_k+b_{ik}(iv_k)  \big)\\
                  =a_{ik}(iv_k)-b_{ik}v_k,
\end{equation}
so that the complete matrix of $\ad_{\lG\heR}X$ in the basis $\mB'$ is given by
\[
\ad_{\lG\heR}X=\begin{pmatrix}
                 a&-b\\
         b&a
               \end{pmatrix}.
\]
So,
\[
\ad_{\lG\heR}X\circ\ad_{\lG\heR}X'=\begin{pmatrix}
                 aa'-bb'&\cdot\\
         \cdot&aa'-bb'
               \end{pmatrix}.
\]
Then $B(X,X')=2\tr(aa'-bb')$ while
\begin{equation}
  B(X,Y)=\tr\big(  (a+ib)(a'+ib')  \big)\\
        =\tr(aa'-bb')+i\tr(ab'+ba').
\end{equation}
Thus we have
\begin{equation}
     B_{\lG\heR}=2\Reel B_{\lG},
\end{equation}
so that $\lG\heR$ is semisimple if and only if $\lG$ is semisimple. 

A result about the group of inner automorphism which will be useful later:

\begin{lemma}\label{lem:Int_g_gR}
If $\lG$ is a complex semisimple Lie algebra, then $\Int\lG=\Int\lG\heR$.
\end{lemma}

\begin{proof}
If $\{X_i\}$ is a basis of $\lG$, then $\{X_j,iX_j\}$ is a basis of $\lG\heR$. We define $\dpt{\psi}{\ad\lG}{\ad\lG\heR}$ by
\[
   \psi(\ad(a^jX_j))=\ad(a^jX_j).
\]
It is clearly surjective. On the other hand, if $\ad(a^jX_j)\ad(b^kX_k)$ as elements of $\ad\lG\heR$, then they are equals as elements of $\ad\lG$. The discussion following equations \eqref{eq:schem_ad_int} finish the proof.
\end{proof}

\subsection{Split real form}
%//////////////////////////////

Let $\lG$ be a complex semisimple Lie algebra, $\lH$ a Cartan subalgebra, $\Phi$ the set roots, $\Delta$ the set of non zero roots and $B$, the Killing form. From property \eqref{eq:enuaiv} and the fact that $c(-\alpha,-\beta)=c(\alpha,\beta)$, we find $c(\alpha,\beta)^2=\frac{1}{2}\lbha(1+\lbba)|\alpha|^2$,
 so that $c(\alpha,\beta)^2\geq 0$ which gives $c(\alpha,\beta)\in\eR$. We can define
 
\[
   \lGeR=\lH_0\bigoplus_{\alpha\in\Phi}\eR x_{\alpha}.
\]
Remark that $\lG_{\alpha}$ has dimension one with respect to $\eC$, not $\eR$; then $\eR x_{\alpha}\neq\lG_{\alpha}$, but $\eC x_{\alpha}=\lG_{\alpha}$ and $\lG_{\alpha}=\eR x_{\alpha}\oplus i\eR x_{\alpha}$. Since it is clear that $\bigoplus_{\alpha\in\Delta}( \eR x_{\alpha}\oplus i\eR x_{\alpha} )=\bigoplus_{\alpha\in\Delta}\lG_{\alpha}$, the proposition \ref{prop:lHeR} gives
\begin{equation}
  \lG=\lGeR\oplus i\lGeR.
\end{equation}
Any real form of $\lG$ which contains the $\lHeR$ of a certain Cartan subalgebra $\lH$ of $\lG$ is said a \defe{split real form}{split!real form}. The construction shows that any complex semisimple Lie algebra admits a split real form.

\subsection{Compact real form}
%///////////////////////////////

A \defe{compact real form}{compact!real form}\index{compact!Lie algebra} of a complex Lie algebra is a real form which is compact as Lie algebra. Recall that a real Lie algebra is compact when its analytic group of inner automorphism is compact, see page \pageref{pg:compact_Lie}

\begin{theorem}
Any complex semisimple Lie algebra contains a compact real form.
\end{theorem}

\begin{proof}
Let $\lH$ be a Cartan algebra of the complex semisimple Lie algebra $\lG$ and $ x_{\alpha}$, some root vectors. We consider the space
\begin{equation}
 \lU_0=\underbrace{\sum_{\alpha\in\Phi}\eR ih_{\alpha}}_A+\underbrace{\sum_{\alpha\in\Phi}\eR( x_{\alpha}-\xbma)}_B+\underbrace{\sum_{\alpha\in\Phi}\eR i( x_{\alpha}+\xbma)}_C.
\end{equation}
Since $\lU_0\oplus i\lU_0$ contains all the $\eC h_{\alpha}$, $\lH\subset\lU_0\oplus i\lU_0$; it is also rather clear that $\lU_0$ is a real form of $\lG$ (as vector space), for example, $i\eR( x_{\alpha}-\xbma)+\eR i( x_{\alpha}+\xbma)=\eR i x_{\alpha}$. Now we have to check that $\lU_0$ is a real form of $\lG$ as Lie algebra, i.e. that $\lU_0$ is closed for the Lie bracket. This is a lot of computations:
\[
\begin{split}
[i h_{\alpha},i\hbb]               &=0,\\
[i h_{\alpha},( x_{\alpha}-\xbma)]        &=i(\alpha( h_{\alpha}) x_{\alpha}-(-\alpha)( h_{\alpha})\xbma)\\
                            &=i\alpha( h_{\alpha})( x_{\alpha}+\xbma)\in C,\\
[i h_{\alpha},i( x_{\alpha}+\xbma)]       &=-\alpha( h_{\alpha})( x_{\alpha}-\xbma)\in B,\\
[( x_{\alpha}-\xbma),(\xbb-\xbmb)] &=c(\alpha,\beta)( x_{\alpha+\beta}-x_{-(\alpha+\beta)} )\in B\\
                            &\quad -c(\alpha,\beta)(x_{\alpha-\beta}-x_{\beta-\alpha})\in B,\\
[( x_{\alpha}-\xbma),i(\xbb+\xbmb)]&=ic(\alpha,\beta)(x_{\alpha+\beta}+x_{-(\alpha+\beta)})\in C\\
                            &\quad +ic(\alpha,-\beta)(x_{\alpha-\beta)}+x_{-\alpha+\beta})\in C\\
[i h_{\alpha},(\xbb-\xbmb)]     &=i\beta( h_{\alpha})(\xbb-\xbmb)\in C\\
[i h_{\alpha},i(\xbb+\xbmb)]       &=-\beta( h_{\alpha})(\xbb-\xbmb)\in B\\
[i( x_{\alpha}+\xbma),i(\xbb+\xbmb)]&=-c(\alpha,\beta)(x_{\alpha+\beta}-x_{-(\alpha+\beta)})\\
                             &\quad -c'(\alpha,-\beta)(x_{\alpha-\beta}-x_{-\alpha+\beta}).
\end{split}
\]

From proposition \ref{prop:compact_Killing}, it just remains to prove that the Killing form of $\lU_0$ is strictly negative definite. We know that $B_{\lG}(\lG_{\alpha},\lG_{\beta})=0$ if $\alpha,\beta\in\Phi$ and $\alpha+\beta\neq 0$; then $A\perp B$ and $A\perp C$. It is a lot of computation to compute the Killing form; we know that $B$ is strictly positive definite on $\sum_{\alpha\in\Delta}\eR h_{\alpha}$ (and then strictly negative definite on $A$) a part this, the non zero elements are (recall that if $\alpha\neq 0$, $B( x_{\alpha}, x_{\alpha})=0$ from corollary \ref{cor:Bxy_zero})
\[
\begin{split}
  B( ( x_{\alpha}-\xbma),( x_{\alpha}-\xbma) )&=-2B( x_{\alpha},\xbma)=-2\\
  B(i( x_{\alpha}+\xbma),i( x_{\alpha},\xbma))&=-2.
\end{split}  
\]

What we have in the matrix of $B_{\lG}|_{\lU_0\times\lU_0}$ is a negative definite block (corresponding to $A$), $-2$ on the rest of the diagonal and zero anywhere else. Then it is well negative definite and $\lU_0$ is a compact real from of $\lG$.
\end{proof}

\subsection{Involutions}
%//////////////////////////

Let $\lG$ be a (real or complex) Lie algebra. An automorphism $\dpt{\sigma}{\lG}{\lG}$ which is not the identity such that $\sigma^2$ is the identity is a \defe{involution}{involutive!automorphism}. An involution $\dpt{\theta}{\lF}{\lF}$ of a \emph{real} semisimple Lie algebra $\lF$ such that the quadratic form $B_{\theta}$ defined by
\[
   B_{\theta}(X,Y):=-B(X,\theta Y)
\]
is positive definite is a \defe{Cartan involution}{Cartan!involution}.

\begin{proposition}
Let $\lG$ be a complex semisimple Lie algebra, $\lU_0$ a compact real form and $\tau$, the conjugation of $\lG$ with respect to $\lU_0$. Then $\tau$ is a Cartan involution of $\lG\heR$.
\label{prop:conj_invol}
\end{proposition}

\begin{proof}
From the assumptions, $\lG=\lU_0\oplus i\lU_0$, $\tau_{\lU_0}=id$ and $\tau_{i\lU_0}=-id$; then it is clear that $\tau_{\lG\heR}^2=id|_{\lG\heR}$. If $Z\in\lG$, we can decompose into $Z=X+iY$ with $X$, $Y\in\lU_0$. For $Z\neq 0$, we have
\begin{equation}
    B_{\lG}(Z,\tau Z)=B_{\lG}(X+iY,X-iY)
                     =B_{\lG}(X,X)+B_{\lG}(Y,Y)
             =B_{\lU_0}(X,X)+B_{\lU_0}(Y,Y)<0
\end{equation}
because $B$ restricts itself to $\lU_0$ which is compact. Then
\begin{equation}
  (B_{\lG\heR})_{\tau}(Z,Z')=B_{\lG\heR}(Z,\tau Z)
                            =-2\Reel B_{\lG}(Z,\tau Z')
\end{equation}
is positive definite because $(B_{\lG})_{\tau}$ is negative definite. Thus $\tau$ is a Cartan involution of $\lG\heR$.
\end{proof}

\begin{lemma}
If $\varphi$ and $\psi$ are involutions of a vector space $V$ (we denote by $V_{\psi^+}$ and $V_{\psi^-}$ the subspaces of $V$ for the eigenvalues $1$ and $-1$ of $\psi$ and similarly for $\varphi$), then
\[
[\varphi,\psi]=0\quad\text{iff}\quad \left\{   \begin{aligned}
                                                   V_{\varphi^+}&=(V_{\varphi^+}\cap V_{\psi^+})\oplus(V_{\varphi^+}\cap V_{\psi^-})\\
                           V_{\varphi^-}&=(V_{\varphi^-}\cap V_{\psi^+})\oplus(V_{\varphi^-}\cap V_{\psi^-}),
                                           \end{aligned}
                      \right.
\]
i.e. if and only if the decomposition of $V$ with respect to $\varphi$ is ``compatible''{} with the one with respect to $\psi$.
\label{lem:invol_compat}
\end{lemma}

\begin{proof}
\subdem{Direct sense}
Let us first see that $\varphi$ leaves the decomposition $V=V_{\psi^+}\oplus V_{\psi^-}$ invariant. If $x=x_{\psi^+}+x_{\psi^-}$,
\[
   \varphi(x_{\psi^+})=(\varphi\circ\psi)(x_{\psi^+})=(\psi\circ\varphi)(x_{\psi^+}).
\]
Then $\varphi(x_{\psi^+})\in V_{\psi^+}$, and the matrix of $\varphi$ is block-diagonal with respect to the decomposition given by $\psi$. Thus $V_{\psi^+}$ and $V_{\psi^-}$ split separately into two parts with respect to $\varphi$.

\subdem{Inverse sense} 
If $x\in V$, we can write $x=x_{++}+x_{+-}+x_{-+}+x_{--}$ where the first index refers to $\psi$ while the second one refers to $\psi$; for example, $x_{+-}\in V_{\psi^+}\cap V_{\varphi^-}$. The following computation is easy:
\begin{equation}
\begin{split}
(\varphi\circ\psi)(x)&=\varphi(x_{++}+x_{+-}-x_{-+}-x_{--})\\
                 &=x_{++}-x_{+-}-x_{-+}+x_{--}\\
         &=\psi(x_{++}-x_{+-}-x_{-+}-x_{--})\\
         &=(\psi\circ\varphi)(x).
\end{split}
\end{equation}
\end{proof}

\begin{theorem}
Let $\lF$ be a real semisimple Lie algebra, $\theta$ a Cartan involution on $\lF$ and $\sigma$, another involution (not specially Cartan). Then there exists a $\varphi\in\Int\lF$ such that $[\varphi\theta\varphi^{-1},\sigma]=0$ 
\label{tho:sigma_theta_un}
\end{theorem}

\begin{proof}
If $\theta$ is a Cartan involution, then $B_{\theta}$ is a scalar product on $\lF$. Let $\omega=\sigma\theta$. By using $\sigma^2=\theta^2=1$, $\theta=\theta^{-1}$ and the invariance property \ref{prop:auto_2} of the Killing form,
\begin{equation}
B(\omega X,\theta Y)=B(X,\omega^{-1}\theta Y)
                    =B(X,\theta\sigma\theta Y)
            =B(X,\theta\omega Y).
\end{equation}
Then $B_{\theta}(\omega X,Y)=B_{\theta}(X,\omega Y)$. This is a general property of scalar product that in this case, the matrix of $\omega$ is symmetric while the one of $\omega^2$ is positive definite. If we consider the classical scalar product whose matrix is $(\delta_{ij})$, the property is written as $A_{ij}v_jw_j=v_iA_{ij}w_j$ (with sum over $i$ and $j$); this implies the symmetry of $A$. To see that $A^2$ is positive definite, we compute (using the symmetry):
\[
   A_{ij}A_{jk}v_iv_k=v_iA_{ij}v_kA_{kj}=\sum_j(v_iA_{ij})^2>0.
\]
The next step is to see that there is an unique linear transformation $\dpt{A}{\lF}{\lF}$ such that $\omega^2=e^A$, and that for any $t\in\eR$, the transformation $e^{tA}$ is an automorphism of $\lF$.

We choose an orthonormal (with respect to the inner product $B_{\theta}$) basis $\{X_1,\ldots,X_n\}$  of $\lF$ in which $\omega$ is diagonal. In this basis, $\omega^2$ is also diagonal and has positive real numbers on the diagonal; then the existence and unicity of $A$ is clear. Now we take some notations:
\begin{subequations}
\begin{align}
  \omega(X_i)&=\lambda_iX_i\\
  \omega^2(X_i)&=e^{a_i}X_i,
\end{align}  
\end{subequations}
(no sum at all) where the $a_i$ are the diagonals elements of $A$. The structure constants are as usual defined by
\begin{equation}
   [X_i,X_j]=c_{ij}^kX_k.  
\end{equation}
Since $\sigma$ and $\theta$ are automorphisms, $\omega^2$ is also one. Then 
\[
\omega^2[X_i,X_j]=c_{ij}^k\omega^2(X_k)=c_{ij}^ke^{a_k}X_k
\]
can also be computed as
\[
   \omega^2[X_i,X_j]=[\omega^2X_i,\omega^2X_j]=e^{a_i}e^{a_j}c_{ij}^kX_k,
\]
so that $c_{ij}^ke^{a_k}=c_{ij}^ke^{a_i}e^{a_j}$, and then $\forall t\in\eR$,
\[
   c_{ij}^ke^{ta_k}=c_{ij}^ke^{ta_i}e^{ta_j},
\]
which proves that $e^{tA}$ is an automorphism of $\lF$. By lemma \ref{lem:autom_derr}, $A$ is thus a derivation of $\lF$. The semi-simplicity makes $\partial\lF=\ad\lF$, then $A\in\ad\lF$ and $e^{tA}\in\Int\lF$ because it clearly belongs to the identity component of $\Aut\lF$.

Now we can finish de proof by some computations. Remark that $\omega=e^{A/2}$ and $[e^{tA},\omega]=0$ because it can be seen as a common matrix commutator. Since $\omega^{-1}=\theta\sigma$, we have $\theta\omega^{-1}\theta=\sigma\theta$, or $\theta\omega^2\theta=\omega^2$ and
\begin{equation}\label{eq:eAth}
   e^{A}\theta=\theta e^{-A}.
\end{equation}
From this, one can deduce that $e^{tA}\theta=\theta e^{-tA}$. Indeed, as matricial identity, equation \eqref{eq:eAth} reads
\[
    (e^{A}\theta)_{ik}=(e^{A})_{ij}\theta_{jk}
                      =e^{a_i}\theta_{ik}
              =e^{-a_k}\theta_{ik}.
\]
Then for any $ik$ such that $\theta_{ik}\neq 0$, we find $e^{a_i}=e^{-a_k}$ and then also $e^{ta_i}=e^{-ta_k}$. Thus $(e^{tA}\theta)_{ik}=(e^{tA})_ij\theta_{jk}=e^{ta_i}\theta_{ik}=\theta_{ik}e^{-ta_k}=(\theta e^{-tA})_{ik}$. So we have
\begin{equation}
  e^{tA}\theta=\theta e^{-tA}.
\end{equation}
Now we consider $\varphi=e^{A/4}\in\Int\lF$ and $\theta_1=\varphi\theta\varphi^{-1}$. We find $\theta_1\sigma=e^{A/2}\omega^{-1}$ and $\sigma\theta^{-1}=e^{-A/2}\omega$. Since $\omega^2=A$, we have $e^{A/2}=e^{-A/2}\omega^2$ and thus $\theta_1\sigma=\sigma\theta_1$.

\end{proof}

\begin{corollary}
Any real Lie algebra has a Cartan involution.
\end{corollary}

\begin{proof}
Let $\lF$ be a real Lie algebra and $\lG$ be his complexification: $\lG=\lF\heC$. Let $\lU_0$ be a compact real form of $\lG$ and $\tau$ the induced involution (the conjugation) on $\lG$. By the proposition \ref{prop:conj_invol}, we know that $\tau$ is  a Cartan involution of $\lG\heR$. We also consider $\sigma$, the involution of $\lG$ with respect to the real form $\lF$. It is in particular an involution on the real Lie algebra $\lF$. Then one can find a $\varphi\in\Int\lG\heR$ such that $[\varphi\tau\varphi^{-1},\sigma]=0$ on $\lG\heR$. Let $\lU_1=\varphi\lU_0$ and $X\in\lU_1$. We can write $X=\varphi Y$ for a certain $Y\in\lU_0$. Then
\[
   \varphi\tau\varphi^{-1} X=\varphi\tau Y=\varphi Y=X,
\]
so that $\varphi\tau\varphi^{-1}=id|_{\lU_1}$. Note that $\lU_1$ is also a real compact form of $\lG$ because the Killing form is not affected by $\varphi$. Let $\tau_1$ be the involution of $\lG$ induced by $\lU_1$. We have
\[
   \tau_1|_{\lU_1}=\varphi\tau\varphi^{-1}_{\lU_1}=\id|_{\lU_1}.
\]
Since $\varphi$ is $\eC$-linear, we have in fact $\tau_1=\varphi\tau\varphi^{-1}$. Now we forget $\lU_0$ and we consider the compact real form $\lU_1$ with his involution $\tau_1$ of $\lG$ which satisfy $[\tau_1,\sigma]=0$ on $\lG\heR$ This relation holds also on $i\lG\heR$, then 
\[
   [\tau_1,\sigma]=0
\]
on $\lG=\lF\heC$. Let $X\in\lF$, i.e. $\sigma X=X$; it automatically fulfils 
\[
  \sigma\tau_1 X=\tau_1\sigma X=\tau_1 X,
\]
so that $\tau_1$ restrains to an involution on $\lF$ (because $\tau_1\lF\subset\lF$). Let $\theta=\tau_1|_{\lF}$. For $X$, $Y\in\lF$, we have
\begin{equation}
B_{\theta}(X,Y)=-B_{\lF}(X, \theta Y)
             =-B_{\lF}(X,\tau Y)
         =\frac{1}{2}(B_{\lG\heR})_{\tau_1}(X,Y),
\end{equation}
which shows that $\theta$ is a Cartan involution. The half factor on the last line comes from the fact that $\lG\heR=(\lF\heC)\heR=\lF\oplus i\lF$.

\end{proof}

\begin{corollary}
Any two Cartan involutions of a real semisimple Lie algebra are conjugate by an inner automorphism. \index{inner!automorphism}
\label{cor:Cartan_conj_inner}
\end{corollary}

\begin{proof}
Let $\sigma$ and $\sigma'$ be two Cartan involutions of $\lF$. We can find a $\varphi\in\inf\lF$ such that $[\varphi\sigma\varphi^{-1},\sigma']=0$. Thus it is sufficient to prove that any two Cartan involutions which commute are equals. So let us consider $\theta$ and $\theta'$, two Cartan involutions such that $[\theta,\theta']=0$. By lemma \ref{lem:invol_compat}, we know that the decompositions into $+1$ and $-1$  eigenspaces with respect to $\theta$ and $\theta'$ are compatibles. If we consider $X\in\lF$ such that $\theta X=X$ and $\theta' X=-1$ (it is always possible if $\theta\neq\theta'$), we have
\[
\begin{split}
  0<B_{\theta}(X,X)=-B(X,\theta X)=-B(X,X)\\
  0<B_{\theta'}(X,X)=-B(X,\theta' X)=B(X,X)
\end{split}
\]
which is impossible.
\end{proof}

\begin{corollary}
Any two real compact form of a complex semisimple Lie algebra are conjugate by an inner automorphism.
\end{corollary}

\begin{proof}
We know that any real form of $\lG$ induces an involution (the conjugation) and that if the real form is compact, the involution is Cartan on $\lG\heR$. Let $\lU_0$ and $\lU_1$ be two compact real forms of $\lG$ and $\tau_0$, $\tau_1$ the associated involutions of $\lG$ (which are Cartan involutions of $\lG\heR$). For a suitable $\varphi\in\Int\lG\heR$, 
\[
   \tau_0=\varphi\tau_1\varphi^{-1}.
\]
The fact that $\Int\lG=\Int\lG\heR$ (lemma \ref{lem:Int_g_gR}) finish the proof.

\end{proof}
\subsection{Cartan decomposition}
%-------------------------------

Examples of Cartan and Iwasawa decomposition are given in sections \ref{SecToolSL}, \ref{SecIwaSOunn},\ref{subsecIwasawa_un}, \ref{SecSympleGp} and \ref{SecIwasldeuxC}. An example of how it works to prove isomorphism of Lie algebras is provided in subsection \ref{sssIsomsoslplussl}.

Let $\lF$ be a real semisimple Lie algebra. A vector space decomposition $\lF=\lK\oplus\lP$ is a \defe{Cartan decomposition}{Cartan!decomposition} if the Killing form is negative definite on $\lK$ and positive definite on $\lP$ and the following commutators hold:
\begin{equation}\label{eq:comm_Cartan}
   [\lK,\lK]\subseteq\lK,\quad[\lK,\lP]\subseteq\lP,\quad[\lP,\lP]\subseteq\lK.
\end{equation}
If $X\in\lK$ and $Y\in\lP$, we have $(\ad X\circ\ad Y)\lK\subseteq\lP$ and $(\ad X\circ\ad Y)\lP\subseteq\lK$, therefore $B_{\lF}(X,Y)=0$.

Let $\dpt{\theta}{\lF}{\lF}$ be a Cartan involution, $\lK$ its $+1$ eigenspace and $\lP$ his $-1$ one. It is easy to see that the relations \eqref{eq:comm_Cartan} are satisfied for the decomposition  $\lF=\lK\oplus\lP$. For example, for $X,X'\in\lK$, using the fact that $\theta$ is an automorphism, 
\[
   [X,X']=[\theta X,\theta X']=\theta[X,X'],
\]
which proves that $[\lK,\lK]\subseteq\lK$. Since $\theta$ is a Cartan involution, $B_{\theta}$ is positive definite. For $X\in\lK$,
\[
  B(X,X)=B(X,\theta X)=-B_{\theta}(X,X)
\]
proves that $B$ is negative definite on $\lK$; in the same way we find that $B$ is also positive definite on $\lP$. Then the Cartan involution gives rise to a Cartan decomposition. We are going to prove that any Cartan decomposition defines a Cartan involution.

Let us now do the converse. Let $\lF=\lK\oplus\lP$ be a Cartan decomposition of the real semisimple Lie algebra $\lF$. We define $\theta=\id|_{\lK}\oplus(-\id)|_{\lP}$. If $X,X'\in\lK$, the definition of a Cartan algebra makes $[X,X']\in\lK$ and so
\[
  \theta[X,X']=[X,X']=[\theta X,\theta X'],
\]
and so on, we prove that $\theta$ is an automorphism of $\mF$. It remains to prove that $B_{\theta}$ is positive definite. If $X\in\lK$,
\[
   B_{\theta}(X,X)=-B(X,\theta X)=-B(X,X).
\]
Then $B_{\theta}$ is positive definite on $\lK$ because on this space, $B$ is negative definite by definition of a Cartan involution. The same trick shows that $B_{\theta}$ is also positive definite on $\lP$. We had seen that $\lP$ and $\lK$ where $B_{\theta}$-orthogonal spaces. Thus $B_{\theta}$ is positive definite and $\theta$ is a Cartan involution.

Let $\lF=\lK\oplus\lP$ be a Cartan decomposition. Then it is quite easy to see that $\lK\oplus i\lP$ is a compact real form of $\lG=(\lFeC)$.

\begin{proposition}
Let $\lL$ and $\lQ$ be the $+1$ and $-1$ eigenspaces of an involution $\sigma$. Then $\sigma$ is a Cartan involution if and only if $\lL\oplus i\lQ$ is  a compact real form of $\lFeC$.
\end{proposition}

\begin{proof}
First remark that $\lL\oplus i\lQ$ is always a real form of $\lFeC$. The direct sense is yet done. Then we suppose that $B_{\lFeC}$ is negative definite on $\lL\oplus i\lQ$ and we have to show that $\lL\oplus\lQ$ is a Cartan decomposition of $\lF$. The condition about the brackets on $\lL$ and $\lQ$ is clear from their definitions. If $X\in\lL$, $B(X,X)<0$ because $B$ is negative definite on $\lL$. If $Y\in\lQ$, $B(Y,Y)=-B(iY,iY)>0$ because $B$ is negative definite on $i\lQ$.
\end{proof}

\section{Root spaces in the real case}
%----------------------------------------

Let $\lF$ be a real semisimple Lie algebra with a Cartan involution $\theta$ and the corresponding Cartan decomposition $\lF=\lK\oplus\lP$. We consider $B$, a ``Killing like''{} form, i.e. $B$ is a symmetric nondegenerate invariant bilinear form on $\lF$ such that $B(X,Y)=B(\theta X,\theta Y)$ and $B_{\theta}:=-B(X,\theta X)$ is positive definite. Then $B$ is negative definite on the compact real form $\lK\oplus i\lP$. Indeed if $Y\in\lP$,
\begin{equation}
  B(iY,iY)=-B(\theta Y,\theta Y)
          =B(Y,\theta Y)
      =-B_{\theta}(Y,Y)<0.
\end{equation}
The case with $X\in\lK$ is similar. It is easy to see that $B_{\theta}$ is in fact a scalar product on $\lF$, so that we can define the orthogonality and the adjoint from $B_{\theta}$. If $\dpt{A}{\lF}{\lF}$ is an operator on $\lF$, his adjoint is the operator $A^*$ given by the formula
\[
   B_{\theta}(A X,Y)=B_{\theta}(X,A^*Y)
\]
for all $X$, $Y\in\lF$.

\begin{proposition}
With this definition, when $X\in\lF$, the adjoint operator of $\ad X$ is given by means of the Cartan involution:
\[ 
(\ad X)^*= \ad(\theta X). 
\]
\end{proposition}

\begin{proof}
This is a simple computation
\begin{equation}
B_{\theta}\big(  (\ad\theta X)Y,Z \big)=-B\big(  Y,[\theta X,\theta Y]  \big)
                                     =-B_{\theta}(Y,[X,Z])
                     =-B_{\theta}\big( (\ad X)^*Y,Z \big).
\end{equation}
\end{proof}

Let $\lA$ be a maximal abelian subalgebra of $\lP$ (the existence comes from the finiteness of the dimensions). If $H\in\lA$, the operator $\ad H$ is self-adjoint because
\begin{equation}
(\ad H)^*X=(-\ad\theta H)X
          =[H,X]
      =(\ad H)X,
\end{equation}
where we used the fact that $H\in\lP$.  For $\lambda\in\lA^*$, we define the space
\begin{equation}
  \lF_{\lambda}=\{ X\in\lF\tq\forall H\in\lA,\, (\ad H)X=\lambda(H)X\}.
\end{equation}
If $\lF_{\lambda}\neq 0$ and $\lambda\neq 0$, we say that $\lambda$ is a \defe{restricted root}{restricted root (real case)}\index{root!restricted (real case)} of $\lF$. We denote by $\Sigma$ the set of restricted roots of $\lF$. We may sometimes write $\Sigma_{\lF}$ if the Lie algebra is ambiguous.

The main properties of the real root spaces are given in the following proposition.

\begin{proposition}     \label{PropPropRacincesReelles}
The set $\Sigma$ of the restricted roots of a real semisimple Lie algebra $\lF$ has the following properties:
\label{prop:enuc}
\begin{enumerate}
\item\label{enuci} $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$,
\item\label{enucii} $[\lF_{\lambda},\lF_{\mu}]\subseteq\lF_{\lambda+\mu}$,
\item\label{enuciii} $\theta\lF_{\lambda}=\lF_{-\lambda}$,
\item\label{enuciv} $\lambda\in\Sigma$ implies $-\lambda\in\Sigma$,
\item\label{enucv} $\lF_0=\lA\oplus\lM$ where $\lM=\mZ_{\lK}(\lA)$ and $\lA\perp\lM$.
\end{enumerate}
\end{proposition}

\begin{proof}
Proof of \ref{enuci}. The operators $\ad H$ with $H\in\lA$ form an abelian algebra of self-adjoint operators, then they are simultaneously diagonalisable. Let $\{X_i\}$ be a basis which realize this diagonalisation, and $\lF_i=\Span X_i$, so that $\lF=\oplus_i\lF_i$. We have $(\ad H)\lF_i=\lF_i$ and then $(\ad H)X_i=\lambda_i(H)X_i$ for a certain $\lambda_i\in\lA^*$. This shows that $\lF_i\subseteq\lF_{\lambda_i}$.\quext{pourquoi ça n'implique pas que $\dim\lF_{\lambda_i}=1$ ? Réponse par Philippe : tu as oublié les valeurs propres nulles  dans ta base ce qui doit entrainer quelques modifs dans ton texte(par  ex.  $adH f_i = f_i$ pas toujours ) }

Proof of \ref{enucii}. Let $H\in\lA$, $X\in\lF_{\lambda}$ and $Y\in\lF_{\mu}$. We have
\begin{equation}
   (\ad H)[X,Y]=[[H,X],Y]+[X,[H,Y]]
               =\big(  \lambda(H)+\mu(H) \big) [X,Y].
\end{equation}

Proof of \ref{enuciii}. Using the fact that $\theta H=-H$ because $H\in\lP$,
\begin{equation}
  (\ad H)\theta X=\theta[\theta H,X]
                 =-\theta\lambda(H)X
         =-\lambda(H)(\theta X).
\end{equation}

Proof of \ref{enuciv}. It is a consequence of \ref{enuciii} because if $\lF_{\lambda}\neq 0$, then $\theta\lF_{_{\lambda}}\neq 0$.

Proof of \ref{enucv}. By \ref{enuciii}, $\theta\lF_0=\lF_0$, then $\lF_0=(\lK\cap\lF_0)\oplus(\lP\cap\lF_0)$. If $X\in\lF_0$, then it commutes with all the elements of $\lA$ and by the maximality property of $\lA$, provided that $X\in\lP$, it also must belongs to $\lA$. This fact makes $\lA=\lP\cap\lF_0$. Now,
\[
  \lM=\mZ_{\lK}(\lA)=\{X\in\lK\tq [X,\lA]=0\}=\lK\cap\lF_0.
\]
All this gives $\lF_0=\mZ_{\lK}(\lA)\oplus\lA$.
\end{proof}

We choose a positivity notion on $\lA^*$, we consider $\Sigma^+$, the set of restricted positive roots and we define\nomenclature{$\lN$}{Restricted roots}
\[
  \lN=\bigoplus_{\lambda\in\Sigma^+}\lF_{\lambda}.
\]

From finiteness of the dimension, there are only a finitely many forms $\lambda\in\lA^*$ such that $\lF_{\lambda}\neq 0$. Then, taking, more and more commutators in $\lN$, the formula $[\lF_{\lambda},\lF_{\mu}]\subseteq\lF_{\lambda+\mu}$ shows that the result finish to fall into a $\lF_{\mu}=0$. On the other hand, since $\lA\subset\lF_0$, we have $[\lA,\lN]=\lN$. If $a_1,a_2\in\lA$ and $n_1,n_2\in\lN$,
\begin{equation}
   [a_1+n_1,a_2+n_2]=\underbrace{[a_1,a_2]}_{=0}+\underbrace{[a_1,n_2]}_{\in\lN}
                      \quad+\underbrace{[n_1,a_2]}_{\in\lN}+\underbrace{[n_1,n_2]}_{\in\lN},
\end{equation}
then $[\lA\oplus\lN,\lA\oplus\lN]=\lN$. This proves the three following important properties:

\begin{enumerate}
\item $\lN$ is nilpotent.
\item $\lA$ is abelian.
\item $\lA\oplus\lN$ is a solvable Lie subalgebra of $\lF$.
\end{enumerate}

\subsection{Iwasawa decomposition}
%----------------------------------

\begin{theorem}
Let $\lF$ be a real semisimple Lie algebra and $\lK$, $\lA$, $\lN$ as before. Then we have the following direct sum:
\begin{equation}
   \lF=\lK\oplus\lA\oplus\lN.
\end{equation}
\end{theorem}

This is the \defe{Iwasawa decomposition}{Iwasawa!decomposition}\index{decomposition!Iwasawa} for the real semisimple Lie algebra $\lF$.

\begin{proof}
We yet know the direct sum $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$. Roughly speaking, in $\lN$ we have only vectors of $\Sigma^+$, in $\theta\lN$, only of $\Sigma^-$ and in $\lA$, only in ``zero''. Then the sum $\lA\oplus\lN\oplus\theta\lN$ is direct.

Now we prove that the sum $\lK+\lA+\lN$ is also direct. It is clear that $\lA\cap\lN=0$ because $\lA\subseteq\lF_0$. Let $X\in\lK\cap(\lA\oplus\lN)$. Then $\theta X=X$. But $\theta X\in\lA\oplus\theta\lN$. Thus $X\in\lA\oplus\lN\cap\lA\oplus\lN$ which implies $X\in\lA$. All this makes $X\in\lP\oplus\lK$ and $X=0$.

Now we prove that $\lK\oplus\lA\oplus\lN=\lF$. An arbitrary $X\in\lF$ can be written as 
\[
   X=H+X_0+\sum_{\lambda\in\Sigma}X_{\lambda}
\]
where $H\in\lA$, $X_0\in\lM$ and $X_{\lambda}\in\lF_{\lambda}$. Now there are just some manipulations\ldots
\begin{equation}
  \sum_{\lambda\in\Sigma}X_{\lambda}=\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+X_{\lambda})
                                  =\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+\theta X_{-\lambda})
                  +\sum_{\lambda\in\Sigma^+}(X_{\lambda}+\theta X_{-\lambda}),
\end{equation}
but $\theta(X_{-\lambda}+\theta X_{-\lambda})=X_{-\lambda}+\theta X_{-\lambda}$, then $X_{-\lambda}+X_{-\lambda}\in\lK$. Moreover, $X_{\lambda}, \theta X_{-\lambda}\in\lF_{\lambda}$, then $X_{\lambda}-\theta X_{-\lambda}\in\lF_{\lambda}\subseteq\lN$. Then
\begin{equation}
  X=X_0+\sum_{\lambda\in\Sigma^+}(X_{-\lambda}+\theta X_{-\lambda})+H+\sum_{\lambda\in\Sigma^+}(X_{\lambda}-\theta X_{-\lambda})
\end{equation}
where the two first term belong to $\lK$, $H\in\lA$ and the last term belongs to $\lN$.
\end{proof}

\begin{lemma}
There exists a basis $\{X_i\}$ of $\lF$ in which 

\begin{enumerate}
\item\label{enudi} The matrices of $\ad\lK$ are symmetric,
\item\label{enudii} The matrices of $\ad\lA$ are diagonal and real,
\item\label{enudiii} The matrices of $\ad\lN$ are upper triangular with zeros on the diagonal.
\end{enumerate}
\end{lemma}

\begin{proof}
We have the orthogonal decomposition $\lF=\lF_0\bigoplus_{\lambda\in\Sigma}\lF_{\lambda}$ given by proposition \ref{prop:enuc}. Let $\{X_i\}$ be an orthogonal basis of $\lF$ compatible with this decomposition and in such an order that $i<j$ implies $\lambda_i\geq\lambda_j$. From the orthogonality of the basis it follows that the matrix of $B_{\theta}$ is diagonal. Thus the adjoint is the transposition.

\ref{enudi} If $X\in\lK$, $(\ad X)^t=(\ad X)^*=-\ad\theta X=-\ad X$.

\ref{enudii} Each $X_i$ is a restricted root; then $(\ad H)X_i=\lambda_i(H)X_i$, then the diagonal of $\ad H$ is made of $\lambda_i(H)$ whose are real.

\ref{enudiii} If $Y_i\in\lF_{\lambda_i}$ with $\lambda_i\in\Sigma^+$, $(\ad Y_i)X_j$ has only components in $\lF_{\lambda_i+\lambda_j}$ with $\lambda_i+\lambda_j>\lambda_j$ because $\lambda_i\in\Sigma^+$.
\end{proof}


\begin{lemma}
Let $\lH$ be a subalgebra of the real semisimple Lie algebra $\lF$. Then $\lH$ is a Cartan subalgebra if and only if $\lHeC$ is Cartan in $\lFeC$.
\end{lemma}

\begin{proof}
\subdem{Direct sense} If $\lH$ is nilpotent in $\lF$, it is cleat that $\lHeC$ is nilpotent in $\lFeC$. We have to prove that $[x,\lHeC]\subseteq\lHeC$ implies $x\in\lHeC$. As set, $\lFeC=\mF\oplus i\lF$  (but not as vector space !), then we can write $x=a+ib$ with $a$, $b\in\lF$. The assumption makes that for any $h\in\lH$, there exists $h',h''\in\lH$ such that 
\[
   [a+ib,h]=h+ih''.
\]
This equation can be decomposed in $\lF$-part and $i\lF$-part: for any $h\in\lH$, there exists a $h'\in\lH$ such that $[a,h]=h'$,  and for any $h\in\lH$, there exists a $h''\in\lH$ such that $[b,h]=h''$. Thus $a$, $b\in\lH$ because $\lH$ is Cartan in $\lF$.

\subdem{Inverse sense} The assumption is that $[x,\lHeC]\subset\lHeC$ implies $x\in\lHeC$. In particular consider a $x\in\lH$ such that $[x,\lH]\subset\lH$. Then $x\in\lHeC$ because $[x,\lHeC]\subset\lHeC$. But $\lHeC\cap\lF=\lH$.
\end{proof}

In the complex case, the Cartan subalgebras all have same dimensions because they are maximal abelian.


\section{Iwasawa decomposition of Lie groups}%\label{sec:Iwasawa}
%+++++++++++++++++++++++++++++++++++++++++++++++

In this section, we show the main steps of the Iwasawa decomposition for a semisimple Lie group. For proofs, the reader will see \cite{Knapp} VI.4 and \cite{Helgason} III,\S\ 3,4 and VI,\S\ 3. In the whole section, $G$ denotes a semisimple group, and $\lG$ its real (finite dimensional) Lie algebra. The two main examples that are widely used are $\SL(2,\eR)$ and $\SO(2,n)$.

\subsection{Cartan decomposition}
%-------------------------------

If $\lG$ is a finite dimensional Lie algebra and $X$, $Y\in\lG$, the composition of the adjoint $\dpt{\ad X\circ \ad Y}{\lG}{\lG}$ makes sense.
\begin{definition}
An involutive automorphism $\theta$ on a \emph{real} semi simple Lie algebra $\lG $ for which the form $B_{\theta}$,
\begin{equation}
          B_{\theta}(X,Y):=-B(X,\theta Y)
\end{equation}
($B$ is the Killing form on $\lG$) is positive definite is a \defe{Cartan involution}{Cartan!involution}.
\ifthenelse{\value{siTHZ}=1}{}{\index{involution!Cartan}}
\end{definition}

\begin{proposition}
There exists a Cartan involution for every real semisimple Lie algebra.
\end{proposition}

\begin{probleme}
The theorem 4.1 in \cite{Helgason} is maybe a proof of this proposition.
\end{probleme}

See \cite{Helgason}, theorem 4.1.  Since $\theta^2=id$, the eigenvalues of a Cartan involution are $\pm 1$, and we can define the \defe{Cartan decomposition}{Cartan!decomposition}\index{decomposition!Cartan} $\lG$
\begin{equation}
                    \lG=\lK\oplus\lP
\end{equation}
into $\pm1$-eigenspaces of $\theta$ in such a way that $\theta=(-\id)|_{\lP}\oplus \id|_{\lK}$. These eigenspaces are subject to the following commutation relations:
\begin{equation} \label{Ieq:comm_KP}
[\lK,\lK]\subseteq\lK,\quad[\lK,\lP]\subseteq\lP,\quad [\lP,\lP]\subseteq\lK.
\end{equation}
%
The dimension of a maximal abelian subalgebra of $\lP$ is the \defe{rank}{rank of a Lie algebra} of $\lG$. One can prove that it does not depend on the choices (Cartan involution and maximal abelian subalgebra). We denote by $\lA$ such a maximal abelian subalgebras of $\lP$.


\begin{lemma}
If $\lG_0$ is a real semisimple Lie algebra and $\theta$ a Cartan involution, then for all $X\in\lG_0$,
\begin{equation}
                (\ad X)^*=-\ad(\theta X),
\end{equation}
where the star on an operator on $\lG$ is defined by
\begin{equation}
                  B_{\theta}(X,AY)=B_{\theta}(A^*X,Y).
\end{equation}
\end{lemma}

\begin{lemma}
The set of operators $\ad(\lA)$ is an abelian algebra whose elements are self-adjoint.
\end{lemma}

\begin{proof}
We have to prove that $(\ad H)^*=(\ad H)$ and $[\ad H,\ad I]=0$ for every $H$, $I\in \lA$.  First, note that $H\in\lA\subset\lP$, thus $\theta H=-H$, and $(\ad H)^*=-\ad(\theta H)=\ad H$.

For the second, $\ad H\circ \ad I=\ad(H\circ I)$ so that $[\ad H,\ad I]=\ad[H,I]=0$ because $\lA$ is abelian.
\end{proof}

\subsection{Root space decomposition}
%--------------------------------

From the lemma, the operators $\ad(H)$ with $H\in\lA$ are simultaneously diagonalisable. That means that there exists a basis $\{ X_i \}$ of $\lG$ and linear maps $\lambda_i\colon \lA\to \eR$ such that 
\[
    \ad(H)X_i=\lambda_i(H)X_i.
\]
 For each $\lambda\in\lA^*$, we define
\begin{equation}
        \lG_{\lambda}=\{X\in\lG|(\ad H)X=\lambda(H)X,\forall H\in\lA\}.
\end{equation}
Elements $0\neq\lambda\in\lA^*$ such that $\lG_{\lambda}\neq 0$ are called \defe{restricted roots}{root!restricted} of $\lG$. The set of restricted roots is denoted by $\Sigma$.

\begin{proposition}     \label{prop:somme_de_G}
 The restricted root together with $\lA$ itself span the whole space:
\begin{equation}    \label{eq:somme_de_G}
             \lG=\lG_0 \oplus_{\lambda\in\Sigma}\lG_{\lambda},
\end{equation}
This decomposition is called the \defe{restricted root space decomposition}{root!space!decomposition}\index{decomposition!root space}. 
\end{proposition}

\begin{proof}
We first prove that the sum is direct. If the sum is not so, we can find a $H^*\in\lG_0$ and $X_i\in\lG_{\lambda_i}$ ($\lambda_i\in\Sigma$) such that
\begin{equation}\label{eq:2906r2}
              H^*+\sum_iX_i=0
\end{equation}
Let us consider
\[
      N=\{H\in\lG_0|\textrm{ the $\lambda_i(H)$ are all differents and not zero}\}
\]
A $H$ which is not in $N$ fulfils some relations as $\lambda_i(H)=\lambda_j(H)$ which are linear equations, so the complement of $N$ is an union of hyperplanes and thus $N$ is not empty. This allows us to consider a $H\in N$.

We have choice the $X_i$ in $\lG_{\lambda_i}$, \emph{i.e.}
\begin{equation} \label{eq:2906r1}
(\ad A)X_i=\lambda_i(A)X_i
\end{equation}
 for all $A\in\lA$. In other words, $X_i$ diagonalise $\ad A$ with eigenvalues $\lambda_i(A)$. Now, let us consider $\ad H$ for a $H\in N$. Since all the $\lambda_i(H)$ are different and not zero, the equation \eqref{eq:2906r1} implies that all the $X_i$ (and $H^*$) are in separate eigenspaces of $\ad H$. Thus they are linearly independent, hence the equation \eqref{eq:2906r2} is not possible. The sum \eqref{eq:somme_de_G} is thus a direct sum. For the rest of
  the proof, see~\cite{Helgason} theorem 4.2.
\end{proof}

Other properties of the root spaces are listed in the following proposition.
\begin{proposition}
The spaces $\lG_{\lambda_i}$ satisfy also:
\begin{enumerate}
\item $[\lG_{\lambda},\lG_{\mu}]\subseteq\lG_{\lambda+\mu}$,
\item $\theta\lG_{\lambda}=\lG_{-\lambda}$; in particular, if $\lambda$ belongs to $\Sigma$, then $-\lambda$ belongs to $\Sigma$ too,
\item $\lG_0=\lA\oplus\mZ_{\lK}(\lA)$ orthogonally.
\end{enumerate}
\end{proposition}

\begin{probleme}
Il faut définir quelque part ce qu'est cet espace $\mZ_{\lK}(\lA)$
\end{probleme}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Positivity, convex cone and partial ordering}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubsecPosiCconePartOrder}

\begin{definition}
    Let $V$ be a vector space. A \defe{positivity notion}{positive!element!in a vector space} (see \cite{Knapp}, page~154) is the data of a subset $V^+$ of $V$ such that
    %\label{def_posi}
    \begin{enumerate}
        \item
            for every nonzero $v\in V$, we have $v\in V^+$ \emph{xor} $-v\in V^+$,
        \item 
            for every $v$, $w\in V^+$ and every $\mu\in\eR^+$, the elements $v+w$ and $\mu v$ are positive.
    \end{enumerate}
\end{definition}

If $v\in V^+$, we say that $v$ is \defe{positive}{positive!vector} and we note $v>0$. 

\begin{definition}      \label{DefConvexCone}
    A \defe{convex cone}{convex!cone} in a vector space \(A\) is a subset $C$ such that
    \begin{enumerate}
        \item
            $x\in C$ and $t\in\eR^+$ imply $tx\in C$, \label{enuli}
        \item
            $x,y\in C$ implies $x+y\in C$,\label{enulii}
        \item
            $C\cap(-C)=\{ 0 \}$.\label{enuliii}
    \end{enumerate}
\end{definition}

To a convex cone $C$ is attached a notion of positivity by defining $x\geq 0$ if and only if $x\in C$. The converse is also true: if we have a notion of positivity on $V$, we define the corresponding convex cone by\label{PgConeAndPositive}
\begin{equation}
    V^+=\{ x\in V\tq x\geq 0 \}.
\end{equation}

A \defe{linear partial ordering relation}{ordering!linear partial relation} is a partial ordering $\leq$ such that
\begin{itemize}
    \item $A\leq B$ implies $A+C\leq B+C$ for all $C$,
    \item $\lambda A\leq\lambda B$ for all $\lambda\in\eR^+$.
\end{itemize}
From a positivity notion gives rise to a linear partial ordering on \(V\) by defining \(x\geq y\) if and only if \(y-x\geq 0\).

\subsection{Iwasawa decomposition}
%--------------------------------

Let us consider a notion of positivity on $\lA^*$ and denote by $\Sigma^+$ the set of positive roots. We define
\begin{equation}
      \lN:=\oplus_{\lambda\in\Sigma^+}\lG_{\lambda}.
\end{equation}
The \defe{Iwasawa decomposition}{Iwasawa!decomposition}\index{decomposition!Iwasawa} is given by the following theorem (\cite{Knapp}, theorem 5.12):

\begin{theorem}
Let $G$ be a linear connected semisimple group and $A=\exp\lA$, $N=\exp\lN$ where $\lA$ and $\lN$ are the previously defined algebras. Then $A$, $N$ and $AN$ are simply connected subgroups of $G$ and the multiplication map
\begin{equation}
\begin{aligned}
  \phi\colon A\times N\times K&\to G \\ 
 (a,n,k)&\mapsto ank 
\end{aligned}
\end{equation}
is a global analytic diffeomorphism. In particular, the Lie algebra $\lG$ decomposes as vector space direct sum
\begin{equation}
            \lG=\lA\oplus\lN\oplus\lK.
\end{equation}
 The group $AN$ is a solvable subgroup of $G$ which is called the \defe{Iwasawa group}{Iwasawa!group}, or Iwasawa component of $G$.
\label{ThoIwasawaVrai}
\end{theorem}

\begin{remark}
It can be proved that this theorem is independent of the choices: the Cartan involution, the maximal abelian subalgebra $\lA$ and the notion of positivity.
\end{remark} 
Notice that $A$, $N$ and $K$ are unique up to isomorphism. Their matricial representation of course depend on choices.

This theorem from \cite{Helgason}, chapter VI, Theorem 3.4. will be useful.

\begin{theorem}
The Lie algebra $\lA\oplus\lK$ is solvable.
\end{theorem}
This theorem implies that the group $AN$ is solvable.\quext{J'esère que ce que je raconte ici n'est pas trop débile pcq j'ai pas été fouiller à fond.} Before to go into concrete situations, let us prove an useful property of the $\lK$ part of $\lG$ :

\begin{theorem}
\[
      \Stab(\lK)=K
\]
 for the adjoint action of $G$ on $\lK$.
\label{tho:Stab_K}
\end{theorem}
The proof of it is given by two lemmas. \cite{Humphreys}

\begin{lemma}
For any $k\in K$,
\[
   \Ad(k)\lK=\lK,
\]
\label{lem:stab_1}
\end{lemma}
and
\begin{lemma}
 If for any $L\in\lK$, $\Ad(x)L$ belongs to $\lK$, then $x\in K$.
\label{lem:stab_2}
\end{lemma}

\begin{proof}[Proof of lemma \ref{lem:stab_1}]
   Let us take a $L\in\lK$ and define $M\in K$ $k=e^M$. We have $\Ad(k)L=e^{\ad M}L$. But in general, we have the relations \eqref{Ieq:comm_KP} which give $e^{\ad M}L\in\lK$. Then $\Ad(k)\lK\subset\lK$.

   In order to show that $\lK\subset\Ad(k)\lK$, let us consider a $L\in\lK$. We have to find a $N\in\lK$ such that $\Ad(k)N=L$. It is clear that $N=\Ad(k^{-1})L$ fulfils the conditions.
\end{proof}

\begin{proof}[Proof of lemma \ref{lem:stab_2}]
Let us consider $X\in\lG$ such that $x=e^X$. We have $e^{\ad X}L\in\lK$ for all $L\in\lK$. This implies that all the terms of the expansion of $e^{\ad X}L$ are in $\lK$. In particular, $[X,L]\in\lK$ for all $L\in\lK$. Let us consider the Cartan decomposition of $X$ : $X=X_k+X_p$. We need $X$ such that
\[
   [X_k,L]+[X_p,L]\in\lK
\]
for any $L\in\lK$. But inclusions \eqref{Ieq:comm_KP} make $[X_p,L]\in\lP$. Then the $X_p$ part of $X$ must vanish (because $\lG=\lK\oplus\lP$ is a direct sum).
\end{proof}

\section{Representations}
%++++++++++++++++++++++++++++++
Source :\cite{Lie_groups}

We are interested in the adjoint representation on a common vector space; we will not discuss the importance of some more complicated features as the ``locally convex''\ condition. We only mention it.

\begin{definition}
If $V$ is a locally convex space, a \defe{continuous representation}{representation!of Lie group} of a Lie group $G$ on $V$ is a left invariant action $\dpt{\pi}{G\times V}{V}$ such that for any $x\in G$, the map $\dpt{\pi(x)}{V}{V}$ is a linear endomorphism of $V$.
\end{definition}

If $\lG$ is a Lie algebra, a \defe{representation}{representation!of Lie algebra}\index{Lie!algebra!representation} of $\lG$ in $V$ is a bilinear map $\dpt{\sigma}{\lG}{\End(V)}$ such that 
\begin{equation}
    \sigma([X,Y])v=[\sigma(X),\sigma(Y)]v=\sigma(X)\sigma(Y)v-\sigma(Y)\sigma(X)v.
\end{equation}
In other words, $\dpt{\sigma}{\lG}{\End{V}}$ is an algebra homomorphism.

A vector space equipped with a representation of a Lie algebra $\lG$ is a \defe{$\lG$-module}{$g$-module@$\protect\lG$-module}. A \emph{complete} locally convex space equipped with a representation of a Lie group is a \defe{$G$-module}{$G$-module}. 

Let us write down  Schur's lemma:
\begin{lemma}
If $\dpt{\phi}{\lG}{\gl(V)}$ is irreducible, then the only endomorphism of $V$ which commutes with all $\phi(\lG)$ are multiples of identity.
\label{lem:Schur}
\end{lemma}

If $\pi$ is a representation of $G$ in a (eventually complex) vector space $V$, an \defe{invariant subspace}{invariant!vector subspace} is a vector subspace $W\subset V$ such that $\pi(x)W\subset W$ for any $x\in G$. A continuous representation in a complete locally compact vector space $V$ is \defe{irreducible}{irreducible!representation} if $\{0\}$ and $V$ are the only two invariant closed subspaces of $V$.

In the case of finite dimensional vector space, any subspace is closed; in this class, we find back the usual notion of irreducibility.

An \defe{unitary}{unitary!representation}\index{representation!unitary} representation of $G$ is a continuous representation $\pi$ of $G$ in a complex (or real) Hilbert space $H$ such that $\pi(x)$ is unitary for any $x\in G$. This is: $\pi$ is unitary if and only if $\forall x\in G$, $v$, $w\in H$,
\begin{equation}
\scal{\pi(x)v}{w}=\scal{v}{\pi(x)^{-1} w}.
\end{equation}
A continuous and finite dimensional representation is \defe{unitarisable}{representation!unitarisable} if there exists an hermitian product for which the representation is unitary.

Now a great proposition without proof:

\begin{proposition}
Let $G$ be a compact Lie group\quext{Verifie si il faut que ce soit de Lie}. Then every representation on a finite dimensional vector space is unitarisable.
\end{proposition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Other results about Cartan algebras}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



\begin{lemma}
A Cartan subalgebra of a semisimple complex Lie algebra is maximally abelian. 
\end{lemma}

\begin{proof}
    If \( \lH\) is a Cartan subalgebra of \( \lG\), proposition \ref{PropCartanLzXtjs} provides \( H_0\in\lG\) such that \( \lH=\lG_0(H_0)\); in particular \( H_0\in\lH\). We are going to prove that if \( H_1,H_2\in\lH\), then for every \( Y\in\lG\) we have \( B\big( [H_1,H_2],Y \big)=0\), so that the non degeneracy of the Killing form will conclude that \( [H_1,H_2]=0\).

    
    Let $X\in\lG(H_0,\lambda)$, $H\in\lH$. The map $\ad X\circ\ad H$ sends $\lG(H_0,\mu)$ to $\lG(H_0,\lambda+\mu)$. If we choose a basis of $\lG$ made up with basis of the spaces $\lG(H_0,\lambda_i)$ (by the primary decomposition theorem) it is clear that $B(H,X)=\tr(\ad H\circ\ad X)=0$. In particular with \( H=[H_1,H_2]\) we get \( B\big( [H_1,H_2],X \big)=0\).

    On the other hand, $\lH$ is solvable because it is nilpotent. Since the adjoint action provides a representation of \( \lH\) on \( \lH\), corollary \ref{cor:de_Lie_Vu} says that we have  basis of $\lH$ in which all the matrices of are upper triangular. Now if $A$, $B$ and $C$ are upper triangular matrices, $ABC$ and $BAC$ have same elements on the diagonal;in particular they traces are the equal: $\tr(ABC)=\tr(BAC)$. Let us consider $H_1$, $H_2$, $H\in\lH$ By Jacobi, $\ad[H_1,H_2]=[\ad H_1,\ad H_2]$, then
    \begin{equation}
    \begin{split}
    \tr(\ad[H_1,H_2]\ad H)&=\tr(\ad H_1\ad H_2\ad H)-\tr(\ad H_2\ad H_1\ad H)\\
    &=\tr(\ad H_2\ad H_1\ad H)-\tr(\ad H_1\ad H_2\ad H)\\
    &=0.
    \end{split}
    \end{equation}
    Up to now we had seen that $B([H_1,H_2],H)=0$ and $B(H,X)=0$ if $X\in\oplus_{\lambda\neq 0}\lG(H_0,\lambda)$. In the latter, we can consider $[H_1,H_2]$ as $H$. Then 
    \[
    B([H_1,H_2],Y)=0
    \]
    for all $Y\in\lG$. Then $[H_1,H_2]=0$ because the Killing form is nondegenerate ($\lG$ is semisimple). This proves that $\lH$ is abelian.

    Now it remains to see that $\lH$ is contained in no larger abelian subalgebra of $\lG$. For this, we naturally consider a larger abelian subalgebra $\lH'$ of $\lG$. For any $H'\in\lH'$ and $H\in\lH$, we have $[H,H']=0$. In particular $[H',H_0]=0$; the property 
    \[
    \lH=\lG(H_0,0)=\{X\in\lG\tq (\ad H_0)^kX=0\text{ for a cerain $k\in\eN$}\}.
    \]
    makes $H'\in\lH$.
\end{proof}


\begin{proposition}\label{prop:G_x_central}
    Let $\lG$ be a Lie algebra, $x\in\lG$ and\nomenclature{$\lG^x$}{An algebra derived from $\lG$}
        \begin{equation}
            \lG^x=\{y\in\lG\tq\exists n\in\eN:(\ad x)^ny=0\}.
        \end{equation}
    Then $\lG^x$ is a subalgebra of $\lG$ which is its own centralizer in $\lG$.
\end{proposition}

\begin{proof}
Since $\ad(x)$ is a derivation of $\lG$ (cf. \ref{sec:adj_gp}),
\[
(\ad x)^n([u,v])=\sum_{k=0}^n \binom{n}{k} [ (\ad x)^ku,(\ad x)^{n-k}v ];
\]
then $[\lG^x,\lG^x]\subset\lG^x$. This proves that $\lG^x$ is a subalgebra of $\lG$. Let $y\in\lG$ be such that $[y,\lG^x]\subset\lG^x$. Clearly $[x,y]\in\lG^x$ (because $x\in\lG^x$) then $(\ad x)^ny=(\ad x)^{n-1}[x,y]$, so that $y\in\lG^x$.
\end{proof}

\begin{lemma}
If $\dpt{A}{V}{V}$ is a linear operator on a finite dimensional vector space, then there exists a positive integer $p$ such that $A^p(V)=A^{p+1}(V)$.
\label{lem:A_n_stabilise}
\end{lemma}

\begin{proof}
We build a basis of $V$ in the following manner. Since $A(V)$ is a subspace of $V$, we can begin our basis with $\{Y_i\}$, a basis of the component of $A(V)$ in $V$. Next, $A^2(V)$ is a subspace of $A(V)$, then we can consider $\{X^1_i\}$, a basis of the vector space $A(V)\setminus A^2(V)$, and so on\ldots $\{X^p_i\}$ are vectors in $A^n(V)$ but not in $A^{n+1}V$. Since the vector space has only a finite number of basis vectors, there is a $p$ such that $\{X^p_i\}=\emptyset$.
\end{proof}

Now we consider $W=\{u\in V\tq\exists n\in\eN:A^nu=0\}$ and $v\in V$. There exists a $v'\in V$ such that $A^p(v)=A^{p+1}(v')$. Writing  $v= A(v')+(v-A(v'))$, we find
\begin{equation}\label{eq:VAW}
V\subset A(V)+W
\end{equation}
because $A^p(v)-A^{p+1}(v')=0$, $v-A(v')\in W$.


If we apply $A$ on this, we find $A(V)\subset A^2(V)+A(W)$. Reinserting it into the right hand side of \eqref{eq:VAW}, we find $V\subset A^2(V+W)$ and repeating $p$ times this process, we find $V=A^p(V)+W$ and the sum is direct because none of the elements of $A^p(V)$ is annihilated by $A$:
\begin{equation}\label{eq:ApoplusW}
V=A^p(V)\oplus W.
\end{equation}

\begin{proposition}
Let $\lG$ be a Lie algebra and $x\in\lG$. Then there exists a subspace $\lG_x$ of $\lG$ such that $\lG=\lG_x\oplus\lG^x$ and $[\lG^x,\lG_x]\subset\lG_x$.
\label{prop:G_x_G_x}
\end{proposition}

\begin{proof}
We claim that the space is given by 
\begin{equation}
\lG_x=(\ad x)^p\lG
\end{equation}
where $p$ is taken large enough to have $(\ad x)^p\lG=(\ad x)^{p+1}\lG$. The lemma and the discussion below show the correctness of the definition of $\lG_x$ and that $\lG=\lG_x\oplus\lG^x$. It remains to be proved that $[\lG^x,\lG_x]\subset\lG_x$. For we will prove (by induction with respect to $m$) for any $m$ that $(\ad x)^my=0$ implies $(\ad y)\lG_x\subset\lG_x$.

For $m=1$, the induction assumption becomes $[x,y]=0$ and Jacobi gives $\ad x\circ\ad y=\ad y\circ\ad x$, then $(\ad y)\lG_x=(\ad x)^p(\ad y)\lG\subset\lG_x$. Now we suppose that $(\ad x)^{m-1}z=0$ implies $(\ad z)\lG_x\subset\lG_x$ and we consider $y\in\lG$ such that $(\ad x)^my=0$ and $u\in\lG_x$. We are going to show that $(\ad y)u\in\lG_x$. Let $f$ be the characteristic polynomial of $\ad x$: 
\[
f(t)=\det\big( \ad x-t\mtu \big)
\]
where $\ad x$ and $\mtu$ are taken on $\lG_x$. Since $(\ad x)u=0$, $f(0)\neq 0$ and by the Cayley-Hamilton theorem, $f(\ad x)u=0$. Then
\begin{equation}
( f(\ad x)\ad y )u=(  f(\ad x)\ad y-(\ad y)f(\ad x)   )u,
\end{equation}
and, on the other hand, $\forall q\in\eN$,
\[
(\ad x)^q\ad y-\ad y(\ad x)^q=\sum_{r=0}^{q-1}(\ad x)^r(\ad[x,y])(\ad x)^{q-r-1}.
\]
It follows that $f(\ad x)\ad y-(\ad y)f(\ad x)$ is a linear combination of terms of the form 
\[
(\ad x)^a(\ad[x,y])(\ad x)^b
\]
and the induction hypothesis shows that $f(\ad x)(\ad y)u\in\lG_x$.

Now we consider a $n$ such that $(\ad x)^n\lG^x=0$; the fact that $f(0)\neq 0$ implies the existence of polynomials $g(t)$ and $h(t)$ such that $g(t)t^n+h(t)f(t)=1$. If we decompose $(\ad y)u=v+w$ with respect to $\lG=\lG_x\oplus\lG^x$ we find
\begin{equation}
\begin{split}
(\ad y)u&=[ g(\ad x)(\ad x)^n+h(\ad x)f(\ad x) ](\ad y)u\\
&=f(\ad x)(\ad x)^nv+h(\ad x)f(\ad x)(\ad y)u\in\lG_x.
\end{split}
\end{equation}

\end{proof}

\begin{proposition}
Let $\lG$ be a Lie algebra and $x\in\lG$ such that $\lG^x$ is as small as possible. Then $\lG^x$ is a Cartan subalgebra.
\end{proposition}

\begin{proof}
From proposition \ref{prop:G_x_central}, it is sufficient to prove that $\lG^x$ is nilpotent. Let $y\in\lG^x$ and $f_y(t)$ be the characteristic polynomial of $\ad y$. Since it is a subalgebra, $\lG^x$ is stable under $\ad y$ and proposition \ref{prop:G_x_G_x} makes $\lG_x$ also stable under $\ad y$. Then $\ad y$ can be written under a bloc-diagonal form with respect to the decomposition $\lG=\lG_x\oplus\lG^x$, so that the characteristic polynomial can be factorised as
\begin{equation} 
f_y(t)=g_y(t)h_y(t)
\end{equation}
where $g_y$ and $h_y$ are the characteristic polynomials of the restrictions of $\ad y$ to $\lG^x$ and $\lG_x$. Let $(y_1,\ldots,y_m)$ be a basis of $\lG^x$ and $t^n$, the greatest power of $t$ which divide all the $g_y(t)$ with $y\in\lG^x$. The coefficient of $t^n$ in $g_{c^iy_i}(t)$ is a polynomial with respect to the $c^i$ because of the expression 
\[
g_{c^iy_i}(t)=\det\Big( \ad(c^iy_i)-t\mtu \Big).
\]
Let $u$ be this polynomial: $g_{c^iy_i}(t)=\ldots+u(c^1,\ldots,c^m)t^n$. By definition of $n$, this is not an identically zero polynomial and there are no terms with $t^{n-1}$. For the same reasons, we have a polynomial $v$ such that
\begin{equation}
h_{c^iy_i}(0)=v(c^1,\ldots,c^m).
\end{equation}
We know that none of the non-zero elements in $\lG_x$ are annihilated by $\ad x$ (because of the definition of $\lG^x$). Then $h_x(0)\neq 0$ and $v$ is not identically zero. With all this we can find some $c^i\in\eC$ such that $u(c^1,\ldots,c^m)v(c^1,\ldots,c^m)\neq 0$. If $y=c^iy_i$, the coefficient of $t^n$ in $f_y(t)$ is $u(c)v(c)\neq 0$, so that $f_y(t)$ is not divisible by $t^{n+1}$.

But in the other hand $\lG^x$ has minimal dimension, then $\dim\lG^y\geq m=\dim\lG^x$. Moreover $t^{\dim\lG^y}$ divide $f_y(t)$ because there is a certain power of $\ad y$ which has zero as eigenvalue with multiplicity $\dim\lG^y$\quext{This is not a good reason.}. Since $f_y(t)$ can not be divided by $t^{n+1}$ this shows that $n+1>\dim\lG^y$ and $n\geq\dim\lG^y\geq m$.

Now we consider $y$, any element of $\lG^x$. From the fact that $t^n$ divide all the $g_y(t)$ and that $n\geq m$, we see that $t^m$ divide $g_y(t)$. But the degree of $g_y(t)$ is $\dim\lG^x=m$. Finally, $g_y(t)=m$ and $\ad y$ is nilpotent on $\lG^x$ for any $y\in\lG^x$.

The Engel's theorem  \ref{tho:Engel} makes $\lG^x$ nilpotent.
\end{proof}


The following holds for complex or real Lie algebras and comes from \cite{Sagle} see also \cite{SamelsonNotesLieAlg}. We denote by $\eK$ the base field of $\lG$, i.e. $\eR$ or $\eC$. For $X\in\lG$ and $\lambda\in\eK$ we define
\begin{equation}
\lG(X,\lambda)=\{Y\in\lG\tq (\ad X-\lambda\mtu)^nY=0\textrm{ for a certain $n\in\eN$}\}.
\end{equation}
A first useful result is given in
\begin{lemma}
If $Z\in\lG$, then
\[
[\lG(Z,\lambda),\lG(Z,\mu)]\subset\lG(Z,\lambda+\mu),
\]
in particular $\lH$ is a subalgebra of $\lG$.
\label{lem:lambda_mu_plus}
\end{lemma}

\begin{proof}
We consider $X_{\lambda}\in\lG(Z,\lambda)$ and $X_{\mu}\in\lG(Z,\mu)$. We have
\begin{equation}
\begin{split}
\big( \ad Z-(\lambda+\mu)I\big)[X_{\lambda},X_{\mu}]&=[ (\ad Z-\lambda I)X_{\lambda},X_{\mu} ]\\
            &\quad+[X_{\lambda}, (\ad Z-\mu I)X_{\mu} ].
\end{split}                    
\end{equation}
By induction,
\begin{equation}
\big( \ad Z-(\lambda+\mu)\mtu\big)^n[X_{\lambda},X_{\mu}]=\sum_{i=0}^{\infty}\binom{n}{i}
[ (\ad Z-\lambda I)^iX_{\lambda} ,(\ad Z-\mu I)^{n-i}X\mu].
\end{equation}
It will become zero for large enough $n$.
\end{proof}


An element $X\in\lG$ is \defe{regular}{regular} if $\dim\lG(X,0)$ is minimum\angl. This minimum is the \defe{rank}{rank of a Lie algebra} of $\lG$. 

\begin{proposition}
    If $X\in\lG$ is a regular element then the algebra
    \begin{equation}
        \lH=\lG(X,0)=\{Y\in\lG\tq(\ad X)^nY=0\text{ for some $n\in\eN$}\}
    \end{equation}
    is nilpotent.
\end{proposition}

\begin{proof}
We have to show that for any $H\in\lH$, the endomorphism $\ad H$ of $\lH$ is nilpotent. Consider the characteristic polynomial of $\ad X$
\[
p(t)=\det(\ad X-t\mtu)=t^rq(t)
\]
where $t^r$ is the maximal factorization of $t$; in other words, $q(t)$ is not divisible by $t$ and $r=\dim\lH$. In particular
\begin{equation}
\lH=\{Y\in\lG\tq (\ad X)^rY=0\}.
\end{equation}
Let 
\begin{equation}
\lK=\{Y\in\lG\tq q(\ad X)Y=0\}
\end{equation}
From the Cayley-Hamilton theorem (\ref{ThoCayleyHamilton}), $p(\ad X)=0$, then $(\ad X)^rq(\ad X)=0$ and $\lG=\lH\oplus\lK$. Moreover $\lH$ and $\lK$ are $\ad X$-invariants: $(\ad X)\lH\subseteq\lH$ and $(\ad X)\lK\subseteq\lK$.

Every weight of $\ad X$ are in $\eC$. As we know that $\lH$ is Cartan in $\lG$ if and only if $\lHeC$ is Cartan in $\lGeC$, we can suppose that $\lG$ is a complex algebra by considering $\lGeC$ if $\lG$ is real. So all the weight are in the base field and we can define
\[
\lK=\sum_{\lambda\in\Delta}\lG(X,\lambda).
\]
where $\Delta$ is the set of all the non zero weight of $\ad X$. A property\quext{Que je dois encore faire, cf Sagle} of the weight space is that 
\[
\lG=\lG(X,\lambda_1)\oplus\ldots\oplus\lG(X,\lambda_m)
\]
if the $\lambda_i$'s are the weight of $\ad X$. Now we prove that $\sum_{\lambda\neq 0}\lG(X,\lambda)=\lK$. First consider a $Y\in\lG(X,\lambda)$ which can be decomposed as $Y=H+K$ with $H\in\lH$ and $K\in\lK$. Then $(\ad X-\lambda\mtu)^n(H+K)=(\ad X-\lambda\mtu)^nH+(\ad X-\lambda\mtu)^nK$ where the first term is not zero (because $H\in\lH$) and lies in $\lH$ while the second term lies in $\lK$. Then the sum can be zero only if $H=0$.

% Il faut continuer la preuve à partir du bas de /100: la démonstration de l'inclusion inverse.

\end{proof}

Let $\lG$ be a complex semisimple Lie algebra, $H\in\lG$ and $0=\lambda_0,\lambda_1,\ldots,\lambda_r$, the eigenvalues of $\ad H$. For any $\lambda\in\eC$, one can consider 
\begin{equation}
\lG(H,\lambda)=\{X\in\lG\tq (\ad H-\lambda I)^kX=0\}.
\end{equation}
From the Jordan decomposition, $g(H,\lambda)=0$ except if $\lambda$ is one of the $\lambda_i$, and
\begin{equation}\label{eq:g_sum_g_H}
\lG=\bigoplus_{i=0}^{r}g(H,\lambda_i).
\end{equation}

An element $H\in\lG$ is \defe{regular}{regular} if 
\[
\dim g(H,0)=\min_{X\in\lG}\dim g(X,0).
\]
Let $H_0$ be a regular element and $\lH=\lG(H_0,0)$.

\begin{lemma}
    The algebra $\lH=\lG(H_0,0)$ is nilpotent
\end{lemma}

\begin{proof}
Let $0=\lambda_0,\lambda_1,\ldots,\lambda_r$ be the eigenvalues of $\ad H_0$ and
\[
\lG'=\sum_{i=1}^{r} \lG(H_0,\lambda_i)
\]
which is a subspace of $\lG$. From the lemma,
\[
[\lG(H_0,0),\lG(H_0,\lambda_i)]\subset\lG(H_0,\lambda_i)\subset\lG'.
\]
For each $H\in\lH$, we denote $H'$, the restriction of $\ad H$ to $\lG'$ and $d(H)=\det H'$. The function $H\to d(H)$ is a polynomial on $\lH$ in the sense of the coordinates on $\lH$ as vector space. If $H'_0$ has a zero eigenvalue we would have $\ad(H_0)X=0$ for some $X\in\lG'$. In this case $[H_0,X]=0$, but $X\in\lG(H_0,\lambda_i)$, then for a certain $k$, $(\ad H_0-\lambda_i)^kX=0$, so that $\lambda_iX=0$. Since $\lG$ is defined by excluding $\lambda_0$, $X=0$. Thus $H_0'$ has only non zero eigenvalues and $d(H_0)\neq 0$.

We know that a polynomial which is zero on an open set is identically zero; then on any open set of $\lH$, $d$ has a non zero value somewhere. In particular,
\[
S=\{H\in\lH\tq d(H)\neq 0\}
\]
is dense in $\lH$. We consider a $H\in S$. The endomorphism $H'$ has only non zero eigenvalues, so that $\lG(H,0)\subset\lH$ from lemma \ref{lem:lambda_mu_plus}; but $H_0$ is regular, then $\lG(H,0)\subset\lH$. Thus the restriction of $\ad H$ to $\lH$ is nilpotent because it is nilpotent on $\lG(H,0)$\quext{Ce paragraphe n'est pas vraiment clair\ldots}.

If $l=\dim\lH$, then $(\ad_{\lH}H)^l=0$ because $\ad_{\lH}H$ is nilpotent. By continuity, this equation is true for any $H\in\lH$ from the density of $S$ in $\lH$. Then $\lH$ is nilpotent.

\end{proof}


Here is an alternative proof (that I do not really understand) for theorem \ref{TholGCartalphaplusbeta}.
\begin{theorem}    
Let $\lG$ be a complex Lie algebra with Cartan subalgebra $\lH$. Then $\lG_0=\lH$.
\end{theorem}    

\begin{proof}
Since $\lH$ is Cartan, it is nilpotent. So $\lH\subset\lG_0$. If $v\in\lG_0$, there exists a $n$ such that for any $z\in\lH$, $(\ad z)^nv=0$. The fact that $\lH$ is nilpotent makes $(\ad z_n)\circ\ldots\circ(\ad z_1)v=0$ for any $z\in\lG_0$ and for all $z_1,\ldots,z_n\in\lH$. If we write $(\ad z_1)v$ with $v\in\lG_0\setminus\lH$, we can always choose $z_1$ in order the result to \emph{not} be $\lH$. Next we can choose $z_2\in\lH$ such that $(\ad z_2)\circ(\ad z_1)v$ is also not in $\lH$ and so on\ldots Since $\lG_0$ is nilpotent, we always finish on zero. If $n$ is the maximum of adjoint that we can take before to fall into zero; we have
\[
[h, (\ad z_{n-1})\circ(\ad z_1)v ]=0
\]
for all $h\in\lH$ and with a good choice of $z_i$, it contradicts the fact that $\lH$ is Cartan.
\end{proof}

\section{Universal enveloping algebra}  \label{subsec:env_alg}
%----------------------------------------

Let $\mA$ be a Lie algebra. One knows that the composition law $(X,Y)\to[X,Y]$ is often non associative. In order to build an associative Lie algebra which ``looks like''\ $\mA$, one considers $T(\mA)$, the tensor algebra of $\mA$ (as vector space) and $\mJ$ the two-sided ideal in $T(\mA)$ generated by elements of the form 
\[
   X\otimes Y-Y\otimes X -[X,Y]
\]
for $X$, $Y\in\mA$. The \defe{universal enveloping algebra}{universal!enveloping algebra} of $\mA$ is the quotient \nomenclature{$U(\mA)$}{Universal enveloping algebra} 
\begin{equation}
     U(\mA)=T(\mA)/\mJ.
\end{equation}
For $X\in\mA$, we denote by $X^*$\nomenclature{$X^*$}{Image of a tensor in the universal enveloping algebra} the image of $X$ by canonical projection $\dpt{\pi}{T(\mA)}{U(\mA)}$ and by $1$ the unit in $U(\mA)$. One has $1\neq 0$ if and only if $\mA\neq\{0\}$.

A property without proof\quext{La preuve est à partir de 21\# de Lie} (see \cite{Helgason} page 90):

\begin{proposition}
Le $V$ be a vector space on $K$. Then there is a natural bijection between the representations of $\mA$\index{representation! of $U(\mA)$} on $V$ and the ones of $U(\mA)$ on $V$. If $\rho$ is a representation of $\mA$ on $V$, the corresponding $\rho^*$ of $U(\mA)$ is given by
\[
   \rho(X)=\rho^*(X^*)
\]
($X\in\mA$).
\end{proposition}

Let $\{X_1,\ldots,X_n\}$ be a basis of $\mA$. For a $n$-uple of complex numbers $(t_i)$, one defines
\begin{equation}
X^*(t)=\sum_{i=1}^nt_iX^*_i.
\end{equation}
On the other hand, we consider a $n$-uple of positive integers $M=(m_1+\ldots m_n)$, and the notation 
\begin{equation}
\begin{split}
   |M|&=m_1+\ldots+m_n\\
   t^M&=t_1^{m_1}\cdots t_n^{m_n}.
\end{split}
\end{equation}

When $|M|>0$, we denote by $X^*(M)\in U(\mA)$ the coefficient of $t^M$ in the expansion of $(|M|!)^{-1} (X^*(t))^{|M|}$. If $|M|=0$, the definition is $X^*(0)=1$. Once again a proposition without proof\quext{\`A la page /23}:

\begin{proposition}
The smallest vector subspace of $U(\mA)$ which contains all the elements of the form $X^*(M)$ is $U(\mA)$ itself:
\[
   U(\mA)=\Span\{ X^*(M):M\in \eN^n \}.
\]

\end{proposition}


\begin{corollary}
Let $\mA$ be a Banach algebra of dimension $n$, $\mB$ a Banach subalgebra of dimension $n-r$ and a basis $\{X_1,\ldots,X_n\}$ of $\mA$ such that the $n-r$ last basis vectors are in $\mB$. We denotes by $B$ the vector subspace of $U(\mA)$ spanned by the elements of the form $X^*(M)$ with $m=(0,\ldots,0,m_{r+1},\ldots,m_n)$. Then $B$ is a subalgebra of $U(\mA)$.
 \label{cor:/24}
\end{corollary}

\begin{definition}
Two Lie groups $G$ and $G'$ are \defe{isomorphic}{isomorphism!of Lie groups} when there exists a differentiable group isomorphism between $G$ and $G'$.

They are \defe{locally isomorphic}{locally!isomorphic!Lie groups} when there exists neighbourhoods $\mU$ and $\mU'$of $e$ and $e'$ and a differentiable diffeomorphism $\dpt{f}{\mU}{\mU'}$ such that

$\forall x,y,xy\in\mU$, $f(xy)=f(x)f(y)$, \\and

$\forall x',y',x'y'\in\mU'$, $f^{-1}(x'y')=f^{-1}(x')f^{-1}(y')$.
\end{definition}

Now a great theorem without proof:
\begin{theorem}
Two Lie groups are locally isomorphic if and only if their Lie algebras are isomorphic.
 \label{tho:loc_isom}
\end{theorem}

The following universal property of the \emph{universal} enveloping algebra explains the denomination:

\begin{proposition}
Let $\dpt{\sigma}{\mG}{\mU(\mG)}$ the canonical inclusion and $A$ an unital complex associative algebra. A linear map $\dpt{\varphi}{\mG}{A}$ such that
\begin{equation}
\varphi[X,Y]=\varphi(X)\varphi(Y)-\varphi(Y)\varphi(X)
\end{equation}
can be extended in only one way to an algebra homomorphism $\dpt{\varphi_0}{\mU(\mG)}{A}$ such that $\varphi_0\circ\sigma=\varphi$ and $\varphi(1)=1$
\label{prop:extunifmap}
\end{proposition}
For a proof, see \cite{Knapp_reprez}.

\subsection{Adjoint map in \texorpdfstring{$\mU(\mG)$}{U(G)}}   \label{ssadjunif}
%/////////////////////////////////////////////

We know that $\dpt{ \Ad(g) }{ \mG }{ \mG }$ fulfils
\[ 
  \Ad(g)[X,y]=[  \Ad(g)X,\Ad(g)Y  ],
\]
and we can define $\dpt{ \Ad(g) }{ \mG }{ \mU(\mG) }$ by $\Ad(g)X=X$ where in the right hand side, $X$ denotes the class of $X$ for the quotients of the tensor algebra which defines the universal enveloping algebra.

When $[A,B]$ is seen in $\mU(\mG)$, we have $[A,B]=A\otimes B-B\otimes A$. Then $\dpt{ \Ad(g) }{ \mG }{ \mU(\mG) }$ fulfils proposition \ref{prop:extunifmap} and is extended in an unique way to $\dpt{ \Ad(g) }{ \mU(\mG) }{ \mU(\mG) }$ with $\Ad(g)1=1$.
 
\begin{lemma} 
    If $D\in\mU(\mG)$, the following properties are equivalent:
    \begin{itemize}
        \item $D\in\mZ(\mG)$
        \item $D\otimes X=X\otimes D$ for all $X\in \mG$
        \item $e^{\ad X}D=D$ for all $X\in\mG$
        \item $\Ad(g)D=D$ for all $g\in G$.
    \end{itemize}
     \label{lem:equivDAd}
\end{lemma}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Invariant fields}
%---------------------------------------------------------------------------------------------------------------------------

If $X\in\lG$, we have the associated left invariant vector field on $G$ given by $\tilde X_x=dL_xX$. That field is left invariant as operator on the functions because
\begin{equation}
    \tilde X_x(u)=\tilde X_e(L^*_xu)
\end{equation}
as the following computation shows
\begin{equation}
        \tilde X_e(L^*u)=\Dsdd{ (L_x^*u)\big(  e^{tX} \big) }{t}{0}
        =\Dsdd{ u\big( x e^{tX} \big) }{t}{0}
        =\Dsdd{ u\big( \tilde X_x(t) \big) }{t}{0}
        =\tilde X_x(u)
\end{equation}
because the path defining $\tilde X_x$ is $x e^{tX}$.

We can perform the same construction in order to build left invariant fields based on $\mU(\lG)$. If $X$ and $Y$ are elements of $\lG$, the  differential operator on $ C^{\infty}(G)$ associated to $XY\in\mU(\lG)$ is given by
\begin{equation}
    (XY)(f)=\DDsdd{ f\big( X(s)Y(t) \big) }{t}{0}{s}{0}
\end{equation}
The path defining the field $\widetilde{XY}$ is
\begin{equation}
    \widetilde{XY}_x=xX(s)Y(t).
\end{equation}
Thus we have
\begin{equation}        \label{EqInvarUgField}
    \widetilde{(XY)}_e(L^*u)=\widetilde{(XY)}_xu
\end{equation}

\begin{lemma}       \label{LemAdesthioo}
    If \( X,Y\in\lG\) we have
    \begin{equation}
        [\ad(X),\ad(Y)]=\ad([X,Y]).
    \end{equation}
\end{lemma}

\begin{proof}
    Let \( f\in\lG\) and compute the action of \( [\ad(X),\ad(Y)]\):
    \begin{subequations}
        \begin{align}
            [\ad(X),\ad(Y)]f&=\ad(X)[Yf,fY]-\ad(Y)(Xf-fX)\\
            &=(XY-YX)f+f(YX-XY)\\
            &=\ad([X,Y])f.
        \end{align}
    \end{subequations}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
                    \subsection{Representation of Lie groups}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}
    Let $G$ be a Lie group and $\mG$ its Lie algebra. A representation $\varphi\colon G\to \End(V)$ of the group induces a representation $\phi\colon \mU(\mG)\to \End(V)$ of the universal enveloping algebra with the definitions
    \begin{subequations}
        \begin{align}
            \phi(X)     &=d\varphi_e(X),\\
            \phi(XY)    &=\phi(X)\circ\phi(Y)
        \end{align}
    \end{subequations}
    where $e$ is the unit in $G$ and $X$, $Y$ are any elements of $\mG$.
\end{proposition}

\begin{proof}
    We have
    \begin{equation}
        \phi(X)=\Dsdd{ \varphi( e^{tX})v }{t}{0}=d\varphi_e(X)v.
    \end{equation}
    Notice that, by linearity of the action of $\varphi( e^{tX})$ on $v$, one can leave $v$ outside the derivation. Now, neglecting the second order terms in $t$ in the derivative, and using the Leibnitz formula, we have
    \begin{equation}
        \begin{aligned}[]
            \phi([X,Y])v    &=  \Dsdd{ \varphi( e^{tXY} e^{-tXY}) }{t}{0}v\\
                    &=  \Dsdd{ \varphi( e^{tXY})\varphi(\mtu) }{t}{0}v+\Dsdd{ \varphi(\mtu)\varphi( e^{-tXY}) }{t}{0}v\\
                    &=  \phi(XY)v-\phi(YX)v\\
                    &=  \big( \phi(X)\phi(Y)-\phi(Y)\phi(X) \big)v\\
                    &=  [\phi(X),\phi(Y)]v,
        \end{aligned}
    \end{equation}
    which is the claim.
\end{proof}

