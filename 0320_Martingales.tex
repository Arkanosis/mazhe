% This is part of Mes notes de mathématique
% Copyright (c) 2012-2013
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Convergence de martingales}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    Si \( \tribA\) est une tribu, une \defe{filtration}{filtration} de \( \tribA\) est une suite croissante de sous-tribus \( \tribB_i\subseteq\tribB_{i+1}\subseteq\tribA\).

    Nous disons qu'une suite de variables aléatoires \( (X_n)\) est \defe{adaptée}{processus!adapté à une filtration} à une filtration \( (\tribF_n)\) si \( X_i\) est \( \tribF_i\)-mesurable pour tout \( i\).
\end{definition}

Ces définitions impliquent immédiatement que si \( (X_n)\) est adapté à \( (\tribF_n)\) alors \( X_n\) est \( \tribF_k\)-mesurable pour \( k\geq n\).

\begin{definition}
    Une \defe{martingale}{martingale} adaptée à la filtration \( (\tribB_n)_{n\in \eN}\) est une suite de variables aléatoires \( M_n\in L^1(\Omega,\tribA,P)\) telle que
    \begin{enumerate}
        \item
            \( M_n\) est \( \tribB_n\)-mesurable,
        \item
            \( E(M_{n+1}|\tribB_n)=M_n\).
    \end{enumerate}

    Le processus \( M_n\) est une \defe{sur-martingale}{sur-martingale} si \( E(M_{n+1}|\tribB_n)\leq M_n\), et c'est une \defe{sous-martingale}{sous-martingale} si \( E(M_n|\tribB_n)\geq M_n\).
\end{definition}

\begin{example}
    Si \( M\in L^1(\Omega,\tribA,P)\) et si \( (\tribB_n)_{n\in \eN}\) est une filtration, nous pouvons considérer la martingale \( M_n=E(M|\tribB_n)\).
\end{example}

\begin{example}     \label{ExtFFKTr}
    Soit \( (X_i)_{i\geq 1}\) une suite de variables aléatoires indépendantes et centrées. On pose
    \begin{equation}
        S_n=X_1+\ldots +X_n
    \end{equation}
    et la filtration \( \tribB_n=\sigma(X_1,\ldots, X_n)\). Pour montrer que cela est une martingale, nous commençons par remarquer que
    \begin{equation}
        E(X_{n+1}|\tribB_n)=E(X_{n+1})=0
    \end{equation}
    par indépendance des tribus \( \tribB_n\) et \( \sigma(X_{n+1})\). Ici c'est le lemme \ref{LemxUZFPV} qui joue.

    Ensuite nous argumentons que \( E(X_1+\ldots +X_n|\tribB_n)=X_1+\ldots +X_n\). En effet d'une part \( X_1+\ldots +X_n\) est \( \tribB_n\)-mesurable et évidemment la condition intégrale de l'espérance conditionnelle est satisfaite.

    Plus généralement si \( X\) est une variable aléatoire et si \( \sigma(X)\subset\tribB\) alors \( E(X|\tribB)=X\).
\end{example}

\begin{lemma}   \label{LemqanhgJ}
    Soit \( (M_n)\) une martingales adaptée à la filtration \( (\tribF_n)\) et \( n\geq k\). Alors
    \begin{subequations}
        \begin{align}
            E(M_n|\tribF_k)&=M_k\\
            E(M_k|\tribF_n)&=M_k.
        \end{align}
    \end{subequations}
\end{lemma}

\begin{proof}
    La seconde relation revient seulement à dire que \( M_k\) est \( \tribF_n\)-mesurable, ce qui est évident parce que \( \tribF_k\subset\tribF_n\).

    Nous prouvons la première par récurrence (à l'envers) sur \( k\). D'abord si \( k=n\), l'égalité \( E(M_n|\tribF_n)=M_n\). Nous supposons maintenant que \( E(M_n|\tribF_k)=M_k\), et nous prouvons que \( E(M_n|\tribF_{k-1})=M_{k-1}\). Si \( B_{k-1}\in \tribF_{k-1}\), nous avons
    \begin{equation}
        \int_{B_{k-1}}M_{k-1}=\int_{B_{k-1}}M_{k}=\int_{B_{k-1}}M_n.
    \end{equation}
    La première égalité est la définition d'une martingale, et la seconde est l'hypothèse de récurrence.
\end{proof}

\begin{theorem}[\cite{GubinelliMartin,PMCmartinLP}]     \label{ThobysyWI}
    Soit \( (M_n)_{n\geq 0}\) une martingale bornée dans \( L^2(\Omega)\), c'est à dire telle que\index{martingale!bornée dans \( L^2(\Omega)\)}
    \begin{equation}
        \alpha=\sup_{n\geq 0}E(M_n^2)<\infty.
    \end{equation}
    Alors la suite \( M_n\) converge dans \( L^2(\Omega)\).
\end{theorem}

\begin{proof}
    Nous écrivons \( M_n\) en somme télescopique
    \begin{equation}
        M_n=M_0+\sum_{k=1}^n\Delta_k
    \end{equation}
    où \( \Delta_k=M_k-M_{k-1}\). Nous commençons par monter que les incréments sont orthogonaux au sens où \( E(\Delta_n\Delta_k)=0\). Pour \( n>k\), la variable aléatoire \( E\big( \Delta_n\Delta_k|\tribF_{n-1} \big)\) est la variable aléatoire \( \tribF_{n-1}\)-mesurable telle que
    \begin{equation}
        \int_{B_{n-1}}E\big( \Delta_n\Delta_k|\tribF_{n-1} \big)=\int_{B_{n-1}}\Delta_n\Delta_k
    \end{equation}
    pour tout \( B_{n-1}\in\tribF_{n-1}\). En particulier avec \( B_{n-1}=\Omega\) nous trouvons
    \begin{equation}
        E\Big( E\big( \Delta_n\Delta_k|\tribF_{n-1} \big)\Big)=E(\Delta_n\Delta_k)
    \end{equation}
    par la définition de l'espérance \eqref{EqdCBLst}. Par conséquent, en utilisant le lemme \ref{LemqanhgJ} nous avons\footnote{À ce niveau je crois qu'il y a une faute dans \cite{PMCmartinLP} qui conditionne par rapport à \( \tribF_n\).}
    \begin{equation}
        E(\Delta_n\Delta_k)=E\Big( E(\Delta_n\Delta_k|\tribF_{n-1}) \Big)=E\Big( \Delta_kE(\Delta_n|\tribF_{n-1}) \Big)=0
    \end{equation}
    parce que \( E(\Delta_n|\tribF_{n-1})=E(M_n|\tribF_{n-1})-E(M_{n-1}|\tribF_{n-1})=0\).

    Utilisant l'orthogonalité des incréments, nous avons
    \begin{equation}
        E(M_n^2)=E(M_0^2)+\sum_{k=1}^nE(\Delta_k^2).
    \end{equation}
    En prenant le supremum (par rapport à \( n\) des deux côtés),
    \begin{equation}
        E(M_0^2)+\sum_{k=1}^{\infty}E(\Delta_k^2)=\alpha<\infty.
    \end{equation}
    Cela prouve que la suite \( \sum_{k=1}^n\Delta_k\) converge dans \( L^2(\Omega)\). Nous en déduisons immédiatement que \( (M_n)\) est de Cauchy dans \( L^2(\Omega)\) parce que si \( k,l>n\), nous avons (en utilisant encore l'orthogonalité des incréments)
    \begin{equation}
        E\big( | M_k-M_l |^2 \big)=\sum_{i=k+1}^lE(\Delta_i^2)\leq\sum_{i=k+1}^{\infty}E(\Delta_i^2),
    \end{equation}
    qui tend vers zéro lorsque \( n\to\infty\).
\end{proof}

Le théorème suivant complète la conclusion du théorème \ref{ThobysyWI}.
\begin{theorem}[\cite{PMCmartinLP}] \label{ThofcttYW}
    Soit \( (M_n)_{n\in \eN}\) une martingale bornée dans \( L^2\). Alors \( (M_n)\) converge dans \( L^2(\Omega)\) et presque sûrement vers une même variable aléatoire \( M_{\infty}\) qui vérifie
    \begin{equation}        \label{EqmDMfZf}
        M_n=E(M_{\infty}|\tribF_n).
    \end{equation}
\end{theorem}

Notons en particulier que la variable aléatoire \( M_{\infty}\) est presque sûrement finie parce qu'en vertu de \eqref{EqmDMfZf} nous avons
\begin{equation}
    \int_{\Omega}M_{\infty}=\int_{\Omega}M_n<\infty.
\end{equation}

\begin{example}
    Soient des variables aléatoires indépendantes \( V_k\sim\dE(2^n\lambda)\) et la variable aléatoire somme
    \begin{equation}
        S_n=\sum_{k=1}^nV_k.
    \end{equation}
    Nous allons montrer que \( S_n\stackrel{p.s.}{\longrightarrow}X\) où \( X\) est une variable aléatoire presque sûrement finie. Nous posons
    \begin{equation}
        M_n=S_n-\sum_{k=1}^n\frac{1}{ 2^k\lambda }
    \end{equation}
    Cela est une martingale adaptée à la filtration \( \tribF_n=\sigma(V_1,\ldots, V_n)\) en vertu de l'exemple \ref{ExtFFKTr}. Nous montrons à présent qu'elle est bornée dans \( L^2(\Omega)\) au sens où \( \sum_{n\geq 1}E(M_n^2)<\infty\). Nous avons
    \begin{equation}
        E(M_n^2)=E\left( \big[ S_n-\sum_k\frac{1}{ 2^k\lambda } \big]^2 \right)=E\left( \big[ \sum_k(V_k-\frac{1}{ 2^k\lambda }) \big]^2 \right).
    \end{equation}
    La variable aléatoire \( V_k-1/2^k\lambda\) est une variable aléatoire centrée de variance \( 1/(2^k\lambda)^2\) (voir proposition \ref{PropTxGcWn}). Étant donné que \( M_n\) est centrée, \( \Var(M_n)=E(M_n^2)\) et nous avons
    \begin{equation}
        E(M_n^2)=\sum_{k=1}^n\Var\left( V_k-\frac{1}{ 2^k\lambda } \right)=\sum_{k=1}^n\frac{1}{ (2^k\lambda)^2 },
    \end{equation}
    cette dernière somme étant bornée par \( l=\sum_{k=1}^{\infty}\frac{1}{ (2^k\lambda)^2 }\), nous avons
    \begin{equation}
        E(M_n^2)\leq l
    \end{equation}
    avec \( l\) indépendant de \( n\). C'est pour cela que \( (M_n)_{n\in \eN}\) est une martingale bornée dans \( L^2(\Omega)\). Par le théorème \ref{ThofcttYW} nous avons \( M_n\to M_{\infty}\) et en faisant \( n\to \infty\) dans
    \begin{equation}
        S_n=M_n+\sum_{k=1}^n\frac{1}{ 2^k\lambda },
    \end{equation}
    nous trouvons
    \begin{equation}
        S_n\to M_{\infty}+\sum_{k=1}^{\infty}\frac{1}{ 2^k\lambda }=M_{\infty}+\frac{1}{ \lambda }
    \end{equation}
    qui est presque sûrement finie.
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Temps d'arrêt}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    Soit \( (\Omega,\tribF_n,\tribF,P)\) un espace de probabilité filtré. Une application \( T\colon \Omega\to \bar \eN\) est un \defe{temps d'arrêt}{temps d'arrêt} adapté à la filtration \( (\tribF_n)\) si pour tout \( n\in \eN\) nous avons \( \{ T\leq n \}\in\tribF_n\).

    Le temps d'arrêt $T$ est \defe{borné}{borné!temps d'arrêt} si il existe \( k\in \eN\) tel que \( T(\omega)\leq k\) pour presque tout \( \omega\in \Omega\).
\end{definition}

\begin{theorem}[Théorème d'arrêt borné\cite{PMCmartinLP}]
    Soit \( (X_n)\) une sur-martingale et \( T_1\leq T_2\), deux temps d'arrêts bornés. Alors
    \begin{enumerate}
        \item
            les variables aléatoires \( X_{T_1}\) et \( X_{T_2}\) sont intégrables,
        \item
            \( E(X_{T_2}|  \sigma(T_1) )\leq X_{T_1}\) presque sûrement.
    \end{enumerate}
    Si par contre \( (X_n)\) est une martingale alors \( X_{T_1}\) et \( X_{T_2}\) sont bornées, et \( E(X_{T_2}|\sigma(T_1))=X_{T_1}\).
\end{theorem}

\begin{theorem}[Théorème d'arrêt]   
    Soit \( (X_n)\) une martingale et un temps d'arrêt \( T\). Nous supposons que 
    \begin{enumerate}
        \item
            \( P(T<\infty)=1\)
        \item
            \( E(| X_T |)<\infty\)
        \item
            \( \lim_{n\to \infty} E(X_n\mtu_{T>n})=0\).
    \end{enumerate}
    Alors
    \begin{equation}
        E(X_T)=E(X_0).
    \end{equation}
\end{theorem}

\begin{theorem}[Premier théorème d'arrêt de Doob\cite{FUFFxBX}] \label{ThoZTrdjtZ}
    Soit \( (X_n)\) une martingale et \( T\) un temps d'arrêt; tous deux pour la filtration \( (\tribF_n)\). Nous supposons qu'une des trois propriétés suivantes soit vérifiée :
    \begin{enumerate}
        \item
            \( T\) est presque sûrement bornée.
        \item
            \( E(T)<\infty\) et il existe une constante \( c\) telle que
            \begin{equation}
                E\big( | X_{n+1}-X_n |\,|\tribF_n \big)\leq c
            \end{equation}
            sur l'événement \( \{ T\geq n \}\).
        \item   \label{ItemQVWZuBkiii}
            Il existe une constante \( c\) telle que \( | X_{T\wedge n} |\leq c\) presque sûrement\footnote{Il est d'usage assez classique de noter \( a\wedge b\)\nomenclature[P]{\( a\wedge b\)}{\( \min(a,b)\)} le minimum de \( a\) et \( b\).}.
    \end{enumerate}
    Alors \( X_T\) est une variable aléatoire presque sûrement bien définie nous avons
    \begin{equation}
        E(T_T)=E(X_0).
    \end{equation}
    Si \( (X_n)\) est une sur-martingale, alors la conclusion est \( E(X_T)\leq E(X_0)\) et si \( (X_n)\) est une sous-martingale, la conclusion est \( E(X_T)\geq E(X_0)\).
\end{theorem}
%TODO : la preuve est sur la page de wikipédia https://en.wikipedia.org/wiki/Optional_stopping_theorem

\begin{remark}
    Sous l'hypothèse \ref{ItemQVWZuBkiii}, il est possible d'avoir \( T=\infty\) sur un ensemble de mesure non nulle. Sur cet ensemble, la variable aléatoire \( X_T\) doit être définie de façon plus fine.
\end{remark}

\begin{probleme}
    D'après la \href{https://en.wikipedia.org/wiki/Talk:Optional_stopping_theorem}{page de discussion} de l'article sur Wikipédia, il semblerait que la seconde condition soit mal énoncée. Je n'ai pas vérifié.
\end{probleme}
% Lorsque cela sera fait, il faut enlever la question de la liste.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Martingale terminée}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    Nous disons que la martingale \( (M_n)_{n\geq 1}\) est \defe{terminée}{terminée!martingale} si il existe \( M\in L^1(\Omega,\tribA,P)\) telle que \( M_n=E(M|\tribA_n)\) pour tout \( n>1\).
\end{definition}

\begin{definition}  \label{DefOZlZnse}
    Un ensemble \( H\subset L^1(\Omega,\mu)\) est \defe{équi-intégrable}{équi-intégrable} si
    \begin{equation}
        \lim_{a\to \infty}\left( \sup_{f\in H}\int_{  | f |>a   }| f(x) |d\mu(x) \right)=0.
    \end{equation}
\end{definition}
Notons dans cette définition que vu que \( f\in L^1\) nous avons toujours
\begin{equation}
    \lim_{a\to \infty}\int_{| f |>a}| f(x) |d\mu(x)=0.
\end{equation}
L'équi-intégrabilité donne une sorte d'uniformité en \( f\) de cette limite.

\begin{theorem} \label{ThoEFbpVXb}
    Si \( (M_n)\) est une martingale, nous avons équivalence entre
    \begin{enumerate}
        \item
            \( (M_n)\) converge dans \( L^1\);
        \item
            \( (M_n)\) est terminée;
        \item
            l'ensemble \( \{ M_n \}_{n\geq 1}\) est équi-intégrable.
    \end{enumerate}
\end{theorem}
%TODO : une preuve.

Attention : en vertu de la proposition \ref{PropWoywYG} et surtout de l'exemple \ref{ExPOmxICc}, la convergence \( L^1\) n'implique pas la convergence presque partout.

\begin{theorem}[Théorème de Doob\cite{ProbaDanielLi}]   \label{ThoHBvnTRk}
    À propos de convergence de martingales.
    \begin{enumerate}
        \item
            Toute martingale terminée converge presque sûrement et pour la norme \( L^1\).
        \item
            Toute martingale bornée dans \( L^2\) converge presque sûrement et pour la norme \( L^2\).
    \end{enumerate}
\end{theorem}
\index{théorème!Doob}
\index{convergence!de martingales}
% TODO : la preuve est dans la référence.

\begin{proposition}[\cite{HPVCqkr}]
    Soit \( (M_n)\) une martingale et \( T\) un temps d'arrêt (pour la même filtration \( (\tribB_n)\)). Alors le processus \( V_n=M_{n\wedge T}\) est une martingale.
\end{proposition}

\begin{proof}
    Nous décomposons \( V_n\) de la façon suivante :
    \begin{equation}    \label{EqYJjUZrv}
        V_n=M_{n\wedge T}=M_n\mtu_{T\geq n}+M_T\mtu_{T<n}=M_n\mtu_{T\geq n}+\sum_{k< n}M_k\mtu_{T=k}.
    \end{equation}
    Nous avons, grâce au lemme \ref{LemBWNlKfA},
    \begin{equation}
        \{ T\geq n \}=\complement\{ T<n \}=\complement\{ T\leq n-1 \}\in\tribB_{n-1}
    \end{equation}
    et, si \( k\leq n\),
    \begin{equation}
        \{ T=k \}=\underbrace{\{ T\leq k \}}_{\in\tribB_k}\setminus\underbrace{\{ T\leq k-1 \}}_{\in\tribB_{k-1}}\in\tribB_k\subset\tribB_n.
    \end{equation}
    La forme \eqref{EqYJjUZrv} donne donc manifestement la \( \tribB_n\)-mesurabilité de \( V_n\).

    En ce qui concerne l'espérance nous devons calculer
    \begin{equation}
        E(V_{n+1}|\tribB_n)=E(M_{n+1}\mtu_{T\geq n+1}|\tribB_n)+\sum_{k<n+1}E(M_k\mtu_{T=k}|\tribB_n)
    \end{equation}
    où nous avons utilisé la proposition \ref{PropZBnsCgh}. Étant donné que \( \mtu_{T\geq n+1}\) et \( \mtu_{T=k}\) sont des variables aléatoires \( \tribB_n\)-mesurables nous pouvons utiliser la proposition \ref{PropRNBtfql} pour les sortir :
    \begin{equation}
        E(V_{n+1}|\tribB_n)=\mtu_{T\geq n+1}M_n+\sum_{k\leq n}\mtu_{T=k}M_k=M_{T\wedge n}=V_n.
    \end{equation}
    Pour cela nous avons utilisé \( E(M_{n+1}|\tribB_n)=M_n\) (parce que \( (M_n)\) est une martingale) et \( E(M_k|\tribB_n)=M_k\) parce que \( M_k\) est \( \tribB_n\)-mesurable.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Décomposition de martingales}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}[Processus croissant prévisible\cite{PMCmartinLP}]
    Un processus \( X_n\) adapté à la filtration \( \tribF_n\) est un processus \defe{croissant prévisible}{processus!croissant prévisible} si
    \begin{enumerate}
        \item
            \( A_0=0\)
        \item
            \( A_n\leq A_{n+1}\); c'est cette condition qui correspond à «croissant»,
        \item
            \( A_{n+1}\) est \( \tribF_n\)-mesurable; c'est cette condition qui correspond à «prévisible».
    \end{enumerate}
\end{definition}

\begin{proposition}[Décomposition de Doob pour une sous-martingale\cite{PMCmartinLP}]
    Toute sous-martingale \( (X_n)\) s'écrit de façon unique sous la forme 
    \begin{equation}\label{EqCCsAwbZ}
        X_n=M_n+A_n 
    \end{equation}
    où \( (M_n)\) est une martingale et \( (A_n)\) est un processus croissant prévisible.
\end{proposition}

\begin{proof}
    Nous considérons le processus
    \begin{subequations}
        \begin{numcases}{}
            A_0=0\\
            A_{n+1}=A_n+E(X_{n+1}-X_n|\tribF_n).
        \end{numcases}
    \end{subequations}
    Nous vérifions que cela est un processus croissant prévisible. D'abord \( E(X_{n+1}-X_n|\tribF_n)=E(X_{n+1}|\tribF_n)-E(X_n|\tribF_n)\). Le second terme est égal à \( X_n\) parce que cette variable aléatoire est \( \tribF_n\)-mesurable tandis que \( (X_n)\) étant une sous-martingale nous avons \( E(X_{n+1}|\tribF_n)\geq X_n\). Nous avons donc bien \( A_{n+1}\geq A_n\) et le processus \( (A_n)\) est croissant.

    En ce qui concerne la prévisibilité nous devons prouver que \( A_{n+1}\) est \( \tribF_n\)-mesurable. D'une part \( A_n\) est \( \tribF_n\)-mesurable et d'autre part par définition de l'espérance conditionnelle, la variable aléatoire \( E(X_{n+1}-X_n|\tribF_n)\) est également \( \tribF_n\)-mesurable.

    Nous posons alors \( M_n=X_n-A_n\) et nous devons prouver que cela est une martingale. Nous avons
    \begin{equation}
        E(M_{n+1}-M_n|\tribF_n)=E(X_{n+1}-X_n|\tribF_n)-E(A_{n+1}-A_n|\tribF_n).
    \end{equation}
    Le second terme vaut
    \begin{equation}
        E(A_{n+1}-A_n|\tribF_n)=E\Big( E(X_{n+1}-X_n|\tribF_n)|\tribF_n \Big)=E(X_{n+1}-X_n|\tribF_n)
    \end{equation}
    par la proposition \ref{PropRGcscXj}. Le processus \( (M_n)\) est donc une martingale. La preuve de l'existence d'une décomposition \eqref{EqCCsAwbZ} est achevée.

    Nous passons maintenant à l'unicité en posant \( X_n=M_n+A_n=M'_n+A'_n\). Nous avons \( A_0=A'_0=0\) et \( A'_n=X_n-M'_n\), donc
    \begin{equation}
        A'_{n+1}-A'_n=X_{n+1}-X_n+M'_{n+1}-M'_n=X_{n+1}-X_n-(M'_{n+1}-M'_n).
    \end{equation}
    Nous appliquons \( E(.|\tribF_n)\) des deux côtés de cette égalité :
    \begin{equation}
        \underbrace{E(A'_{n+1}-A'_n|\tribF_n)}_{=A'_{n+1}-A'_n}=E(X_{n+1}-X_n|\tribF_n)-\underbrace{E(M'_{n+1}-M'_n|\tribF_n)}_{=0}.
    \end{equation}
    Nous avons utilisé le que que \( (M_n)\) étant une martingale, \( E(M_{n+1}-M_n\tribF_n)=0\), et idem avec \( (M_n')\). Donc
    \begin{equation}
        A'_{n+1}-A'_n=E(X_{n+1}-X_n|\tribF_n)=E(M_{n+1}-M_n|\tribF_n)+E(A_{n+1}-A_n|\tribF_n)=A_{n+1}-A_n.
    \end{equation}
    Nous avons donc montré que \( A_{n+1}-A_n=A'_{n+1}-A'_n\) et donc que \( A_n=A'_n\) pour tout \( n\). Nous en déduisons immédiatement que \( M_n=M'_n\) pour tout \( n\) et l'unicité de la décomposition.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Problème de la ruine du joueur}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous considérons un joueur compulsif qui joue à un jeu très simple\footnote{Le gros des choses dites à propos de la ruine du joueur provient de \cite{KXjFWKA}.} : il joue à pile ou face contre la banque avec une pièce truquée. Si pile sort, la banque donne \( 1\) au joueur et si c'est face, c'est le joueur qui donne \( 1\) à la banque. Nous nommons \( a\) la fortune initiale du joueur,  \( b\) celle de la banque et \( p\) la probabilité d'obtenir pile.

Nous supposons que le jeu se poursuit jusqu'à la ruine du joueur ou de la banque. La modélisation est comme suit : nous considérons \( (Y_n)\) une suite de variables aléatoires indépendantes et identiquement distribuées de loi
\begin{equation}
    Y_n\sim p\delta_1+(1-p)\delta_{-1}.
\end{equation}
C'est le résultat financier pour le joueur du \( n\Ieme\) lancé. La fortune du joueur au bout de \( n\) lancés est la variable aléatoire
\begin{equation}
    S_n=a+\sum_{j=1}^nY_j.
\end{equation}
Nous notons \( Y_0=a\).

Nous considérons la filtration
\begin{equation}
    \tribA_n=\sigma\big( S_i\tq 0\leq i\leq n \big)=\sigma\big( Y_i\tq 0\leq i\leq n \big),
\end{equation}
et le temps d'arrêt du jeu :
\begin{equation}
    T=\inf\{ n\geq 1\tq S_n\in\{ 0,a+b \} \};
\end{equation}
c'est le temps qu'il faut pour que tout l'argent appartienne soit au joueur soit à la banque.

Nous voulons étudier les paramètres suivants :
\begin{enumerate}
    \item
        \( \rho=P(S_T=a+b)\), c'est à dire la probabilité que ce soit le joueur qui gagne contre la banque.
    \item
        \( P(T<\infty)\), c'est à dire la probabilité que le jeu se finisse.
    \item
        \( E(T)\), la durée moyenne du jeu.
\end{enumerate}

\begin{lemma}   \label{LemEOAmVyZ}
    Le processus \( S_n\) du problème de la ruine du joueur est vérifie
    \begin{equation}    \label{EqZQtwVMXyiB}
        E(S_n|\tribA_{n-1})=S_{n-1}+p-q.
    \end{equation}
    De plus le processus \( S_n\) est
    \begin{enumerate}
        \item
            une martingale si \( p=q=\frac{ 1 }{2}\),
        \item
            une sous-martingale si \( p>q\).
    \end{enumerate}
\end{lemma}

\begin{proof}
    Pour \( n\geq 1 \) nous avons
    \begin{equation}
        E(S_n|\tribA_{n-1})=a+\sum_{j=1}^nE(Y_j|\tribA_{n-1})=a+\sum_{j=1}^{n-1}E(Y_j|\tribA_{n-1})+E(Y_n|\tribA_{n-1}).
    \end{equation}
    Si \( j\leq n-1\) alors \( Y_j\in m(\tribA_{n-1})\). Mais nous savons que si \( X\) est \( \tribF\)-mesurable, alors \( E(X|\tribF)=X\) (c'est la définition de l'espérance conditionnelle), donc \( \sum_{j=1}^{n-1}E(Y_j|\tribA_{n-1})=\sum_{j=1}^{n-1}Y_j\).

    En ce qui concerne le terme \( j=n\) nous utilisons le fait que \( \sigma(Y_n)\) soit une tribu indépendante de \( \tribA_{n-1}\); nous avons donc au final pour tout \( j\) que \( E(Y_j|\tribA_{n-1}E(Y_j)=p-q\). Nous avons donc
    \begin{equation}
        E(S_n|\tribA_{n-1})=S_{n-1}+p-q.
    \end{equation}
    Si \( p=q=\frac{ 1 }{2}\) alors c'est une martingale, et si \( p>q\) c'est une sous-martingale.
\end{proof}

\begin{lemma}   \label{LemXDlNxtE}
    La variable aléatoire \( T\) est un temps d'arrêt.
\end{lemma}

\begin{proof}
    Par définition \( T=\inf\{ n\geq 1\tq S_n\in\{ 0,a+b \} \}\). Vu que les variables aléatoires \( S_i\) avec \( i\leq n\) sont \( \tribF_n\)-mesurables, les ensembles \( \big\{ S_k\notin\{ 0,a+b \} \big\}\) avec \( k\leq n\) sont \( \tribF_n\)-mesurables. Donc les ensembles
    \begin{equation}
        \{ T=n \}=\bigcap_{k\leq n}\big\{ S_k\notin\{ 0,a+b \} \big\}\cap\big\{ S_n\in\{ 0,a+b \} \big\}
    \end{equation}
    sont \( \tribF_n\)-mesurables. Nous en concluons que l'ensemble \( \{ T\leq n \}\) est également mesurable.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Le cas où la pièce est truquée}
%---------------------------------------------------------------------------------------------------------------------------

Nous supposons être dans le cas \( p>q\). 

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Introduction d'une martingale}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Considérons le processus
\begin{subequations}
    \begin{numcases}{}
        A_0=0\\
        A_n=A_{n-1}+E(S_n-S_{n-1}|\tribA_{n-1}).
    \end{numcases}
\end{subequations}
Vu que \( E(S_n|\tribA_{n-1})=S_{n-1}+p-q\) (lemme \ref{LemEOAmVyZ}) et que \( E(S_{n-1}|\tribA_{n-1})=S_{n-1}\) (parce que \( S_{n-1}\) est dans la tribu de \( \tribA_{n-1}\)), nous avons \( A_n=A_{n-1}+(p-q)\) et donc
\begin{equation}
    A_n=n(p-q).
\end{equation}
Ce processus \( (A_n)\) est croissant et prévisible. Nous introduisons le processus
\begin{equation}    \label{EqMUajTwl}
    M_n=S_n-A_n
\end{equation}
et nous montrons que c'est une martingale\footnote{Ceci est un peu le contraire de la décomposition de Doob.}. Nous conditionnons la définition \eqref{EqMUajTwl} par rapport à \( \tribA_{n-1}\) :
\begin{subequations}
    \begin{align}
        E(M_n\tribA_{n-1})&=E(S_n|\tribA_{n-1})-\underbrace{E(A_n|\tribA_{n-1})}_{=A_n}\\
        &=A_n-A_{n-1}+E(S_{n-1}|\tribA_{n-1})-A_n\\
        &=E(S_{n-1}|\tribA_{n-1})-A_{n-1}.
    \end{align}
\end{subequations}
Mais \( S_{n-1}\) est \( \tribA_{n-1}\)-mesurable, donc \( E(S_{n-1}|\tribA_{n-1})=S_{n-1}\) et
\begin{equation}
    E(M_n|\tribA_{n-1})=S_{n-1}-A_{n-1}=M_{n-1},
\end{equation}
ce qui signifie que \( (M_n)\) est une martingale.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Finitude du temps d'arrêt}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Nous montrons maintenant, en étudiant \( M_{T\wedge n}\) que \( T\) est intégrable et nous prouvons que \( P(T=\infty)=0\).

Nous voulons maintenant étudier la variable aléatoire \( M_{T\wedge n}\) où nous rappelons que le lemme \ref{LemXDlNxtE} nous indique que \( T\) est un temps d'arrêt. Le temps d'arrêt \( T\wedge n\) est borné (par \( n\) évidemment) et nous pouvons donc lui appliquer le théorème d'arrêt \ref{ThoZTrdjtZ} pour dire que 
\begin{equation}
    E(M_{T\wedge n})=E(M_0).
\end{equation}
Le membre de droite est simple parce que \( M_0=S_0-A_0=S_0=a\) parce que c'est l'argent de départ du joueur. Pour l'autre :
\begin{equation}    \label{EqKEkJvBg}
    E(M_{T\wedge n})=E(S_{T\wedge n})-E(A_{T\wedge n}).
\end{equation}
D'une part, \( E(A_{T\wedge n})=E\big( (T\wedge n)(p-q) \big)\) et d'autre part, \( E(S_{T\wedge n})\leq a+b\) parce que \( S_T\) vaut zéro ou \( a+b\) (avec des probabilités encore inconnues\footnote{Mais on y travaille.}). En combinant avec ce qui était dit juste au-dessus et remarquant que \( (p-q)E(T\wedge n)\geq 0\) nous pouvons écrire
\begin{equation}    \label{EqHWtxOcW}
    0\leq (p-q)E(T\wedge n)\leq b.
\end{equation}
La suite de variables aléatoires \( T\wedge n\) est donc croissante, positive et intégrable\footnote{Je rappelle que les constantes sont des fonctions intégrables sur \( \Omega\). Oui, je sais, quand on est habitué à faire de l'analyse sur \( \eR^n\) c'est un truc qu'on perd toujours un peu de vue.} et donc nous avons du travail pour le théorème de la convergence monotone \ref{ThoConvMonFtBoVh}. La variable aléatoire \( T\) est alors mesurable et
\begin{equation}    \label{EqABPXmgr}
    \lim_{n\to \infty} E(T\wedge n)=E(T).
\end{equation}
Notons que nous n'avons pas encore prouvé que \( E(T)<\infty\), mais en passant à la limite dans \eqref{EqHWtxOcW} nous écrivons
\begin{equation}
    0\leq (p-q)E(T)\leq b.
\end{equation}
Maintenant nous avons prouvé que \( T\) est intégrable et même \( L^1\). Par conséquent 
\begin{equation}
    P(T=\infty)=0.
\end{equation}
Le jeu se termine donc presque certainement après un temps fini.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Temps moyen de jeu}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Nous commençons par prouver que \( S_{T\wedge n}\stackrel{p.s.}{\longrightarrow}S_T\). Vu que \( T \) est presque sûrement finie, il suffit de prouver que
\begin{equation}    \label{EqRVoKxsN}
    (S_{T\wedge n})(\omega)\to S_T(\omega)
\end{equation}
pour tout \( \omega\) tel que \( T(\omega)=k\) pour tout \( k\in \eN\). Soit donc \( \omega\in \Omega\) tel que \( T(\omega)=k\) et \( n>k\). Nous avons
\begin{equation}
    (S_{T\wedge n})(\omega)=S_{T(\omega)\wedge n}(\omega)=S_k(\omega)=(S_T)(\omega).
\end{equation}

\begin{remark}
    Notons la différence subtile entre \( S_T(\omega)\) et \( (S_T)(\omega)\). La première est la variable aléatoire
    \begin{equation}
        \omega'\mapsto S_{T(\omega')}(\omega)
    \end{equation}
    et la seconde est le nombre \( S_{T(\omega)}(\omega)\).
\end{remark}

Nous avons les bornes \( 0\leq S_{T\wedge n}\leq a+b\) et comme \( a+b\) est intégrable, \( S_{T\wedge n}\) l'est aussi et nous pouvons parler de \( E(S_{T\wedge n})\). Repartons de \eqref{EqKEkJvBg} :
\begin{equation}    \label{EqLKdCOQg}
    a=E(M_0)=E(M_{T\wedge n})=E(S_{T\wedge n})-E(A_{T\wedge n})=E(S_{T\wedge n})-(p-q)E(T\wedge n).
\end{equation}
La variable aléatoire \( S_{T\wedge n}\) est majorée par \( a+b\) indépendamment de \( n\); donc le théorème de la convergence dominée \ref{ThoConvDomLebVdhsTf} donne \( \lim_{n\to \infty} E(S_{T\wedge n})=E(S_T)\). En ce qui concerne le second terme, la convergence dominée ne fonctionne pas parce que \( T\wedge n\) n'est pas a priori majoré par quelque chose d'indépendant de \( n\), mais le théorème de la convergence monotone donne \( \lim_{n\to \infty} E(T\wedge n)=E(T)\). Au final en passant à la limite dans \eqref{EqLKdCOQg} nous avons
\begin{equation}
    a=E(S_T)-(p-q)E(T).
\end{equation}
Étant donné que \( T>0\) et \( p-q>0\) nous pouvons récrire cela sous la forme
\begin{equation}
    0\leq (p-q)E(T)=E(S_T)-a.
\end{equation}
Par définition de \( T\) nous avons aussi
\begin{equation}
    E(S_T)=(a+b)P(S_T=a+b)+0\cdot P(S_T=0)=\rho(a+b).
\end{equation}
Nous déduisons
\begin{equation}
    E(T)=\frac{ (a+b)\rho-a }{ p-q }.
\end{equation}
Ne crions pas victoire trop vite : nous n'avons pas encore d'expression de \( \rho=P(S_T=a+b)\). Le temps moyen de jeu n'est donc pas encore tout à fait connu.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Probabilité de victoire du joueur}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Nous avons besoin d'exprimer \( \rho\) en termes de \( a\), \( b\) et \( p\). Pour cela nous introduisons la variable aléatoire
\begin{equation}
    U_n=\left( \frac{ p }{ q } \right)^{S_n}.
\end{equation}
Nous commençons par prouver que c'est une martingale en calculant
\begin{equation}
    E(U_n|\tribA_{n-1})=E\left( \left( \frac{ q }{ p } \right)^{S_n-1}\left( \frac{ q }{ p } \right)^{Y_n}|\tribA_{n-1} \right)
\end{equation}
Nous utilisons la proposition \ref{PropRNBtfql}. Dans notre cas, \( S_{n-1}\) et \( Y_n\) sont des variables aléatoires \( \tribA_n\)-mesurables; la variable aléatoire \( Y_n\) est même \( \tribA_{n-1}\)-mesurable et sort donc du conditionnement; nous avons donc
\begin{equation}    \label{EqWTkXcEK}
    E(U_n|\tribA_{n-1})=\left( \frac{ q }{ p } \right)^{S_{n-1}}E\left( \left( \frac{ q }{ p } \right)^{Y_n} \right)
\end{equation}
Nous allons utiliser le théorème de transfert \ref{PropintdPintdPXeR} : 
\begin{equation}
    E(s^{Y_n})=\int_{\Omega}s^{Y_n(\omega)}DP(\omega)=\int_{Y_n=1}sdP(\omega)+\int_{Y_n=-1}\frac{1}{ s }dP(\omega).
\end{equation}
Mais nous savons que \( P(Y_n=1)=p\) et \( P(Y_n=-1)=1-p=q\), donc
\begin{equation}
    E(s^{Y_n})=ps+\frac{ 1-p }{ s }
\end{equation}
et
\begin{equation}
    E\left( \left( \frac{ p }{ q } \right)^{Y_n} \right)=p+q=1.
\end{equation}
Donc
\begin{equation}
    E(U_n|\tribA_{n-1})=\left( \frac{ q }{ p } \right)^{S_n-1}=U_{n-1},
\end{equation}
ce qui prouve que \( (U_n)\) est une martingale.

Par définition nous avons toujours \( S_n\geq 0\) tant que \( n\leq T\)\footnote{Pour \( n>T\) le jeu est terminé, donc on ne se pose pas la question.}, donc \( U_{T\wedge n}\in\mathopen[ 0 , 1 \mathclose]\). Il est donc évident que si \( a\geq 1\) nous avons
\begin{equation}
    \int_{| U_{T\wedge n} |>a}| U_{T\wedge n} |dP=0
\end{equation}
parce que le domaine d'intégration est vide. Donc les variables aléatoires \( V_n=U_{T\wedge n}\) sont équi-intégrables\footnote{Définition \ref{DefOZlZnse}.} et le théorème \ref{ThoEFbpVXb} montre que la martingale \( (V_n)\) est terminée; par ricochet\footnote{Nous rappellons que la convergence \( L^1\) n'implique pas la convergence presque partout.} le théorème de Doob \ref{ThoHBvnTRk} montre qu'il existe une variable aléatoire \( X\) telle que $V_n\stackrel{p.s.}{\longrightarrow}X$. Nous allons prouver que \( X=U_T\) presque partout. Nous savions déjà (voir l'équation \eqref{EqRVoKxsN} et ses alentours) que
\begin{equation}
    S_{n\wedge T}\stackrel{p.s.}{\longrightarrow}S_T.
\end{equation}
Nous avons alors (au sens du presque sûrement) :
\begin{equation}
    \lim_{n\to \infty} V_n=\lim_{n\to \infty} U_{T\wedge n}=\lim_{n\to \infty} \left( \frac{ q }{ p } \right)^{S_{T\wedge n}}=\left( \frac{ q }{ p } \right))^{S_T}=U_T.
\end{equation}
Donc par unicité de la limite presque partout nous avons \( X=U_T\) presque partout. Par le théorème de transfert \ref{PropintdPintdPXeR} nous évaluons
\begin{equation}
    E(U_T)=\left( \frac{ q }{ p } \right)^0P(S_T=0)+\left( \frac{ q }{ p } \right)^{a+b}P(S_T=a+b)=(1-\rho)+\left( \frac{ q }{ p } \right)^{a+b}\rho.
\end{equation}
<++>

