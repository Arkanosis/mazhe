% This is part of Mes notes de mathématique
% Copyright (c) 2011-2012
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Parties libres, génératrices et bases}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    Si \( E\) est un espace vectoriel, une partie finie \( (u_i)_{1\leq i\leq n}\) de \( E\) est \defe{libre}{libre!partie} si l'égalité
    \begin{equation}
        a_1 u_1+\ldots +a_nu_n=0
    \end{equation}
    implique \( a_i=0\) pour tout \( i\).

    Une partie infinie est libre si toute ses parties finies le sont.
\end{definition}
La définition de liberté dans le cas des parties infinies a son importance lorsqu'on parle d'espaces vectoriels de dimension infinies (en dimension finie, aucune partie infinie n'est libre) parce que cela fera une différence entre une base algébrique et une base hilbertienne par exemple.

\begin{theorem} \label{ThoBaseIncompjblieG}
    Soit \( E\) un espace vectoriel de dimension finie et \( \{ e_i \}_{i\in I}\) une partie génératrice de \( E\).

    \begin{enumerate}
        \item
            Il existe \( J\subset I\) tel que \( \{ e_i \}_{i\in J}\) est une base. Autrement dit : de toute partie génératrice nous pouvons extraire une base.
        \item
            Soit \( \{ f_1,\ldots, f_l \}\) une partie libre. Alors nous pouvons la compléter en utilisant des éléments \( e_i\). C'est à dire qu'il existe \( J\subset I\) tel que \( \{ f_k \}\cup\{ e_i \}_{i\in J}\) soit une base.
    \end{enumerate}
\end{theorem}

Soit \( F\) un sous espace vectoriel de l'espace vectoriel \( E\). La \defe{codimension}{codimension} de \( F\) dans \( E\) est
\begin{equation}
    \codim_E(F)=\dim(E/F).
\end{equation}

Le théorème suivant est valable également en dimension infinie; ce sera une des rares incursions en dimension infinie de ce chapitre.
\begin{theorem}[Théorème du rang]\index{théorème!du rang}       \label{ThoGkkffA}
       Soient \( E\) et \( F\) deux espaces vectoriels (de dimensions finies ou non) et soit \( f\colon E\to F\) une application linéaire. Alors le rang de \( f\) est égal à la codimension du noyau, c'est à dire
       \begin{equation}
           \rang(f)+\dim\ker f=\dim E.
       \end{equation}

       Dans le cas de dimension infinie affin d'éviter les problèmes d'arithmétique avec l'infini nous énonçons le théorème en disant que si \( (u_s)_{s\in S}\) est une base de \( \ker f\) et si \( \big( f(v_t) \big)_{t\in T}\) est une base de \( \Image(f)\) alors  \( (u_s)_{s\in s}\cup (v_t)_{t\in T}\) est une base de \( E\).
\end{theorem}

\begin{proof}
    Nous devons montrer que 
    \begin{equation}
          (u_s)_{s\in S}\cup (v_t)_{t\in T}
    \end{equation}
    est libre et générateur.

    Soit \( x\in E\). Nous définissons les nombres \( x_t\) par la décomposition de \( f(x)\) dans la base \( \big( f(v_t) \big)\) :
    \begin{equation}
        f(x)=\sum_{t\in T}x_tf(v_t).
    \end{equation}
    Ensuite le vecteur \( x=\sum_tx_tv_t\) est dans le noyau de \( f\), par conséquent nous le décomposons dans la base \( (u_s)\) :
    \begin{equation}
        x-\sum_tx_tv_t=\sum_s\in S x_su_s.
    \end{equation}
    Par conséquent
    \begin{equation}
        x=\sum_sx_su_s+\sum_tx_tv_t.
    \end{equation}
    
    En ce qui concerne la liberté nous écrivons
    \begin{equation}
        \sum_tx_tv_t+\sum_sx_su_s=0.
    \end{equation}
    En appliquant \( f\) nous trouvons que 
    \begin{equation}
        \sum_tx_tf(v_t)=0
    \end{equation}
    et donc que les \( x_t\) doivent être nuls. Nous restons avec \( \sum_sx_su_s=0\) qui à son tour implique que \( x_s=0\).
\end{proof}
Un exemple d'utilisation de ce théorème en dimension infinie sera donné dans le cadre du théorème de Fréchet-Riesz, théorème \ref{ThoQgTovL}.

\begin{proposition}[\cite{RombaldiO}]   \label{PropTVKbxU}
    Soit \( E\), un espace vectoriel sur un corps infini et \( (F_k)_{k=1,\ldots, r}\), des sous-espaces vectoriels de \( E\) tels que \( \bigcup_{i=1}^rF_i=E\). Alors \( E=F_k\) pour un certain \( k\).

    Autrement dit, l'union finie de sous espaces propres ne peut être égal à l'espace complet.
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Dualité}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E\), un espace vectoriel, et \( F\) une sous espace de \( E\). L'\defe{orthogonal}{orthogonal!sous espace} de \( F\) est la partie \( F^{\perp}\subset E^*\) donnée par
\begin{equation}    \label{Eqiiyple}
    F^{\perp}=\{ \alpha\in E^*\tq \forall x\in F,\alpha(x)=0 \}.
\end{equation}
Cette définition d'orthogonal via le dual n'est pas du pur snobisme. En effet, la définition «usuelle» qui ne parle pas de dual,
\begin{equation}
    F^{\perp}=\{ y\in E\tq \forall x\in F,y\cdot x=0 \},
\end{equation}
demande la donnée d'un produit scalaire. Évidemment dans le cas de \( \eR^n\) munie du produit scalaire usuel et de l'identification usuelle entre \( \eR^n\) et \( (\eR^n)^*\) via une base, les deux notions d'orthogonal coïncident.

La définition \eqref{Eqiiyple} est intrinsèque : elle ne dépend que de la structure d'espace vectoriel.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Déterminants}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



\begin{lemma}   \label{LemcDOTzM}
    Soit \( \eK\) un corps fini autre que \( \eF_2\)\footnote{Je ne comprends pas très bien à quel moment joue cette hypothèse.}, soit un groupe abélien \( M\) et un morphisme \( \varphi\colon \GL(n,\eK)\to M\). Alors il existe un unique morphisme \( \delta\colon \eK^*\to M\) tel que \( \varphi=\delta\circ\det\).
\end{lemma}

\begin{proof}
    D'abord le groupe dérivé de \( \GL(n,\eK)\) est \( \SL(n,\eK)\) parce que les éléments de \( D\big( \GL(n,\eK) \big)\) sont de la forme \( ghg^{-1}h^{-1}\) dont le déterminant est \( 1\). De plus le groupe \( \SL(n,\eK)\) est normal dans \( \GL(n,\eK)\). Par conséquent \( \GL(n,\eK)/\SL(n,\eK)\) est un groupe et nous pouvons définir l'application relevée
    \begin{equation}
        \tilde \varphi\colon \frac{ \GL(n,\eK) }{ \SL(n,\eK) }\to M
    \end{equation}
    vérifiant \( \varphi=\tilde \varphi\circ\pi\) où \( \pi\) est la projection. 

    Nous pouvons faire la même chose avec l'application
    \begin{equation}
        \det\colon \GL(n,\eK)\to \eK^*
    \end{equation}
    qui est un morphisme de groupes dont le noyau est \( \SL(n,\eK)\). Cela nous donne une application
    \begin{equation}
        \tilde \det\colon \frac{ \Gl(n,\eK) }{ \SL(n,\eK) }\to \eK^*
    \end{equation}
    telle que \( \det=\tilde \det\circ\pi\). Cette application \( \tilde \det\) est un isomorphisme. En effet elle est surjective parce que le déterminant l'est et elle est injective parce que son noyau est précisément ce par quoi on prend le quotient. Par conséquent \( \tilde \det \) possède un inverse et nous pouvons écrire
    \begin{equation}
        \varphi=\tilde \varphi\circ\tilde \det^{-1}\circ\tilde \det\circ\pi.
    \end{equation}
    État donné que \( \tilde \det\circ\pi=\det\), nous avons alors \( \varphi=\delta\circ\det\) avec \( \delta=\tilde \varphi\circ\tilde \det^{-1}\). Ceci conclu la partie existence de la preuve.

    En ce qui concerne l'unicité, nous considérons \( \delta'\colon \eK^*\to M\) telle que \( \varphi=\delta'\circ\det\). Pour tout \( u\in \GL(n,\eK)\) nous avons \( \delta'(\det(u))=\varphi(u)=\delta(\det(u))\). L'application \( \det\) étant surjective depuis \( \GL(n,\eK)\) vers \( \eK^*\), nous avons \( \delta'=\delta\).
\end{proof}

\begin{theorem}
    Soit \( p\geq 3\) un nombre premier et \( V\), un \( \eF_p\)-espace vectoriel de dimension finie \( n\). Pour tout \( u\in\GL(V)\) nous avons
    \begin{equation}
        \epsilon(u)=\left(\frac{\det(u)}{p}\right).
    \end{equation}
\end{theorem}
Ici \( \epsilon\) est la signature de \( u \) vue comme une permutation des éléments de \( \eF_p\).

\begin{proof}
    Commençons par prouver que
    \begin{equation}
        \epsilon\colon \GL(V)\to \{ -1,1 \}.
    \end{equation}
    est un morphisme. Si nous notons \( \bar u\in S(V)\) l'élément du groupe symétrique correspondant à la matrice \( u\in \GL(V)\), alors nous avons \( \overline{ uv }=\bar u\circ\bar v\), et la signature étant un homomorphisme (proposition \ref{ProphIuJrC}), 
    \begin{equation}
        \epsilon(uv)=\epsilon(\bar u\circ\bar v)=\epsilon(\bar u)\epsilon(\bar v).
    \end{equation}
    Par ailleurs \( \{ -1,1 \}\) est abélien, donc le lemme \ref{LemcDOTzM} s'applique et nous pouvons considérer un morphisme \( \delta\colon \eF_p^*\to \{ -1,1 \}\) tel que \( \epsilon=\delta\circ\det\).

    Nous allons utiliser le lemme \ref{Lemoabzrn} pour montrer que \( \delta\) est le symbole de Legendre. Pour cela il nous faudrait trouver un \( x\in \eF_p^*\) tel que \( \delta(x)=-1\). Étant donné que \( \det\) est surjective, nous cherchons ce \( x\) sous la forme \( x=\det(u)\). Par conséquent nous aurions
    \begin{equation}
        \delta(x)=(\delta\circ\det)(u)=\epsilon(u),
    \end{equation}
    et notre problème revient à trouver une matrice \( u\in\GL(V)\) dont la permutation associée soit de signature \( -1\).

    Soit \( n=\dim V\); en conséquence de la proposition \ref{PropHfrNCB}\ref{ItemiEFRTg}, l'espace \( \eE_q=\eF_{p^n}\) est un \( \eF_p\)-espace vectoriel de dimension \( n\) et est donc isomorphe en tant qu'espace vectoriel à \( V\). Étant donné que \( \eF_q\) est un corps fini, nous savons que \( \eF_q^*\) est un groupe cyclique à \( q-1\) éléments. Soit \( y\), un générateur de \( \eF_q^*\) et l'application
    \begin{equation}
        \begin{aligned}
            \beta\colon \eF_q&\to \eF_q \\
            x&\mapsto yx. 
        \end{aligned}
    \end{equation}
    Cela est manifestement \( \eF_p\)-linéaire (ici \( y\) et \( x\) sont des classes de polynômes et \( \eF_p\) est le corps des coefficients). L'application \( \beta\) fixe zéro et à part zéro, agit comme le cycle
    \begin{equation}
        (1,y,y^2,\ldots, y^{q-2}).
    \end{equation}
    Nous savons qu'un cycle de longueur \( n\) est de signature \( (-1)^{n+1}\). Ici le cycle est de longueur \( q-1\) qui est pair (pare que \( p\geq 3\)) et par conséquent, l'application \( \beta\) est de signature \( -1\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Endomorphismes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynôme caractéristique}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( A\) un anneau commutatif et \( \eK\), un corps commutatif. L'injection canonique \( A\to A[X]\) se prolonge en une injection
\begin{equation}
   \eM(A)\to\eM\big( A[X] \big).
\end{equation}
Si \( u\in\eM_n(A)\), nous définissons le \defe{polynôme caractéristique de \( u\)}{polynôme!caractéristique}\index{caractéristique!polynôme} :
\begin{equation}
    \chi_u(X)=\det(X\mtu_n-u).
\end{equation} 
Ce faisons nous assimilons la matrice \( u\) et l'endomorphisme \( u\colon E\to E\) qu'elle définit. 

Nous avons un morphisme d'algèbre
\begin{equation}
    \begin{aligned}
        \varphi_u\colon\eK[X]&\to \End(E) \\
        P&\mapsto P(u). 
    \end{aligned}
\end{equation}
Cet endomorphisme ne peut pas être injectif parce que \(\eK[X]\) est de dimension infinie tandis que \( \End(E)\) est de dimension finie. Il possède donc un noyau, c'est à dire qu'il existe \( P\in\eK[X]\) tel que \( P(X)=0\). Le \defe{polynôme minimal}{polynôme!minimal} de \( u\) est le polynôme unitaire de plus petit degré qui annule \( u\). Nous le notons \( \mu_u\)\nomenclature[A]{\( \mu_A\)}{polynôme minimal de \( A\)} :
\begin{equation}
    \mu_u(u)=0.
\end{equation}

\begin{lemma}
    Le polynôme \( \chi_u\) est unitaire et de degré \( n\).
\end{lemma}

\begin{lemma}       \label{LemjcztYH}
    Soit \( u\) un endomorphisme et \( E_{\lambda}(u)\)\nomenclature[A]{\( E_{\lambda}(u)\)}{Espace propre de \( u\)} ses espaces propres. La somme des \( V_{\lambda}\) est directe.
\end{lemma}

\begin{proof}
    Soit \( v_i\in V_{\lambda_i}\) un choix de vecteurs propres de \( u\). Si la somme n'est pas directe, nous pouvons considérer une combinaison linéaire des \( v_i\) qui soit nulle :
    \begin{equation}
        v_1+\ldots+v_p=0.
    \end{equation}
    Appliquons \( (A-\lambda_1\mtu)\) à cette égalité :
    \begin{equation}
        (\lambda_2-\lambda_1)v_1+\ldots+(\lambda_p-\lambda_1)v_p=0.
    \end{equation}
    En appliquant encore successivement les opérateurs \( (A-\lambda_i\mtu)\) nous réduisons le nombre de termes jusqu'à obtenir \( v_p=0\).
\end{proof}


\begin{theorem}     \label{ThoNhbrUL}
    Soit \( E\) un \(\eK\)-espace vectoriel de dimension finie \( n\) et un endomorphisme \( u\in\End(E)\). Alors
    \begin{enumerate}
        \item
            Le polynôme caractéristique divise \( (\mu_u)^n\) dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes facteurs irréductibles dans \(\eK[X]\).
        \item
            Les polynômes caractéristiques et minimaux ont mêmes racines dans \(\eK[X]\).
        \item
            Le polynôme caractéristique est scindé si et seulement si le polynôme minimal est scindé.
    \end{enumerate}
\end{theorem}


Si \( \lambda\in\eK\) est une racine de \( \chi_u\), l'ordre de l'annulation est la \defe{multiplicité algébrique}{multiplicité!algébrique d'une valeur propre} de la valeur propre \( \lambda\) de \( u\).

\begin{theorem}
    Soit \( u\in\End(E)\) et \( \lambda\in\eK\). Les conditions suivantes sont équivalentes
    \begin{enumerate}
        \item\label{ItemeXHXhHi}
            \( \lambda\in\Spec(u)\)
        \item\label{ItemeXHXhHii}
            \( \chi_u(\lambda)=0\)
        \item\label{ItemeXHXhHiii}
            \( \mu_u(\lambda)=0\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    \ref{ItemeXHXhHi} \( \Leftrightarrow\) \ref{ItemeXHXhHii}. Dire que \( \lambda\) est dans le spectre de \( u\) signifie que l'opérateur \( u-\lambda\mtu\) n'est pas inversible, ce qui est équivalent à dire que \( \det(u-\lambda\mtu)\) est nul ou encore que \( \lambda\) est une racine du polynôme caractéristique de \( u\). 

    \ref{ItemeXHXhHii} \( \Leftrightarrow\) \ref{ItemeXHXhHiii}. Cela est une application directe du théorème \ref{ThoNhbrUL} qui précise que le polynôme caractéristique a les mêmes racines dans \(\eK\) que le polynôme minimal.
\end{proof}

\begin{lemma}
    Une matrice triangulaire supérieure avec des \( 1\) sur la diagonale n'est diagonalisable que si elle est diagonale (c'est à dire si elle est la matrice unité).
\end{lemma}

\begin{proof}
    Si \( A\) est une matrice triangulaire supérieure de taille \( n\) telle que \( A_{ii}=1\), alors \( \det(A-\lambda\mtu)=(1-\lambda)^n\), ce qui signifie que \( \Spec(A)=\{ 1 \}\). Pour la diagonaliser, il faudrait une matrice \( P\in\GL(n,\eK)\) telle que \( \mtu=P^{-1}AP\), ce qui est uniquement possible si \( A=\mtu\).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrices semblables}
%---------------------------------------------------------------------------------------------------------------------------

Sur l'ensemble \( \eM_n(\eK)\) des matrices \( n\times n\) à coefficients dans \(\eK\) nous introduisons la relation d'équivalence \( A\sim B\) si et seulement si il existe une matrice \( P\in\GL(n,\eK)\) telle que \( B=P^{-1}AP\). Deux matrices équivalentes en ce sens sont dites \defe{semblables}{semblables!matrices}.

Le polynôme caractéristique est un invariant sous les similitudes. En effet si \( P\) est une matrice inversible,
\begin{subequations}
    \begin{align}
        \chi_{PAP^{-1}}&=\det(PAP^{-1}-\lambda X)\\
        &=\det\big( P^{-1}(PAP^{-1}-\lambda X)P^{-1} \big)\\
        &=\det(A-\lambda X).
    \end{align}
\end{subequations}

La permutation de lignes ou de colonnes ne sont pas de similitudes, comme le montrent les exemples suivants :
\begin{equation}
    \begin{aligned}[]
        A&=\begin{pmatrix}
            1    &   2    \\ 
            3    &   4    
        \end{pmatrix}&
        B&=\begin{pmatrix}
            2    &   1    \\ 
            4    &   3    
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Nous avons \( \chi_A=x^2-5x-2\) tandis que \( \chi_B=x^2-5x+2\) alors que le polynôme caractéristique est un invariant de similitude.


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Polynômes d'endomorphismes}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( u\) un endomorphisme d'un \( \eK\)-espace vectoriel \( E\) et \( P\), un polynôme. Nous disons que \( P\) est un polynôme \defe{annulateur}{polynôme!annulateur} de \( u\) si \( P(u)=0\) en tant que endomorphisme de \( E\).

\begin{lemma}       \label{LemQWvhYb}
    Si \( P\) et \( Q\) sont des polynômes dans \( \eK[X]\) et si \( u\) est un endomorphisme d'un \( \eK\)-espace vectoriel \( E\), nous avons
    \begin{equation}
        (PQ)(u)=P(u)\circ Q(u).
    \end{equation}
\end{lemma}

\begin{proof}
    Si \( P=\sum_i a_iX^i\) et \( Q=\sum_j b_jX^j\), alors le coefficient de \( X^k\) dans \( PQ\) est
    \begin{equation}        \label{EqCoefGPyVcv}
        \sum_la_lb_{k-l}.
    \end{equation}
    Par conséquent \( (PQ)(u)\) contient \( \sum_la_lb_{k-l}u^k\). Par ailleurs \( P(u)\circ Q(u)\) est donné par
    \begin{equation}
        \sum_ia_iu^i\left( \sum_jb_ju^j \right)(x)=\sum_{ij}a_ib_ju^{i+j}(x).
    \end{equation}
    Le coefficient du terme en \( u^k\) est bien le même que celui donné par \eqref{EqCoefGPyVcv}.
\end{proof}

\begin{theorem}[Décomposition des noyaux ou lemme des noyaux]       \label{ThoDecompNoyayzzMWod}
    Soit \( u\) un endomorphisme du \( \eK\)-espace vectoriel \( E\). Soit \( P\in\eK[X]\) un polynôme tel que \( P(u)=0\). Nous supposons que \( P\) s'écrive comme le produit \( P=P_1\ldots P_n\) de polynômes deux à deux étrangers. Alors
    \begin{equation}
        E=\ker P_1(u)\oplus\ldots\oplus\ker P_n(u).
    \end{equation}
    De plus les projecteurs associés à cette décomposition sont des polynômes en \( u\).
\end{theorem}

\begin{proof}
    Nous posons 
    \begin{equation}
        Q_i=\prod_{j\neq i}P_i.
    \end{equation}
    Par le lemme \ref{LemuALZHn} ces polynômes sont étrangers entre eux et le théorème de Bezout (théorème \ref{ThoBezoutOuGmLB}) donne l'existence de polynômes \( R_i\) tels que
    \begin{equation}
        R_1Q_1+\ldots+R_nQ_n=1.
    \end{equation}
    Si nous appliquons cette égalité à \( u\) et ensuite à \( x\in E\) nous trouvons
    \begin{equation}        \label{EqqVcpUy}
        \sum_{i=1}^n(R_iQ_i)(u)(x)=x,
    \end{equation}
    et en particulier si nous posons \( E_i=\Image\big(P_iQ_i(u)\big)\) nous avons
    \begin{equation}
        E=\sum_{i=1}^nE_i.
    \end{equation}
    Cette dernière somme n'est éventuellement pas une somme directe. Si \( i\neq j\), alors \( Q_iQ_j\) est multiple de \( P\) et nous avons, en utilisant le lemme \ref{LemQWvhYb}, 
    \begin{equation}
        (R_iQ_i)(u)\circ (R_jQ_j)(u)=\big( R_iQ_iR_jQ_j \big)(u)=S_{ij}(u)\circ P(u)=0
    \end{equation}
    où \( S_{ij}\) est un polynôme. 

    Nous pouvons voir \( E\) comme un \( \eK\)-module et appliquer le théorème \ref{ThoProjModpAlsUR}. Les opérateurs \( R_iQ_i(u)\) ont l'identité comme somme et sont orthogonaux, et nous avons donc la décomposition en somme directe :
    \begin{equation}
        E=\bigoplus_{i=1}^nR_iQ_i(u)E.
    \end{equation}

    Afin de terminer la preuve, nous devons montrer que \( R_iQ_i(u)E=\ker P_i(u)\). D'abord nous avons
    \begin{equation}
        P_iR_iQ_i(u)=(R_iP)(u)=R_i(u)\circ P(u)=0,
    \end{equation}
    par conséquent \( \Image(R_iQ_i(u))\subset \ker P_i(u)\). Pour obtenir l'inclusion inverse, nous reprenons l'équation \eqref{EqqVcpUy} avec \( x\in\ker P_i(u)\). Elle se réduit à
    \begin{equation}
        (R_iQ_i)(u)x=x.
    \end{equation}
    Par conséquent \( x\in\Image\big( R_iQ_i(u) \big)\).
\end{proof}

\begin{corollary}
    Soit \( E\), un \( \eK\)-espace vectoriel de dimension finie et \( f\), un endomorphisme semi-simple dont la décomposition du polynôme caractéristique \( \chi_f\) en facteurs irréductibles sur \( \eK[X]\) est \( \chi_f=M_1^{\alpha_1}\cdots M_r^{\alpha_r}\). Si \( F\) est un sous-espace stable par \( f\), alors
    \begin{equation}
        F=\bigoplus_{i=1}^r\ker M_i^{\alpha_i}(f)\cap F
    \end{equation}
\end{corollary}

\begin{proof}
    Nous posons \( E_i=\ker M_i^{\alpha_i}(f)\) et \( F_i=E_i\cap F\). Les polynômes \( M_i^{\alpha_i}\) sont deux à deux étrangers et \( \chi_f(f)=0\), donc le lemme des noyaux (\ref{ThoDecompNoyayzzMWod}) s'applique et
    \begin{equation}
        E=E_1\oplus\ldots\oplus E_r
    \end{equation}
    <++>
\end{proof}
<++>

\begin{proposition}     \label{PropAnnncEcCxj}
    Si \( P\) est un polynôme tel que \( P(u)=0\), alors le polynôme minimal \( \mu_u\) divise \( P\)
\end{proposition}

\begin{proof}
    L'ensemble \( I=\{ Q\in \eK[X]\tq Q(u)=0 \}\) est un idéal par le lemme \ref{LemQWvhYb}. Le polynôme minimal de \( u\) est un élément de degré plus bas dans \( I\) et par conséquent \( I=(\mu_u)\) par le théorème \ref{ThoCCHkoU}. Nous concluons que \( \mu_u\) divise tous les éléments de \( I\).
\end{proof}

\begin{lemma}
    Soit \( f\) un endomorphisme cyclique d'un espace vectoriel \( E\) de dimension finie et \( y\), un vecteur cyclique de \( f\). Alors le polynôme minimal de \( f\) en \( y\) est le polynôme minimal de \( f\).
\end{lemma}

\begin{proof}
    En utilisant les notations de la définition \ref{Decyyumy}, nous devons démontrer que \( \mu_{f,y}=\mu_f\). Bien entendu, \( \mu_f\in I_{y,f}\), donc \( \mu_{f,y}\) divise \( \mu_f\). Montrons que \( \mu_{f,y}\) est un polynôme annulateur de \( f\). Dans ce cas \( \mu_f\) divisera \( \mu_{f,y}\) et le lemme sera démontré.

    Le vecteur \( y\) étant cyclique, tout élément de \( E\) s'écrit sous la forme \( x=P(f)y\) où \( P\) est un polynôme (de degré égal à la dimension de \( E\)). En utilisant le lemme \ref{LemQWvhYb} nous avons
    \begin{equation}
            \mu_{f,y}(f)x=\big( \mu_{f,y}(f)\circ P(f) \big)y
            =\big( P(f)\circ \mu_{f,y}(f) \big)y
            =0.
    \end{equation}
\end{proof}

Si \( f\) est un endomorphisme de l'espace vectoriel \( E\) et si \( x\in E\), nous notons 
\begin{equation}
    E_{f,x}=\Span\{ f^k(x)\tq k\in \eN \}.
\end{equation}

\begin{proposition}[\cite{RombaldiO}]\label{PropNrZGhT}
    Soit \( f\), un endomorphisme de \( E\) et \( x\in E\). Alors
    \begin{enumerate}
        \item
            L'espace \( E_{f,x}\) est stable par \( f\).
        \item\label{ItemfzKOCo}
            L'espace \( E_{f,x}\) est de dimension
            \begin{equation}
                p_{f,x}=\dim E_{f,x}=\deg(\mu_{f,x})
            \end{equation}
            où \( \mu_{f,x}\) est le générateur unitaire de \( I_{f,x}\).
        \item   \label{ItemKHNExH}
            Le polynôme caractéristique de \( f|_{E_{f,x}}\) est \( \mu_{f,x}\).
        \item   \label{ItemHMviZw}
            Nous avons
            \begin{equation}
                \chi_{f|_{E_{f,x}}}(f)x=\mu_{f,x}(f)x=0.
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le fait que \( E_{f,x}\) soit stable par \( f\) est classique. Le point \ref{ItemHMviZw} est un une application du point \ref{ItemKHNExH}. Les deux gros morceaux sont donc les points \ref{ItemfzKOCo} et \ref{ItemKHNExH}.
\end{proof}
<++>

\begin{theorem}[Cayley-Hamlilton]\index{théorème!Cayley-Hamilton}
    Le polynôme caractéristique est un polynôme annulateur.
\end{theorem}

\begin{proof}
    Nous devons prouver que \( \chi_f(f)x=0\) pour tout \( x\in E\). Pour cela nous nous fixons un \( x\in E\), nous considérons l'espace \( E_{f,x}\) et \( \chi_{f,x}\), le polynôme caractéristique de \( f|_{E_{f,x}}\). Étant donné que \( E_{f,x}\) est stable par \( f\), le polynôme caractéristique de \( f|_{E_{j,x}}\) divise \( \chi_f\), c'est à dire qu'il existe un polynôme \( Q_x\) tel que
    \begin{equation}
        \chi_f=Q_x\chi_{f,x},
    \end{equation}
    et donc aussi
    \begin{equation}
        \chi_f(f)x=Q_x(f)\big( \chi_{f,x}(f)x \big)=0
    \end{equation}
    parce que la proposition \ref{PropNrZGhT} nous indique que \( \chi_{f,x}\) est un polynôme annulateur de \( f|_{E_{f,x}}\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Diagonalisation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Ici encore \( \eK\) est un corps commutatif.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes diagonalisables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LemgnaEOk}
    Soit \( F\) un sous espace stable par \( u\). Soit une décomposition du polynôme minimal
    \begin{equation}
        \mu_u=P_1^{n_1}\ldots P_r^{n_r}
    \end{equation}
    où les \( P_i\) sont des polynômes irréductibles unitaires distincts. Si nous posons \( E_i=\ker P_i^{n_i}\), alors
    \begin{equation}
        F=(F\cap E_1)\oplus\ldots \oplus(F\cap E_r).
    \end{equation}
\end{lemma}

\begin{theorem}     \label{ThoDigLEQEXR}
    Soit \( E\), un espace vectoriel de dimension \( n\) sur le corps commutatif \( \eK\) et \( u\in\End(E)\). Les propriétés suivantes sont équivalentes.
    \begin{enumerate}
        \item       \label{ItemThoDigLEQEXRi}
            Il existe un polynôme \( P\in\eK[X]\) non constant, scindé sur \(\eK\) dont toutes les racines sont simples tel que \( P(u)=0\).
        \item\label{ItemThoDigLEQEXRii}
            Le polynôme minimal \( \mu_u\) est scindé sur \(\eK\) et toutes ses racines sont simples
        \item\label{ItemThoDigLEQEXRiii}
            Tout sous espace de \( E\) possède un supplémentaire stable par \( u\).
        \item\label{ItemThoDigLEQEXRiv}
            L'endomorphisme \( u\) est diagonalisable.
    \end{enumerate}

\end{theorem}

\begin{proof}
    \ref{ItemThoDigLEQEXRi}\( \Rightarrow\)\ref{ItemThoDigLEQEXRii}. Étant donné que \( P(u)=0\), il est dans l'idéal des polynôme annulateurs de \( u\), et le polynôme minimal \( \mu_u\) le divise parce que l'idéal des polynôme annulateurs est généré par \( \mu_u\) par le théorème \ref{ThoCCHkoU}.

    \ref{ItemThoDigLEQEXRii}\( \Rightarrow\)\ref{ItemThoDigLEQEXRiv}. Étant donné que le polynôme minimal est scindé à racines simples, il s'écrit sous forme de produits de monômes tous disctincts, c'est à dire
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots(X-\lambda_r)
    \end{equation}
    où les \( \lambda_i\) sont des éléments distincts de \( \eK\). Étant donné que \( \mu_u(u)=0\), le théorème de décomposition des noyaux (théorème \ref{ThoDecompNoyayzzMWod}) nous enseigne que
    \begin{equation}
        E=\ker(u-\lambda_1)\oplus\ldots\oplus\ker(u-\lambda_r).
    \end{equation}
    Mais \( \ker(u-\lambda_i)\) est l'espace propre \( E_{\lambda_i}(u)\). Donc \( u\) est diagonalisable.

    \ref{ItemThoDigLEQEXRiv}\( \Rightarrow\)\ref{ItemThoDigLEQEXRiii}. Soit \( \{ e_1,\ldots, e_n \}\) une base qui diagonalise \( u\), soit \( F\) un sous espace de \( E\) un \( \{ f_1,\ldots, f_r \}\) une base de \( F\). Par le théorème \ref{ThoBaseIncompjblieG} (qui généralise le théorème de la base incomplète), nous pouvons compléter la base de \( F\) par des éléments de la base \( \{ e_i \}\). Le complément ainsi construit est invariant par \( u\).

    \ref{ItemThoDigLEQEXRiii}\( \Rightarrow\)\ref{ItemThoDigLEQEXRiv}. En dimension un, tout endomorphisme est diagonalisable, nous supposons donc que \( \dim E=n\geq 2\). Nous procédons par récurrence sur le nombre de vecteurs propres connus de \( u\). Supposons avoir déjà trouvé \( p\) vecteurs propres \( e_1,\ldots, e_p\) de \( u\). Considérons \( H\), un hyperplan qui contient les vecteurs \( e_1,\ldots, e_p\). Soit \( F\) un supplémentaire de \( H\) stable par \( u\); par construction \( \dim F=1\) et si \( e_{p+1}\in F\), il doit être vecteur propre de \( u\).

    \ref{ItemThoDigLEQEXRiv}\( \Rightarrow\)\ref{ItemThoDigLEQEXRi}. Nous supposons maintenant que \( u\) est diagonalisable. Soient \( \lambda_1,\ldots, \lambda_r\) les valeurs propres deux à deux distinctes, et considérons le polynôme
    \begin{equation}
        P(x)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Alors \( P(u)=0\). En effet si \( e_i\) est un vecteur propre pour la valeur propre \( \lambda_i\), 
    \begin{equation}
        P(u)e_i=\prod_{j\neq i}(u-\lambda_j)\circ(u-\lambda_i)e_i=0
    \end{equation}
    par le lemme \ref{LemQWvhYb}. Par conséquent \( P(u)\) s'annule sur une base.
\end{proof}

\begin{corollary}
    Si \( u\) est diagonalisable et si \( F\) est une sous espace stable par \( u\), alors
    \begin{equation}
        F=\sum_{\lambda}E_{\lambda}(u)\cap F
    \end{equation}
    où \( E_{\lambda}(u)\) est l'espace propre de \( u\) pour la valeur propre \( \lambda\). En particulier la restriction de \( u\) à \( F\), \( u|_F\) est diagonalisable.
\end{corollary}

\begin{proof}
    Par le théorème \ref{ThoDigLEQEXR}, le polynôme \( \mu_x\) est scindé et ne possède que des racines simples. Notons le
    \begin{equation}
        \mu_u(X)=(X-\lambda_1)\ldots (X-\lambda_r).
    \end{equation}
    Les espaces \( E_i\) du lemme \ref{LemgnaEOk} sont maintenant les espaces propres.

    En ce qui concerne la diagonalisabilité de \( u|_F\), notons que nous avons une base de \( F\) composée de vecteurs dans les espaces \( E_{\lambda}(u)\). Cette base de \( F\) est une base de vecteurs propres de \( u\).
\end{proof}

\begin{lemma}
    Soit \( E\) un \( \eK\)-espace vectoriel et \( u\in\End(E)\). Si \( \Card\big( \Spec(u) \big)=\dim(E)\) alors \( u\) est diagonalisable.
\end{lemma}

\begin{proof}
    Soient \( \lambda_1,\ldots, \lambda_n\) les valeurs propres distinctes de \( u\). Nous savons que les espaces propres correspondants sont en somme directe (lemme \ref{LemjcztYH}). Par conséquent \( \Span\{ E_{\lambda_i}(u) \}\) est de dimension \( n\) est \( u\) est diagonalisable.
\end{proof}

\begin{proposition}     \label{PropGqhAMei}
    Soit \( (u_i)_{i\in I}\) une famille d'endomorphismes qui commutent deux à deux.
    \begin{enumerate}
        \item       \label{ItemGqhAMei}
            Si \( i,j\in I\) alors tout sous espace propre de \( u_i\) est stable par \( u_j\). Autrement dit \( u_j\big(E_{\lambda}(u)\big)\subset E_{\lambda}(u)\).
        \item
            Si les \( u_i\) sont diagonalisables, alors ils le sont simultanément.
    \end{enumerate}
\end{proposition}

\begin{proof}

    Supposons que \( u_i\) et \( u_j\) commutent et soit \( x\) un vecteur propre de \( u_i\) : \( u_ix=\lambda x\). Nous montrons que \( u_jx\in E_{\lambda}(u)\). Nous avons
    \begin{equation}
        u_i\big( u_j(x) \big)=u_j\big( u_i(x) \big)=\lambda u_j(x).
    \end{equation}
    Par conséquent \( u_j(x)\) est vecteur propre de \( u_i\) de valeur propre \( \lambda\).

    Montrons maintenant l'affirmation à propos des endomorphismes simultanément diagonalisables. Si \( \dim E=1\), le résultat est évident. Nous supposons également qu'aucun des \( u_i\) n'est multiple de l'identité. Nous effectuons une récurrence sur la dimension.

    Soit \( u_0\) un des \( u_i\) et considérons ses valeurs propres deux à deux distinctes \( \lambda_1,\ldots, \lambda_r\). Pour chaque \( k\) nous avons
    \begin{equation}
        E_{\lambda_k}(u_0)\neq E,
    \end{equation}
    sinon \( u_0\) serait un multiple de l'identité. Par contre nous avons
    \begin{equation}
        E=\bigoplus_{k}E_{\lambda_k}(u_0).
    \end{equation}
    Par le point \ref{ItemGqhAMei}, nous avons \( u_i\colon E_{\lambda_k}(u_0)\to E_{\lambda_k}(u_0)\), et nous pouvons considérer la famille d'opérateurs
    \begin{equation}
        \left( u_i|_{E_{\lambda_k}(u_0)} \right)_{i\in I}.
    \end{equation}
    Ce sont tous des opérateurs qui commutent et qui agissent sur un espace de dimension plus petite. Par hypothèse de récurrence nous avons une base de \( E_{\lambda_k}(u_0)\) qui diagonalise tous les \( u_i\).
\end{proof}

\begin{proposition} \label{PropleGdaT}
    Soit \( p\) un nombre premier et \( P\) un élément de \( \eF_p[X]\). L'anneau \( \eF_p[X]/(P)\) est intègre si et seulement si \( P\) est irréductible dans \( \eF_p[X]\).
\end{proposition}

\begin{proof}
    Supposons que \( P\) soit réductible dans \( \eF_p[X]\), c'est à dire qu'il existe \( Q,R\in \eF_p[X]\) tels que \( P=QR\). Dans ce cas, \( \bar Q\) est diviseur de zéro dans \( \eF_p[X]/(P)\) parce que \( \bar Q\bar R=0\).

    Nous supposons maintenant que \( \eF_p[X]/(P)\) ne soit pas intègre : il existe des polynômes \( R,Q\in \eF_p[X]\) tels que \( \bar Q\bar R=0\). Dans ce cas le polynôme \( QR\) est le produit de \( P\) par un polynôme : \( QR=PA\). Tous les facteurs irréductibles de \( A \) étant soit dans \( Q\) soit dans \( R\), il est possible de modifier un peu \( Q\) et \( R\) pour obtenir \( QR=P\), ce qui signifie que \( P\) n'est pas irréductible.
\end{proof}

\begin{theorem}[Théorème des deux carrés]
    Un nombre premier est somme de deux carrés si et seulement si \( p=2\) ou \( p=1\mod 4\).
\end{theorem}
\begin{remark}
    Il n'est pas dit que les nombres dans \( [1]_4\) sont premiers (\( 9=8+1\) ne l'est pas par exemple). Le théorème signifie que (à part \( 2\)), si un nombre premier est dans \( [1]_4\) alors il est somme de deux carrés, et inversement, si un nombre premier est somme de deux carrés, il est dans \( [1]_4\).
\end{remark}

\begin{proof}
    Nous notons \( \Sigma=\{ a^2+b^2\tq a,b\in \eN \}\). Soit \( p\) un nombre premier dans \( \Sigma\). Si \( a=2k\), alors \( a^2=4k^2\) et \( a^2=0\mod 4\). Si au contraire \( a\) est impair, \( a=2k+1\) et \( a^2=4k^2+1+4k=1\mod 4\). La même chose est valable pour \( b\). Par conséquent, \( a^2+b^2\) est automatiquement \( [0]_4\), \( [1]_4\) ou \( [2]_4\). Évidemment les nombres de la forme \( 0\mod 4\) ne sont pas premiers; parmi les \( 2\mod 4\), seul \( p=2\) est premier (et vaut \( 1^2+1^2\)).

    Nous avons démontré que les seuls premiers de la forme \( a^2+b^2\) sont \( p=2\) et les \( p=1\mod 4\). Il reste à faire le contraire : démontrer que si un nombre premier \( p\) vaut \( 1\mod 4\), alors il est premier. Nous considérons l'anneau
    \begin{equation}
        \eZ[i]=\{ a+bi\tq a,b\in \eZ \}.
    \end{equation}
    puis l'application
    \begin{equation}
        \begin{aligned}
            N\colon \eZ[i]&\to \eN \\
            a+bi&\mapsto a^2+b^2. 
        \end{aligned}
    \end{equation}
    Un peu de calcul dans \( \eC\) montre que pour tout \( z,z'\in \eZ[i]\), \( N(zz')=N(z)N(z')\).

    Déterminons maintenant les éléments inversibles de \( \eZ[i]\). Si \( z\in \eZ[i]^*\), alors il existe \( z'\in \eZ[i]^*\) tel que \( zz'=1\). Dans ce cas nous aurions
    \begin{equation}
        1=N(zz')=N(z)N(z'),
    \end{equation}
    ce qui est uniquement possible avec \( N(z)=N(z')=1\), c'est à dire \( z=\pm 1\) ou \( z=\pm i\). Nous avons donc
    \begin{equation}
        \eZ[i]^*=\{ \pm 1,\pm i \}.
    \end{equation}
    Nous montrons que \( \eZ[i]\) est un anneau euclidien en montrant que \( N\) est un stathme. Soient \( t,t\in \eZ[i]\setminus\{ 0 \}\) et 
    \begin{equation}
        \frac{ z }{ t }=x+iy
    \end{equation}
    dans \( \eC\). Nous considérons \( q=a+bi\) où \( a\) et \( b\) sont les entiers les plus proches de \( x\) et \( y\). Si il y a \emph{ex aequo}, on prend au hasard\footnote{Dans l'exemple \ref{ExwqlCwvV}, nous prenions toujours l'inférieur parce que le stathme tenait compte de la positivité.}. Alors nous avons
    \begin{equation}
        | \frac{ z }{ t }-q |\leq \frac{ | 1+i | }{ 2 }=\frac{ \sqrt{2} }{2}<1.
    \end{equation}
    On pose \( r=z-qt\) qui est bien un élément de \( \eZ[i]\). De plus
    \begin{equation}
        | r |=| z-qt |=| t | |\frac{ z }{ t }-q |<| t |,
    \end{equation}
    c'est à dire que \( | r |^2<| t |^2\) et donc \( N(r)<N(t)\). L'anneau \( \eZ[i]\) étant euclidien, il est principal (proposition \ref{Propkllxnv}).

    Pour la suite, nous allons d'abord montrer que \( p\in\Sigma\) si et seulement si \( p\) n'est pas irréductible dans \( \eZ[i]\), puis nous allons voir quels sont les irréductibles de \( \eZ[i]\).

    Soit \( p\), un nombre premier dans \( \Sigma\). Si \( p=a^2+b^2\), alors nous avons \( p=(a+ib)(a-bi)\), mais étant donné que \( p\) est premier, nous avons \( a\neq 0\) et \( b\neq 0\). Du coup \( p\) n'est pas inversible dans \( \eZ[i]\), mais il peut être écrit comme le produit de deux non inversibles. Le nombre \( p\) est donc non irréductible dans \( \eZ[i]\).

    Dans l'autre sens, nous supposons que \( p\) est un nombre premier non irréductible dans \( \eZ[i]\). Nous avons alors \( p=zz'\) avec ni \( z\) ni \( z'\) dans \( \{ \pm 1,\pm i \}\). En appliquant \( N\) nous avons
    \begin{equation}
        p^2=N(p)=N(z)N(z').
    \end{equation}
    Vu que \( p\) est premier, cela est uniquement possible avec \( N(z)=N(z')=p\) (avoir \( N(z)=1\) est impossible parce que cela dirait que \( z\) est inversible). Si \( z=a+ib\), alors \( p=N(z)=a^2+b^2\), et donc \( p\in \Sigma\).

    Nous cherchons maintenant les éléments irréductibles de \( \eZ[i]\). Nous savons déjà que \( \eZ[i]\) est un anneau principal et n'est pas un corps; la proposition \ref{PropomqcGe} s'applique donc et \( p\) sera non irréductible si et seulement si l'idéal \( (p)\) sera non premier. Le fait que \( (p)\) soit un idéal non premier implique que le quotient \( \eZ[i]/(p)\) est non intègre (c'est la définition d'un idéal premier). Nous cherchons donc les nombres premiers pour lesquels le quotient \( \eZ[i]/(p)\) n'est pas intègre.

    Nous commençons par écrire le quotient \( \eZ[i]/(p)\) sous d'autres formes. D'abord en remarquant que si \( I\) et \( J\) sont deux idéaux, on a \( (\eA/I)/J\simeq (\eA/J)/I\), du coup, en tenant compte du fait que \( \eZ[i]=\eZ[X]/(X^2+1)\), nous avons
    \begin{equation}
        \eZ[i]/(p)=(\eZ[X]/(p))/(X^2+1)=\eF_p[X]/(X^2+1).
    \end{equation}
    Nous avons donc équivalence des propositions suivantes :
    \begin{subequations}
        \begin{align}
            p\in\Sigma\\
            \eF_p[X]/(X^2+1)\text{ n'est pas intègre}\\
            X^2+1\text{ n'est pas irréductible dans \( \eF_p\)} \label{EqZkdrvh}\\
            \text{\( X^2+1\) admet une racine dans \( \eF_p\)}\\
            -1\in (\eF_p^*)^2\\
            \exists y\in \eF_p^*\tq y^2=-1.
        \end{align}
    \end{subequations}
    Le point \eqref{EqZkdrvh} vient de la proposition \ref{PropleGdaT}. Maintenant nous utilisons le fait que \( p\) soit un premier impair (parce que le cas de \( p=2\) est déjà complètement traité), donc \( (p-1)/2\in \eN\) et nous avons, pour le \( y\) de la dernière ligne,
    \begin{equation}
        (-1)^{(p-1)/2}=(y^2)^{(p-1)/2}=y^{p-1}=1
    \end{equation}
    parce que dans \( \eF_p\) nous avons \( y^{(p-1)}=1\) par le petit théorème de Fermat (théorème \ref{ThoOPQOiO}). Du coup \( p\) doit vérifier
    \begin{equation}
        1=(-1)^{(p-1)/2},
    \end{equation}
    c'est à dire \( \frac{ p-1 }{2}=0\mod 2\) ou encore \( p=1\mod 4\).

\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Théorème de Burnside}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LemwXXzIt}
    Soit \( P\), un polynôme sur \( \eK\). Une racine de \( P\) est une racine simple si et seulement si elle n'est pas racine de \( P'\).
\end{lemma}

\begin{lemma}
    Un endomorphisme \( u\colon E\to E\) est nilpotent si et seulement si \( u^p\) est de trace nulle pour tout \( p\) entre \( 1\) et \( \dim E\).
\end{lemma}

\begin{probleme}
    Trouver une preuve de cela.
\end{probleme}

\begin{theorem}     \label{ThoBurnsideoPuCtS}
    Toute représentation d'un groupe d'exposant fini sur \( \eC^n\) a une image finie.
\end{theorem}

Dans le cas d'un groupe abélien, la démonstration est facile. Étant donné que \( G\) est d'exposant fini, il existe \( \alpha\in \eN^*\) tel que \( g^{\alpha}=e\) pour tout \( g\in G\). Le polynôme \( P(X)=X^{\alpha}-1\) est scindé à racines simples. En effet tout polynôme sur \( \eC\) est scindé. Le fait qu'il soit à racines simples provient du lemme \ref{LemwXXzIt} parce que si \( a^{\alpha}=1\), alors il n'est pas possible d'avoir \( \alpha a^{\alpha-1}=0\).

Par ailleurs \( P(g)=0\). Le fait que nous ayons un polynôme annulateur de \( g\) scindé à racines simples implique que \( g\) est diagonalisable (théorème \ref{ThoDigLEQEXR}). Le fait que \( G\) soit abélien montre qu'il existe une base de \( \eC^n\) dans laquelle tous les éléments de \( G\) sont diagonaux. Nous devons par conséquent montrer qu'il existe un nombre fini de matrices de la forme
\begin{equation}
    \begin{pmatrix}
        \lambda_1    &       &       \\
            &   \ddots    &       \\
            &       &   \lambda_n
    \end{pmatrix}.
\end{equation}
Nous savons que \( \lambda_i^{\alpha}=1\) parce que \( g^{\alpha}=\mtu\), par conséquent chacun des \( \lambda_i\) est une racine de l'unité dont il n'existe qu'un nombre fini.

Le résultat reste vrai si \( G\) n'est pas abélien, mais la preuve devient plus compliquée. C'est le \wikipedia{fr}{Théorème_de_Burnside_(problème_de_1902)}{théorème de Burnside}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas complexe}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons maintenant le cas de l'espace \( E=\eC^n\) comme espace vectoriel de dimension \( n\) sur \( \eC\). Il est muni d'une forme sesquilinéaire
\begin{equation}    \label{EqFormSesqQrjyPH}
    \langle x, y\rangle =\sum_{k=1}^nx_k\bar y_k
\end{equation}
pour tout \( x,y\in\eC^n\).
\begin{lemma}
    Pour un opérateur hermitien,
    \begin{enumerate}
        \item
            le spectre est réel,
        \item
            deux vecteurs propres à des valeurs propres distinctes sont orthogonales\footnote{Pour la forme \eqref{EqFormSesqQrjyPH}.}.
    \end{enumerate}
\end{lemma}

\begin{proof}
    Soit \( v\) un vecteur de valeur propre \( \lambda\). Nous avons d'une part 
    \begin{equation}
        \langle Av, A\rangle =\lambda\langle v, v\rangle =\lambda\| v \|^2,
    \end{equation}
    et d'autre part, en utilisant le fait que \( A\) est hermitien,
    \begin{equation}
        \langle Av, v\rangle =\langle v, A^*v\rangle =\langle v, Av\rangle =\bar\lambda\| v \|^2,
    \end{equation}
    par conséquent \( \lambda=\bar\lambda\) parce que \( v\neq 0\).

    Soient \( \lambda_i\) et \( v_i\) (\( i=1,2\)) deux valeurs propres de \( A\) avec leurs vecteurs propres correspondants. Alors d'une part
    \begin{equation}
        \langle Av_1, v_2\rangle =\lambda_1\langle v_1, v_2\rangle ,
    \end{equation}
    et d'autre part
    \begin{equation}
        \langle Av_1, v_2\rangle =\langle v_1, Av_2\rangle =\lambda_2\langle v_1, v_2\rangle .
    \end{equation}
    Nous avons utilisé le fait que \( \lambda_2\) était réel. Par conséquent, soit \( \lambda_1=\lambda_2\), soit \( \langle v_1, v_2\rangle =0\).
\end{proof}

La preuve de Schur provient de \cite{NormHKNPKRqV}.

\begin{lemma}[Lemme de Schur complexe]\index{lemme!Schur complexe}  \label{LemSchurComplHAftTq}
    Si \( A\in\eM(n,\eC)\), il existe une matrice unitaire \( U\) telle que \( UAU^{-1}\) soit triangulaire supérieure.
\end{lemma}

\begin{proof}
    Étant donné que \( \eC\) est algébriquement clos, nous pouvons toujours considérer un vecteur propre \( v_1\) de \( A\), de valeur propre \( \lambda_1\). Nous pouvons utiliser un procédé de Gram-Schmidt pour construire une base orthonormée \( \{ v,u_2,\ldots, u_n \}\) de \( \eR^n\), et la matrice (unitaire)
    \begin{equation}
        Q=\begin{pmatrix}
             \uparrow   &   \uparrow    &       &   \uparrow    \\
             v   &   u_2    &   \cdots    &   u_n    \\ 
             \downarrow   &   \downarrow    &       &   \downarrow
         \end{pmatrix}.
    \end{equation}
    Nous avons \( Q^{-1}AQe_1=Q^{-1} Av=\lambda Q^{-1} v=\lambda e_1\), par conséquent la matrice \( Q^{-1} AQ\) est de la forme
    \begin{equation}
        Q^{-1}AQ=\begin{pmatrix}
            \lambda_1    &   *    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( *\) représente une ligne quelconque et \( A_1\) est une matrice de \( \eM(n-1,\eC)\). Nous pouvons donc répéter le processus sur \( A_1\) et obtenir une matrice triangulaire supérieure (nous utilisons le fait qu'un produit de matrices orthogonales est orthogonales).  
\end{proof}
En particulier les matrices hermitiennes, anti-hermitiennes et unitaires sont trigonables par une matrice unitaire, qui peut être choisie de déterminant \( 1\).


Le théorème suivant et la preuve proviennent de \cite{LecLinAlgAllen}, \wikipedia{en}{Spectral_theorem}{wikipedia} et \href{http://planetmath.org/encyclopedia/TheoremForNormalTriangularMatrices.html}{PlanetMath}.
\begin{theorem}[Théorème spectral pour les matrices normales]\index{Théorème!spectral!matrices normales}
    Soit \( A\in\eM(n,\eC)\) une matrice de valeurs propres \( \lambda_1,\ldots, \lambda_n\) (non spécialement distinctes). Alors les conditions suivantes sont équivalentes :
    \begin{enumerate}
        \item   \label{ItemJZhFPSi}
            \( A\) est normale,
        \item   \label{ItemJZhFPSii}
            \( A\) se diagonalise par une matrice unitaire,
        \item
            \( \sum_{i,j=1}^n| A_{ij} |^2=\sum_{j=1}^n| \lambda_j |^2\),
        \item
            il existe une base orthonormale de vecteurs propres de \( A\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nous allons nous contenter de prouver \ref{ItemJZhFPSi}\( \Leftrightarrow\)\ref{ItemJZhFPSii}. Soit \( Q\) la matrice unitaire donnée par la décomposition de Schur (lemme \ref{LemSchurComplHAftTq}) : \( A=QTQ^{-1}\). Étant donné que \( A\) est normale nous avons
    \begin{equation}
        QTT^*Q^{-1}=QT^*TQ^{-1},
    \end{equation}
    ce qui montre que \( T\) est également normale. Or une matrice triangulaire supérieure normale est diagonale. En effet nous avons \( T_{ij}=0\) lorsque \( i>j\) et
    \begin{equation}
        (TT^*)_{ii}=(T^*T)_{ii}=\sum_{k=1}^n| T_{ki} |^2=\sum_{k=1}^n| T_{ik} |^2.
    \end{equation}
    Écrivons cela pour \( i=1\) en tenant compte de \( | T_{k1} |^2=0\) pour \( k=2,\ldots, n\),
    \begin{equation}
        | T_{11} |^2=| T_{11} |^2+| T_{12} |^2+\ldots+| T_{1n} |^2,
    \end{equation}
    ce qui implique que \( T_{11}\) est le seul non nul parmi les \( T_{1k}\). En continuant de la sorte avec \( i=2,\ldots, n\) nous trouvons que \( T\) est diagonale.

    Dans l'autre sens, si \( A\) se diagonalise par une matrice unitaire, \( UAU^*=D\), nous avons
    \begin{equation}
        DD^*=UAA^*U^*
    \end{equation}
    et 
    \begin{equation}
        D^*D=UA^*AU^*,
    \end{equation}
    qui ce prouve que \( A\) est normale.
\end{proof}

\begin{lemma}
    Les groupes \( \gU(n)\) et \( \SU(n)\) sont connexes par arcs.
\end{lemma}

\begin{proof}
    Soit \( A\), une matrice unitaire et \( Q\) une matrice unitaire qui diagonalise \( A\). Étant donné que les valeurs propres arrivent par paires complexes conjuguées,
    \begin{equation}
        QAQ^{-1}=\begin{pmatrix}
            e^{i\theta_1}    &       &       &       &   \\  
            &    e^{-i\theta_1}    &       &       &   \\  
            &       &    \ddots    &       &   \\  
            &       &       &    e^{i\theta_r}    &   \\  
            &       &       &       &        e^{-i\theta_r}
        \end{pmatrix}.
    \end{equation}
    Le chemin \( U(t)\) obtenu en remplaçant \( \theta_i\) par \( t\theta_i\) avec \( t\in\mathopen[ 0 , 1 \mathclose]\) joint \( QAQ^{-1}\) à l'identité. Par conséquent \( Q^{-1}U(t)Q\) joint \( A\) à l'unité.
\end{proof}

\begin{theorem}
    Les matrices \wikipedia{fr}{Endomorphisme_normal}{normales} forment un espace connexe par arc.
\end{theorem}

\begin{proof}
    Soit \( A\) une matrice normale, et \( U\) une matrice unitaire qui diagonalise \( A\). Nous considérons \( U(t)\), un chemin qui joint \( \mtu\) à \( U\) dans \( \gU(n)\). Pour chaque \( t\), la matrice
    \begin{equation}
        A(t)=U(t)^{-1} AU(t)
    \end{equation}
    est normale. Nous avons donc trouvé un chemin dans les matrices normales qui joint \( A\) à une matrice diagonale. Il est à présent facile de la joindre à l'identité.

    Toutes les matrices normales étant connexes à l'identité, l'ensemble des matrices normales est connexe.
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Densité}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{PropDigDensVxzPuo}
    Les matrices diagonalisables sont denses dans \( \eM(n,\eC)\).
\end{proposition}

\begin{proof}
    D'après le lemme de Schur \ref{LemSchurComplHAftTq}, une matrice de \( \eM(n,\eC)\) est de la forme
    \begin{equation}
        A=Q\begin{pmatrix}
            \lambda_1    &   *    &   *    \\
              0  &   \ddots    &   *    \\
            0    &   0    &   \lambda_n
        \end{pmatrix}Q^{-1}.
    \end{equation}
    Les valeurs propres sont sur la diagonale. La matrice est diagonalisable si les éléments de la diagonales sont tous différents. Il suffit maintenant de considérer \( n\) suites \( (\epsilon^{(r)}_k)_{k\in\eN}\) convergentes vers zéro telles que pour chaque \( k\) les nombres \( \lambda_r+\epsilon^{(r)}_k\) soient tous différents. La suite de matrices
    \begin{equation}
        A_k=Q\begin{pmatrix}
            \lambda_1+\epsilon^{(1)}_k    &   *    &   *    \\
              0  &   \ddots    &   *    \\
              0    &   0    &   \lambda_n+\epsilon^{(n)}_k
        \end{pmatrix}Q^{-1}.
    \end{equation}
    est alors diagonalisable pour tout \( k\) et nous avons \( \lim_{k\to \infty} A_k=A\).
\end{proof}

\begin{proposition}
    Si \( A\in\eM(n,\eC)\) alors
    \begin{equation}
        e^{\tr(A)}=\det( e^{A}).
    \end{equation}
\end{proposition}

\begin{proof}
    Le résultat est un simple calcul pour les matrices diagonalisable. Si \( A\) n'est pas diagonalisable, nous considérons une suite de matrices diagonalisables \( A_k\) dont la limite est \( A\) (proposition \ref{PropDigDensVxzPuo}). La suite
    \begin{equation}
        a_k= e^{\tr(A_k)}
    \end{equation}
    converge vers \(  e^{\tr(A)}\) tandis que la suite 
    \begin{equation}
        b_k=\det( e^{A_k})
    \end{equation}
    converge vers \( \det( e^{A})\). Mais nous avons \( a_k=b_k\) pour tout \( k\); les limites sont donc égales.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Diagonalisation : cas réel}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}[Lemme de Schur réel]\index{lemme!Schur réel}  \label{LemSchureRelnrqfiy}
    Soit \( A\in\eM(n,\eR)\). Il existe une matrice orthogonale \( Q\) telle que \( Q^{-1}AQ\) soit de la forme
    \begin{equation}        \label{EqMtrTSqRTA}
        QAQ^{-1}=\begin{pmatrix}
            \lambda_1    &   *    &   *    &   *    &   *\\  
            0    &   \ddots    &   \ddots    &   \ddots    &   \vdots\\  
            0    &   0    &   \lambda_r    &   *    &   *\\  
            0    &   0    &   0    &   \begin{pmatrix}
                a_1    &   b_1    \\ 
                c_1    &   d_1    
            \end{pmatrix}&   *\\  
            0    &   0    &  0     &   0    &   \begin{pmatrix}
                a_s    &   b_s    \\ 
                c_s    &   d_s    
            \end{pmatrix}
        \end{pmatrix}.
    \end{equation}
    Le déterminant de \( A\) est le produit des déterminants des blocs diagonaux et les valeurs propres de \( A\) sont les \( \lambda_1,\ldots, \lambda_r\) et celles de ces blocs.
\end{lemma}

\begin{proof}
    Si la matrice \( A\) a des valeurs propres réelles, nous procédons comme dans le cas complexe. Cela nous fournit le partie véritablement triangulaire avec les valeurs propres \( \lambda_1,\ldots, \lambda_r\) sur la diagonale. Supposons donc que \( A\) n'a pas de valeurs propres réelles. Soit donc \( \alpha+i\beta \) une valeur propre (\( \beta\neq 0\)) et \( u+iv\) un vecteur propre correspondant où \( u\) et \( v\) sont des vecteurs réels. Nous avons
    \begin{equation}
        Au+iAv=A(u+iv)=(\alpha+i\beta)(u+iv)=\alpha u-\beta v+i(\alpha v+\beta v),
    \end{equation}
    et en égalisant les parties réelles et imaginaires,
    \begin{subequations}
        \begin{align}
            Au&=\alpha u-\beta v\\
            Av&=\alpha v+\beta u.
        \end{align}
    \end{subequations}
    Sur ces relations nous voyons que ni \( u\) ni \( v\) ne sont nuls. De plus \( u\) et \( v\) sont linéairement indépendants (sur \( \eR\)), en effet si \( v=\lambda u\) nous aurions \( Au=\alpha u-\beta\lambda u=(\alpha-\beta\lambda)u\), ce qui serait une valeur propre réelle alors que nous avions supposé avoir déjà épuisé toutes les valeurs propres réelles.

    Étant donné que \( u\) et \( v\) sont deux vecteurs réels non nuls et linéairement indépendants, nous pouvons trouver une base orthonormée \( \{ q_1,q_2 \}\) de \( \Span\{ u,v \}\). Nous pouvons étendre ces deux vecteurs en une base orthonormée \( \{ q_1,q_2,q_3,\ldots, q_n \}\) de \( \eR^n\). Nous considérons à présent la matrice orthogonale dont les colonnes sont formées de ces vecteurs : \( Q=[q_1\,q_2\,\ldots q_n]\).

    L'espace \( \Span\{ e_1,e_2 \}\) est stable par \( Q^{-1} AQ\), en effet nous avons
    \begin{equation}
        Q^{-1} AQe_1=Q^{-1} Aq_1=Q^{-1}(aq_1+bq_2)=ae_1+be_2.
    \end{equation}
    La matrice \( Q^{-1}AQ\) est donc de la forme
    \begin{equation}
        Q^{-1} AQ=\begin{pmatrix}
            \begin{pmatrix}
                \cdot    &   \cdot    \\ 
                \cdot    &   \cdot    
            \end{pmatrix}&   C_1    \\ 
            0    &   A_1    
        \end{pmatrix}
    \end{equation}
    où \( C_1\) est une matrice réelle \( 2\times (n-1)\) quelconque et \( A_1\) est une matrice réelle \( (n-2)\times (n-2)\). Nous pouvons appliquer une récurrence sur la dimension pour poursuivre.

    Notons que si \( A\) n'a pas de valeurs propres réelles, elle est automatiquement d'ordre pair parce que les valeurs propres complexes viennent par couple complexes conjuguées.

    En ce qui concerne les valeurs propres, il est facile de voir en regardant \eqref{EqMtrTSqRTA} que les valeurs propres sont celles des blocs diagonaux. Étant donné que \( QAQ^{-1}\) et \( A\) ont même polynôme caractéristique, ce sont les valeurs propres de \( A\).
\end{proof}

\begin{theorem} \label{ThoeTMXla}
    Le spectre d'une matrice symétrique réelle est réel. Les matrices symétriques sont diagonalisables par une matrice orthogonale.
\end{theorem}

\begin{proof}
    Soit \( A\) une matrice réelle symétrique. Si \( \lambda\) est une valeur propre complexe pour le vecteur propre complexe \( v\), alors d'une part \( \langle Av, v\rangle =\lambda\langle v, v\rangle \) et d'autre part \( \langle Av, v\rangle =\langle v, Av\rangle =\bar\lambda\langle v, v\rangle \). Par conséquent \( \lambda=\bar\lambda\).
    
    Le lemme de Schur réel \ref{LemSchureRelnrqfiy} donne une matrice orthogonale qui trigonalise \( A\). Les valeurs propres étant toutes réelles, la matrice \( QAQ^{-1}\) est même triangulaire (il n'y a pas de blocs dans la forme \eqref{EqMtrTSqRTA}). Prouvons que \( QAQ^{-1}\) est symétrique :
    \begin{equation}
        (QAQ^{-1})^t=(Q^{-1})^tA^tQ^t=QA^tQ^{-1}=QAQ^{-1}
    \end{equation}
    où nous avons utilisé le fait que \( Q\) était orthogonale (\( Q^{-1}=Q^t\)) et que \( A\) était symétrique (\( A^t=A\)). Une matrice triangulaire supérieure symétrique est obligatoirement une matrice diagonale.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Sous espaces caractéristiques}
%---------------------------------------------------------------------------------------------------------------------------

Sources : \cite{MneimneReduct} et \wikipedia{fr}{Décomposition_de_Dunford}{divers articles sur wikipédia}.

Lorsqu'un opérateur n'est pas diagonalisable, les valeurs propres jouent quand même un rôle important.

Soit \( E\) un \( \eK\)-espace vectoriel et \( f\in\End(E)\). Pour \( \lambda\in \eK\) nous définissons
\begin{equation}
    F_{\lambda}(f)=\{ v\in E\tq (f-\lambda\mtu)^nv=0, n\in\eN \}.
\end{equation}
C'est l'ensemble de nilpotence de l'opérateur \( f-\lambda\mtu\).

\begin{lemma}
    L'ensemble \( F_{\lambda}(f)\) est non vide si et seulement si \( \lambda\) est une valeur propre de \( f\). L'espace \( F_{\lambda}(f)\) est invariant sous \( f\).
\end{lemma}

\begin{proof}
    Si \( F_{\lambda}(f)\) est non vide, nous considérons \( v\in F_{\lambda}(f)\) et \( n\) le plus petit entier non nul tel que \( (f-\lambda)^nv=0\). Alors \( (f-\lambda)^{n-1}v\) est un vecteur propre de \( f\) pour la valeur propre \( \lambda\). Inversement si \( v\) est une valeur propre de \( f\) pour la valeur propre \( \lambda\), alors \( v\in F_{\lambda}(f)\).

    En ce qui concerne l'invariance, remarquons que \( f\) commute avec \( f-\lambda\mtu\). Si \( x\in F_{\lambda}(f)\) il existe \( n\) tel que \( (f-\lambda\mtu)^nx=0\). Nous avons aussi
    \begin{equation}
        (f-\lambda\mtu)^nf(x)=f\big( (f-\lambda\mtu)^nx \big)=0,
    \end{equation}
    par conséquent \( f(x)\in F_{\lambda}(f)\).
\end{proof}

\begin{remark}
    Toute matrice sur \( \eC\) n'est pas diagonalisable. Considérons en effet l'endomorphisme \( f\) donné par la matrice
    \begin{equation}
        \begin{pmatrix}
            a&    \alpha    &   \beta    \\
            0    &   a    &   \gamma    \\
            0    &   0    &   b
        \end{pmatrix}
    \end{equation}
    où \( a\neq b\), \( \alpha\neq 0\), \( \beta\) et \( \gamma\) sont des nombres complexes quelconques.
    Son polynôme caractéristique est 
    \begin{equation}
        \chi_f(\lambda)=(a-\lambda)^2(b-\lambda)
    \end{equation}
    de telle façon à ce que les valeurs propres soient \( a\) et \( b\). Nous trouvons les vecteurs propres pour la valeur \( a\) en résolvant
    \begin{equation}
        \begin{pmatrix}
            a    &   \alpha    &   \beta    \\
            0    &   a    &   \gamma    \\
            0    &   0    &   b
        \end{pmatrix}\begin{pmatrix}
            x    \\ 
            y    \\ 
            z    
        \end{pmatrix}=\begin{pmatrix}
            ax    \\ 
            ay    \\ 
            az    
        \end{pmatrix}.
    \end{equation}
    L'espace propre \( E_a(f)\) est réduit à une seule dimension générée par \( (1,0,0)\). De la même façon l'espace propre correspondant à la valeur propre \( b\) est donné par 
    \begin{equation}
        \begin{pmatrix}
            \frac{1}{ b-a }\left( \beta+\frac{ \alpha\gamma }{ b-a } \right)    \\ 
            \frac{ \gamma }{ b-a }    \\ 
            1    
        \end{pmatrix}.
    \end{equation}
    Il n'y a donc pas trois vecteurs propres linéairement indépendants, et l'opérateur \( f\) n'est pas diagonalisable.

    Par contre nous pouvons voir que
    \begin{equation}
        (f-\alpha\mtu)^2\begin{pmatrix}
             0   \\ 
            1    \\ 
            0    
        \end{pmatrix}=
        \begin{pmatrix}
            a    &   \alpha    &   \beta    \\
            0    &   a    &   \gamma    \\
            0    &   0    &   b
        \end{pmatrix}
        \begin{pmatrix}
            \alpha    \\ 
            0    \\ 
            0    
        \end{pmatrix}-\begin{pmatrix}
            a\alpha    \\ 
            0    \\ 
            0    
        \end{pmatrix}=\begin{pmatrix}
            0    \\ 
            0    \\ 
            0    
        \end{pmatrix},
    \end{equation}
    de telle sorte que le vecteur \( (0,1,0)\) soit également dans l'espace caractéristique \( F_a(f)\).

    Dans cet exemple, la multiplicité algébrique de la racine \( a\) du polynôme caractéristique vaut \( 2\) tandis que sa multiplicité géométrique vaut seulement \( 1\).
\end{remark}

Le théorème suivant est aussi appelé le théorème de \defe{décomposition primaire}{décomposition!primaire}.


\begin{theorem}[Théorème spectral, décomposition primaire]\index{théorème!spectral}     \label{ThoSpectraluRMLok}
    Soit \( E\) espace vectoriel de dimension finie sur le corps algébriquement clos \( \eK\) et \( f\in\End(E)\). Alors
    \begin{equation}
        E=F_{\lambda_1}(f)\oplus\ldots\oplus F_{\lambda_k}(f)
    \end{equation}
    où la somme est sur les valeurs propres distinctes de \( f\).

    Les projecteurs sur les espaces caractéristique forment un système complet et orthogonal.
\end{theorem}

\begin{proof}
    Soit \( P\) le polynôme caractéristique de \( u\) et une décomposition
    \begin{equation}
        P=(u-\lambda_1)^{\alpha_1}\ldots(u-\lambda_r)^{\alpha_r}
    \end{equation}
    en facteurs irréductibles. La le théorème de noyaux (\ref{ThoDecompNoyayzzMWod}) nous avons
    \begin{equation}        \label{EqDeFVSaYv}
        E=\ker(u-\lambda_1)^{\alpha_1}\oplus\ldots\oplus\ker(u-\lambda_r)^{\alpha_r}.
    \end{equation}
    Les projecteurs sont des polynômes en \( u\) et forment un système orthogonal. Il nous reste à prouver que \( \ker(u-\lambda_i)^{\alpha_i}=F_{\lambda_i}(u)\). L'inclusion
    \begin{equation}    \label{EqzmNxPi}
        \ker(u-\lambda_i)^{\alpha_i}\subset F_{\lambda_i}(u)
    \end{equation}
    est évidente. Nous devons montrer l'inclusion inverse. Prouvons que la somme des \( F_{\lambda_i}(u)\) est directe. Si \( v\in F_{\lambda_i}(u)\cap F_{\lambda_j}(u)\), alors il existe \( v_1=(u-\lambda_i)^nv\neq 0\) avec \( (u-\lambda_i)v_1=0\). Étant donné que \( (u-\lambda_i)\) commute avec \( (u-\lambda_j)\), ce \( v_1\) est encore dans \( F_{\lambda_j}(u)\) et par conséquent il existe \( w=(u-\lambda_j)^mv_1\) non nul tel que 
    \begin{subequations}
        \begin{numcases}{}
            (u-\lambda_i)w=0\\
            (u-\lambda_j)w=0.
        \end{numcases}
    \end{subequations}
    Ce \( w\) serait donc un vecteur propre simultané pour les valeurs propres \( \lambda_i\) et \( \lambda_j\), ce qui est impossible parce que les espaces propres sont linéairement indépendants. Les espaces \( F_{\lambda_i}\) sont donc en somme directe et
    \begin{equation}
        \sum_i\dim F_{\lambda_i}(u)\leq \dim E.
    \end{equation}
    En tenant compte de l'inclusion \eqref{EqzmNxPi} nous avons même
    \begin{equation}
        \dim E=\sum_i\dim\ker(u-\lambda_i)^{\alpha_i}\leq\sum_i F_{\lambda_i}(u)\leq \dim E.
    \end{equation}
    Par conséquent nous avons \( \dim\ker(u-\lambda_i)^{\alpha_i}=\dim F_{\lambda_i}(u)\) et l'égalité des deux espaces.
    
\end{proof}

Le théorème suivant généralise le théorème de diagonalisabilité \ref{ThoDigLEQEXR} au cas où le polynôme minimum est seulement scindé.

\begin{probleme}
    \begin{enumerate}
\item 
    Dans le cas où le corps n'est pas algébriquement clos, il paraît qu'il faut remplacer «diagonalisable» par «semi-simple».
    \end{enumerate}
\end{probleme}

\begin{definition}
    Un endomorphisme d'un espace vectoriel est \defe{semi-simple}{semi-simple!endomorphisme} si tout sous espace stable par \( u\) possède un supplémentaire stable.
\end{definition}
Si l'espace vectoriel est sur un corps algébriquement clos, alors les endomorphismes semi-simples sont les endomorphismes diagonaux.


\begin{theorem}[Décomposition de Dunford]\index{décomposition!Dunford}\index{Dunford!décomposition}
    Soit \( E\) un espace vectoriel sur le corps algébriquement clos \( \eK\) et \( u\in\End(E)\) un endomorphisme de \( E\). Alors \( u\) se décompose de façon unique sous la forme
    \begin{equation}
        u=s+n
    \end{equation}
    où \( s\) est diagonalisable, \( n\) est nilpotent et \( [s,n]=0\).

    De plus \( s\) et \( n\) sont des polynômes en \( u\) et commutent avec \( u\).
\end{theorem}

\begin{proof}
    Le théorème spectral \ref{ThoSpectraluRMLok} nous indique que
    \begin{equation}
        E=\bigoplus_iF_{\lambda_i}(f).
    \end{equation}
    Nous considérons l'endomorphisme \( s\) de \( E\) qui consiste à dilater d'un facteur \( \lambda\) l'espace caractéristique \( F_{\lambda}(f)\) :
    \begin{equation}
        s=\sum_i\lambda_ip_i
    \end{equation}
    où \( p_i\colon E\to F_{\lambda_i}(u)\) est la projection de \( E\) sur \( F_{\lambda_i}(u)\).

    Nous allons prouver que \( [s,f]=0\) et \( n=f-s\) est nilpotent. Cela impliquera que \( [s,n]=0\).

    Si \( x\in F_{\lambda}(f)\), alors nous avons \( sf(x)=\lambda f(x)\) parce que \( f(x)\in F_{\lambda}(f)\) tandis que \( fs(x)=f(\lambda x)=\lambda f(x)\). Par conséquent \( f\) commute avec \( s\).

    Pour montrer que \( f-s\) est nilpotent, nous en considérons la restriction
    \begin{equation}
        f-s\colon F_{\lambda}(f)\to F_{\lambda}(f).
    \end{equation}
    Cet opérateur est égal à \( f-\lambda\mtu\) et est par conséquent nilpotent.

    Prouvons à présent l'unicité. Soit \( u=s'+n'\) une autre décomposition qui satisfait aux conditions : \( s'\) est diagonalisable, \( n'\) est nilpotent et \( [n',s']=0\). Commençons par prouver que \( s'\) et \( n'\) commutent avec \( u\). En multipliant \( u=s'+n'\) par \( s'\) nous avons
    \begin{equation}
        s'u=s'^2+s'n'=s'^2+n's'=(s'+n')s'=us',
    \end{equation}
    par conséquent \( [u,s']=0\). Nous faisons la même chose avec \( n'\) pour trouver \( [u,n']=0\). Notons que pour obtenir ce résultat nous avons utilisé le fait que \( n'\) et \( s'\) commutent, mais pas leur propriétés de nilpotence et de diagonalisibilité.
    
    
    Si \( s'+n'=s+n\) est une autre décomposition, \( s'\) et \( n'\) commutent avec \( u\), et par conséquent avec tous les polynômes en \( u\). Ils commutent en particulier avec \( n\) et \( s\). Les endomorphismes \( s\) et \( s'\) sont alors deux endomorphismes diagonalisables qui commutent. Par la proposition \ref{PropGqhAMei}, ils sont simultanément diagonalisables. Dans la base de simultanée diagonalisation, la matrice de l'opérateur \( s'-s=n-n'\) est donc diagonale. Mais \( n-n'\) est également nilpotent, en effet si \( A\) et \( B\) sont deux opérateurs nilpotents,
    \begin{equation}
        (A+B)^n=\sum_{k=0}^n\binom{k}{n}A^kB^{n-k}.
    \end{equation}
    Si \( n\) est assez grand, au moins un parmi \( A^k\) ou \( B^{n-k}\) est nul.

    Maintenant que \( n-n'\) est diagonal et nilpotent, il est nul et \( n=n'\). Nous avons alors immédiatement aussi \( s=s'\).
\end{proof}


%---------------------------------------------------------------------------------------------------------------------------
\subsection{Calcul de l'exponentielle d'une matrice}
%---------------------------------------------------------------------------------------------------------------------------

Nous reprenons l'exemple de \cite{MneimneReduct}. Soit \( A\) une matrice dont le polynôme minimum s'écrit
\begin{equation}
    P(X)=(X-1)^2(X-2).
\end{equation}
Par le théorème \ref{ThoDecompNoyayzzMWod} de décomposition des noyaux nous avons
\begin{equation}
    E=\ker(A-1)^2\oplus\ker(A-2).
\end{equation}
En suivant les notations de ce théorème nous avons \( P_1(X)=(X-1)^2\), \( P_2(X)=X-2\) et
\begin{subequations}
    \begin{align}
        Q_1(X)&=X-2\\
        Q_2(X)&=(X-1)^2.
    \end{align}
\end{subequations}
Les polynômes \( R_i\) dont l'existence est assurée par le théorème de Bezout sont
\begin{equation}
    \begin{aligned}[]
        R_1(X)&=-X\\
        R_2(X)&=1.
    \end{aligned}
\end{equation}
Nous avons
\begin{equation}
    R_1Q_1+R_2Q_2=1.
\end{equation}
Le projecteur \( p_i\) sur \( \ker P_i\) est \( R_iQ_i\) :
\begin{equation}
    \begin{aligned}[]
        p_1&=-A(A-2)=\pr_{\ker(u-1)^2}\\
        p_2&=(A-1)^2=\pr_{\ker(u-2)}.
    \end{aligned}
\end{equation}
Passons maintenant au calcul de l'exponentielle. Nous avons évidemment
\begin{equation}
    e^A=e^Ap_1+e^Ap_2.
\end{equation}
Étant donné que \( p_1\) est le projecteur sur le noyau de \( (A-1)^2\), nous avons
\begin{equation}
    e^Ap_1=ee^{A-1}p_1=ep_1+e(u-1)1=ep_1=-Ae(A-2).
\end{equation}
En effet \( e^{A-1}p_1=\sum_{k=0}^{\infty}(A-1)^k\circ p_1\). De la même façon nous avons
\begin{equation}
    e^Ap_2=e^2e^{A-2}p_2=e^2p_2=e^2(A-1)^2.
\end{equation}
Au final,
\begin{equation}
    e^A=-Ae(A-2)+e^2(A-1)^2.
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Résolution de systèmes d'équations différentielles}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons l'équation différentielle
\begin{equation}    \label{EqOOsXZJ}
    f'(t)=Af(t)
\end{equation}
pour la fonction \( f\colon \eR\to \eR^n\) et \( A\) est une matrice ne dépendant pas de \( t\). Nous supposons que \( A\) est diagonalisable pour les vecteurs propres \( v_i\) et les valeurs propres \( \lambda_i\) correspondantes.

La matrice 
\begin{equation}
    R(t)=\big[  e^{\lambda_1t}v_1\, \ldots  e^{\lambda_nt}v_n \big]
\end{equation}
est la \defe{matrice résolvante}{résolvante} du système. Alors la solution du système \eqref{EqOOsXZJ} pour la condition initiale \( f(0)=f_0\) est 
\begin{equation}
    f(t)=R(t)f_0.
\end{equation}
En effet
\begin{equation}
    AR(t)=\left[  A\begin{pmatrix}
        \uparrow    \\ 
        e^{\lambda_1t}v_1    \\ 
        \downarrow    
    \end{pmatrix}\,\ldots\,A\begin{pmatrix}
        \uparrow    \\ 
        e^{\lambda_nt}v_n    \\ 
            \downarrow
    \end{pmatrix}\right]=R'(t).
\end{equation}
Par conséquent \( f'(t)=R'(t)f_0=AR(t)f_0=Af(t)\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Valeurs singulières}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Soit \( M\) une matrice \( m\times n\) sur \( \eK\) (\( \eK\) est \( \eR\) ou \( \eC\)). Un nombre réel \( \sigma\) est une \defe{valeur singulière}{valeur!singulière} de \( M\) si il existent des vecteurs unitaires \( u\in \eK^m\), \( v\in \eK^n\) tels que
    \begin{subequations}
        \begin{align}
            Mv&=\sigma u\\
            M^*u&=\sigma v.
        \end{align}
    \end{subequations}
\end{definition}

\begin{theorem}[Décomposition en valeurs singulières]
    Soit \( M\in \eM(m\times n,\eK)\) où \( \eK=\eR,\eC\). Alors \( M\) se décompose en
    \begin{equation}
        M=ADB
    \end{equation}
    où
    il existe deux matrices unitaires \( A\in \gU(m\times m)\), \( B\in \gU(n\times n)\) et une matrice (pseudo)diagonale \( D\in \eM(m\times n)\) tels que
    \begin{enumerate}
        \item 
            \( A\in\gU(m\times m)\), \( B\in\gU(n\times n)\) sont deux matrices unitaires;,
        \item
            \( D\) est (pseudo)diagonale,
        \item
            les éléments diagonaux de \( D\) sont les valeurs singulières de \( M\),
        \item
            le nombre d'éléments non nuls sur la diagonale de \( D\) est le rang de \( M\).
    \end{enumerate}
\end{theorem}

\begin{corollary}
    Soit \( M\in \eM(n,\eC)\). Il existe un isomorphisme \( f\colon \eC^n\to \eC^n\) tel que \( fM\) soit autoadjoint.
\end{corollary}

\begin{proof}
    Si \( M=ADB\) est la décomposition de \( M\) en valeurs singulières, alors nous pouvons prendre \( f=\overline{ B }^tA^{-1}\) qui est une matrice inversible. Pour la vérification que ce \( f\) répond bien à la question, ne pas oublier que \( D\) est réelle, même si \( M\) ne l'est pas.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Matrice compagnon et endomorphismes cycliques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice compagnon}
%---------------------------------------------------------------------------------------------------------------------------

Soit le polynôme \( P=X^n-a_{n-1}X^{n-1}-\ldots-a_1X-a_0\) dans \( \eK[X]\). La \defe{matrice compagnon}{matrice!compagnon} de \( P\) est la matrice\nomenclature[A]{\( C(P)\)}{matrice compagnon} donnée par
\begin{equation}
    C(P)=\begin{pmatrix}
        0    &   \cdots    &   \cdots    &   0    &   a_0\\  
        1    &   0    &       &   \vdots    &   a_1\\  
        0    &   \ddots    &   \ddots    &   \vdots    &   \vdots\\  
        \vdots    &   \ddots    &   \ddots    &   0    &   a_{n-2}\\  
        0    &   \cdots    &   0    &   1    &   a_{n-1}    
    \end{pmatrix}
\end{equation}
si \( n\geq 2\) et par \( (a_0)\) si \( n=1\). Si \( f\) est l'endomorphisme associé à la matrice \( C(P)\) nous avons
\begin{equation}
    f(e_i)=\begin{cases}
        e_{i+1}    &   \text{si \( i<n\)}\\
        (a_0,\ldots, a_{n-1})    &    \text{si \( i=n\)}.
    \end{cases}
\end{equation}
Cet endomorphisme est conçu pour vérifier \( P(f)e_1=0\).

\begin{lemma}[\cite{RapportArgreg2011}] \label{LemkVNisk}
    Soit \( P\), un polynôme sur un corps commutatif \( \eK\). Si \( f\) est l'endomorphisme associé à la matrice compagnon de \( P\), alors \( P\) est la polynôme caractéristique de \( f\). En d'autres termes, \( P=\chi_f\).
\end{lemma}

\begin{proof}
    La propriété \( P(f)e_1=0\) nous indique que le polynôme minimal ponctuel de \( f\) en \( e_1\) divise \( P\). L'ensemble des puissances de \( f\) appliquées à \( e_1\), \( \big( f^i(e_1) \big)_{i=1,\ldots, n-1}\) est libre, donc le polynôme minimal ponctuel en \( e_1\) est de degré \( n\) au minimum. En reprenant les notations du théorème \ref{ThoCCHkoU}, nous avons \( I_{e_1}=(P)\) parce que \( P\) est de degré minimum dans \( I_{e_1}\) et \( \chi_f\in I_{e_1}\).

    Donc \( P\) divise \( \chi_f\) et est de degré égal à celui de \( \chi_f\). Étant donné qu'ils sont tous deux unitaires, ils sont égaux.
\end{proof}

\begin{definition}[Matrices, endomorphismes et vecteurs cycliques]
    Une matrice est \defe{cyclique}{cyclique!matrice}\index{matrice!cyclique} si elle est semblable à une matrice compagnon. Un endomorphisme \( f\colon E\to E\) est \defe{cyclique}{cyclique!endomorphisme}\index{endomorphisme!cyclique} si il existe un vecteur \( x\in E\) tel que \( \{ f^k(x)\tq k=1,\ldots, n-1 \}\) est une base de \( E\). Un vecteur ayant cette propriété est un \defe{vecteur cyclique}{vecteur!cyclique} pour \( f\).
\end{definition}

\begin{lemma}
    Un endomorphisme est cyclique si et seulement si sa matrice associée est cyclique.
\end{lemma}

\begin{lemma}   \label{LemSGmdnE}
    Une matrice est cyclique si et seulement si ses polynômes minimal et caractéristiques coïncident.
\end{lemma}

\begin{lemma}   \label{LemAGZNNa}
    Si \( f\colon E\to E\) est un endomorphisme cyclique et si \( y\) est un vecteur cyclique de \( f\), alors le polynôme minimal de \( f\) est égal au polynôme minimal de \( f\) au point \( y\) : \( \mu_{f}=\mu_{f,y}\).
\end{lemma}

\begin{proof}
    Montrons que \( \mu_{f,y}\) est un polynôme annulateur de \( f\), ce qui prouvera que \( \mu(f)\) divise \( \mu_{f,y}\). Étant donné que \( y\) est cyclique, tout élément de \( E\) s'écrit sous la forme \( x=Q(f)y\). Prenons un polynôme \( P\) annulateur de \( f\) en \( y\) : \( P(f)y=0\). Nous montrons que \( P\) est alors un polynôme annulateur de \( f\). En effet, nous avons
    \begin{equation}
        P(f)x=\big( P(f)\circ Q(f) \big)y=\big( Q(f)\circ P(f) \big)y=0
    \end{equation}
    où nous avons utilisé le lemme \ref{LemQWvhYb}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Décomposition de Frobenius}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Décomposition de Frobenius \cite{AutourFrobCompa,Vialivs}]
    Soit \( E\), un \( \eK\)-espace vectoriel où \( \eK\) est \( \eR\) ou \( \eC\), et \( f\in \End(E)\). Alors il existe une suite de sous espaces \( E_1,\ldots, E_r\) stables par \( f\) tels que
    \begin{enumerate}
        \item   \label{ItemmpwjnSs}
            \( E=\bigoplus_{i=1}^rE_i\);
        \item
            pour chaque \( E_i\), l'endomorphisme restreint \( f_i=f|_{E_i}\) est cyclique;
        \item
            si \( \mu_i\) est le polynôme minimal de \( f_i\) alors \( \mu_{i+1}\) divise \( \mu_i\);
    \end{enumerate}
    Une telle décomposition vérifie automatiquement \( \mu_1=\mu_f\) et \( \mu_1\cdots \mu_r=\chi_f\), et la suite \( (\mu_i)_{i=1,\ldots, r}\) ne dépend que de \( f\) et non du choix de la décomposition du point \ref{ItemmpwjnSs}.
\end{theorem}

Les polynômes \( \mu_i\) sont les \defe{invariants de similitude}{invariant!de similitude} de l'endomorphisme \( f\).

\begin{proof}
    Nous commençons par montrer que si une telle décomposition existe, alors
    \begin{subequations}    \label{subEqzcGouz}
        \begin{align}
            \chi_f=\prod_{i=1}^r\mu_i  \label{EqTaxsvb}\\
            \mu_f=\mu_1
        \end{align}
    \end{subequations}
    où \( \chi_f\) est le polynôme caractéristique de \( f\) et \( \mu_f\) est le polynôme minimal\footnote{Cette partie de la preuve provient de \cite{MoncetIVS}.}. D'abord le polynôme caractéristique de \( f\) devra être égal au produit des polynômes caractéristique des \( f|_{E_i}\), mais ces derniers endomorphismes étant cycliques, leurs polynôme caractéristiques sont égaux à leurs polynômes minimaux (lemme \ref{LemSGmdnE}). Cela prouve l'égalité \eqref{EqTaxsvb}. Ensuite tous les \( \mu_i\) doivent diviser le polynôme minimal, donc \( \ppcm(\mu_1,\ldots, \mu_r)\) divise \(\mu_f\). Cependant le polynôme minimal ne doit contenir une et une seule fois chacun des facteurs irréductibles du polynôme caractéristique, et chacun de ces facteurs sont dans les polynômes \( \mu_i\). Par conséquent \( \ppcm(\mu_1,\ldots, \mu_r)=\mu_f\). Mais par ailleurs \( \mu_1=\ppcm(\mu_1,\ldots, \mu_r)\), donc \( \mu_1=\mu_f\).
    
    Mais le produit des \( \mu_i\) est le polynôme caractéristique, donc tous les facteurs irréductibles du polynôme minimal sont dans les \( \mu_i\); cela signifie que \( \mu_f=\ppcm(\mu_1,\ldots, \mu_r)\).

    Soit \( d\), le degré du polynôme minimal de \( f\) et \( y\in E\) tel que \( \mu_f=\mu_{f,y}\) (voir lemme \ref{LemSYsJJj}). Le plus petit espace stable sous \( f\) contenant \( y\) est
    \begin{equation}
        E_y=\Span\{ y,f(y),\ldots, f^{d-1}(y) \}.
    \end{equation}
    Nous notons \( e_i=f^{i-1}(y)\). Notons que les vecteurs donnés forment bien une base de \( E_y\) parce que si les \( e_i\) n'était pas linéairement indépendants, alors soit \( \sum_ka_ke_k=0\). Donc ce cas nous aurions
    \begin{equation}
        \big( \sum_ka_kX^k \big)(f)y=0,
    \end{equation}
    ce qui contredirait la minimalité de \( \mu_{f,y}\).

    La difficulté du théorème est de trouver un complément de \( E_y\) qui soit également stable sous \( f\). Nous commençons par étendre\footnote{Pour autant que j'aie compris, cette extension manque dans \cite{AutourFrobCompa}. Corrigez moi si je me trompe.} \( \{ e_1,\ldots, e_d \}\) en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Ensuite nous allons montrer que
    \begin{equation}
        E=E_y\oplus F
    \end{equation}
    avec
    \begin{equation}
        F=\{ x\in E\tq  e^*_d\big( f^k(x) \big)=0\forall k\in \eN \}.
    \end{equation}
    Par construction, \( F\) est invariant sous \( f\). Montrons pour commencer que \( E_y\cap F=\{ 0 \}\). Un élément de \( E_y\) s'écrit
    \begin{equation}
        z=a_1e_1+\ldots +a_ke_k
    \end{equation}
    avec \( k\leq d\). Étant donné que \( f\) décale les vecteurs de base, nous avons \( e^*_d\big( f^{d-k}(z) \big)=a_k\). Du coup \( z\in F\) si et seulement si \( a_1=\ldots=a_d=0\), c'est à dire que \( E_y\cap F=\{ 0 \}\).

    Nous montrons maintenant que \( \dim F=n-d\). Pour cela nous considérons l'application
    \begin{equation}
        \begin{aligned}
            T\colon \eK[F]&\to E^* \\
            g&\mapsto e^*_d\circ g. 
        \end{aligned}
    \end{equation}
    Cette application est injective. En effet un élément général de \( \eK[f]\) est
    \begin{equation}
        g=a_1\id+a_2f+\ldots +a_pf^{p-1}
    \end{equation}
    avec \( p\leq d\). Si \( T(g)=0\), alors nous avons en particulier
    \begin{equation}
        0=T(g)e_{_d-p+1}=e^*_d(a_1e_{d-p+1}+a_2e_{d-p+2}+\ldots +a_pe_d)=a_p.
    \end{equation}
    Donc \( a_p=0\) et en appliquant maintenant \( T(g)\) à \( e_{d-p}\) nous obtenons \( a_{p-1}=0\). Au final nous trouvons que \( g=0\) et donc que \( T\) est injective.

    Étant donné que \( \dim\eK[f]=d\) et que \( T\) est injective, \( \dim\Image(T)=d\). Nous regardons l'orthogonal de l'image :
    \begin{subequations}
        \begin{align}
            (\Image(T))^{\perp}&=\{ x\in E\tq T(g)x=0\forall g\in\eK[f] \}\\
            &=\{ x\in E\tq e^*_d\big( g(x) \big)=0\forall g\in \eK[f] \}\\
            &=F.
        \end{align}
    \end{subequations}
    Par conséquent \( F^{\perp}=\Image(T)\). Vu que \( \dim\Image(T)=d\), nous avons donc \( \dim F=n-d\) et il est établi que \( E=E_y\oplus F\). 

    Nous avons donc trouvé \( F\), stable par \( f\) et tel que \( E=E_y\oplus F\). Nous devons maintenant nous assurer que cette décomposition tombe bien pour les polynômes minimaux. Si \( P_1\) est le polynôme minimal de \( f|_{E_yj}\), alors par le lemme \ref{LemAGZNNa} nous avons \( P_1=\mu_{f,y}=\mu_f\) parce que \( f|_{E_y}\) est cyclique sur \( E_y\). Mettons \( P_2\), le polynôme minimal de \( f|_F\). Étant attendu que \( F\) est stable par \( f\), le polynôme \( P_2\) divise \( P_1\). En recommençant la construction sur \( F\), nous construisons un nouvel espace \( F'\) stable sous \( F\) et vérifiant \( \mu_{f|_{F'}}=P_2\), etc.

    Nous passons maintenant à la partie unicité du théorème. Soient deux suites \( F_1,\ldots, F_r\) et \( G_1,\ldots, G_s\) de sous espaces stables par \( f\) et vérifiant
    \begin{enumerate}
        \item
            \( E=\bigoplus_{i=1}^rF_i\),
        \item
            \( f|_{F_i}\) est cyclique,
        \item
            \( \mu_{f|_{F_{i+1}}}\) divise \( \mu_{f|_{F_i}}\),
    \end{enumerate}
    et, \emph{mutatis mutandis}, les mêmes conditions pour la famille \( \{ G_i \}\). Nous posons \( P_i=\mu_{f_{F_i}}\) et \( Q_i=\mu_{f|_{G_i}}\). Nous allons montrer par récurrence que \( P_i=Q_i\) et \( \dim F_i=\dim G_i\). Il ne sera cependant pas garanti que \( F_i=G_i\). D'abord, \( P_1=Q_1\) parce qu'ils sont tous deux égaux à \( \mu_f\) par les relations \eqref{subEqzcGouz}. Nous supposons que \( P_i=Q_i\) pour \( i\leq 1\leq j-1\) et nous tentons de montrer que \( P_j=Q_j\).

    Nous avons 
    \begin{equation}    \label{EqMrCtZO}
        P_j(f)=P_j(f)|_{F_1}\oplus\ldots\oplus P_j(f)|_{F_{j-1}}.
    \end{equation}
    En effet étant donné que \( P_{j+k}\) divise \( P_j\), nous avons\footnote{En vertu du lemme \ref{LemQWvhYb}.} \( P_{j}(f)=A(f)\circ P_{j+k}(f)\), mais \( P_{j+k}(f)F_{j+k}=0\), donc \( P_j(f)F_{j+k}=0\). Les espaces \( G_i\) n'ayant a priori aucun rapport avec les polynômes \( P_i\), nous écrivons
    \begin{equation}    \label{EqJreLiO}
        P_j(f)=P_j(f)|_{G_1}\oplus\ldots\oplus P_j(f)|_{G_{j-1}}\oplus P_j(f)|_{G_j}\oplus\ldots\oplus P_j(f)|_{G_s}.
    \end{equation}
    Pour \( 1\leq i\leq j-1\), nous avons supposé \( P_i=Q_i\). Étant donné que \( f|_{F_i}\) est semblable à \( C_{_i}\) et \( f|_{G_i}\) est semblable à \( C_{Q_i}\), la matrice de \( f|_{E_i}\) est semblable à la matrice de \( f|_{G_i}\). En particulier,
    \begin{equation}
        \dim P_j(f)F_i=\dim P_j(f)G_i.
    \end{equation}
    En prenant les dimensions des images dans les égalités \eqref{EqMrCtZO} et \eqref{EqJreLiO}, nous trouvons que
    \begin{equation}
        P_j(f)|_{G_j}=\ldots=P_j(f)|_{G_s}=0.
    \end{equation}
    Par conséquent \( P_j\in I_{f|G_j}\) et donc \( P_j\) divise \( Q_j\), qui est générateur de \( I_{f|_{G_j}}\). La situation étant symétrique entre \( P\) et \( Q\), nous montrons de même que \( Q_j\) divise \( P_j\) et donc que \( P_j=Q_j\).

    Ceci achève la démonstration du théorème de décomposition de disponible.

\end{proof}


Sous forme matricielle, ce théorème dit que toute matrice est semblable à une matrice de la forme bloc-diagonale
\begin{equation}
    f=\begin{pmatrix}
        C_{\mu_1}    &       &       \\
            &   \ddots    &       \\
            &       &   C_{\mu_r}
    \end{pmatrix}
\end{equation}

\begin{remark}
    Si nous travaillons sur \( \eR\), la réduite de Frobenius restera une matrice réelle, même si les valeurs propres sont complexes. En effet le procédé de Frobenius ne regarde absolument pas les valeurs propres, mais seulement les facteurs irréductibles du polynôme caractéristique. La réduite de Frobenius ne tente pas de résoudre ces polynômes, mais se contente d'en utiliser les matrices compagnon.

    La situation sera différente dans le cas de la forme normal de Jordan.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Forme normale de Jordan}
%---------------------------------------------------------------------------------------------------------------------------

Il existe une preuve directe de la décomposition de Jordan ne nécessitant pas la décomposition de Frobenius\cite{LecLinAlgAllen}. Cette dernière passe par les espaces caractéristiques\footnote{Aussi appelés «espaces propres généralisés».} et est à mon avis plus compliquée que la démonstration de Frobenius elle-même. Nous allons donc nous contenter de donner la décomposition de Jordan comme un cas particulier de Frobenius.

\begin{theorem}[Décomposition de Jordan]\index{décomposition!Jordan}\index{Jordan!décomposition}
    Soit \( E\) un espace vectoriel sur \( \eK\), et \( f\in\End(E)\) un endomorphisme dont le polynôme caractéristique \( \chi_f\) est scindé\footnote{C'est pour cette hypothèse que \( \eK=\eR\) n'est pas le bon cadre.}. Il existe une base de \( E\) dans laquelle la matrice de \( f\) s'écrit sous la forme
    \begin{equation}
        M=\begin{pmatrix}
            J_{n_1}(\lambda_1)    &       &       \\
                &   \ddots    &       \\
                &       &   J_{n_k}(\lambda_k)
        \end{pmatrix}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( f\) (avec éventuelle répétitions) et \( J_n(\lambda)\) représente le bloc \( n\times n\)
    \begin{equation}
        J_n(\lambda)=\begin{pmatrix}
            \lambda    &   1    &       &       &   \\  
                &   \lambda    &   1    &       &   \\  
                &       &   \lambda    &       &   \\  
                &       &       &   \ddots    &   1\\  
                &       &       &       &   \lambda    
        \end{pmatrix}.
    \end{equation}
    En d'autres termes, \( J_n(\lambda)_{ii}=\lambda\) et \( J_n(\lambda)_{i-1,i}=1\).    
\end{theorem}

\begin{proof}
    Nous commençons par le cas où \( f\) est nilpotente; nous notons \( M\) sa matrice. Dans ce cas la seule valeur propre est zéro et le polynôme caractéristique est \( X^m\) pour un certain \( m\). Nous savons par le lemme \ref{LemkVNisk} que (la matrice de) \( f\) est semblable à sa matrice compagnon. En l'occurrence pour \( f\) nous avons
    \begin{equation}
        C_{X^m}=\begin{pmatrix}
             0   &       &       &  0     \\
             1   &   \ddots    &       &   \vdots    \\
                &   \ddots    &   \ddots    &    \vdots   \\ 
                &       &   1    &   0     
         \end{pmatrix}.
    \end{equation}
    Ensuite le changement de base (qui est une similitude) \( (e_1,\ldots, e_n)\mapsto(e_n,\ldots, e_1)\) montre que \( C_{X^m}\) est semblable à un bloc de Jordan \( J_m(0)\).

    Supposons à présent que \( f\) ne soit pas nilpotente. Par l'hypothèse de polynôme caractéristique scindé, nous supposons que \( f\) a \( m\) valeurs propres distinctes et que son polynôme caractéristique est
    \begin{equation}
        \chi_f=(X-\lambda_1)^{l_1}\ldots (X-\lambda_m)^{l_m}.
    \end{equation}
    Le lemme des noyaux (théorème \ref{ThoDecompNoyayzzMWod}) nous enseigne que
    \begin{equation}
        E=\bigoplus_{i=1}^m\underbrace{\ker(f-\mu_i\mtu)^{l_i}}_{F_i}.
    \end{equation}
    La restriction de \( f-\lambda_i\mtu\) à \( F_i\) est par construction un endomorphisme nilpotent, et donc peut s'écrire comme un bloc de Jordan avec des zéros sur la diagonale. En utilisant la décomposition
    \begin{equation}
        f|_{F_i}=(f-\lambda_i\mtu)|_{F_i}+\lambda_i\mtu_{F_i},
    \end{equation}
    nous voyons que \( f|_{F_i}\) s'écrit comme un bloc de Jordan avec \( \lambda_i\) sur la diagonale.
\end{proof}

\begin{remark}
    Nous pouvons calculer la forme normale de Jordan pour une matrice complexe ou réelle, mais dans les deux cas nous devons nous attendre à obtenir une matrice complexe parce que les valeurs propres d'une matrice réelle peuvent être complexes. Cependant nous demandons que le polynôme caractéristique de \( f\) soit scindé sur \( \eK\). En pratique, la décomposition de Jordan n'est garantie que sur les corps algébriquement clos, c'est à dire sur \( \eC\).

    La suite des invariants de similitude sur laquelle repose Frobenius, elle, est disponible sur tout corps, y compris \( \eR\).
\end{remark}

Une application de la décomposition de Jordan est l'existence d'un logarithme pour les matrices.
\begin{proposition}
    Toute matrice inversible complexe est une exponentielle\index{exponentielle!de matrice}.
\end{proposition}

\begin{proof}
    Soit \( A\in \GL(n,\eC)\); nous allons donner une matrice \( B\in \eM(n,\eC)\) telle que \( A=\exp(B)\). D'abord remarquons qu'il suffit de prouver le résultat pour une matrice par classe de similitude. En effet si \( A=\exp(B)\) et si \( M\) est inversible alors 
    \begin{subequations}    \label{EqqACuGK}
        \begin{align}
            \exp(MBM^{-1})&=\sum_k\frac{1}{ k! }(MBM^{-1})^k\\
            &=\sum_k\frac{1}{ k! }MB^kM^{-1}\\
            &=M\exp(B)M^{-1}.
        \end{align}
    \end{subequations}
    Donc \( MAM^{-1}=\exp(MBM^{-1})\). Nous pouvons donc nous contenter de trouver un logarithme pour les blocs de Jordan. Nous supposons donc que \( A=(\mtu+N)\) avec \( N^m=0\). En nous inspirant de \eqref{EqweEZnV}, nous posons
    \begin{equation}
        D(t)=tN-\frac{ t^2 }{ 2 }N^2+\ldots +(-1)^m\frac{ t^{m-1} }{ m-1 }N^{m-1}
    \end{equation}
    et nous allons prouver que \(  e^{D(1)}=\mtu+N\). Notons que \( N\) étant nilpotente, cette somme ainsi que toutes celles qui viennent sont finies. Il n'y a donc pas de problèmes de convergences dans cette preuve (si ce n'est les passages des équations \eqref{EqqACuGK}).

    Nous posons \( S(t)= e^{D(t)}\) (la somme est finie), et nous avons
    \begin{equation}
        S'(t)=D'(t) e^{D(t)}
    \end{equation}
    Afin d'obtenir une expression qui donne \( S'\) en termes de \( S\), nous multiplions par \( (\mtu+tN)\) en remarquant que \( (\mtu+tN)D'(t)=N\) nous avons
    \begin{equation}
        (\mtu+tN)S'(t)=NS(t).
    \end{equation}
    En dérivant à nouveau,
    \begin{equation}    \label{EqKjccqP}
        (\mtu+tN)S''(t)=0.
    \end{equation}
    La matrice \( (\mtu+tN)\) est inversible parce que son noyau est réduit à \( \{ 0 \}\). En effet si \( (\mtu+tN)x=0\), alors \( Nx=-\frac{1}{ t }x\), ce qui est impossible parce que \( N\) est nilpotente. Ce que dit l'équation \eqref{EqKjccqP} est alors que \( S''(t)=0\). Si nous développons \( S(t)\) en puissances de \( t\) nous nous arrêtons au terme d'ordre \( 1\) et nous avons
    \begin{equation}
        S(t)=S(0)+tS'(0)=\mtu+tD'(0)=1+tN.
    \end{equation}
    En \( t=1\) nous trouvons \( S(1)=\mtu+N\). La matrice \( D(1)\) donnée est donc bien un logarithme de $\mtu+N$.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Mini introduction au produit tensoriel}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SeOOpHsn}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définitions}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E\), un espace vectoriel de dimension finie. Si \( \alpha\) et \( \beta\) sont deux formes linéaires sur un espace vectoriel \( E\), nous définissons \( \alpha\otimes \beta\) comme étant la \( 2\)-forme donnée par
\begin{equation}
    (\alpha\otimes \beta)(u,v)=\alpha(u)\beta(v).
\end{equation}
Si \( a\) et \( b\) sont des vecteurs de \( E\), ils sont vus comme des formes sur \( E\) via le produit scalaire et nous avons
\begin{equation}
    (a\otimes b)(u,v)=(a\cdot u)(b\cdot v).
\end{equation}
Cette dernière équation nous incite à pousser un peu plus loin la définition de \( a\otimes b\) et de simplement voir cela comme la matrice de composantes
\begin{equation}
    (a\otimes b)_{ij}=a_ib_j.
\end{equation}
Cette façon d'écrire a l'avantage de ne pas demander de se souvenir qui est une vecteur ligne, qui est un vecteur colonne et où il faut mettre la transposée. Évidemment \( (a\otimes b)\) est soit \( ab^t\) soit \( a^tb\) suivant que \( a\) et \( b\) soient ligne ou colonne.

\begin{lemma}   \label{LemMyKPzY}
    Soient \( x,y\in E\) et \( A,B\) deux opérateurs linéaires sur \( E\) vus comme matrices. Alors
    \begin{equation}        \label{EqXdxvSu}
        (Ax\otimes By)=A(x\otimes y)B^t.
    \end{equation}
\end{lemma}

\begin{proof}
    Calculons la composante \( ij\) de la matrice \( (Ax\otimes By)\). Nous avons
    \begin{subequations}
        \begin{align}
            (Ax\otimes By)_{ij}&=(Ax)_i(By)_j\\
            &=\sum_{kl}A_{ik}x_kB_{jl}y_l\\
            &=A_{ik}(x\otimes y)_{kl}B_{jl}\\
            &=\big( A(x\otimes y)B^t \big)_{ij}.
        \end{align}
    \end{subequations}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\section{Norme opérateur}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SeckwyQjK}

Soit \( E\) un espace vectoriel (pas spécialement de dimension finie) et un opérateur \( A\colon E\to E\). La \defe{norme opérateur}{norme!opérateur} de \( A\) est le nombre
\begin{equation}    \label{EqThUCEJ}
    \| A \|_{\infty}=\sup_{\| x \|=1}\| Ax \|.
\end{equation}
où dans le membre de droite, la norme est celle choisie sur \( E\). Cette norme donne lieu à la \defe{topologie forte}{topologie!forte} sur l'espace des opérateurs.

La proposition suivante est valable également en dimension infinie. C'est elle qui montre que le produit scalaire est continu dans un espace de Hilbert par exemple.
\begin{proposition}     \label{PropmEJjLE}
    Soient \( V\) et \( W\) deux espaces vectoriels et \( T\colon V\to W\) une application linéaire bornée. Alors elle est continue.
\end{proposition}

\begin{proof}
    Pour tout \( x,y\in V\) nous avons
    \begin{equation}
        \| T(x)-T(y) \|=\| T(x-y) \|\leq \| T \|\| x-y \|.
    \end{equation}
    En particulier si \( x_n\) est une suite qui converge vers \( x\) alors
    \begin{equation}
        0\leq \| T(x_n)-T(x) \|\leq \{ T \}\| x-x_n \|\to 0
    \end{equation}
    et \( T\) est continue.
\end{proof}

La topologie forte n'est pas la seule possible. Il existe aussi par exemple la \defe{topologie faible}{topologie!faible} donnée par la notion de convergence \( A_i\to A\) si et seulement si \( A_ix\to Ax\) pour tout \( x\in E\).Il faut noter que la topologie faible n'est pas une topologie métrique. Cela même si la condition \( A_ix\to Ax\), elle, est métrique vu qu'elle est écrite dans \( E\).

et que dans le cas où \( E\) est de dimension infinie, elle est réellement différente de la topologie forte. Nous verrons à la sous-section \ref{subsecaeSywF} que dans le cas des projections sur un espaces de Hilbert, l'égalité
\begin{equation}
    \sum_{i=1}^{\infty}\pr_{u_i}=\id
\end{equation}
est vraie pour la topologie faible, mais pas pour la topologie forte.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
