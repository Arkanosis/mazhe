% This is part of Mes notes de mathématique
% Copyright (C) 2010-2013,2016
%   Laurent Claessens
% See the file LICENCE.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Estimation de l'ordre de convergence}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Comment estimer numériquement l'ordre \( p\) de convergence de la méthode ? Soit une suite \( (x_n)\) convergente vers \( \alpha\). Considérons les \( 4\) termes \( x_{n-3}\), \( x_{n-2}\), \( x_{n-1}\), \( x_n\). Alors nous pouvons écrire l'approximation
\begin{equation}
    \frac{ | x_n -x_{n-1}| }{ | x_{n-1}-x_{n-2} | }\simeq \left( \frac{ | x_{n-1}-x_{n-2} | }{ | x_{n-1}-x_{n-3} | } \right)^p.
\end{equation}
Cette approximation ne serait pas trop mauvaise tant que \( n\) est assez grand pour que la convergence soit bien engagée. Passons au logarithme :
\begin{equation}
    \ln \frac{ | x_n -x_{n-1}| }{ | x_{n-1}-x_{n-2} | }\simeq p\ln \left( \frac{ | x_{n-1}-x_{n-2} | }{ | x_{n-1}-x_{n-3} | } \right).
\end{equation}
et donc
\begin{equation}
    p\simeq \frac{ \ln\left( \frac{ | x_n -x_{n-1}| }{ | x_{n-1}-x_{n-2} } \right) }{ \ln \left(\frac{ | x_{n-1}-x_{n-2} | }{ | x_{n-1}-x_{n-3} | } \right)}.
\end{equation}
Avec cette approximation, en réalité nous calculons une suite \( (p_i)\) qui sont les approximation de \( p\) à partir des termes \( i\) à \(i+3 \) de la suite \( (x_n)\). Il s'agit d'une suite d'estimations de \( p\).

\begin{enumerate}
    \item
Dans le cas de la bisection, nous obtenons toujours \( p_i=1\).
\item
    Dans le cas de la méthode de Newton (\ref{SECooIKXNooACLljs}) nous avons \( p=2\). Mais les premières valeurs de \( p_i\) peuvent être aussi bien \( 0\) que \( 7\). Après quelque itérations pourtant les \( p_i\) se regroupent autour de \( 2\).
\end{enumerate}
En tout cas, le plus important est de savoir si \( p>1\) ou non. Rappel : nous voulons la superlinéarité parce que nous voulons utiliser le test d'arrêt de la différence entre deux termes, voir \ref{NTooVXLXooXlAGEq}. 

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Autres méthodes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Méthode de Schröder}
%---------------------------------------------------------------------------------------------------------------------------

La formule est
\begin{equation}
    x_{n+1}=x_n-\frac{ f(x_n)f'(x_n) }{ f'(x_n)^2-f(x_n)f''(x_n) }
\end{equation}
Cette méthode est d'ordre \( 2\) pour toute racine et toute valeur de multiplicité. Le problème de cette méthode est qu'elle demande \( 3\) valutations. Son efficacité :
\begin{equation}
    E=\sqrt[3]{ 2 }\simeq 1.25
\end{equation}
Cela est donc moins efficace que Newton.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Halley}
%---------------------------------------------------------------------------------------------------------------------------

Il a \( p=3\) lorsque \( \alpha\) est racine simple. Mais encore \( p=1\) pour les racines multiples. Plus efficace que Newton pour les racines simples, mais même problème pour les racines multiples.

\begin{equation}
    x_{n+1}=x_n-\frac{ 2f(x_n)f'(x_n) }{ 2f'(x_n)^2-f(x_n)f''(x_n) }
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Méthode des sécantes variables}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooIUEUooVcHAoc}

Supposons de ne pas avoir \( f\) analytique, mais seulement la possibilité de calculer \( f(x)\) pour tout \( x\). Newton ne fonctionne pas, mais la bisection fonctionne.

Nous pouvons approximer
\begin{equation}
    f'(x_n)=\frac{ f(x_n)-f(x_{n-1}) }{ x_n-x_{n-1} }.
\end{equation}
En substituant dans la formule de Newton, nous obtenons
\begin{equation}
    x_{n+1}=x_n-\frac{ f(x_n)(x_n-x_{n-1}) }{ f(x_n)-f(x_{n-1}) }.
\end{equation}

Il s'agit de prendre la droite qui passe par \( (x_{n-1},f(x_{n-1}))\) et par \( (x_n,f(x_n))\) et de prendre l'intersection de cette droite avec l'axe \( y=0\). Cela donne le \( x_{n+1}\).

Pour cette méthode, il ne faut pas seulement \( x_0\) mais également \( x_1\).

L'ordre de convergence est le nombre d'or
\begin{equation}    \label{EQooQEFCooUsGVjP}
    p=\frac{ 1-\sqrt{ 5 } }{ 2 }\simeq 1.618.
\end{equation}
Cela est donc superlinéaire.

La nombre de valutations est \( s=1\) (il y a deux apparitions de \( f\) dans la formule, mais l'une des deux est récupérée dans l'itération suivante). Donc l'efficacité est
\begin{equation}
    E=p.
\end{equation}
Donc bien efficace.

\begin{proposition}
    Si \( \alpha\) est racine simple, il existe un voisinage de \( \alpha\) tel que pour tout choix de \( x_0\), \( x_1\) dans ce voisinage, la méthode converge.
\end{proposition}

Psychologiquement, on est tenté de prendre \( x_0\) et \( x_1\) de part et d'autre de \( \alpha\) (pensant à la bisection), mais en réalité ce n'est pas obligatoire du tout et n'a aucune influence. Il faut seulement les prendre très proches de \( \alpha\).

\begin{remark}
    La méthode de la sécante est souvent écrite sous la forme
    \begin{equation}        \label{EQooYVKLooKTFjwv}
        x_{n+1}=\frac{ x_{n-1}f(x_n)-x_nf(x_{n-1}) }{ f(x_n)-f(x_{n-1}) }.
    \end{equation}
    C'est évidemment algébriquement équivalent. 

    Les formules \eqref{EQooQEFCooUsGVjP} et \eqref{EQooYVKLooKTFjwv} ont toutes deux des erreurs de cancellation. Laquelle est la plus grave ?

    Dans la première, si la fraction est mal calculée, elle ne fait que modifier \( x_n\). C'est à dire qu'on peut espérer qu'à la prochaine itération, ça aille mieux. En tout cas, dans ce cas si la fraction est mal calculée, ça ne détruit pas tout.

    Dans la seconde, c'est la valeur elle-même qui risque d'être mal calculée. Et si la fraction est mal calculée, alors on casse complètement l'éventuel bonne approximation que nous avions déjà.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Aitken}
%---------------------------------------------------------------------------------------------------------------------------

La méthode du \( \Delta^2\) de Aitken est une méthode d'accélération de la convergence.

Soit \( (x_n)\) une suite qui converge. Nous voudrions une nouvelle suite \( (y_n)\) telle que
\begin{equation}
    \lim_{n\to \infty} \frac{ y_n-\alpha }{ x_n-\alpha }
\end{equation}
C'est la définition d'une convergence accélérée.

La façon de faire est :
\begin{equation}
    y_n=\frac{ x_{n+2}x_n-x_{n+1}^2 }{ x_{n+2}-2x_{n+1}+x_n }=x_n-\frac{ (x_{n+1}-x_n) }{ x_{n+2}-2x_{x+1}+x_n }.
\end{equation}
La première expressions a deux cancellations (la seconde une seule) et de plus la première est $y_n$ elle-même alors que la seconde est une correction.

Donc la seconde expression est numériquement meilleure.

L'opérateur \( \Delta\) appliqué à une suite est :
\begin{equation}
    (\Delta x)_n=x_{n+1}-x_n
\end{equation}
Donc
\begin{equation}
    (\Delta^2x)_n= (\Delta x)_{n+1}-(\Delta x)_n=x_{n+2}-x_{n+1}-x_{n+1}+x_n=x_{n+2}-2x_{n+1}+x_n.
\end{equation}
L'accélération a alors la formule
\begin{equation}
    y_n=\frac{ (\Delta x)_n^2 }{ (\Delta^2x)_n }.
\end{equation}

Le problème est que ça accélère tellement que l'on arrive vite à des erreurs de cancellations, et donc à une précision en pics oscillants.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Équations algébrique}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

C'est une équation du type \( P(x)=0\) où \( P\) est un polynôme. Soit un polynôme de degré \( n\). Nous en savons des choses.

\begin{enumerate}
    \item
        L'équation a exactement \( n\) solutions dans \( \eC\) en comptant les multiplicités.
    \item
        Les racines complexes arrivent par paire complexes conjuguée. Elles sont donc toujours en nombre pair.
\end{enumerate}

Si donc nous avons \( n=3\), nous ne pouvons pas avoir \( 2\) racine réelles. Il y en a donc \( 1\) ou \( 3\) réelles. Pas zéro ni deux.

Quelque méthodes : Müller, matrice compagnon, Laguerre.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Résoudre un système linéaire}
%---------------------------------------------------------------------------------------------------------------------------

Pour résoudre un système linéaire d'équations, nous échelonnons la matrice du système. Soit à résoudre le système $Ax=b$ où
\begin{equation}
	\begin{aligned}[]
		A&=\begin{pmatrix}
			2   &   4   &   -6  \\
			1   &   5   &   3   \\
			1   &   3   &   2
		\end{pmatrix}, &\text{et}&&b=\begin{pmatrix}
			-4  \\
			10  \\
			5
		\end{pmatrix}.
	\end{aligned}
\end{equation}
En termes de problèmes, on écrit $F\big( x,(A,b) \big)=Ax-b$. La donnée de ce problème est le couple $(A,b)$.

En ce qui concerne l'algorithme, on pose comme premier problème
\begin{equation}
	F_1\big(x_1,(A_1,b_1)\big)=A_1x_1-b_1=0
\end{equation}
avec $A_1=A$ et $b_1=b$.

Ensuite, on commence à échelonner et le second problème est
\begin{equation}
	F_2\big(x_2,(A_2,b_2)\big)=A_2x_2-b_2=0
\end{equation}
avec
\begin{equation}
	\begin{aligned}[]
		A&=\begin{pmatrix}
			2   &   4   &   -6  \\
			0   &   3   &   6   \\
			0   &   1   &   5
		\end{pmatrix}, &\text{et}&&b=\begin{pmatrix}
			-4  \\
			12  \\
			13
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Le troisième problème sera
\begin{equation}
	F_3\big(x_3,(A_3,b_3)\big)=A_3x_3-b_3=0
\end{equation}
avec
\begin{equation}
	\begin{aligned}[]
		A&=\begin{pmatrix}
			2   &   4   &   -6  \\
			0   &   3   &   6   \\
			0   &   0   &   3
		\end{pmatrix}, &\text{et}&&b=\begin{pmatrix}
			-4  \\
			12  \\
			3
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Ce problème est facile à résoudre «à la main». Nous nous arrêtons donc ici avec l'algorithme, et nous trouvons le $x_3$ qui résous le problème $F_3$.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Caractéristiques}
%---------------------------------------------------------------------------------------------------------------------------

L'algorithme de résolution de systèmes linéaires d'équations a les propriétés suivantes, à mettre en contraste avec celles de Newton :
\begin{enumerate}

	\item
		Pour résoudre le problème numéro $n$, il n'a pas fallu résoudre le problème numéro $n-1$.
	\item
		Toutes les solutions $x_n$ des problèmes intermédiaires sont solutions du problème de départ. Nous avons $F_n(x,d_n)=0$ pour tout $n$ (ici, $d_n=(A_n,b_n)$).
	\item
		D'un problème à l'autre, les données changent énormément : la matrice échelonnée peut être très différente de la matrice de départ.

\end{enumerate}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définitions}
%---------------------------------------------------------------------------------------------------------------------------

	Nous allons maintenant formaliser en donnant quelques définitions pour nommer les propriétés que nous avons vues. D'abord, un algorithme est une suite de problèmes. Un \defe{algorithme}{algorithme} pour résoudre un problème $F(x,d)=0$ est une suite de problèmes $\{F_n(x_n,d_n)=0\}_{n\in\eN}$.

\begin{definition}
	Un tel algorithme est dit  \defe{fortement consistant}{algorithme!fortement consistant} si pour toutes données admissibles $d_n$, on a
	\begin{equation}
		F_n(x,d_n)=0\quad\forall \;n,
	\end{equation}
	où $x$ est la solution de $F(x,d)=0$.
\end{definition}
L'algorithme des matrices est fortement consistant, mais pas l'algorithme de Newton.

\begin{definition}
	Un algorithme est \defe{consistant}{algorithme!consistant} si $\lim_{n\to\infty}F_n(x,d_n)=0$.
\end{definition}
Dans le cas de l'algorithme de Newton, c'est plutôt une telle consistance qu'on attend.

L'algorithme est dit \defe{stable}{algorithme!stable} si pour tout $n$ le problème correspondant est stable.  Dans ce cas, on note $K^{\mbox{num}}$ le  \defe{conditionnement relatif asymptotique}{conditionnement!relatif asymptotique} défini par
\begin{equation}
	K^{\mbox{num}}=\limsup_nK_n
\end{equation}
où $K_n$ est le conditionnement relatif du problème $F_n(x_n,d_n)=0$.

\begin{definition}      \label{DefAlgoConverge}
	Un algorithme est dit \defe{convergent}{algorithme!convergent} (en $d$) si pour tout $\epsilon>0$, il existe $N=N(\epsilon)$ et $\delta=\delta(N,\epsilon)$ tels que pour $n\geq0$ et $|d-d_n|<\delta$, on ait $|x(d)-x_n(d_n)|<\epsilon$.
\end{definition}

\begin{remark}      \label{RemConvAlgoNewton}
Dans le cas de l'algorithme de Newton, nous avons vu que la donnée $d_n$ du problème $F_n$ était en fait la même que la donnée initiale $d$, donc nous avons $d_n=d$, et par conséquent nous avons toujours $| d-d_n |<\delta$. Dans ce cas, la définition de la convergence revient à demander que la suite numérique des $x_n$ converge vers la solution $x$.
\end{remark}

\begin{remark}
Dans le cas des matrices par contre, les données sont très différentes les unes des autres, nous avons donc en général que $| d-d_n |>\delta$. Mais en revanche nous savons que tous les problèmes intermédiaires $F_n$ acceptent une solution unique\footnote{Nous n'envisageons que le cas où le déterminant est non nul.} $x_n(d_n)=x(d)$. Par conséquent, $| x_n(d_n)-x(d) |$ est toujours plus petit que $\epsilon$. L'algorithme des matrice est donc toujours un algorithme convergent.
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Représentations numériques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Dans cette section les séquences de chiffres écrites entre crochet sont à comprendre comme des séquences de chiffres qui représentent une quantité suivant un codage donné.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Entier relatif en complément à deux (binaire)}
%---------------------------------------------------------------------------------------------------------------------------

Si nous avons \( m\) bits pour coder un entier relatif, une idée serait de prendre le premier bit pour le signe (\( 0\) pour positif et \( 1\) pour négatif) et les autres pour la valeur absolue. Deux inconvénients :
\begin{enumerate}
    \item
        Il y a deux codages pour le zéro, donc gaspillage.
    \item
        L'algorithme pour faire la somme passe mal. Par exemple pour faire \( 1+(-1)\), le \( 1\) est codé comme \( [001]\) et le \( -1\) par \( [101]\) et la somme se ferait naïvement comme
        \begin{equation*}
            \begin{array}[]{ccc}
                0&0&1\\
                1&0&1\\
                \hline
                1&1&0\\
            \end{array}
        \end{equation*}
        Donc le résultat est \( [110]\) qui s'interprète comme \( -2\). Complètement faux.
\end{enumerate}
Une solution est d'utiliser le \defe{complément à deux}{complément!à deux}, qui est la façon usuelle de représenter des entiers signés. 
\begin{description}
    \item[Les entiers positifs] se codent normalement, en laissant à zéro le premier bit (donc si nous disposons de \( m\) bits, nous codons sur \( m-1\) bits).
    \item[Les entiers négatifs] se codent en trois étapes.
        \begin{itemize}
            \item coder la valeur absolue
            \item inverser tous les bits (d'où le nom de «complément à deux» )
            \item soustraire \( 1\).
        \end{itemize}
\end{description}

\begin{example}
    Pour coder \( -1\) nous faisons
    \begin{itemize}
        \item Nous codons \( 1\) : \( [001]\)
        \item Nous inversons tous les bits : \( [110]\)
        \item Nous faisons \( -1\) : \( [101]\).
    \end{itemize}
\end{example}

Avec ce système, la somme passe bien : calculer \( 1+(-1)\) donne
    \begin{equation*}
        \begin{array}[]{ccc}
            0&0&1\\
            1&0&1\\
            \hline
            1&1&0\\
            \hline
        \end{array}
    \end{equation*}
La réponse est donc \( [110]\) qu'il faut interpréter via le complément à deux. 
\begin{equation}
    110\stackrel{+1}{\longrightarrow}111\stackrel{\text{complément}}{\longrightarrow}000.
\end{equation}
Et ce dernier \( [000]\) s'interprète comme zéro.

\begin{definition}[Entier signé en complément à deux\cite{ooAPFIooUfhqqG}]
    La suite de bits \( [a_{m-1}\ldots a_0]\) s'interprète via la formule
    \begin{equation}        \label{EQooXFHKooHRXDmZ}
        -a_{m-1}2^{m-1}+\sum_{i=0}^{m-2}a_i2^i.
    \end{equation}
\end{definition}

Le premier bit donne effectivement le signe du nombre, mais l'interprétation d'un nombre n'est pas aussi simple que ce que l'on pourrait croire de prime abord.

\begin{example}[Entier signé en \( 8\) bits]
    Que pouvons nous faire avec \( 8\) bits ? Le plus grand nombre est codé par \( [01111111]\) qui vaut \( \sum_{k=0}^62^k=2^7-1=127\). (avez-vous utilisé la somme \eqref{EqASYTiCK} ?)

    Le plus petit nombre codable en \( 8\) bits n'est pas \( [11111111]\) mais bien \( [10000000]\) (cela est plus clair en regardant la formule \eqref{EQooXFHKooHRXDmZ} qu'en tentant de suivre la construction du complément à deux) qui signifie \( -2^7=-128\).

    Nous pouvons donc coder tous les nombres de \( -128\) à \( 127\).
\end{example}

Plus généralement un système qui codes des entiers signés en \( N\) bits utilisant le complément à deux peut coder de \( -(2^{N-1})\) à \( 2^{N-1}-1\).

\begin{normaltext}[Le dépassement]
    Que se passe-t-il lorsque nous commettons un dépassement ? Calculons sur \( 3\) bits la somme \( [011]+[001]\) qui revient à ajouter \( 1\) au nombre le plus grand :
    \begin{equation*}
        \begin{array}[]{ccc}
            0&1&1\\
            0&0&1\\
            \hline
            1&0&0\\
        \end{array}
    \end{equation*}
    qui signifie \( -2^2=-4\). Lors d'un dépassement, nous retombons automatiquement sur le plus petit. 
    
    Ce phénomène est bien connu des personnes qui programment sans faire attention dans certains languages de programmation qui ne font pas attention à votre place.
\end{normaltext}


\begin{definition}[Représentation en virgule fixe]
	Soit $x$ un réel. On définit sa \defe{représentation en virgule fixe}{représentation!virgule fixe} par
	\begin{equation}
		x=\{[x_nx_{n-1}...x_0,x_{-1}...x_{-m}], b, s\}
	\end{equation}
	avec  $b\in\eN, b\geq2$, $s\in\{0,1\}$ et $x_j\in\eN,x_j<b$ suivant la formule
	\begin{equation}
		x=(-1)^{s}\sum_{j=-m}^nx_j.b^j.
	\end{equation}
\end{definition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Représentation en virgule flottante}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Représentation en virgule flottante]     \label{DEFooLYONooBNskty}
    La \defe{représentation en \href{https://docs.python.org/tutorial/floatingpoint.html}{virgule flottante} normalisée}{Représentation!virgule flottante normalisée} en base \( b\) d'un nombre est la donnée de 
    \begin{enumerate}
        \item
            Un bit \( s\) pour le signe
        \item
            Un entier \emph{non signé} \( q\) de \( e\) chiffres pour l'exposant
        \item
            Une suite de chiffres \( [a_1\ldots a_m]\) pour la mantisse.
    \end{enumerate}
    Ces données s'interprètent via la formule
    \begin{equation}        \label{EQooAGWJooRuBbBn}
        \fl( s,q,[a_1,\ldots, a_m]  )=(-1)^s\sum_{j=1}^mb^ja_j\times b^{q-d}
    \end{equation}
    où \( d=b^{e-1}\) est le \defe{décalage}{décalage}.
\end{definition}
Une idée à retenir est que l'exposant est un entier non signé parce qu'il est plus simple d'introduire un décalage dans la formule \eqref{EQooAGWJooRuBbBn} que de compliquer l'écriture de l'exposant.

En écriture binaire, la représentation en virgule flottante est un peu différente parce qu'il y a une idée supplémentaire; donc la définition suivante n'est pas un cas particulier de \ref{DEFooLYONooBNskty} avec \( b=2\). Nous allons voir la simple précision.
\begin{definition}[Représentation en simple précision (binaire)]
    La représentation en \defe{précision simple}{précision!simple} d'un nombre se fait sur \( 32\) bits répartis comme suit :
    \begin{enumerate}
        \item
            \( 1\) bit pour le signe,
        \item
            \( 8\) bits pour l'exposant interprété comme nombre entier non signé
        \item
            \( 23\) bits pour la mantisse
    \end{enumerate}
    Nous définissons le décalage comme \( d=2^{8-1}-1=127\).

    Soit le triple 
    \begin{equation}
        \big( s,q,[a_1,\ldots, a_{32}] \big)
    \end{equation}

    Selon que l'exposant réel \( q-d\) soit égal à \( 0\), \( 2^8-1=127\) ou autre chose, les règles d'interprétation sont différentes. Il y a donc trois cas.
    \begin{description}
        \item[Exposant réel générique\cite{ooMPTNooYbSwJS}] Si \( q-d\) n'est ni \( 0\) ni \( 127\) alors la représentation est dite \defe{normalisée}{représentation!normalisée}. Dans ce cas se trouve l'idée de génie qui rend tout compliqué : il y a un \( 1\) sous-entendu en première position de la mantisse. Donc la mantisse \( [a_1,\ldots, a_{32}]\) est à lire comme le nombre
            \begin{equation}
                1,a_1\ldots a_{32}=1+\sum_{j=1}^{32}a_j2^{-j}.
            \end{equation}
            \begin{example}
                La mantisse \( [011100\ldots 0]\) signifie \(1,0111=1+2^{-2}+2^{-3}+2^{-4}=1+\frac{1}{ 4 }+\frac{1}{ 8 }+\frac{1}{ 16 } \).
            \end{example}
        \item[Exposant réel égal à \( 0\)] Si \( q-d\) est égal à zéro, alors l'idée est de pas mettre de \( 1\) au début de la mantisse, mais de le mettre au bout de la mantisse; le but est d'être capable de coder des nombres plus petits.

    \end{description}
    <++>

\end{definition}

\begin{definition}
	L'\defe{erreur relative}{erreur relative} commise en remplaçant un nombre réel $x$ par une valeur approchée $\hat{x}$ est définie par
	\begin{equation}
		\epsilon_x:=\left|\frac{x-\hat{x}}{x}\right|.
	\end{equation}
\end{definition}

L'erreur relative n'est pas influencée par l'ordre de grandeur de \( x\). En effet, l'ordre de grandeur de \( \hat x\) est certainement la même que celle de \( x\), dans la majorité des cas sans problèmes. Du coup si \( x'=200x\) alors \( \hat{x'}\simeq 200\hat{x}\) et le \( 200\) se simplifie.

Le nombre de chiffres significatifs correct dans l'approximation est donné par \( -\log_{10}(\epsilon_x)\). La partie entière de ce nombre est le nombre de chiffres tout à fait exacts et la partie décimale donne une idée sur le fait que le chiffre suivant est plus ou moins bien.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Problèmes pour écrire des nombres}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{remark}
	Si nous voulons donner \( x\in \eR\) à un ordinateur, nous sommes soumis à deux erreurs :
	\begin{enumerate}
		\item
			D'abord, vu que nous ne pouvons pas taper sur le clavier toutes les décimales de \( x\), nous faisons une \defe{erreur de troncature}{erreur!troncature}.
		\item
			L'ordinateur devant convertir cela en base deux, il commet une seconde erreur, dite \defe{erreur d'assignation}{erreur!assignation}.
	\end{enumerate}
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Troncature : la base}
%---------------------------------------------------------------------------------------------------------------------------

Supposons que nous voulions écrire le nombre (écrit ici en base \( 10\))
\begin{equation}
	0.4567894251
\end{equation}
de façon plus facile à lire, on peut demander de ne laisser que \( t\) chiffres significatifs. Disons \( t=3\).

\begin{description}
	\item[Technique de troncature] On garde \( 3\) chiffres significatifs : \( 0.456\). Facile.
	\item[Technique d'arrondi] Vu que le premier qu'on supprime est un \( 7\), le dernier qu'on garde est majoré de \( 1\) : on écrit \( 0.457\).
\end{description}

Que faire si le premier chiffre rejeté est un \( 5\) ? En première approximation, nous pouvons prendre la règle suivante : si le premier chiffre rejeté est un \( 5\), il faut augmenter de \( 1\) de dernier chiffre gardé parce qu'il y a presque certainement encore un chiffre non nul derrière.

\begin{remark}
	Les ordinateurs travaillent tous en mode d'arrondi.
\end{remark}

\begin{example}
    Si on doit entrer le nombre \( 0.38358546\) dans un ordinateur qui ne garde que \( 3\) chiffres significatifs, il faut taper \( 0.384\) au clavier (erreur classique dans les exercices).
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Troncature : le drift}
%---------------------------------------------------------------------------------------------------------------------------

Soit une machine ne pouvant retenir que \( 3\) chiffres significatifs et effectuant les arrondis vers le haut lorsque le chiffre à éliminer est un \( 5\). Nous notons \( \oplus\) et \( \ominus\) les opérations d'addition et soustraction avec arrondis\cite{ooAGVZooTIcZZb}. Les égalités comprenant plus de trois chiffres significatifs sont des égalités au sens de la machine. Nous écrirons donc sans états d'âme :
\begin{equation}
    1\oplus0.555=1.555=1.56.
\end{equation}

Considérons la suite numérique
\begin{subequations}
    \begin{numcases}{}
        x_0=1.00\\
        x_n=(x_{n-1}\ominus y)\oplus y
    \end{numcases}
\end{subequations}
avec \( y=-0.555\).

Nous avons
\begin{equation}
    x_1=(1\oplus 0.555)\ominus 0.555=1.56\ominus 0.555=1.005=1.01
\end{equation}
et ensuite
\begin{equation}
    x_2=(1.01\oplus 0.555)\ominus 0.555=1.565\ominus 0.555=1.57\ominus 0.555=1.015=1.02.
\end{equation}
Et ainsi de suite. La suite est donc croissante alors que la définition nous donnerait envie d'avoir \( x_n=x_0\) pour tout \( n\).

\begin{remark}
    En réalité, cette suite se stabilise à \( x_n=10\) pour tout \( n\) à partir de \( n=845\). En effet,
    \begin{equation}
        (10\oplus 0.555)\ominus 0.555=10.555\ominus 0.555=10.6\ominus 0.555=10.045=10.
    \end{equation}
    Le fait est qu'à ce moment, l'erreur de troncature est assez loin dans les décimales pour que le premier chiffre négligé soit un ``0'' au lieu d'un ``5''.
    
    Notons toutefois que cette stabilité n'est pas là pour nous rassurer parce qu'elle n'en est pas moins complètement fausse.
\end{remark}

La règle de troncature adoptée dans Sage est d'arrondir au nombre pair le plus proche lorsque le premier nombre à négliger est un \( 5\). Donc \( 12.5\) s'arrondit à \( 12\) plutôt que \( 13\).

\begin{example}
	Soient les expressions (algébriquement égales) :
	\begin{enumerate}
		\item
			\(A= x(x+1)\)
		\item
			\(B= x^2+x\)
	\end{enumerate}
	Nous savons que
	\begin{equation}
		x=\fl(x)=10^{-30}
	\end{equation}
	et
	\begin{equation}
		1=\fl(1)
	\end{equation}
	parce que pour \( 1\) et \( 10^{-30}\), il n'y a pas d'erreurs d'assignation.

	En précision simple, \( 10^{-30}+1=1\) parce qu'en précision simple, il n'y a que \( 7\) ou \( 8\) chiffres significatifs\footnote{Erreur de « relation normale».}.

	Nous avons $A=10^{-30}$, mais \( x^2\) donne un \info{underflow} parce que \( 10^{-60}\) ne peut pas être représenté en précision simple. En pratique, beaucoup de logiciels en font \( 0\). Dans ce cas, en réalité \( B\) donne effectivement \( 10^{-30}\) après avoir fait \( x^2+x=0+x=10^{-30}\).
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelque bonnes règles}
%---------------------------------------------------------------------------------------------------------------------------

\begin{enumerate}
	\item
		Si on a plusieurs nombres à additionner ou soustraire, il vaut mieux commencer par sommer ou soustraire ceux dont on sait qu'ils ont le même ordre de grandeur. Il n'y a donc pas tout à fait «associativité» des erreurs.
	\item
		Les opérations délicates sont l'addition et la soustraction. La multiplication et la division sont sans dangers, à part l'erreur de dépassement du maximum. Dans une multiplication, on perd au pire quelque chiffres significatifs, mais certainement les derniers, pas les premiers.
\end{enumerate}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Erreur de ``cancellation''}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Lorsque deux nombres sont de même ordre de grandeur, avec plusieurs nombres significatifs identiques. La cancellation est le fait que, suite à la soustraction, tous les chiffres significatifs ou presque se sont simplifiés et qu'il ne reste plus que des chiffres non significatifs. 

\begin{example}[\cite{ooIZQWooYJmQmW}]
    Sur une machine ne gardant que \( 4\) chiffres significatifs, faire
    \begin{equation}
        0.5678\times 10^6-0.5677\times 10^6 = 0.0001\times 10^6=0.1000\times 10^3.
    \end{equation}
    Le fait est que les trois derniers zéros ne sont pas significatifs, mais maintenant la machine nous fait croire qu'ils le sont.

    Une autre façon de voir ce problème est d'imaginer qu'il faille faire la différence entre \( 0.5678\,289798\times 10^6\) et \( 0.5677\,3136907\) sur cette machine. Certes la machine nous autorise à avoir \( 4\) chiffres significatifs, donc au moment d'entrer les nombres nous perdons un beau paquet de chiffres. Mais au moment de faire la différence, nous perdons (presque) tout le reste. Donc là où nous pouvions espérer avoir \( 4\) chiffres significatifs de la différence, nous n'en avons que \( 1\). Les trois derniers zéros de la réponse (\( 0.1000\times 10^3\)) sont faux.
\end{example}

\begin{example}
	Soit à résoudre l'équation \( ax^2+bx+c=0\) avec \( a,b,c\neq 0\) et \( b^2-4ac>0\). Solution :
	\begin{equation}
		x_{1,2}=\frac{ -b\pm\sqrt{b^2-4ac} }{ 2a }.
	\end{equation}

	Supposons que \( | 4ac |\ll b^2\) avec tout de même pas tellement petit qu'on se perd dans la précision. Bref, on suppose que seules quelque dernières décimales de \( b^2-4ac\) sont différentes de zéro.

	On a :
	\begin{subequations}
		\begin{align}
			\sqrt{b^2-4ac}&=\sqrt{\tilde b}= | \tilde b | \\
			x_1&=\frac{ -b-\sqrt{b^2-4ac} }{ 2a }\\
			x_2&=\frac{ -b+\sqrt{b^2-4ac} }{ 2a }
		\end{align}
	\end{subequations}
	Si \( b>0\), nous avons une erreur de cancellation dans \( x_2\) parce qu'on fait la différence entre deux nombres presque égaux. Donc \( x_2\) mal calculé. Par contre \( x_1\) est bien calculé.


	Si par contre \( b<0\), c'est le contraire.


	Avec \( a=10^{-3}\), \( b=0.8\), \( c=-1.2\times 10^{-5}\). À la main nous obtenons : \( x_1=-800\), \( x_2=1.5\times 10^{-5}\), et un ordinateur se tromperait \ldots


\lstinputlisting{sageSnip001.sage}

	Donc Sage ne tombe pas dans le piège.
\end{example}

Comment résoudre ce problème ? Ou, autre façon de poser la question : comment Sage a fait pour résoudre le problème ?

Utilisons les relations coefficients-racines :
\begin{subequations}
	\begin{align}
		x_1+x_2&=-b/a\\
		x_1x_2&=c/a
	\end{align}
\end{subequations}
La première lie les deux racines par des opérations de addition et soustractions, et donc n'est pas intéressantes. La seconde est bien. Si nous connaissons \( x_1\), nous calculons
\begin{equation}
	x_2=\frac{ c }{ ax_1 }.
\end{equation}

Quitte à redéfinir \( x_1\) et \( x_2\), la solution bien calculée est :
\begin{equation}
	x_1=\frac{ -b-\signe(b)\sqrt{b^2-4ac} }{ 2a }.
\end{equation}

\begin{example}
	Nous considérons :
	\begin{equation}
		f(x)=cos(x+\delta)-\cos(x).
	\end{equation}
	Cela a une erreur de cancellation lorsque \( | \delta |\ll | x |\). On élimine l'erreur de cancellation par
	\begin{equation}
		f(x)=-2\sin(\delta/2)\sin\left( x+\frac{ \delta }{ 2 } \right).
	\end{equation}

	\begin{probleme}
		Pourquoi la condition pour avoir l'erreur est \( \delta\ll x\) et non simplement \( \delta\ll 1\) ?
	\end{probleme}

\end{example}

\begin{example}
	Pour
	\begin{equation}
		f(x)=\sqrt{x+\delta}-\sqrt{x}.
	\end{equation}
	On fait la coup du binôme conjugué :
	\begin{equation}
		f(x)=\frac{ \delta }{ \sqrt{x+\delta}+\sqrt{x} }.
	\end{equation}
	Plus d'erreur de cancellation, vu qu'au dénominateur nous avons une somme de deux positifs.
\end{example}

Les erreurs de cancellation ne se résolvent pas en augmentant la précision des nombres donnés.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Erreur d'absorption}
%---------------------------------------------------------------------------------------------------------------------------

L'addition d'un nombre avec un nombre très différent peut faire perdre de l'information sur le plus petit. Par exemple avec \( 4\) chiffres significatifs,
\begin{equation}
    0.5678\oplus 0.0001237=0.5679
\end{equation}
où nous avons perdu presque toute l'information du petit nombre.

Une situation particulièrement ennuyeuse est celle où justement c'est le petit nombre qui nous intéresse parce que le grand est censé se simplifier : 
\begin{equation}
    (0.0001327\oplus 0.5678)\ominus 0.5678=0.5679\ominus 0.5678=0.0001
\end{equation}
qui ne possède qu'un seul chiffre significatif correct alors que voyant le calcul, la réponse aurait pu être trouvée.

Moralité : si certains manipulations algébrique peuvent faire apparaître des simplifications avant de passer le calcul à la machine, il est bon de les effectuer.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Calcul d'une dérivée}
%---------------------------------------------------------------------------------------------------------------------------

Pour calculer la dérivée de \( f\) en \( a\), il est loisible d'utiliser la formule
\begin{equation}
    f'(a)=\lim_{h\to 0} \frac{ f(a+h)-f(a) }{ h }.
\end{equation}
Le numérateur est alors sujet à une erreur d'absorption dans le calcul de \( a+h\) et ensuite une erreur de cancellation dans le calcul de la différence.

En utilisant la formule
\begin{equation}
    f'(a)=\lim_{h\to 0} \frac{ f(a+h)-f(a-h) }{ 2h }
\end{equation}
nous pouvons espérer avoir une erreur de cancellation plus petite.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Équations non linéaire}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Certains équations non linéaires sont résoluble explicitement, par exemples les polynômes de degré jusqu'à \( 4\) ou des choses comme
\begin{equation}
	\sin^2(x)+3\sin(x)+5=0.
\end{equation}
Mais ces exemples sont très rares.

Nous allons étudier des équations du type \( f(x)=0\), dans \( \eR\).

\begin{enumerate}
	\item
Un problème écrit sous la forme \( x=g(x)\) peut utiliser des théorèmes de points fixes.
\item
	Un problème sous la forme \( f(x)=0\) peut utiliser des méthodes de bisection, Newton ou autres.
\end{enumerate}
Il y a évidemment beaucoup de façons de transformer un problème pour passer d'une forme à l'autre.

\begin{example}
	Soit \( f(x)=x^2-a=0\) avec \( a>0\). Nous pouvons l'écrire
	\begin{equation}
		x^2+x-a=x
	\end{equation}
	qui donne une forme \( g(x)=x\) pour \( g(x)=x^2+x-a\).

	Ou encore \( x=\frac{ a }{ x }\) et donc \( g(x)=a/x\) (si par ailleurs on sait que \( x\neq 0\)). Notons que \( x\neq 0\) n'est pas une hypothèse très forte parce qu'on la vérifie directement sur \( a\).
\end{example}

\begin{example}
	Soit l'équation à résoudre
	\begin{equation}
		f(x)=x^2-2-\ln(x)=0
	\end{equation}
	Les solutions de cette équations peuvent être vues comme les intersections avec l'axe \( X\) du graphe \( y=x^2-2-\ln(x)\). Tracer peut donc aider. Par ailleurs, il faut noter que
	\begin{equation}
		\lim_{x\to \pm\infty} f(x)=\infty,
	\end{equation}
	donc les solutions sont certainement contenues dans un compact de \( \eR\).

	À part tracer nous pouvons écrire
	\begin{equation}
		x^2-2=\ln(x).
	\end{equation}
	Et là, ce sont deux fonctions dont nous pouvons tracer le graphe pour trouver graphiquement les points d'intersection. Une étude de fonction montre vite qu'il y a exactement deux solutions, qu'elles sont strictement positives. Pour trouver des bornes, il faut calculer par exemple pour \( x=2\) les valeurs de \( \ln(x)\) et \( x^2-2\) pour voir si le graphe de \( x^2-2\) est déjà plus haut.
\end{example}

La majorité des méthodes numériques de résolution d'équation du type \( f(x)=0\) ou \( x=g(x)\) seront sous la forme de suites. Avec questions à la clefs :
\begin{enumerate}
	\item
		Quel point de départ choisir ?
	\item
		Convergence ?
	\item
		Est-ce que la limite est bien une solution ?
	\item
		Vu que la limite est unique, comment faire si l'équation a plusieurs solutions ? (souvent c'est le choix du point initial qui va jouer sur ce point)
\end{enumerate}

\begin{normaltext}
	Si la fonction est très plate, il est possible d'avoir
	\begin{equation}
		| f(\tilde \alpha) |\leq \epsilon
	\end{equation}
	sans que \( \tilde \alpha\) ne soit une bonne approximation.

	Lorsqu'on fait tourner une méthode itérative résolvant \( f(x)=0\), il n'est pas suffisant de s'arrêter lorsque
	\begin{equation}
		f(x_n)\leq \epsilon_1.
	\end{equation}
	Il faut aussi s'assurer que, si \( \bar x\) est la solution exacte, \( | x_n-\bar x |\leq \epsilon_2\). Ici \( \epsilon_1\) et \( \epsilon_2\) sont deux «précisions» que nous nous fixons au départ.

	Évidemment, vérifier la condition \( | x_n-\bar x |\leq \epsilon_2\), il faudrait savoir \( \bar x\). Et savoir \( \bar x\) c'est justement le problème. Nous sommes donc amenés à faire des estimation de \( | x_n-\bar x |\).
\end{normaltext}

\begin{normaltext}
    Lorsque nous effectuons une méthode itérative, il faut donc contrôler deux grandeurs :
    \begin{subequations}
        \begin{align}
            | \bar x-x_n |\leq \epsilon_1\\
            | x_{n+1}-x_n |\leq \epsilon_2.
        \end{align}
    \end{subequations}
\end{normaltext}

\begin{proposition}
Soit \( p\) l'ordre de convergence de la suite \( (x_n)\) vers \( \bar x\). Si \( p>1\) et \( | x_{n+1}-x_n |\leq \epsilon_2\) alors \( | \bar x-x_n |\leq \epsilon_2\).
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode de bisection}
%---------------------------------------------------------------------------------------------------------------------------

Il y a ce théorème des valeurs intermédiaires.
\begin{theorem}
	Soit \( f\) continue sur \( \mathopen[ a , b \mathclose]\) telle que \( f(a)f(b)<0\). Alors il existe au moins une solution à l'équation \( f(x)=0\) sur l'intervalle \( \mathopen] a , b \mathclose[\).
\end{theorem}

Pour démarrer une bisection, il est toujours bon de prendre l'intervalle \( \mathopen[ a , b \mathclose]\) de façon à ne contenir qu'une seule solution.

Soit donc un premier intervalle \( \mathopen[ a_0 , b_O \mathclose]\) tel que \( f(a_0)f(b_0)<0\) et ne contenant qu'une seule solution. À chaque itération nous considérons la moitié de l'intervalle précédent, mais la moitié contenant la solution.

Le test d'arrêt de la méthode de bisection se base uniquement sur la taille de l'intervalle qui reste. En effet si nous avons
\begin{equation}
	| b_n-a_n |\leq \epsilon
\end{equation}
nous avons certainement
\begin{equation}
	| \bar x-x_n |\leq \frac{ \epsilon }{2}
\end{equation}
où \( x_n\) est le point du milieu de \( \mathopen[ a_n , b_n \mathclose]\).

\begin{normaltext}
    La fonction \( f\) n'intervient dans la méthode que via son signe, pas via ses valeurs exactes.
\end{normaltext}

\begin{normaltext}
    Notons que le théorème des valeurs intermédiaires n'est pas très puissant pour choisir l'intervalle de départ; penser à la fonction
    \begin{equation}
        f(x)=x^2-5
    \end{equation}
    sur l'intervalle \( \mathopen[ -10 , 10 \mathclose]\). Il y a bien deux solutions dans l'intervalle, mais elles sont invisibles du théorème des valeurs intermédiaires. La fonction \( x\mapsto x^2\) a sa solution en \( x=0\), mais elle aussi n'est pas visible.
\end{normaltext}

\begin{normaltext}
    Certes la méthodes de bisection assure la convergence vers une solution, mais elle n'assure pas la convergence monotone. Il peut arriver que \( | \bar x-x_n |<| \bar x-x_{n+1} |\). C'est le cas lorsque la solution est très proche du milieu de l'intervalle choisit. Le \( x_0\) est alors proche de \( \bar x\) alors que \( x_1\) sera à une distance de \( \bar x\) d'environ un quart de l'intervalle de départ.
\end{normaltext}

Supposons déjà avoir trouvé un intervalle \( \mathopen[ a , b \mathclose]\) dans lequel se trouve une unique solution à \( f(x)=0\). Voici un algorithme possible.

\lstinputlisting{codeSnip_1.py}

Plusieurs remarques :
\begin{enumerate}
    \item
Le fait de retourner le nombre d'itérations effectuées permet à l'utilisateur de savoir la précision et si le nombre maximum d'itérations est dépassé. Si ce \info{n} retourné est égal à \info{nmax}, l'utilisateur sait que le \info{x} retourné n'est pas fiable.
\item
    La ligne \info{from \_\_future\_\_ import division} fait en sorte que l'opération \info{/} est bien la division usuelle. Sinon, le défaut en python 2 est que \info{/} soit la division \emph{entière}, c'est à dire que \( 1/2=0\) en python 2.o
    En python 3, le symbole \info{/} désigne bien la division usuelle, mais Sage utilise Python 2.
\item
    Même si l'intervalle \( \mathopen[ a , b \mathclose]\) contient plus d'une solution, la méthode fonctionne et donne une solution. Il est simplement éventuellement très compliqué de savoir laquelle.
\item
    Nous faisons \info{amp=toll+1} parce que nous voulons absolument lancer le cycle au moins une fois. Sinon, le \info{x} à retourner ne serait pas définit au moment de sortir du cycle (si le cycle n'est pas exécuté).
\item
    Calculer le point milieu d'un intervalle \( \mathopen[ a , b \mathclose]\) est par la formule \( (a+b)/2\) sauf que cette opération est numériquement dangereuse parce qu'à cause de l'arithmétique en précision finie, il est possible que cela tombe \emph{exactement} sur \( a\) ou \( b\). D'où le fait de calculer le point milieu par
    \begin{equation}
        x=a+\frac{ amp }{2}.
    \end{equation}
\item
    Dans le cas \info{Problème ZERO} nous déduisons \( f(x)=0\). Attention que c'est pas que \( f(x)=0\) mais simplement que en mettant \( x\) dans \( f\), la \emph{machine} retourne son zéro.

    Il peut cependant avoir une fonction telle que \( f(1)=10^{-50}\) et \( f(2)=0\). L'algorithme de bisection risque de s'arrêter si \( x_n=1\). Parce que la machine risque de calculer \( f(x_n)=0\).

    Quoi qu'il en soit, nous y mettons \info{amp=0} pour être sûr de sortir de la boucle dès la prochaine vérification.
\item
    Il y a moyen de sauver les valeurs de \( f(a)\) et \( f(x)\) pour ne pas les recalculer, et en particulier au moment de faire \info{b=x} nous pouvons poser \(\info{fa=fx}\).
\end{enumerate}

Si \( \tau\) est la précision de la solution voulue, nous pouvons fixer a priori le nombre d'itérations à faire grâce à la formule
\begin{equation}
    n\geq\left\lceil  \log_2\big( \frac{ b-a }{ \tau } \big)  \right\rceil.
\end{equation}
Il y a un ``\( \geq\)'' et non une égalité parce qu'en arithmétique numérique, le nombre obtenu à droite pourrait ne pas être le bon à \( 1\) près.

Ici pour \( \nu\in \eR\) le nombre \( \lceil\nu\rceil\) est le plus petit entier à être plus grand ou égal à \( \nu\).

\begin{normaltext}
    Notons l'importance de la continuité de \( f\). Par exemple que ferait la bisection sur la fonction \( f(x)=1/x\) pour l'intervalle $\mathopen[ -3 , 1 \mathclose]$ ? 

    Il y a changement de signe sans avoir de racine.
\end{normaltext} 

Vu que \( 2^{10}\) est déjà \( 1024\). Donc si on veut de la précision de l'ordre de \( 1/1000\), dix itération suffisent. Si donc nous avons besoin de \( 200\) itérations pour atteindre la précision voulue, c'est l'occasion de trouver un intervalle plus petit. Par exemple en traçant la fonction, en faisant un zoom et en trouvant des valeurs de \( a\) et \( b\) qui sont déjà proches.


\begin{normaltext}
    Dans le monde réel, il arrive souvent d'utiliser une méthode de bisection pour se donner un point de départ pour une autre méthode.
\end{normaltext}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Efficacité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    L'\defe{efficacité}{efficacité!d'une méthode itérative} est le nombre
    \begin{equation}
        E=\sqrt[s]{ p }
    \end{equation}
    où \( p\) est l'ordre de convergence de la méthode et \( s\) est le nombre de fois qu'il faut calculer une valeur de la fonction à chaque itération (nous ne comptons pas l'initialisation).
\end{definition}
Que le nombre de valutations intervienne est logique parce que chaque valutation provoque une erreur possible.

\begin{example}[Bisection]
    Pour la méthode de bisection, nous avons \( s=1\) parce que chercher \( x_{n+1}\), il faut seulement calculer \( f(x_n)\).
\end{example}

\begin{example}[Newton]
    Pour l'algorithme de Newton nous avons \( p=2\) et il y a deux valutations à chaque itérations (une fois \( f\) et une fois \( f'\)), donc \( s=2\) et \( E=\sqrt{ 2 }\).
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Exercices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\Exo{mazhe-0002}
\Exo{mazhe-0003}
\Exo{mazhe-0004}

