% This is part of Mes notes de mathématique
% Copyright (C) 2010-2013,2016
%   Laurent Claessens
% See the file LICENCE.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Estimation de l'ordre de convergence}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Comment estimer numériquement l'ordre \( p\) de convergence de la méthode ? Soit une suite \( (x_n)\) convergente vers \( \alpha\). Considérons les \( 4\) termes \( x_{n-3}\), \( x_{n-2}\), \( x_{n-1}\), \( x_n\). Alors nous pouvons écrire l'approximation
\begin{equation}
    \frac{ | x_n -x_{n-1}| }{ | x_{n-1}-x_{n-2} | }\simeq \left( \frac{ | x_{n-1}-x_{n-2} | }{ | x_{n-1}-x_{n-3} | } \right)^p.
\end{equation}
Cette approximation ne serait pas trop mauvaise tant que \( n\) est assez grand pour que la convergence soit bien engagée. Passons au logarithme :
\begin{equation}
    \ln \frac{ | x_n -x_{n-1}| }{ | x_{n-1}-x_{n-2} | }\simeq p\ln \left( \frac{ | x_{n-1}-x_{n-2} | }{ | x_{n-1}-x_{n-3} | } \right).
\end{equation}
et donc
\begin{equation}
    p\simeq \frac{ \ln\left( \frac{ | x_n -x_{n-1}| }{ | x_{n-1}-x_{n-2} } \right) }{ \ln \left(\frac{ | x_{n-1}-x_{n-2} | }{ | x_{n-1}-x_{n-3} | } \right)}.
\end{equation}
Avec cette approximation, en réalité nous calculons une suite \( (p_i)\) qui sont les approximation de \( p\) à partir des termes \( i\) à \(i+3 \) de la suite \( (x_n)\). Il s'agit d'une suite d'estimations de \( p\).

\begin{enumerate}
    \item
Dans le cas de la bisection, nous obtenons toujours \( p_i=1\).
\item
    Dans le cas de la méthode de Newton (\ref{SECooIKXNooACLljs}) nous avons \( p=2\). Mais les premières valeurs de \( p_i\) peuvent être aussi bien \( 0\) que \( 7\). Après quelque itérations pourtant les \( p_i\) se regroupent autour de \( 2\).
\end{enumerate}
En tout cas, le plus important est de savoir si \( p>1\) ou non. Rappel : nous voulons la superlinéarité parce que nous voulons utiliser le test d'arrêt de la différence entre deux termes, voir \ref{NTooVXLXooXlAGEq}. 

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Autres méthodes}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Méthode de Schröder}
%---------------------------------------------------------------------------------------------------------------------------

La formule est
\begin{equation}
    x_{n+1}=x_n-\frac{ f(x_n)f'(x_n) }{ f'(x_n)^2-f(x_n)f''(x_n) }
\end{equation}
Cette méthode est d'ordre \( 2\) pour toute racine et toute valeur de multiplicité. Le problème de cette méthode est qu'elle demande \( 3\) évaluations de \( f\). Son efficacité :
\begin{equation}
    E=\sqrt[3]{ 2 }\simeq 1.25
\end{equation}
Cela est donc moins efficace que Newton.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Halley}
%---------------------------------------------------------------------------------------------------------------------------

Il a \( p=3\) lorsque \( \alpha\) est racine simple. Mais encore \( p=1\) pour les racines multiples. Plus efficace que Newton pour les racines simples, mais même problème pour les racines multiples.

\begin{equation}
    x_{n+1}=x_n-\frac{ 2f(x_n)f'(x_n) }{ 2f'(x_n)^2-f(x_n)f''(x_n) }
\end{equation}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Méthode des sécantes variables}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooIUEUooVcHAoc}

Supposons de ne pas avoir \( f\) analytique, mais seulement la possibilité de calculer \( f(x)\) pour tout \( x\). Newton ne fonctionne pas, mais la bisection fonctionne.

Nous pouvons approximer
\begin{equation}
    f'(x_n)=\frac{ f(x_n)-f(x_{n-1}) }{ x_n-x_{n-1} }.
\end{equation}
En substituant dans la formule de Newton, nous obtenons
\begin{equation}
    x_{n+1}=x_n-\frac{ f(x_n)(x_n-x_{n-1}) }{ f(x_n)-f(x_{n-1}) }.
\end{equation}

Il s'agit de prendre la droite qui passe par \( (x_{n-1},f(x_{n-1}))\) et par \( (x_n,f(x_n))\) et de prendre l'intersection de cette droite avec l'axe \( y=0\). Cela donne le \( x_{n+1}\).

Pour cette méthode, il ne faut pas seulement \( x_0\) mais également \( x_1\).

L'ordre de convergence est le nombre d'or
\begin{equation}    \label{EQooQEFCooUsGVjP}
    p=\frac{ 1-\sqrt{ 5 } }{ 2 }\simeq 1.618.
\end{equation}
Cela est donc superlinéaire.

La nombre d'évaluations est \( s=1\) (il y a deux apparitions de \( f\) dans la formule, mais l'une des deux est récupérée dans l'itération suivante). Donc l'efficacité est
\begin{equation}
    E=p.
\end{equation}
Donc bien efficace.

\begin{proposition}
    Si \( \alpha\) est racine simple, il existe un voisinage de \( \alpha\) tel que pour tout choix de \( x_0\), \( x_1\) dans ce voisinage, la méthode converge.
\end{proposition}

Psychologiquement, on est tenté de prendre \( x_0\) et \( x_1\) de part et d'autre de \( \alpha\) (pensant à la bisection), mais en réalité ce n'est pas obligatoire du tout et n'a aucune influence. Il faut seulement les prendre très proches de \( \alpha\).

\begin{remark}
    La méthode de la sécante est souvent écrite sous la forme
    \begin{equation}        \label{EQooYVKLooKTFjwv}
        x_{n+1}=\frac{ x_{n-1}f(x_n)-x_nf(x_{n-1}) }{ f(x_n)-f(x_{n-1}) }.
    \end{equation}
    C'est évidemment algébriquement équivalent. 

    Les formules \eqref{EQooQEFCooUsGVjP} et \eqref{EQooYVKLooKTFjwv} ont toutes deux des erreurs de cancellation. Laquelle est la plus grave ?

    Dans la première, si la fraction est mal calculée, elle ne fait que modifier \( x_n\). C'est à dire qu'on peut espérer qu'à la prochaine itération, ça aille mieux. En tout cas, dans ce cas si la fraction est mal calculée, ça ne détruit pas tout.

    Dans la seconde, c'est la valeur elle-même qui risque d'être mal calculée. Et si la fraction est mal calculée, alors on casse complètement l'éventuel bonne approximation que nous avions déjà.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Aitken}
%---------------------------------------------------------------------------------------------------------------------------

La méthode du \( \Delta^2\) de Aitken est une méthode d'accélération de la convergence.

Soit \( (x_n)\) une suite qui converge. Nous voudrions une nouvelle suite \( (y_n)\) telle que
\begin{equation}
    \lim_{n\to \infty} \frac{ y_n-\alpha }{ x_n-\alpha }
\end{equation}
C'est la définition d'une convergence accélérée.

La façon de faire est :
\begin{equation}
    y_n=\frac{ x_{n+2}x_n-x_{n+1}^2 }{ x_{n+2}-2x_{n+1}+x_n }=x_n-\frac{ (x_{n+1}-x_n) }{ x_{n+2}-2x_{x+1}+x_n }.
\end{equation}
La première expressions a deux cancellations (la seconde une seule) et de plus la première est $y_n$ elle-même alors que la seconde est une correction.

Donc la seconde expression est numériquement meilleure.

L'opérateur \( \Delta\) appliqué à une suite est :
\begin{equation}
    (\Delta x)_n=x_{n+1}-x_n
\end{equation}
Donc
\begin{equation}
    (\Delta^2x)_n= (\Delta x)_{n+1}-(\Delta x)_n=x_{n+2}-x_{n+1}-x_{n+1}+x_n=x_{n+2}-2x_{n+1}+x_n.
\end{equation}
L'accélération a alors la formule
\begin{equation}
    y_n=\frac{ (\Delta x)_n^2 }{ (\Delta^2x)_n }.
\end{equation}

Le problème est que ça accélère tellement que l'on arrive vite à des erreurs de cancellations, et donc à une précision en pics oscillants.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Équations algébrique}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

C'est une équation du type \( P(x)=0\) où \( P\) est un polynôme. Soit un polynôme de degré \( n\). Nous en savons des choses.

\begin{enumerate}
    \item
        L'équation a exactement \( n\) solutions dans \( \eC\) en comptant les multiplicités.
    \item
        Les racines complexes arrivent par paire complexes conjuguée. Elles sont donc toujours en nombre pair.
\end{enumerate}

Si donc nous avons \( n=3\), nous ne pouvons pas avoir \( 2\) racine réelles. Il y en a donc \( 1\) ou \( 3\) réelles. Pas zéro ni deux.

Quelque méthodes : Müller, matrice compagnon, Laguerre.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Résoudre un système linéaire}
%---------------------------------------------------------------------------------------------------------------------------

Pour résoudre un système linéaire d'équations, nous échelonnons la matrice du système. Soit à résoudre le système $Ax=b$ où
\begin{equation}
	\begin{aligned}[]
		A&=\begin{pmatrix}
			2   &   4   &   -6  \\
			1   &   5   &   3   \\
			1   &   3   &   2
		\end{pmatrix}, &\text{et}&&b=\begin{pmatrix}
			-4  \\
			10  \\
			5
		\end{pmatrix}.
	\end{aligned}
\end{equation}
En termes de problèmes, on écrit $F\big( x,(A,b) \big)=Ax-b$. La donnée de ce problème est le couple $(A,b)$.

En ce qui concerne l'algorithme, on pose comme premier problème
\begin{equation}
	F_1\big(x_1,(A_1,b_1)\big)=A_1x_1-b_1=0
\end{equation}
avec $A_1=A$ et $b_1=b$.

Ensuite, on commence à échelonner et le second problème est
\begin{equation}
	F_2\big(x_2,(A_2,b_2)\big)=A_2x_2-b_2=0
\end{equation}
avec
\begin{equation}
	\begin{aligned}[]
		A&=\begin{pmatrix}
			2   &   4   &   -6  \\
			0   &   3   &   6   \\
			0   &   1   &   5
		\end{pmatrix}, &\text{et}&&b=\begin{pmatrix}
			-4  \\
			12  \\
			13
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Le troisième problème sera
\begin{equation}
	F_3\big(x_3,(A_3,b_3)\big)=A_3x_3-b_3=0
\end{equation}
avec
\begin{equation}
	\begin{aligned}[]
		A&=\begin{pmatrix}
			2   &   4   &   -6  \\
			0   &   3   &   6   \\
			0   &   0   &   3
		\end{pmatrix}, &\text{et}&&b=\begin{pmatrix}
			-4  \\
			12  \\
			3
		\end{pmatrix}.
	\end{aligned}
\end{equation}
Ce problème est facile à résoudre «à la main». Nous nous arrêtons donc ici avec l'algorithme, et nous trouvons le $x_3$ qui résous le problème $F_3$.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Caractéristiques}
%---------------------------------------------------------------------------------------------------------------------------

L'algorithme de résolution de systèmes linéaires d'équations a les propriétés suivantes, à mettre en contraste avec celles de Newton :
\begin{enumerate}

	\item
		Pour résoudre le problème numéro $n$, il n'a pas fallu résoudre le problème numéro $n-1$.
	\item
		Toutes les solutions $x_n$ des problèmes intermédiaires sont solutions du problème de départ. Nous avons $F_n(x,d_n)=0$ pour tout $n$ (ici, $d_n=(A_n,b_n)$).
	\item
		D'un problème à l'autre, les données changent énormément : la matrice échelonnée peut être très différente de la matrice de départ.

\end{enumerate}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définitions}
%---------------------------------------------------------------------------------------------------------------------------

	Nous allons maintenant formaliser en donnant quelques définitions pour nommer les propriétés que nous avons vues. D'abord, un algorithme est une suite de problèmes. Un \defe{algorithme}{algorithme} pour résoudre un problème $F(x,d)=0$ est une suite de problèmes $\{F_n(x_n,d_n)=0\}_{n\in\eN}$.

\begin{definition}
	Un tel algorithme est dit  \defe{fortement consistant}{algorithme!fortement consistant} si pour toutes données admissibles $d_n$, on a
	\begin{equation}
		F_n(x,d_n)=0\quad\forall \;n,
	\end{equation}
	où $x$ est la solution de $F(x,d)=0$.
\end{definition}
L'algorithme des matrices est fortement consistant, mais pas l'algorithme de Newton.

\begin{definition}
	Un algorithme est \defe{consistant}{algorithme!consistant} si $\lim_{n\to\infty}F_n(x,d_n)=0$.
\end{definition}
Dans le cas de l'algorithme de Newton, c'est plutôt une telle consistance qu'on attend.

L'algorithme est dit \defe{stable}{algorithme!stable} si pour tout $n$ le problème correspondant est stable.  Dans ce cas, on note $K^{\mbox{num}}$ le  \defe{conditionnement relatif asymptotique}{conditionnement!relatif asymptotique} défini par
\begin{equation}
	K^{\mbox{num}}=\limsup_nK_n
\end{equation}
où $K_n$ est le conditionnement relatif du problème $F_n(x_n,d_n)=0$.

\begin{definition}      \label{DefAlgoConverge}
	Un algorithme est dit \defe{convergent}{algorithme!convergent} (en $d$) si pour tout $\epsilon>0$, il existe $N=N(\epsilon)$ et $\delta=\delta(N,\epsilon)$ tels que pour $n\geq0$ et $|d-d_n|<\delta$, on ait $|x(d)-x_n(d_n)|<\epsilon$.
\end{definition}

\begin{remark}      \label{RemConvAlgoNewton}
Dans le cas de l'algorithme de Newton, nous avons vu que la donnée $d_n$ du problème $F_n$ était en fait la même que la donnée initiale $d$, donc nous avons $d_n=d$, et par conséquent nous avons toujours $| d-d_n |<\delta$. Dans ce cas, la définition de la convergence revient à demander que la suite numérique des $x_n$ converge vers la solution $x$.
\end{remark}

\begin{remark}
Dans le cas des matrices par contre, les données sont très différentes les unes des autres, nous avons donc en général que $| d-d_n |>\delta$. Mais en revanche nous savons que tous les problèmes intermédiaires $F_n$ acceptent une solution unique\footnote{Nous n'envisageons que le cas où le déterminant est non nul.} $x_n(d_n)=x(d)$. Par conséquent, $| x_n(d_n)-x(d) |$ est toujours plus petit que $\epsilon$. L'algorithme des matrice est donc toujours un algorithme convergent.
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Équations non linéaire}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Certains équations non linéaires sont résoluble explicitement, par exemples les polynômes de degré jusqu'à \( 4\) ou des choses comme
\begin{equation}
	\sin^2(x)+3\sin(x)+5=0.
\end{equation}
Mais ces exemples sont très rares.

Nous allons étudier des équations du type \( f(x)=0\), dans \( \eR\).

\begin{enumerate}
	\item
Un problème écrit sous la forme \( x=g(x)\) peut utiliser des théorèmes de points fixes.
\item
	Un problème sous la forme \( f(x)=0\) peut utiliser des méthodes de bisection, Newton ou autres.
\end{enumerate}
Il y a évidemment beaucoup de façons de transformer un problème pour passer d'une forme à l'autre.

\begin{example}
	Soit \( f(x)=x^2-a=0\) avec \( a>0\). Nous pouvons l'écrire
	\begin{equation}
		x^2+x-a=x
	\end{equation}
	qui donne une forme \( g(x)=x\) pour \( g(x)=x^2+x-a\).

	Ou encore \( x=\frac{ a }{ x }\) et donc \( g(x)=a/x\) (si par ailleurs on sait que \( x\neq 0\)). Notons que \( x\neq 0\) n'est pas une hypothèse très forte parce qu'on la vérifie directement sur \( a\).
\end{example}

\begin{example}
	Soit l'équation à résoudre
	\begin{equation}
		f(x)=x^2-2-\ln(x)=0
	\end{equation}
	Les solutions de cette équations peuvent être vues comme les intersections avec l'axe \( X\) du graphe \( y=x^2-2-\ln(x)\). Tracer peut donc aider. Par ailleurs, il faut noter que
	\begin{equation}
		\lim_{x\to \pm\infty} f(x)=\infty,
	\end{equation}
	donc les solutions sont certainement contenues dans un compact de \( \eR\).

	À part tracer nous pouvons écrire
	\begin{equation}
		x^2-2=\ln(x).
	\end{equation}
	Et là, ce sont deux fonctions dont nous pouvons tracer le graphe pour trouver graphiquement les points d'intersection. Une étude de fonction montre vite qu'il y a exactement deux solutions, qu'elles sont strictement positives. Pour trouver des bornes, il faut calculer par exemple pour \( x=2\) les valeurs de \( \ln(x)\) et \( x^2-2\) pour voir si le graphe de \( x^2-2\) est déjà plus haut.
\end{example}

La majorité des méthodes numériques de résolution d'équation du type \( f(x)=0\) ou \( x=g(x)\) seront sous la forme de suites. Avec questions à la clefs :
\begin{enumerate}
	\item
		Quel point de départ choisir ?
	\item
		Convergence ?
	\item
		Est-ce que la limite est bien une solution ?
	\item
		Vu que la limite est unique, comment faire si l'équation a plusieurs solutions ? (souvent c'est le choix du point initial qui va jouer sur ce point)
\end{enumerate}

\begin{normaltext}
	Si la fonction est très plate, il est possible d'avoir
	\begin{equation}
		| f(\tilde \alpha) |\leq \epsilon
	\end{equation}
	sans que \( \tilde \alpha\) ne soit une bonne approximation.

	Lorsqu'on fait tourner une méthode itérative résolvant \( f(x)=0\), il n'est pas suffisant de s'arrêter lorsque
	\begin{equation}
		f(x_n)\leq \epsilon_1.
	\end{equation}
	Il faut aussi s'assurer que, si \( \bar x\) est la solution exacte, \( | x_n-\bar x |\leq \epsilon_2\). Ici \( \epsilon_1\) et \( \epsilon_2\) sont deux «précisions» que nous nous fixons au départ.

	Évidemment, vérifier la condition \( | x_n-\bar x |\leq \epsilon_2\), il faudrait savoir \( \bar x\). Et savoir \( \bar x\) c'est justement le problème. Nous sommes donc amenés à faire des estimation de \( | x_n-\bar x |\).
\end{normaltext}

\begin{normaltext}
    Lorsque nous effectuons une méthode itérative, il faut donc contrôler deux grandeurs :
    \begin{subequations}
        \begin{align}
            | \bar x-x_n |\leq \epsilon_1\\
            | x_{n+1}-x_n |\leq \epsilon_2.
        \end{align}
    \end{subequations}
\end{normaltext}

\begin{proposition}
Soit \( p\) l'ordre de convergence de la suite \( (x_n)\) vers \( \bar x\). Si \( p>1\) et \( | x_{n+1}-x_n |\leq \epsilon_2\) alors \( | \bar x-x_n |\leq \epsilon_2\).
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode de bisection}
%---------------------------------------------------------------------------------------------------------------------------

Il y a ce théorème des valeurs intermédiaires.
\begin{theorem}
	Soit \( f\) continue sur \( \mathopen[ a , b \mathclose]\) telle que \( f(a)f(b)<0\). Alors il existe au moins une solution à l'équation \( f(x)=0\) sur l'intervalle \( \mathopen] a , b \mathclose[\).
\end{theorem}

Pour démarrer une bisection, il est toujours bon de prendre l'intervalle \( \mathopen[ a , b \mathclose]\) de façon à ne contenir qu'une seule solution.

Soit donc un premier intervalle \( \mathopen[ a_0 , b_O \mathclose]\) tel que \( f(a_0)f(b_0)<0\) et ne contenant qu'une seule solution. À chaque itération nous considérons la moitié de l'intervalle précédent, mais la moitié contenant la solution.

Le test d'arrêt de la méthode de bisection se base uniquement sur la taille de l'intervalle qui reste. En effet si nous avons
\begin{equation}
	| b_n-a_n |\leq \epsilon
\end{equation}
nous avons certainement
\begin{equation}
	| \bar x-x_n |\leq \frac{ \epsilon }{2}
\end{equation}
où \( x_n\) est le point du milieu de \( \mathopen[ a_n , b_n \mathclose]\).

\begin{normaltext}
    La fonction \( f\) n'intervient dans la méthode que via son signe, pas via ses valeurs exactes.
\end{normaltext}

\begin{normaltext}
    Notons que le théorème des valeurs intermédiaires n'est pas très puissant pour choisir l'intervalle de départ; penser à la fonction
    \begin{equation}
        f(x)=x^2-5
    \end{equation}
    sur l'intervalle \( \mathopen[ -10 , 10 \mathclose]\). Il y a bien deux solutions dans l'intervalle, mais elles sont invisibles du théorème des valeurs intermédiaires. La fonction \( x\mapsto x^2\) a sa solution en \( x=0\), mais elle aussi n'est pas visible.
\end{normaltext}

\begin{normaltext}
    Certes la méthodes de bisection assure la convergence vers une solution, mais elle n'assure pas la convergence monotone. Il peut arriver que \( | \bar x-x_n |<| \bar x-x_{n+1} |\). C'est le cas lorsque la solution est très proche du milieu de l'intervalle choisit. Le \( x_0\) est alors proche de \( \bar x\) alors que \( x_1\) sera à une distance de \( \bar x\) d'environ un quart de l'intervalle de départ.
\end{normaltext}

Supposons déjà avoir trouvé un intervalle \( \mathopen[ a , b \mathclose]\) dans lequel se trouve une unique solution à \( f(x)=0\). Voici un algorithme possible.

\lstinputlisting{codeSnip_1.py}

Plusieurs remarques :
\begin{enumerate}
    \item
Le fait de retourner le nombre d'itérations effectuées permet à l'utilisateur de savoir la précision et si le nombre maximum d'itérations est dépassé. Si ce \info{n} retourné est égal à \info{nmax}, l'utilisateur sait que le \info{x} retourné n'est pas fiable.
\item
    La ligne \info{from \_\_future\_\_ import division} fait en sorte que l'opération \info{/} est bien la division usuelle. Sinon, le défaut en python 2 est que \info{/} soit la division \emph{entière}, c'est à dire que \( 1/2=0\) en python 2.o
    En python 3, le symbole \info{/} désigne bien la division usuelle, mais Sage utilise Python 2.
\item
    Même si l'intervalle \( \mathopen[ a , b \mathclose]\) contient plus d'une solution, la méthode fonctionne et donne une solution. Il est simplement éventuellement très compliqué de savoir laquelle.
\item
    Nous faisons \info{amp=toll+1} parce que nous voulons absolument lancer le cycle au moins une fois. Sinon, le \info{x} à retourner ne serait pas définit au moment de sortir du cycle (si le cycle n'est pas exécuté).
\item
    Calculer le point milieu d'un intervalle \( \mathopen[ a , b \mathclose]\) est par la formule \( (a+b)/2\) sauf que cette opération est numériquement dangereuse parce qu'à cause de l'arithmétique en précision finie, il est possible que cela tombe \emph{exactement} sur \( a\) ou \( b\). D'où le fait de calculer le point milieu par
    \begin{equation}
        x=a+\frac{ amp }{2}.
    \end{equation}
\item
    Dans le cas \info{Problème ZERO} nous déduisons \( f(x)=0\). Attention que c'est pas que \( f(x)=0\) mais simplement que en mettant \( x\) dans \( f\), la \emph{machine} retourne son zéro.

    Il peut cependant avoir une fonction telle que \( f(1)=10^{-50}\) et \( f(2)=0\). L'algorithme de bisection risque de s'arrêter si \( x_n=1\). Parce que la machine risque de calculer \( f(x_n)=0\).

    Quoi qu'il en soit, nous y mettons \info{amp=0} pour être sûr de sortir de la boucle dès la prochaine vérification.
\item
    Il y a moyen de sauver les valeurs de \( f(a)\) et \( f(x)\) pour ne pas les recalculer, et en particulier au moment de faire \info{b=x} nous pouvons poser \(\info{fa=fx}\).
\end{enumerate}

Si \( \tau\) est la précision de la solution voulue, nous pouvons fixer a priori le nombre d'itérations à faire grâce à la formule
\begin{equation}
    n\geq\left\lceil  \log_2\big( \frac{ b-a }{ \tau } \big)  \right\rceil.
\end{equation}
Il y a un ``\( \geq\)'' et non une égalité parce qu'en arithmétique numérique, le nombre obtenu à droite pourrait ne pas être le bon à \( 1\) près.

Ici pour \( \nu\in \eR\) le nombre \( \lceil\nu\rceil\) est le plus petit entier à être plus grand ou égal à \( \nu\).

\begin{normaltext}
    Notons l'importance de la continuité de \( f\). Par exemple que ferait la bisection sur la fonction \( f(x)=1/x\) pour l'intervalle $\mathopen[ -3 , 1 \mathclose]$ ? 

    Il y a changement de signe sans avoir de racine.
\end{normaltext} 

Vu que \( 2^{10}\) est déjà \( 1024\). Donc si on veut de la précision de l'ordre de \( 1/1000\), dix itération suffisent. Si donc nous avons besoin de \( 200\) itérations pour atteindre la précision voulue, c'est l'occasion de trouver un intervalle plus petit. Par exemple en traçant la fonction, en faisant un zoom et en trouvant des valeurs de \( a\) et \( b\) qui sont déjà proches.


\begin{normaltext}
    Dans le monde réel, il arrive souvent d'utiliser une méthode de bisection pour se donner un point de départ pour une autre méthode.
\end{normaltext}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Efficacité}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    L'\defe{efficacité}{efficacité!d'une méthode itérative} est le nombre
    \begin{equation}
        E=\sqrt[s]{ p }
    \end{equation}
    où \( p\) est l'ordre de convergence de la méthode et \( s\) est le nombre de fois qu'il faut calculer une valeur de la fonction à chaque itération (nous ne comptons pas l'initialisation).
\end{definition}
Que le nombre de d'évaluations de \( f\) intervienne est logique parce que chaque évaluation provoque une erreur possible.

\begin{example}[Bisection]
    Pour la méthode de bisection, nous avons \( s=1\) parce que chercher \( x_{n+1}\), il faut seulement calculer \( f(x_n)\).
\end{example}

\begin{example}[Newton]
    Pour l'algorithme de Newton nous avons \( p=2\) et il y a deux évaluations à chaque itérations (une fois \( f\) et une fois \( f'\)), donc \( s=2\) et \( E=\sqrt{ 2 }\).
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Exercices}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\Exo{mazhe-0002}
\Exo{mazhe-0003}
\Exo{mazhe-0004}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Approximations de fonctions}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{enumerate}
    \item
        D'habitude on n'approxime pas une fonction sur tout son domaine, mais seulement sur une partie.
    \item
        Il y a le problème du choix de la classe des fonctions qui vont approximer. Nous allons travailler avec des polynômes.
    \item
        Il nous faut un critère disant si une approximation est bonne ou non.
\end{enumerate}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Critère d'interpolation}
%---------------------------------------------------------------------------------------------------------------------------

À partir de \( n+1\) abscisses points distinctes \( x_i\), nous calculons \( y_i=f(x_i)\). Il y a ce théorème qui dit qu'il existe un unique polynôme de degré (au plus) \( n+1\) passant par les points \( (x_i,y_i)\).

\begin{propositionDef}[Base de Lagrange]
    Étant donnés \( n+1\) valeurs distinctes \( x_i\), l'espace des polynômes de degré \( n\) admet la base
    \begin{equation}
        L_i^{(n)}(x)=\prod_{\substack{j=0\\j\neq i}}^n\frac{ x-x_j }{ x_i-x_j }
    \end{equation}
    pour \( i=0,\ldots, n\).
\end{propositionDef}

Soit par exemple les valeurs de \( f\) données dans
\begin{equation*}
    \begin{array}[]{c|c}
        x_i&y_i=f(x_i)\\
        \hline
        x_0=5&1\\
        x_1=-7&-23\\
        x_2=6&-54\\
        x_3=0&-954\\
    \end{array}
\end{equation*}

Les polynômes de Lagrange pour ces données dépendent seulement des \( x_i\), pas des \( y_i\). En particulier,
\begin{equation}
    L_0^{(3)}(x)=\frac{ (x-x_1)(x-x_2)(x-x_3) }{ (x_0-x_1)(x_0-x_2)(x_0-x_3) },
\end{equation}
etc.

La réponse est que
\begin{subequations}
    \begin{align}
        L_0^{(3)}(x)&=\frac{ x^3+13x^2+42x }{ 660 }\\
        L_1^{(3)}(x)&=\frac{ -x^3-x^2+30x  }{ 84 }\\
        etc.
    \end{align}
\end{subequations}

Ce qu'il y a de bien avec cette base est que en posant \( a_i=f(x_i)\) alors le polynôme
\begin{equation}
    \sum_{i=0}^na_iL_i^{(n)}(x)
\end{equation}
passe par les points \( (x_i,f(x_i))\). Du coup il suffit d'écrire
\begin{equation}
    P_3(x)=L_0^{(3)}(x)-23L_1^{(n)}(x)-54L_2^{(n)}(x)-954 L_3^{(n)}(x)=4x^3+35x^2-84x-954.
\end{equation}

Un inconvénient de cette base est qu'elle est complètement dépendante des points choisis. Si on ajoute un point ou qu'on en prend un à peine différent, tous les coefficients changent. Mais en pratique, ajouter des points est quelque chose qui arrive souvent parce que souvent, après avoir vu le résultat d'un polynôme d'interpolation, on veut ajouter un point pour avoir un meilleur résultat.

\begin{normaltext}
    Une habitude : le premier et le dernier nœud se choisissent aux extrémités de l'intervalle sur lequel nous voulons une approximation.

    Le but d'une approximation est d'avoir des approximations de \( f(x^*)\) pour des valeurs de \( x^*\) qui ne soit pas une des abscisses données (parce que sur ces points, le polynôme et la fonction sont égaux). Nous considérons donc
    \begin{equation}
        f(x^*)\simeq P_n(x^*).
    \end{equation}
    Si \( x^*\) est dans l'intervalle \( I= \mathopen[ x_{min} , x_{max} \mathclose]\) alors nous disons que nous calculons \( f\) par \defe{interpolation}{interpolation}. Si au contraire \( x^*\) est en dehors de cet intervalle nous parlons d'\defe{extrapolation}{extrapolation}.

    Si \( x^*\) est pris à l'extérieur de \( I\), alors l'erreur risque d'être très grande, surtout parce que les polynômes tendent tous vers \( \pm\infty\) lorsque \( x\to \pm\infty\).

    Autant l'interpolation via polynômes est le plus souvent valable, il faut garder à l'esprit que les extrapolation sont souvent mauvaises si \( x^*\) est trop loin des extrémités de \( I\).
\end{normaltext}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Base de Newton}
%---------------------------------------------------------------------------------------------------------------------------

Après la base canonique et la base de Lagrange, nous voyons la base de Newton. Soient encore \( n+1\) points donnés du graphe de \( f\). 

\begin{definition}
    La \defe{base de Newton}{base!de Newton} pour les abscisses \( x_i\) est l'ensemble des polynômes suivants :
    \begin{equation}
        1,(x-x_0),(x-x_0)(x-x_1),\ldots,(x-x_0)(x-x_1)\ldots (x-x_{n-1})
    \end{equation}
\end{definition}
Notons que ces polynômes n'utilisent pas le dernier point des \( x_i\). Le polynôme passant par les points est 
\begin{equation}
    P_n(x)=x_0+\sum_{i=1}^nc_i\prod_{j=0}^{n-1}(x-x_j).
\end{equation}
Le calcul des \( c_i\) n'est pas absolument évident. Mais si nous ajoutons un point d'interpolation, les polynômes déjà calculés sont encore bons; en particulier
\begin{equation}
    P_{n+1}(x)=P_n(x)+c_{n+1}\prod_{j=0}^{n}(x-x_j).
\end{equation}
Et cela est bien, parce que ça donne une façon de les calculer par récurrence.

Il y a plusieurs façons de calculer les \( c_i\).

Les différences divisées sont des façons d'approximer les dérivées.
\begin{definition}
    Soient \( n+1\) nœuds \( x_i\) pour la fonction \( f\). La \defe{différence divisée}{différence!divisée} sont :
    \begin{description}
        \item[Ordre \( 0\)]  
            \begin{equation}
                f[x_i]=f(x_i)
            \end{equation}
        \item[Ordre \( 1\)]
            \begin{equation}
                f[x_i,x_j]=\frac{ f[x_i]-f[x_j] }{ x_i-x_j }.
            \end{equation}
        \item[Ordre \( 2\)]
            \begin{equation}
                f[x_i,x_j,x_k]=\frac{ f[x_i,x_j]-f[x_j,x_k] }{ x_i-x_k }.
            \end{equation}
        \item[Ordre \( n\)]
            \begin{equation}
                f[x_0,\ldots, x_n]=\frac{ f[x_0,\ldots, x_{n-1}]-f[x_1,\ldots, x_n] }{ x_0-x_n }.
            \end{equation}
    \end{description}
\end{definition}
Les ordres font référence à l'ordre de dérivation qui est approximé.

Nous avons alors
\begin{equation}
    c_i=f[x_0,\ldots, x_i].
\end{equation}
Cela donne effectivement une méthode de récurrence pour trouver les coefficients \( c_i\).

\begin{remark}
    Pour calculer \( c_0\), il faut seulement calculer \( f[x_0]=f(x_0)\). Mais pour calculer \( c_1\) il faut \( f[x_0] \) et \( f[x_1]\). Et pour \( c_2\) il faut \( f[x_0,x_1,x_2]\) qui demande \( f[x_0,x_1]\) et \( f[x_0,x_1]\), qui demande etc.

    Il faut donc calculer en réalité tous les \( f[x_i]\) pour terminer le calcul. Par contre, pour ajouter un point, il ne faut pas tout recalculer, et même pas tout conserver en mémoire. Il faut seulement garder en mémoire la dernière diagonale.
\end{remark}

\Exo{mazhe-0005}

Un exercice typique serait de donner tout pour \( 3\) points puis de demander le polynôme qui aurait un quatrième point.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Méthode des minima quadratiques}
%---------------------------------------------------------------------------------------------------------------------------

Soient \( m+1\) points connus sur le graphe de la fonction \( f\) que nous devons approximer. Au lieu d'exiger que notre approximation ne passe par tous les points, nous allons chercher une approximation qui minimise la somme des carrés des erreurs sur ces points.

Soit \( \mF\) une classe de fonctions dans laquelle nous allons chercher l'approximation. Nous cherchons \( g\in \mF\) qui minimise
\begin{equation}    \label{EQooJGQVooKwZZVJ}
   E(g)= \sum_{i=0}^m \big( f(x_i)-g(x_i) \big)^2\omega_i
\end{equation}
où \( \omega_i>0\) est une pondération. Souvent on prend \( \omega_i=1\), mais pas toujours. La fonction \( E\) sur \( \mF\) est la \defe{fonction d'erreur}{erreur}.

\begin{normaltext}
    À part dans les exercices à la main, le nombre de points est grand, du type du milliard. Il est bien entendu pas envisageable de faire passer un polynôme \emph{exactement} par un milliard de points, parce que cela demanderait un polynôme de degré un milliard. 

    Plus généralement, d'un point de vue scientifique, avoir \( n\) paramètres libres pour \( n\) données expérimentales, ça ne passe pas Popper.

    Afin de faire de la science qui passe Popper nous nous restreignons à une classe de fonction \( \mF\) dont la dimension n'est pas grande : \( \dim( \mF)\ll m\). Et nous notons \( \dim(\mF)=n+1\).
\end{normaltext}

\begin{example}
    La qualité d'une expérience peut être influencée par des paramètres extérieurs comme l'humidité, le vent, etc. Donc il est normal d'avoir des expériences moins précises que d'autres. On le pèse moins.
\end{example}

\begin{example}
    Dans un questionnaire, il se met des questions volontairement contradictoires. Si quelqu'un répond «oui» aux deux questions, il y a une indication que la personne a répondu un peu n'importe comment, et il faut moins peser ses réponses.
\end{example}

Soit \( g\in \mF\), et une base \( \{ g_i \}_{i=0,\ldots, n}\) de \( \mF\). Nous écrivons
\begin{equation}
    g=a_0g_0+a_1g_1+\ldots +a_ng_n
\end{equation}
La fonction donnée \( E\) donnée en \eqref{EQooJGQVooKwZZVJ}, est, à partir du moment où \( \mF\) et une base sont choisis, une fonction des paramètres \( a_i\) que nous nommons \( F(a_0,\ldots, a_i)\). Il faut minimiser \( F\), c'est à dire poser
\begin{equation}
    \frac{ \partial F }{ \partial a_j }=0
\end{equation}
pour \( j=0,\ldots, n\). Cela sont \( n+1\) équations pour \( n+1\) inconnues. Notons que ces équations sont linéaires parce que chacun des termes est du type
\begin{equation}
    \left( f(x_i)-\sum_{j=0}^ma_jg_j(x_i) \right)^2,
\end{equation}
et lors de la dérivation par rapport à \( a_j\), nous obtenons du degré \( 1\). 

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Notre espace de Hilbert}
%---------------------------------------------------------------------------------------------------------------------------

Nous allons maintenant formaliser un peu tout cela. Dans \cite{ooPTFGooScbUWC} il est expliqué que si \( \omega\) est une fonction strictement positive, alors l'espace \( L^2_{\omega}\big( \mathopen[ a , b \mathclose] \big)\) dérivé de la norme
\begin{equation}
    \| f \|_{L^2_{\omega}}^2=\int_a^b| f(x) |^2\omega(x)dx
\end{equation}
est un espace de Hilbert. Nous allons tenter le coup avec \( \omega=\sum_{i=0}^m\omega_i\delta_{x_i}\) où \( \delta_a\) est la distribution de Dirac\footnote{Définition dans l'exemple \ref{EXooUGOTooMDCFwD}.} centrée en \( a\).

Sur l'ensemble des fonctions \( \eR\to\eR\) nous considérons la relation d'équivalence \( f\sim g\) si \( f(x_i)=g(x_i)\) pour tout \( i=0,\ldots, m\). Nous notons \(  L^2_{\omega}  \) cet ensemble.

\begin{proposition}
    La formule 
    \begin{equation}        \label{EQooVUKMooIjpkXO}
        \langle f, g\rangle =\sum_{i=0}^mf(x_i)g(x_i)\omega_i
    \end{equation}
    définit un produit scalaire sur \( L^2_{\omega}\). Ce dernier devient un espace de Hilbert.
\end{proposition}

\begin{proof}
    Pour être un produit scalaire (définition \ref{DefVJIeTFj}), la forme considérée doit être symétrique et strictement définie positive. La symétrie de la formule \eqref{EQooVUKMooIjpkXO} ne fait pas de doute. Le fait que ce soit semi-défini positif non plus. Pour le strict,
    \begin{equation}
        \langle f, f\rangle =\sum_{i=0}^m| f(x_i) |^2\omega_i.
    \end{equation}
    Étant donné que \( \omega_i>0\) pour tout \( i\), l'annulation de \( \langle f, f\rangle \) implique l'annulation de \( f(x_i)\) pour tout \( i\). Cela signifie que \( f\) est dans la classe de \( 0\) et donc est nul dans \( L^2_{\omega}\).

    En ce qui concerne la complétude, la proposition \ref{PROPooGJDTooXOoYfw} répond à notre place, étant donné que \( L^2_{\omega}\) est de dimension finie. Une base est donnée par exemple par \( e_i(x)=\delta_{x,x_{i}}\). Ici le \( \delta\) est celui de Kronecker, et non celui de Dirac.
\end{proof}

\begin{lemma}
    Si la classe de fonctions \( \mF\) est un sous-espace vectoriel de \( L^2_{\omega}\) et si \( f\in L^2_{\omega}\) il existe un unique élément \( g\) de \( \mF\) minimisant la distance à \( f\).
\end{lemma}

\begin{proof}
    Le théorème de projection (au choix \ref{ThoWKwosrH} ou \ref{ThoProjOrthuzcYkz}) nous assure l'existence et l'unicité d'un élément de \( \mF\) minimisant la distance à \( f\in L^2_{\omega}\).
\end{proof}

\begin{normaltext}
    Ce lemme est gentil, mais ne nous donne pas de méthodes pour trouver ce minimum. Nous allons donc écrire explicitement un système d'équations permettant de le trouver. Si \( \{ g_{\alpha} \} \) est une base (finie) de \( \mF\) alors nous cherchons le minimisant sous la forme \( f=\sum_{\alpha}a_{\alpha}g_{\alpha}\). 

    Nous devons minimiser
    \begin{equation}
        E(g)=\sum_{i=0}^m\big( f(x_i)-g(x_i) \big)^2\omega_i=\sum_{i=0}^m\big( f(x_i)-\sum_{\alpha}a_{\alpha}g_{\alpha}(x_i) \big)^2\omega_i.
    \end{equation}
    Vu que cela est maintenant plutôt une fonction des coefficients \( a_{\alpha}\) que de la fonction \( g\) nous la notons \( F(a_0,\ldots, a_n)\). Il s'agit d'étudier le système d'équations
    \begin{equation}
        \frac{ \partial F }{ \partial a_{\alpha} }=0.
    \end{equation}
    Un tout petit peu de calcul mène au système
    \begin{equation}       
        \sum_i\sum_{\beta}a_{\beta}\omega_ig_{\alpha}(x_i)g_{\beta}(x_i)=\sum_{i}\omega_ig_{\alpha}(x_i)f(x_i).
    \end{equation}
    À droite nous reconnaissons \( \langle f, g_{\alpha}\rangle \). et à gauche, \( \sum_{\beta}a_{\beta}\langle g_{\alpha}, g_{\beta}\rangle \). Donc le système s'écrit
    \begin{equation}
        \sum_{\beta}a_{\beta}\langle g_{\alpha}, g_{\beta}\rangle =\langle f, g_{\alpha}\rangle .
    \end{equation}
    Il y a une équation pour chaque valeur de \( \alpha\).

    La matrice \( A\in\eM(n+1),\eR\) donnée par  \( \langle g_{\alpha}, g_{\beta}\rangle \) étant strictement définie positive (c'est un produit scalaire), le système a une unique solution. Et comme cette matrice est de plus symétrique, elle est diagonalisable par le théorème spectral \ref{ThoeTMXla}. Toutes ses valeurs propres sont strictement positives.

    Notons pour la curiosité que si l'on considère la matrice \( B\in\eM(m\times n)\) donnée par
    \begin{equation}
        B_{ij}=\sqrt{ \omega_i }g_j(x_l),
    \end{equation}
    alors nous avons \( A=B^tB\).
\end{normaltext}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Droite de régression}
%---------------------------------------------------------------------------------------------------------------------------

La droite de régression est la cas particulier \( n=1\), c'est à dire un système \( 2\times 2\). Nous cherchons \( P=a_0+a_1x\). Et la base choisie est \( g_0(x)=1\), \( g_1(x)=x\). Nous avons
\begin{subequations}
    \begin{align}
        \langle g_0, g_0\rangle =\sum_i\omega_ig_0(x_i)g_0(x_i)=\sum_i\omega_i\\
        \langle g_0, g_1\rangle =\sum_i\omega_ix_i\\
        \langle g_1, g_1\rangle =\sum_i\omega_ix_i^2.
    \end{align}
\end{subequations}
Donc pour approximer une fonction \( f\) il faut résoudre le système
\begin{equation}
    \begin{pmatrix}
        \sum_i\omega_i    &   \sum_i\omega_ix_i    \\ 
        \sum_i\omega_ix_i    &   \sum_i\omega_ix_i^2    
    \end{pmatrix}\begin{pmatrix}
        a_0    \\ 
        a_1    
    \end{pmatrix}=
    \begin{pmatrix}
        \langle f, g_0\rangle     \\ 
        \langle f, g_1\rangle     
    \end{pmatrix}.
\end{equation}
 
Pour calculer les produits \( \langle f, g_{\alpha}\rangle \) il suffit de savoir \( f\) sur les points \( x_i\). Et encore heureux, parce que toute la méthode est basée sur le fait que nous ne connaissons pas \( f\) ailleurs. C'est pour cela que nous avons défini \( L^2_{\omega}\) comme un ensemble quotient.

\begin{example}
    Faisons la droite de régression pour les données avec tous les poids \( \omega_i=1\).
    \begin{equation*}
        \begin{array}[]{c|c}
            x_i&f(x_i)\\
            \hline\hline
            -5&18\\
            -3&7\\
            1&0\\
            3&7\\
            4&16\\
            6&50\\
            8&67\\
        \end{array}
    \end{equation*}
    Nous avons
    \begin{subequations}
        \begin{align}
            \langle g, g_0\rangle =\sum_if(x_i)=165\\
            \langle f, f_1\rangle =\sum_if(x_i)x_i=810.
        \end{align}
    \end{subequations}
    et  

    Nous avons le système
    \begin{equation}
        \begin{pmatrix}
            7    &   14    \\ 
            14    &   160    
        \end{pmatrix}
        \begin{pmatrix}
            a_0    \\ 
            a_1    
        \end{pmatrix}=\begin{pmatrix}
            165    \\ 
            810    
        \end{pmatrix}.
    \end{equation}
    La résolution donne la droite de régression.
\end{example}

\begin{proposition}
    Si tous les poids sont identiques, alors la droite de régression passe par le barycentre des points donnés :
    \begin{subequations}
        \begin{numcases}{}
            x_M=\frac{1}{ m+1 }\sum_{i=0}^mx_i\\
            y_M=\frac{1}{ m+1 }\sum_{i=0}^my_i.
        \end{numcases}
    \end{subequations}
\end{proposition}
Cela donne une vérification possible de la réponse trouvée.

\begin{definition}
    L'\defe{erreur quadratique}{erreur!quadratique} est la fonction \( F(a_0,\ldots, a_n)\) dont il est question plus haut. Et si une solution est connue, son erreur quadratique est la valeur de $F$ pour cette solution.
\end{definition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Système linéaires (généralités)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit un système d'équations linéaires \( Ax=b\) avec \( A\in \eM(n,\eR)\). Le problème est évidemment de savoir si il existe une unique solution \( x\) et de la déterminer. Nous supposons l'existence et l'unicité. C'est à dire que les conditions équivalentes\footnote{L'équivalence est la proposition \ref{PropYQNMooZjlYlA}\ref{ITEMooNZNLooODdXeH}.} sont vérifiées :
\begin{enumerate}
    \item
        \( A\) est inversible, c'est à dire qu'il existe une matrice notée \( A^{-1}\) telle que \( AA^{-1}=A^{-1}A=\mtu\).
    \item
        \( \det(A)\neq 0\).
\end{enumerate}
Note : si  nous avons un système pas carré du type \( Bx=v\) avec \( B\in \eM(n\times m)\) alors nous pouvons nous ramener à un système carré en écrivant
\begin{equation}
    B^tBx=B^tv.
\end{equation}
Mais attention : bien que \( B^tB\) soit symétrique et semi-définie positive, certaines valeurs propres peuvent être nulles.

\begin{normaltext}
    Deux choses générales en calcul numérique :
    \begin{enumerate}
        \item
            On ne calcule pas l'inverse d'une matrice.
        \item
            On ne calcule même pas son déterminant.
    \end{enumerate}
    Par conséquent nous ne faisons pas \( x=A^{-1}v\).

    Il faut garder en tête le fait que dans la pratique, la matrice \( A\) possède des millions de lignes et colonnes, si pas pire. Pour une matrice de taille de l'ordre du million, il y a \( 1000\) milliards d'entrées. Si on compte \( 32\) bits par nombre (précision simple, définition \ref{DEFooEIOZooYLDVjs}), c'est à dire \( 4\) octets, il faut \( 4000\) giga-octets pour enregistrer la matrice. Même pour la mémoire actuellement disponible, ce n'est pas rien. Surtout que souvent, la précision simple n'est pas utilisée, mais la précision double, ce qui donne \( 8000\) giga pour enregistrer la matrice.

    Heureusement, dans la majorité des cas pratiques, les matrices géantes qui apparaissent sont pleines de zéros.

\end{normaltext}

\begin{definition}
    Une matrice est \defe{creuse}{matrice!creuse} si elle possède beaucoup de zéros. Une matrice non creuse est dite \defe{dense}{matrice!dense}.  
    
    Notons que lorsqu'on parle de matrice comprenant beaucoup de «zéros», nous pensons à des éléments très petits, et non de vrai zéros. 
\end{definition}
    Les matrices creuses ne sont pas mémorisées entièrement, mais plutôt comme un dictionnaire \( (i,j,v)\) qui donne la valeur \( v\) de \( A_{ij}\).

\begin{definition}
    Une matrice est de «grande dimension» si elle ne peut pas être mise en mémoire sur un ordinateur donné. Sur certains ordinateurs, ça commence à \( 5000\) inconnues. Mais sur des plus forts, on peut aller jusqu'au million ou le milliard.
\end{definition}

Si la matrice est de petite dimension, il est possible d'utiliser des méthodes dites «directes». Sinon, il faudra utiliser des méthodes itératives.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Les méthodes directes}
%---------------------------------------------------------------------------------------------------------------------------

Une méthode directe consiste à successivement transformer un système \( A^{(0)}x=b^{(0)}\) en de nouveaux systèmes \( A^{(i)}x=b^{(i)}\) dont la solution est identique jusqu'à obtenir un système \( A^{(n-1)}x=b^{(n-1)}\) qui est à résolution immédiate.

L'avantage d'une méthode directe est qu'elle fournit une réponse exacte, pour autant que les calculs intermédiaires soient bien faits (ce qui n'est pas le cas sur un ordinateur).

Une méthode directe fonctionne en général avec un nombre de pas fixés par la taille du système. Par exemple pour un système \( n\times n\), la méthode de Gauss demande exactement \( n\) pas, et il n'y a pas moyen de faire mieux. Or chaque pas demande de recalculer tous les éléments de la matrice. Encore une fois, si la matrice a une taille de l'ordre du milliard, cela fait \( 10^{18}\) éléments à recalculer un milliard de fois (sans compter les éléments du vecteur \( b\)). Infaisable.


Souvent une méthode directe passe par une factorisation \( A=BC\) avec \( B,C\in \eM(n\times n)\).

Quelque types de matrices dont la résolution est immédiate :
\begin{itemize}
    \item Matrice diagonale.
    \item Matrice orthogonale parce que si \( A\) est orthogonale alors \( Ax=v\) se résout par \( x=A^tv\) qui n'est pas particulièrement lourd à faire numériquement.
    \item Matrice triangulaire.
\end{itemize}

\begin{remark}
    Pour une matrice diagonale, le déterminant et l'inverse sont faciles. Mais également pour la triangulaire. Pour une matrice triangulaire, le déterminant est le produit des éléments diagonaux, et il se fait qu'il y a une algorithme facile pour calculer l'inverse.

    Donc en fait les matrices à résolution immédiates sont des matrices pour lesquelles l'inverse et le déterminant sont facile à calculer.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Méthodes itératives}
%---------------------------------------------------------------------------------------------------------------------------

Si la matrice est trop grande, il n'est pas possible de faire des manipulations de matrices à chaque itération.

En général, les méthodes itératives ne convergent pas toujours. Mais lorsqu'une méthode converge, c'est une propriété de la matrice, et donc la convergence aura lieu pour tout vecteur de départ \( x_0\). Cela est très différent du cas des équations non linéaires type Newton pour lesquelles la convergence peut fortement dépendre du point de départ.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Système linéaires (méthodes directes)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Les matrices que nous sommes autorisés à inverser sont les matrices
\begin{itemize}
    \item orthogonales : l'inverse est la transposée
    \item diagonales : l'inverse est diagonale avec les inverses sur la diagonale
    \item triangulaires : nous en parlons maintenant.
\end{itemize}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Inversion de matrice triangulaire}
%---------------------------------------------------------------------------------------------------------------------------

Si \( T\) est une matrice triangulaire (mettons supérieure pour fixer les idées), il est possible d'en calculer l'inverse sans trop d'efforts. Notons \( B\) la matrice inverse que nous allons construire ligne par ligne. Vu que \( BT=\mtu\) nous avons
\begin{equation}
    \delta_{1j}=\sum_{k=1}^nB_{1k}T_{kj}=\sum_{k=1}^jB_{1k}T_{kl}
\end{equation}
parce que \( T_{kj}=0\) pour \( k>j\). Donc nous pouvons calculer les éléments \( B_{1j} \) un par un parce que chacun ne dépend que des précédents. Le même procédé fonctionne pour les autres lignes :
\begin{equation}
    \delta_{ij}=\sum_{k=1}^jB_{ij}T_{kj}.
\end{equation}
Et tu notes que le calcul peut être parallélisé : le calcul de la ligne numéro \( j\) ne dépend pas du résultat des autres lignes.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Transformation gaussienne}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Transformation gaussienne\cite{ooPFJDooUQIwHZ}]
    Soit \( x\in \eR^n\) avec \( x_k\neq 0\). La \( k\)\ieme \defe{transformation gaussienne}{transformation!gaussienne} pour \( x\) est la matrice
    \begin{equation}
        M_k(x)=\mtu-T_k(x)
    \end{equation}
    où \( T_k(x)\) est la matrice unité à qui on a ajouté le vecteur
    \begin{equation}
        \tau_k(x)=
        \begin{pmatrix}
            0    \\ 
            \vdots    \\ 
            0    \\ 
            x_{k+1}/x_k    \\ 
            \vdots    \\ 
            x_n/x_k    
        \end{pmatrix}
    \end{equation}
    à la \( k\)\ieme colonne. 
\end{definition}
Autrement dit, la matrice \( M_k(x)\) est la matrice 
\begin{equation}        \label{EQooMWXLooBDtsKS}
    M_k(x)=\begin{pmatrix}
        1    &       &       &       &       &   \\  
        0    &    \ddots    &                 &       &       &   \\
        \vdots    &   0    &   1               &         &       &   \\
        \vdots    & \vdots      &   -x_{k+1}/x_k    &   1    &       &   \\
        \vdots    &   \vdots    &   \vdots    &   0    &   \ddots    &   \\
        0    &   0    &           -x_n/x_k   &  0   &      &   1
    \end{pmatrix}
\end{equation}
En coordonnées nous avons
\begin{equation}
    \big( M_k(x)\big)_{ij}=\delta_{ij}-\tau_k(x)_i\delta_{kj}.
\end{equation}

\begin{normaltext}
    Les matrices de transformation gaussienne sont des matrice triangulaires de diagonale unitaire (c'est à dire avec des \( 1\) sur la diagonale).
\end{normaltext}

\begin{lemma}
    Si \( x\in \eR^n\) alors nous avons 
    \begin{equation}
        M_k(x)x=\begin{pmatrix}
            x_1    \\ 
            \vdots    \\ 
            x_k    \\ 
            0    \\ 
            \vdots    \\ 
            0    
        \end{pmatrix}.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous avons
    \begin{equation}
        \big( M_k(x)x \big)_i=\sum_lM_k(x)_{il}x_l=\sum_l\big( \delta_{il}-\tau_k(x)_i\delta_{kl} \big)x_l=x_i-\tau_k(x)_ix_k. 
    \end{equation}
    Si \( i\leq k\) nous avons \( \tau_k(x)_i=0\) et donc \(  \big( M_k(x)x \big)_i=x_i   \). Si par contre \( i\geq k+1\) alors \( \tau_k(x)_i=\frac{ x_i }{ x_k }\) et alors \( \big( M_k(x)x \big)_i=0\).
\end{proof}

\begin{lemma}       \label{LEMooPFWWooUmMsVH}
    Si \( y\in \eR^n\) vérifie \( y_i=0\) pour \( i>k\) alors \( M_{k+1}(x)y=y\).
\end{lemma}

\begin{proof}
    C'est une simple vérification :
    \begin{equation}
        \big( M_{k+1}(x)y \big)_i=\sum_l\big( \delta_{il}-\tau_{k+1}(x)_i\delta_{k+1,l} \big)y_l=y_i-\tau_{k+1}(x)_iy_{k+1}.
    \end{equation}
    Mais comme \( y_{k+1}=0\) il nous reste automatiquement \( y_i\).
\end{proof}
Le sens de ce lemme est si un vecteur est déjà «gaussiannisé» au niveau \( k\), alors en lui appliquant une transformation gaussienne de niveau plus élevé que \( k\), il ne change pas. Ce fait est important parce qu'il assure que lorsque l'on avance dans le processus de Gauss, chaque étape ne détruit pas les précédentes.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Méthode de Gauss sans pivot (décomposition LU)}
%---------------------------------------------------------------------------------------------------------------------------

La méthode de Gauss est encore utilisée aujourd'hui dans les vrais problèmes.

La méthode de Gauss est souvent aussi appelée méthode «LU» qui va décomposer \( A=LU\) où \( L\) est triangulaire inférieure et \( U\) est triangulaire supérieure. La décomposition est même plus précise que cela : on demande que \( L\) ait seulement des \( 1\) sur la diagonale.

Si \( A\) est une matrice nous notons 
\begin{equation}
    \Delta_k(A)= (A_{ij})_{1\leq i,j\leq k}
\end{equation}
la matrice tronquée dont nous ne gardons que le carré \( k\times k\) en haut à gauche.

\begin{lemma}       \label{LEMooXEJFooGiYoyb}
    Soit \( S\) une matrice triangulaire inférieure. Soient également \( A\) et \( B\) telles que \( B=SA\). Alors
    \begin{equation}
        \Delta_k(B)=\Delta_k(S)\Delta_k(A).
    \end{equation}
\end{lemma}

\begin{proof}
     En effet nous avons
     \begin{equation}       \label{EQooHBZZooHtjjsE}
         \Delta_k(B)_{ij}=\sum_{l=1}^nS_{il}A_{lj}.
     \end{equation}
     Dans la somme sur \( l\) il ne reste que les termes \( l\leq i\). Mais dans le calcul des éléments de matrice \( \Delta_k(B)_{ij}\), nous avons évidemment \( i,j\leq k\). Donc \( l\leq i\leq k\). Les seuls éléments de matrice de \( A\) qui sont utilisés dans la somme \eqref{EQooHBZZooHtjjsE} sont les éléments \( A_{lj}\) avec \( l,j\leq k\). 
     
     Nous pouvons donc limiter la somme à \( l=k\) au lieu de \( l=n\) et écrire \( \Delta_k(A)_{lj}\) au lieu de \( A_{lj}\).

     Même chose en ce qui concerne \( S\). À partir du moment où \( l\) est limité à \( k\), les éléments \( S_{il}\) et \( \Delta_k(S)_{il}\) sont les mêmes.
\end{proof}

\begin{theorem}[Décomposition \( LU\)\cite{ooDANFooPSmBfd,MonCerveau}]       \label{THOooUXKJooYaPhiu}
    Soit une matrice \( A\) inversible telles que \( \det(\Delta_k(A))\neq 0\) pour tout \( k\). Alors il existe un unique couple de matrices \( (L,U)\) telles que
    \begin{itemize}
        \item \( U\) soit triangulaire supérieure
        \item \( L\) soit triangulaire inférieure, de diagonale unité
        \item \( A=LU\).
    \end{itemize}
    De plus pour tout \( k\leq n\) nous avons
    \begin{equation}
        \Delta_k(A)=\Delta_k(L)\Delta_k(U).
    \end{equation}
\end{theorem}

\begin{proof}
    Nous allons prouver par récurrence le fait suivant : pour tout \( 1\leq k\leq n-1\) il existe des matrices  \( E_i\) ($i=1,\ldots, k$) telles que en posant
    \begin{equation}
        A_k=E_{k}\ldots E_1A,
    \end{equation}
    \begin{itemize}
        \item \( E_j\) est une transformation gaussienne pour la \( j\)\ieme colonne,
        \item pour tout \( j\leq k\), \( A_{ij}=0\) dès que \( i>j\). Autrement dit la matrice \( A_k\) est triangulaire supérieure jusqu'à y compris la \( (k+1)\)\ieme colonne (laquelle est quelconque). Exemple pour fixer les idées : pour une matrice \( A\in \eM(4\times 4) \), la matrice \( A_2\) doit avoir la forme
            \begin{equation}
                A_2=E_2E_1A=\begin{pmatrix}
                     *   &   *    &   *    &   *    \\
                     0   &   *    &   *    &   *    \\
                     0   &   0    &   \oasterisk    &   *    \\ 
                     0   &   0    &   *    &   *     
                 \end{pmatrix}
            \end{equation}
            où les éléments notés \(*\) sont a priori non nuls,
        \item l'élément de matrice \( (A_k)_{ k+1,k+1  }  \) est non nul (celui entouré dans l'exemple).
    \end{itemize}
    
    La chose un peu triste dans cette démonstration est que l'initialisation va être très ressemblante au pas de récurrence.
    \begin{subproof}
        \item[Initialisation : \( k=1\)]

            Vu que \( \Delta_1(A)\) est inversible, l'élément \( A_{11}\) est non nul. Il existe donc une transformation gaussienne \( E_1\) telle que la première colonne de la matrice \( A_1=E_1A\) soit nul sauf la première composante. En particulier \( (A_1)_{21}=0\).

            Par le lemme \ref{LEMooXEJFooGiYoyb}, nous avons \( \Delta_2(A_1)=\Delta_2(E_1)\Delta_2(A)\), donc\footnote{Le déterminant est multiplicatif, proposition \ref{PropYQNMooZjlYlA}\ref{ItemUPLNooYZMRJy}.}
            \begin{equation}
                \det\big( \Delta_2(A_1) \big)=\det\big( \Delta_2(E_1) \big)\det\big( \Delta_2(A) \big).
            \end{equation}
            Étant donnée la forme \eqref{EQooMWXLooBDtsKS}, toutes les matrices du type \( \Delta_k(E_i)\) ont un déterminant unité, et par hypothèse \( \Delta_2(A)\) est inversible, donc de déterminant non nul. Par conséquent \( \det\big( \Delta_2(A_1) \big)\neq 0\). Mais comme ce déterminant est le produit des éléments diagonaux (c'est une matrice triangulaire), ces derniers ne sont pas nuls. Finalement, \( (A_1)_22\neq 0\).

        \item[Le pas de récurrence]

            Nous supposons avoir \( A_k=E_k\ldots E_1A\) avec \( (A_k)_{k+1,k+1}\neq 0\). Alors il existe une transformation gaussienne \( E_{k+1}\) de la \( (k+1)\)\ieme colonne telle que \( A_{k+1}=E_{k+1}A_k\) soit une matrice dont la \( (k+1)\)\ieme colonne n'ait que des zéros en dessous de la \( (k+1)\)\ieme position. Vu le lemme \ref{LEMooPFWWooUmMsVH}, cette transformation n'affecte pas les colonnes précédentes. 

            La matrice \( A_{k+1}\) est donc triangulaire supérieure jusqu'à la \( (k+2)\)\ieme colonne.

            Vu que le produit \( E_k\ldots E_1\) est une matrice triangulaire inférieure, le lemme \ref{LEMooXEJFooGiYoyb} fonctionne encore et nous avons
            \begin{equation}
                \Delta_{k+1}(A_k)=\Delta_{k+1}(E_k\ldots E_1)\Delta_{k+1}(A).
            \end{equation}
            Au niveau des déterminants par hypothèse nous avons \( \det\big( \Delta_{k+1}(A) \big)\neq 0\) et \( \det\big( \Delta_{k+1}(E_k\ldots E_1) \big)=1\). Donc
            \begin{equation}
                \det\big( \Delta_{k+1}(A_k) \big)\neq 0.
            \end{equation}
            Cette matrice état triangulaire, ses éléments diagonaux sont non nuls et nous avons \( (A_k)_{k+1,k+1}\neq 0\).
    \end{subproof}

    En poussant la récurrence jusqu'au bout, la matrice
    \begin{equation}
        A_{n-1}=E_{n-1}\ldots E_nA
    \end{equation}
    est triangulaire supérieure. 

    Nous posons alors \(   L=(E_{n-1}\ldots E_n)^{-1}  \) et \( U=A_{n-1}\). Cela prouve l'existence parce que
    \begin{equation}
        A=(E_{n-1}\ldots E_1)^{-1}A_{n_1}.
    \end{equation}
    Encore une fois, le lemme \ref{LEMooXEJFooGiYoyb} nous donne
    \begin{equation}
        \Delta_k(A)=\Delta_k\Big( (E_{n_1}\ldots E_1)^{-1} \Big)\Delta_k(A_{n-1}),
    \end{equation}
    ou encore \( \Delta_k(A)=\Delta_k(L)\Delta_k(U)\).
    
    En ce qui concerne l'unicité, si \( A=L_1U_1=L_2U_2\) alors \( L_2^{-1}L_1=U_2U_1^{-1} \). Vu qu'à gauche nous avons une matrice triangulaire inférieure et que à droite nous avons une triangulaire inférieure, nous savons que les deux membres représentent une matrice diagonale. Mais à gauche, la diagonale est unitaire. Donc les deux membres représentent la matrice unité.
\end{proof}

\begin{normaltext}
    En pratique, pour résoudre \( Ax=b\), il faut seulement appliquer les transformations gaussiennes à la matrice élargie \( (A|b)\) pour finir sur un système du type
    \begin{equation}
        Ux=b'
    \end{equation}
    qui est immédiatement soluble. Autrement dit, en effectuant les annulations de colonnes, la matrice \( U\) est «gratuite».
    
    Il n'est pas indispensable de calculer la matrice \( L\) qui, elle, demande à chaque étape de se souvenir de la matrice \( E_i\) utilisée. Si il faut résoudre plusieurs systèmes \( Ax_i=b_i\), nous pouvons encore travailler avec la matrice encore plus élargie \( (A|b_1\ldots b_m)\).

    Si par contre nous ne connaissons pas à l'avance l'ensemble des vecteurs \( b\) avec lesquels il faudra résoudre le système, il est bon de calculer la décomposition \( A=LU \) in extenso, c'est à dire de garder une trace des matrices \( L\) et \( U\) séparément. Dans ce cas, résoudre \( Ax=b\) revient à résoudre \( Ly=b\), et ensuite \( Ux=y\). Ce sont deux systèmes de résolution directe parce que les matrices sont triangulaires.
\end{normaltext}

\begin{normaltext}
    Le fait que
    \begin{equation}
        \Delta_k(A)=\Delta_k(L)\Delta_k(U)
    \end{equation}
    nous dit que si après avoir calculer \( L\) et \( U \) nous remarquons que le système est un peu plus petit ou un peu plus grand que prévu, tout le travail n'est pas perdu. En particulier si le système est plus petit que prévu, l'adaptation de \( L\) et \( U\) est immédiate.
\end{normaltext}

Notons que \( U\) et \( L\) sont inversibles, et que \( \det(L)=1\). Donc \( \det(U)=\det(A)\). 

\begin{example}
    Pour travailler la méthode de Gauss pour le système \( Ax=b\), nous introduisons la matrice un peu augmentée \( (A|b)\). Nous faisons un exemple. Soit à résoudre
    \begin{equation}
        \begin{pmatrix}
             2   &   1    &   3       \\
             4   &   3    &   10     \\ 
             -2   &   1    &   7  3
         \end{pmatrix}
         \begin{pmatrix}
             x   \\ 
             y   \\ 
             z   
         \end{pmatrix}=\begin{pmatrix}
             11   \\ 
             28   \\ 
             3   
         \end{pmatrix}.
    \end{equation}
    Nous introduisons la matrice augmentée
    \begin{equation}
        (A|b)^{(0)}=\begin{pmatrix}
             2   &   1    &   3    &   11    \\
             4   &   3    &   10    &   28    \\ 
             -2   &   1    &   7    &   3
         \end{pmatrix}.
    \end{equation}
    Le premier pas consiste à annuler tous les éléments sous la diagonale de la première colonne. Autrement dit, nous prenons le \( 2\) comme pivot. Nous introduisons les multiplicateurs \( l_{ij}= \frac{ A_{ij} }{ A_{i1} }\). La nouvelle matrice est :
    \begin{equation}
        (A|b)^{(1)}=\begin{pmatrix}
             2   &   1    &   3    &   11    \\
             0   &   1    &   4    &   6    \\ 
             0   &   2    &   10    &   14
         \end{pmatrix}
    \end{equation}
    où nous avons utilisé les multiplicateurs \( l_{21}=2\), \( l_{31}=-1\).

    Et la matrice suivante est :
    \begin{equation}
        (A|b)^{(2)}=\begin{pmatrix}
             2   &   1    &   3    &   11    \\
             0   &   1    &   4    &   6    \\ 
             0   &   0    &   2    &   2
         \end{pmatrix}
    \end{equation}
    où nous avons utilisé le multiplicateur \( l_{32}=2\).

    Cela est un système de résolution immédiate :
        \begin{subequations}
            \begin{numcases}{}
                2x+y+3z=11\\
                y+4z=6\\
                2z=2.
            \end{numcases}
        \end{subequations}
    La troisième donne \( z=1\). Ensuite \( y+4=6\), donc \( y=2\). Et la première donne : \( 2x+2+3=11\), c'est à dire \( 2x=6\), enfin : \( x=3\).

    Solution : \( (x,y,z)=(3,2,1)\).

    Nous notons surtout que dans \( (A|b)^{(2)}\) nous avons une matrice triangulaire supérieure. Où est la matrice triangulaire inférieure ? En réalité la matrice \( L\) est la matrice des multiplicateurs :
    \begin{equation}
        L=\begin{pmatrix}
            1    &   0    &   0    \\
            2    &   1    &   0    \\
            -1    &   2    &   1
        \end{pmatrix}.
    \end{equation}
\end{example}

Le problème de cette méthode est que faisant ainsi nous risquons d'avoir un zéro sur un des pivots. Par exemple tomber sur
\begin{equation}
    (A|b)=\begin{pmatrix}
         2   &   1    &   3    &   11    \\
         0   &   0    &   4    &   6    \\ 
         0   &   2    &   10    &   14
     \end{pmatrix}.
\end{equation}
Le zéro sur la deuxième ligne nous ennuie si nous voulons tout faire dans l'ordre. Mais notons qu'en échangeant les deux dernière lignes, tout va bien : le système donné par
\begin{equation}
    (A|b)=\begin{pmatrix}
         2   &   1    &   3    &   11    \\
         0   &   2    &   10    &   14\\
         0   &   0    &   4    &   6   
     \end{pmatrix}
\end{equation}
fonctionne très bien. Et même tellement bien qu'il est de résolution immédiate, dans ce cas.

Un autre problème est que si un des pivots est \( 10^{-14}\), le multiplicateur sera de l'ordre \( 10^{14}\), qui est mal représenté en mémoire. Il est donc bon de prendre les pivots le plus grand possible. Si le pivot est le plus grand nombre en valeur absolue d'une colonne, alors les nombres \( x_{k+i}/x_k\) qui entrent dans la matrice de transformation gaussienne sont des nombres dans \( \mathopen[ -1 , 1 \mathclose]\) qui sont bien représentés en mémoire.

Tout cela nous incite à développer une méthode de Gauss qui permet de tenir une trace des permutations.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrice de permutation élémentaires}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Une \defe{matrice de permutation élémentaire}{matrice!permutation!élémentaire} est une matrice obtenue en permutant deux lignes de la matrice identité. Nous notons \( P_{ij}\) la matrice obtenue en inversant les lignes \( i\) et \( j\) de la matrice identité.
\end{definition}

\begin{example}
    \begin{equation}
        \begin{pmatrix}
            1    &   0    &   0    \\
            0    &   1    &   0    \\
            0    &   0    &   1
        \end{pmatrix}\to
        \begin{pmatrix}
            0    &   1    &   0    \\
            1    &   0    &   0    \\
            0    &   0    &   1
        \end{pmatrix}=P_{12}.
    \end{equation}
\end{example}

\begin{lemma}
    La matrice \( P_{ij}A\) est la matrice \( A\) avec ses lignes \( i\) et \( j\) inversées.
\end{lemma}

\begin{proof}
    Il suffit d'écrire
    \begin{equation}
        (P_{ij}A)_{kl}=\sum_m(P_{ij})_{km}A_{ml}
    \end{equation}
    et de faire trois cas selon que \( k=i\), \( k=j\) ou \( k\) différent de \( i\) et \( j\). Si \( k=i\) alors \( (P_{ij})_{im}=\delta_{mj}\) et si \( k\) est différent de \( i\) et \( j\) alors \( (P_{ij})_{mk}=\delta_{km}\) (troisième cas similaire au premier).
\end{proof}

Et la matrice \( AP_{12}\) est la \( A\) avec ses deux premières \emph{colonnes} échangées.

Avec ces notations, notre matrice \( (A|b)^{0'}\) est 
\begin{equation}
    P_{12}(A|b)^{(0)}.
\end{equation}
Puis la matrice \( (A|b)^{(1')}\) est
\begin{equation}
    P_{23}(A|b)^{(1)}.
\end{equation}
Et la matrice \( P\) qui arrive dans \( PA=LU\) est la matrice \(P= P_{23}P_{21}\), qui est une matrice de permutation non élémentaire. Elle vaut :
\begin{equation}
    P=\begin{pmatrix}
        0    &   1    &   0    \\
        0    &   0    &   1    \\
        1    &   0    &   0
    \end{pmatrix}.
\end{equation}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Méthode de Gauss avec pivot partiel}
%---------------------------------------------------------------------------------------------------------------------------

À chaque pas, nous faisons une permutation de ligne. Nous permutons à chaque pas la première ligne avec celle qui a le pivot le plus grand (en valeur absolue). Donc :
\begin{equation}
    (A|b)^{(0)}=\begin{pmatrix}
         2   &   1    &   3    &   11    \\
         4   &   3    &   10    &   28    \\ 
         -2   &   1    &   7    &   3
     \end{pmatrix}
\end{equation}
Nous commençons par déplacer des lignes :
\begin{equation}
    (A|b)^{(0')}=\begin{pmatrix}
         4   &   3    &   10    &   28    \\ 
         2   &   1    &   3    &   11    \\
         -2   &   1    &   7    &   3
     \end{pmatrix}.
\end{equation}
Les multiplicateurs sont \( l_{21}=1/2\) et \( l_{31}=-1/2\). Le fait est que les multiplicateurs ont toujours le plus grand dénominateur possible et nous avons alors toujours \( 0\leq | l_{ij} |\leq 1\), qui sont des nombres relativement petits, et bien représentés en mémoire.

Nous avons la nouvelle matrice
\begin{equation}
    (A|b)^{(1)}=\begin{pmatrix}
         4   &   3    &   10    &   28    \\ 
         0   &   -1/2    &   -2    &   -3    \\
         0   &   5/2    &   12    &   17
     \end{pmatrix}.
\end{equation}
Le pivot serait \( -1/2\). Nous cherchons un pivot plus grand en dessous de ce \( -1/2\) (et pas au dessus, sinon on casserait les zéros déjà trouvés). 
Nous trouvons le \( 5/2\) qui est plus grand. Nous permutons donc les deux dernières lignes :
\begin{equation}
    (A|b)^{(1')}=\begin{pmatrix}
         4   &   3    &   10    &   28    \\ 
         0   &   5/2    &   12    &   17\\
         0   &   -1/2    &   -2    &   -3   
     \end{pmatrix}
\end{equation}
où le pivot est maintenant \( l_{32}=-1/5\). La matrice suivante :
\begin{equation}
    (A|b)^{(1)}=\begin{pmatrix}
         4   &   3    &   10    &   28    \\ 
         0   &   5/2    &   12    &   17\\
         0   &   0   &   2/5    &   2/5   
     \end{pmatrix}
\end{equation}

Dans ce cas, la matrice \( L\) n'est pas aussi simple à construire parce que nous avons permuté des choses. Dans ce cas, la matrice \( L\) est encore de la forme
\begin{equation}
    L=\begin{pmatrix}
        1    &   0    &   0    \\
        .    &   1    &   0    \\
        .    &   .    &   1
    \end{pmatrix}.
\end{equation}
Mais vu  que nous avons permuté les lignes \( 2\) et \( 3\) au deuxième pas, nous devons permuter \( l_{21}\) et \( l_{31}\) avant de remplir la matrice \( L\) avec les multiplicateurs :
\begin{equation}
    L=\begin{pmatrix}
        1    &   0    &   0    \\
        -1/2    &   1    &   0    \\
        1/2    &  -1/5    &   1
    \end{pmatrix}.
\end{equation}

Notons que ces \( L\) et \( U\) ne sont pas les mêmes que le \( L U\) obtenu sans pivot. Où est l'unicité ? Elle est que en fait maintenant nous n'avons pas \( A=LU\), mais
\begin{equation}
    PA=LU
\end{equation}
où \( P\) est une matrice de permutation. Pour élucider ce point, il nous faut introduire les matrices de permutation élémentaires, ce qui sera encore un bon moment pour l'agrégation.

\begin{lemma}[\cite{ooJZHZooLRqIsV}]        \label{LEMooYIYIooYhnaOt}
    Les matrices de permutation élémentaires ont la relation de «commutation» suivante avec les transformations gaussiennes :
    \begin{equation}
        M_k(x)P_{ij}=P_{ij}M_k\big(  P_{ij}(x) \big).
    \end{equation}
\end{lemma}

\begin{proposition}[Méthode de Gauss avec pivot partiel\cite{ooJZHZooLRqIsV}]
    Soit une matrice inversible \( A\in\eM(n,\eC)\). Il existe
    \begin{itemize}
        \item une matrice de permutation \( P\)
        \item une matrice triangulaire inférieure de diagonale unitaire \( L\),
        \item une matrice triangulaire supérieure inversible \( U\)
    \end{itemize}
    telles que
    \begin{equation}
        PA=LU.
    \end{equation}
\end{proposition}
Notons que cette proposition ne demande que l'hypothèse d'inversibilité pour \( A\). Il n'y a pas d'hypothèses sur tous les mineurs comme c'était le cas avec Gauss sans pivot.

\begin{proof}
    Nous prouvons par récurrence qu'il existe des matrices \( Q_k\), \( E_1\), \ldots, \( E_k\) et \( A_k\) telles que
    \begin{equation}        \label{EQooOBXWooXxwXSe}
        Q_kA=E_1\ldots E_kA_k
    \end{equation}
    avec
    \begin{enumerate}
        \item
            \( Q_k\) est une matrice de permutation
        \item
            \( E_i\) est une transformation gaussienne sur la \( i\)\ieme colonne
        \item
            \( A_k\) est triangulaire supérieure jusqu'à la \( k\)\ieme colonne.
    \end{enumerate}

    Sachant que \( \det(Q_k)=\pm 1\), et que \( \det(E_i)=1\), le passage au déterminant dans \eqref{EQooOBXWooXxwXSe} nous donne \( \det(A_k)\neq 0\) et si nous notons \( \Omega_k(A)\) la matrice tronquée de \( A\), ne gardant que les entrées plus grandes que \( k\), nous avons
    \begin{equation}
        \det(A_k)=\prod_{i=1}^k(A_k)_ii\det\big( \Omega_{k+1}(A_k) \big).
    \end{equation}
    Donc  : \( (A_k)_{ii}\neq 0\) pour \( i\leq k\) et \( \det\big( \Omega_{k+1}(A_k) \big)\neq 0\).

    Pour fixer les idées, voici une image de \( k=2\) :
    \[
    \input{Fig_FCUEooTpEPFoeQ.pstricks}
    \]
    
    Étant donné que \( \det\big( \Omega_{k+1}(A_k) \big)\neq 0\), parmi les nombres \( (A_k)_{i,k+1}\) (\( i\geq k+1\)), au moins un est non nul et nous posons \( r_{k+1}\) tel que \( | (A_k)_{r_{k+1},k+1} | \) soit maximum parmi ces éléments.

    Le nombre \( r_{k+1}\) est enregistré parce qu'il servira à écrire la matrice \( P\) plus tard. Les matrices \( E_i\) ne sont pas enregistrées, parce que nous verrons qu'elles vont encore changer. Seule la dernière sera enregistrée.

    La composante \( (k+1,k+1)\) de la matrice
    \begin{equation}
        P_{r_{k+1},k+1}A_k
    \end{equation}
    est non nulle et peut donc servir de pivot. Soit \( M_{k+1}\) la transformation gaussienne pour la \( (k+1)\)\ieme colonne de la matrice \( P_{r_{k+1},k+1}A_k\). La matrice
    \begin{equation}
        A_{k+1}=M_{k+1}P_{r_{k+1},k+1}A_k
    \end{equation}
    est alors une matrice triangulaire supérieure jusqu'à la \( (k+1)\)\ieme colonne. En posant \( E_{k+1}=M_{k+1}^{-1}\) nous avons
    \begin{equation}
        P_{r_{k+1},k+1}E_{k+1}A_{k+1}=A_k,
    \end{equation}
    et nous nous sentons en droit de récrire l'équation de départ \eqref{EQooOBXWooXxwXSe} :
    \begin{equation}
        Q_kA=E_1\ldots E_kA_k=E_1\ldots E_kP_{r_{k+1},k+1}E_{k+1}A_{k+1}.
    \end{equation}
    Le lemme \ref{LEMooYIYIooYhnaOt} nous permet de ramener la matrice \( P_{r_{k+1},k+1}\) en première position, quitte à modifier un peu (pas beaucoup) chacune des matrices \( E_i\) (\( i=1,\ldots, k\)). C'est pour cela que nous n'enregistrons pas les matrices \( E_i\). Nous avons donc
    \begin{equation}
        P_{r_{k+1},k+1}Q_kA=E'_1\ldots E'_kE_{k+1}A_{k+1}
    \end{equation}
    où
    \begin{itemize}
        \item Le produit \( P_{r_{k+1},k+1}Q_k\) est encore une matrice de permutation, et mieux : elle vaut
            \begin{equation}
                \prod_{i=1}^{k+1}P_{r_i,i}.
            \end{equation}
            Cela montre qu'il est suffisant d'enregistrer les nombres \( r_i\) pour reconstituer cette partie.
        \item
            La matrice \( E'_i\) est une transformation gaussienne pour la \( i\)\ieme colonne.
        \item
            La matrice $A_{k+1}$ est triangulaire supérieure jusqu'à la \( k+1\)\ieme colonne.
    \end{itemize}
    
    La récurrence est maintenant finie et nous pouvons écrire avec \( k=n\) :
    \begin{equation}        \label{EQooFUEUooHVPFwn}
        Q_nA=E_1\ldots E_nA_n
    \end{equation}
    où le produit \( E_1\ldots E_n\) est triangulaire inférieure et \( A_n\) est triangulaire supérieur.

    Maintenant nous enregistrons la matrice \( U=A_n\), le produit \( L=\prod_{i=1}^nE_n\) et les nombres \( r_i\) qui permettent de retrouver \( P\).
\end{proof}

Note : dans l'équation \eqref{EQooFUEUooHVPFwn} nous avons bien entendu massivement renommé les $E_i'$ en \( E_i\). En réalité la matrice \( E_1\) vient avec \( n\) primes sur la tête.

Dans les exemples \ref{ExooNTECooXvTcoh}, \ref{EXooNVRNooJgQmQc} et \ref{EXooNCRSooTfmPFr}, nous allons résoudre le système
\begin{equation}
    \begin{pmatrix}
        10^{-9}    &   1    \\ 
        1    &   1    
    \end{pmatrix}\begin{pmatrix}
        x_1    \\ 
        x_2    
    \end{pmatrix}=\begin{pmatrix}
        1    \\ 
        2    
    \end{pmatrix}
\end{equation}
d'abord de façon exacte, et ensuite en supposant une machin ne tenant que \( 8\) chiffres significatifs en utilisant la méthode de Gauss avec ou sans pivot.

Commençons par voir comment se passe en pratique la décomposition \( PA=LU\) de Gauss avec pivot partiel.

\begin{example}
    Décomposons la matrice
    \begin{equation}
        A=\begin{pmatrix}
            1    &   2    &   3    \\
            2    &   5    &   0    \\
            3    &   8    &   0
        \end{pmatrix}.
    \end{equation}
    
\end{example}
<++>

\begin{example}     \label{ExooNTECooXvTcoh}
    Nous commençons de façon exacte, par la méthode de Gauss sans pivot. La première transformation gaussienne est
    \begin{equation}
        E_1=\begin{pmatrix}
            1    &   0    \\ 
            -10^9    &   1    
        \end{pmatrix}
    \end{equation}
    et nous calculons
    \begin{equation}
        E_1A=\begin{pmatrix}
            1    &   0    \\ 
            -10^9    &   1    
        \end{pmatrix}\begin{pmatrix}
            10^{-9}    &   1    \\ 
            1    &   1    
        \end{pmatrix}=
        \begin{pmatrix}
            10^{-9}    &   1    \\ 
            0    &   1-10^{-9}    
        \end{pmatrix}.
    \end{equation}
    Vu que cette dernière est triangulaire supérieure, nous avons fini la méthode de Gauss et \( U=E_1A\). En ce qui concerne la matrice \( L\), elle est donnée par \( L=E_1^{-1}\), c'est à dire
    \begin{equation}
        L=\begin{pmatrix}
            1    &   0    \\ 
            -10^9    &   1    
        \end{pmatrix}^{-1}=
        \begin{pmatrix}
            1    &   0    \\ 
            10^9    &   1    
        \end{pmatrix}.
    \end{equation}
    Au final nous avons la décomposition \( A=LU\) exacte suivante :
    \begin{equation}
        \begin{aligned}[]
            L&=\begin{pmatrix}
                1    &   0    \\ 
                10^9    &   1    
            \end{pmatrix}&U&=\begin{pmatrix}
                10^{-9}    &   1    \\ 
                0    &   1-10^9    
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Résoudre le système \( Ax=b\) revient à résoudre \( LUx=b\) et donc résoudre successivement les systèmes
    \begin{subequations}
        \begin{numcases}{}
            Ly=b\\
            Ux=y.
        \end{numcases}
    \end{subequations}
    D'abord le système
    \begin{equation}
        \begin{pmatrix}
            1    &   0    \\ 
            10^9    &   1    
        \end{pmatrix}\begin{pmatrix}
            y_1    \\ 
            y_2    
        \end{pmatrix}=\begin{pmatrix}
            1    \\ 
            2    
        \end{pmatrix}
    \end{equation}
    donne \( y_1=1\) et \( y_2=2-10^9\). 

    Ensuite nous résolvons
    \begin{equation}
        \begin{pmatrix}
            10^{-9}    &   1    \\ 
            0    &   1-10^9    
        \end{pmatrix}\begin{pmatrix}
            x_1    \\ 
            x_2    
        \end{pmatrix}=\begin{pmatrix}
            1    \\ 
            2-10^9    
        \end{pmatrix}.
    \end{equation}
    Cela donne
    \begin{equation}
        \begin{pmatrix}
            x_1    \\ 
            x_2    
        \end{pmatrix}=\begin{pmatrix}
            -\frac{ 10^9 }{ 1-10^9 }    \\ 
            \frac{ 2-10^9 }{ 1-10^9 }    
        \end{pmatrix}\simeq\begin{pmatrix}
            1    \\ 
            1    
        \end{pmatrix}.
    \end{equation}

    C'est également le résultat que trouve Sage :
    \lstinputlisting{sageSnip007.sage}
\end{example}

\begin{example}     \label{EXooNVRNooJgQmQc}
    Nous recommençons tout le calcul avec une précision limitée à \( 8\) chiffres significatifs, sans pivot.

    Nous avons à nouveau la transformation gaussienne
    \begin{equation}
        E_1=\begin{pmatrix}
            1    &   0    \\ 
            -10^9    &   1    
        \end{pmatrix},
    \end{equation}
    mais pour calculer \( U\) nous effectuons le produit matriciel
    \begin{equation}
        U=E_1A=\begin{pmatrix}
            1    &   0    \\ 
            -10^9    &   1    
        \end{pmatrix}\begin{pmatrix}
            10^{-9}   &   1    \\ 
            1    &   1    
        \end{pmatrix}=\begin{pmatrix}
            10^{-9}    &   1    \\ 
            0    &   *    
        \end{pmatrix}.
    \end{equation}
    Nous détaillons à présent le calcul de l'élément noté \( *\). Le calcul de \( 10^9\ominus 1)\) donne
    \begin{equation}
        999999999=9.99999999\times 10^{8},
    \end{equation}
    mais la précision étant limitée à \( 8\) chiffres, un arrondit arrive. Étant donné que le premier chiffres supprimé est un \( 9\) nous retombons sur \( 10^9\), et donc notre machine à précision limitée donnera
    \begin{equation}
        U=\begin{pmatrix}
            10^{-9}    &   1    \\ 
            0    &   -10^{9}    
        \end{pmatrix}.
    \end{equation}
    Ensuite le calcul de \( L=E_1^{-1}\) ne cause pas de problèmes :
    \begin{equation}
        L=\begin{pmatrix}
            1    &   0    \\ 
            -10^9    &   1    
        \end{pmatrix}.
    \end{equation}
    Maintenant il s'agit de résoudre les systèmes \( Ly=b\) et \( Ux=y\). Du système
    \begin{equation}
        \begin{pmatrix}
            1    &   0    \\ 
            10^9    &   1    
        \end{pmatrix}\begin{pmatrix}
            y_1    \\ 
            y_2    
        \end{pmatrix}=\begin{pmatrix}
            1    \\ 
            2    
        \end{pmatrix}
    \end{equation}
    nous tirons tout de suite \( y_1=1\) et ensuite \( 10^9+y_2=2\), c'est à dire \( y_2=2-10^9\), qui en précision limitée donne encore \( y_2=-10^9\). À résoudre maintenant :
    \begin{equation}
        \begin{pmatrix}
            10^{-9}    &   1    \\ 
            0    &   -10^9    
        \end{pmatrix}\begin{pmatrix}
            x_1    \\ 
            x_2    
        \end{pmatrix}=\begin{pmatrix}
            1    \\ 
            -10^9    
        \end{pmatrix}.
    \end{equation}
    Cela donne immédiatement \( x_2=1\) et ensuite
    \begin{equation}
        10^{-9}x_1+1=1, 
    \end{equation}
    donc \( x_1=0\). La solution trouvée est
    \begin{equation}
        \begin{pmatrix}
            x_1    \\ 
            x_2    
        \end{pmatrix}=\begin{pmatrix}
            0    \\ 
            1    
        \end{pmatrix},
    \end{equation}
    qui est complètement faux au niveau de la première variable.
\end{example}

\begin{example}     \label{EXooNCRSooTfmPFr}
    Nous résolvons encore le même système en précision limitée, mais en utilisant cette fois la méthode de Gauss avec pivot partiel.    
\end{example}
<++>

Notons que Sage utilise la méthode de Gauss avec pivots :
\lstinputlisting{sageSnip006.sage}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Déterminant}
%---------------------------------------------------------------------------------------------------------------------------

Pour calculer un déterminant lorsque nous avons la décomposition \( A=LU\) nous pouvons faire
\begin{equation}
    \det(A)=\det(LU)=\det(L)\det(U)=\det(U)
\end{equation}
parce que \( L\) est triangulaire avec des \( 1\) sur la diagonale.

Si par contre nous avons fait des pivots, nous avons \( PA=LU\). Il nous faut le déterminant de \( P\), qui n'est autre que \( \pm 1\). Nous avons
\begin{equation}
    \det(P)=(-1)^s
\end{equation}
où \( s\) est le nombre de permutations effectives effectuées. Nous précisons «effectives» parce qu'il ne faut pas compter le pas où nous n'avons pas permuter (les cas où le bon pivot était présent du premier coup). Nous avons alors
\begin{equation}
    \det(A)=(-1)^s\det(U).
\end{equation}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Plusieurs termes indépendants}
%---------------------------------------------------------------------------------------------------------------------------

Mettons un système \( Ax=b\) qu'il faut résoudre pour plusieurs \( b\) différents. C'est typiquement le cas où l'on voudrait calculer l'inverse de \( A\). Mais on va directement se calmer. Soient donc à résoudre \( Ax_1=b_1\), \ldots, \( Ax_n=b_n\).

Les opérations (avec ou sans pivot) que nous faisons ne dépendent que de la matrice \( A\), mais aucune décisions concernant les pivots ou la matrice des multiplicateurs ne dépend de \( b\). Autre façon de dire : si le système \(  (A|b_1)  \) devient \( (U|y_1)\), le système \( (A|b_i)\) devient \( (U|y_i)\) avec le même \( U\).

Nous ne sommes donc pas obligés de faire tout le travail autant de fois qu'il n'y a de systèmes à résoudre. Donc si on a plusieurs systèmes à résoudre avec la même matrice, on fait mieux de retenir une fois pour toute la décomposition \( LU\) (avec ou sans pivots), avant de vraiment résoudre.

Ou alors on peut aussi faire que, au lieu de faire \( (A|b_i)\) plein de fois, faire une seule fois
\begin{equation}
    (A|b_1\ldots b_n).
\end{equation}
Et on fait tout le travail sur tous les vecteurs d'un en même temps.

Soit \( {e_i}\) la base canonique. Si nous notons \( x_n\) les solutions des problèmes \( Ax_i=e_i\), tous les problèmes \( Ax_i=e_i\) s'écrivent d'un seul coup 
\begin{equation}
    AX=Y
\end{equation}
où \( X\) est la matrice des \( x_i\) en colonnes, et \( Y\) est celle des \( e_i\) en colonnes. Oh, mais \( Y=\mtu\) évidemment. Donc
\begin{equation}
    AX=\mtu.
\end{equation}
Si nous supposons que \( A\) est inversible, alors ce \( X\) est l'inverse.

Donc pour calculer l'inverse d'une matrice de dimension non trop grande, il suffit d'utiliser la méthode de Gauss sur les vecteurs de la base canonique. Cette idée est la base du calcul de l'inverse par matrice companion. En effet, si nous partons du problème
\begin{equation}
    (A|\mtu)
\end{equation}
et nous appliquons la méthode de Gauss avec pivot, nous arrivons à
\begin{equation}
    (U|L^{-1} P).
\end{equation}
Attention : le produit \( L^{-1}P\) est une permutation des \emph{colonnes} de \( L^{-1}\). Vu que \( L\) est triangulaire inférieure avec des \( 1\) sur la diagonale, \( L^{-1}\) est triangulaire inférieure avec des \( 1\) sur la diagonale. Donc si la matrice n'est pas trop grande, on peut assez facilement remettre les colonnes de \( L^{-1}P\) dans l'ordre pour recomposer une matrice triangulaire inférieure avec \( 1\) sur la diagonale.

Une autre façon de calculer l'inverse, si \( A=LU\) est connue, il suffit de faire 
\begin{equation}
    A^{-1}=U^{-1}L^{-1}.
\end{equation}
Et il existe un algorithme facile pour l'inverse d'une matrice triangulaire.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Cholesky}
%---------------------------------------------------------------------------------------------------------------------------

Le commandant Cholesky travaillait sur le tir de canon (chose éminemment liée à de nombreuses mathématiques ingénieuses). La méthode de Cholesky est encore utilisée aujourd'hui dans les vrais problèmes.

La méthode de Gauss s'applique sans hypothèses sur la matrice \( A\), à part qu'elle doit être de petite dimension, comme pour toute méthode directe. Souvent nous savons des choses sur la matrice. Ici nous allons supposer que \( A\) est symétrique et définie positive.

Comment numériquement vérifier ces hypothèses ? En ce qui concerne la symétrique, il suffit de faire le test complet :
\begin{equation}
    A^t=A.
\end{equation}
La vérification de cela coûte au maximum \( n^2\) comparaisons (et en fait la moitié de ça moins la diagonale). 

Le fait que \( A\) soit définie positive est facile à vérifier pour utiliser Cholesky parce que il suffit de le faire, et si il n'y a pas de nombres complexes qui arrivent, c'est que la matrice était définie positive.

Un lemme très simple à mettre en oeuvre numériquement nous permet de traiter certains cas.
\begin{lemma}
    Une matrice symétrique possédant un élément négatif sur la diagonale n'est pas définie positive.
\end{lemma}

\begin{proof}
    Un simple calcul ou effort d'imagination montre que \( \langle Me_k, e_k\rangle =M_{kk}\). Donc si \( M\) doit être définie positive, \( M_{kk}\) doit être positive par le lemme \ref{LemWZFSooYvksjw}.
\end{proof}
Ce lemme est un moyen déjà de faire quelque vérifications. Et si les éléments diagonaux de \( A\) sont tous négatifs, on peut prendre \( -A\).

\begin{lemma}       \label{LEMooVEIYooZbShQb}
    Si \( A\) est une matrice symétrique strictement définie positive, alors pour tout \( k\), la matrice tronquée \( \Delta_k(A)\) l'est également.    
\end{lemma}

\begin{proof}
    Le fait que \( \Delta_k(A)\) soit symétrique est évidement. Le fait qu'elle soit définie positive l'est moins. Soit \( y\in \eR^k\) et le vecteur \( \tau y\in \eR^n\), qui est «complété» avec des zéros.

    Nous avons \( \langle \Delta_k(A)y, y\rangle_k=\langle A\tau y, \tau \rangle_n\). En effet
    \begin{equation}
        \langle \Delta_k(A)y, y\rangle =\sum_{i=1}^k\sum_{l=1}^kA_{il}y_ly_i.
    \end{equation}
    Et à droite :
    \begin{equation}
            \langle A\tau y, \tau y\rangle =\sum_{i=1}^n(A\tau y)_i(\tau y)_i
            =\sum_{i=1}^k(A\tau y)_i y_i
            =\sum_{i=1}^k\sum_{l=1}^n A_{il}(\tau y)_ly_i
            =\sum_{i=1}^k\sum_{l=1}^k A_{il}y_ly_i
    \end{equation}
    où nous avons utilisé le fait que \( (\tau y)_i=0\) dès que \( i>k\) et que \( (\tau y)_i=y_i\) sinon.

    En conséquence de quoi \( \langle \Delta_k(A)y, u\rangle >0\) pour tout \( y\in \eR^k\) et la matrice \( \Delta_k(A)\) est strictement définie positive.
\end{proof}

\begin{lemma}       \label{LEMooLBQLooIYvacH}
    Si \( T\) est une matrice triangulaire, alors \( (T_{ii})^{-1}=(T^{-1})_{ii}\).
\end{lemma}

\begin{proof}
    Il suffit de se rendre compte que le coefficient \( ii\) de l'égalité \( \mtu=TT^{-1}\) donne
    \begin{equation}
        1=\sum_lT_{il}(T^{-1})li.
    \end{equation}
    Dans la somme il ne reste que le terme \( l=i\).
\end{proof}

Nous allons chercher une décomposition de type \( LU\) sous la forme \( A=LL^t\), c'est à dire \( U=L^t\). Attention : maintenant nous n'avons plus des \( 1\) sur la diagonale. Ce n'est donc pas exactement la décomposition \( LU\) dont nous parlions plus haut. C'est pour cela que nous n'allons pas la noter \( LL^t\) mais \( BB^t\).

\begin{theorem}[Cholesky\cite{ooDANFooPSmBfd}]
    Soit une matrice réelle symétrique strictement définie positive. Il existe une unique matrice réelle \( B\) telle que
    \begin{itemize}
        \item \( B\) est triangulaire inférieure,
        \item la diagonale de \( B\) est positive,
        \item \( A=BB^t\).
    \end{itemize}
\end{theorem}

\begin{proof}
    Par la décomposition \( LU\) du théorème \ref{THOooUXKJooYaPhiu} nous avons des matrices \( L\) et \( U\) telles que \( A=LU\). Soit \( D\) la matrice diagonale donnée par 
    \begin{equation}
        D_{ii}=\sqrt{ U_{ii} }.
    \end{equation}
    Cette définition fonctionne parce que \( U_{ii}>0\). En effet nous savons que \( \Delta_k(A)=\Delta_k(L)\Delta_k(U)\), et en passant au déterminant, 
    \begin{equation}
        \det\big( \Delta_k(A) \big)=\det\big( \Delta_k(U) \big).
    \end{equation}
    Vu que \( \Delta_k(A)\) est strictement définie positive par le lemme \ref{LEMooVEIYooZbShQb}, son determinant est strictement positif\footnote{Le théorème \ref{ThoeTMXla} donne une diagonalisation par des matrices de déterminant \( 1\). Vu que les valeurs propres forment sur la diagonale, et qu'elles sont toutes positives, el déterminant est positif.} et nous avons
    \begin{equation}
        \det\big( \Delta_k(U) \big)>0.
    \end{equation}
    En appliquant cela à \( k=1\) nous avons \( U_{11}>0\) puis de proche en proche, \( U_ii>0\) pour tout \( i\).

    Nous posons :
    \begin{subequations}
        \begin{align}
            B&=LD&\text{qui est triangulaire inférieure}\\
            C&=D^{-1} U&\text{qui est triangulaire supérieure.}
        \end{align}
    \end{subequations}
    Nous avons bien entendu \( A=BC\) et nous allons prouver que \( C=B^t\). Vu que \( A=A^t\) nous pouvons identifier \( BC\) et \( C^tB^t\) :
    \begin{equation}
        BC=C^tB^t.
    \end{equation}
    En mettant les matrices triangulaires supérieures à gauche et inférieures à droite :
    \begin{equation}
        C(B^t)^{-1}=B^{-1}C^t,
    \end{equation}
    qui sont donc deux matrices diagonales. Nous montrons que cette diagonale est en réalité l'identité.

    D'abord
    \begin{equation}
        B_{ii}=\sum_{l=1}^nL_{il}D_{li}=L_{ii}\sqrt{ U_{ii} }=\sqrt{ U_{ii} }
    \end{equation}
    parce que \( L_{ii}=1\). Notons en passant que la diagonale de \( B\) est positive.  Ensuite 
    \begin{equation}
        C_{ii}=\sum_{l=1}^n(D^{-1})_{il}U_{li}=(D^{-1})_{ii}U_{ii}=\frac{1}{ \sqrt{ U_ii } }U_{ii}=\sqrt{ U_{ii} }.
    \end{equation}
    Donc \( B\) et $C$ ont des diagonales égales. Calculons alors la diagonale de \( B^{-1}C^t\) :
    \begin{equation}
        \big( B^{-1}C^t \big)_{ii}=\sum_l(B^{-1})_{il}(C^t)_{li}=(B^{-1})_{ii}C_{ii}
    \end{equation}
    parce que encore une fois, de la somme il ne reste que le terme \( l=i\).

    Mais \( B\) est une matrice triangulaire qui tombe sous le coup du lemme \ref{LEMooLBQLooIYvacH}. Donc \( (B^{-1})_{ii}=(B_{ii})^{-1}=(C_{ii})^{-1}\). Nous avons alors
    \begin{equation}
        (B^{-1}C^t)_{ii}=1.
    \end{equation}
    Cela conclu l'existence de la décomposition de Cholesky.

    En ce qui concerne l'unicité, soient \( A=BB^t=CC^t\). Nous regroupons les supérieures et les inférieures :
    \begin{equation}        \label{EQooRRJHooJrFBLn}
        B^t(C^t)^{-1}=B^{-1}C.
    \end{equation}
    Ces deux matrices sont donc diagonales et nous posons \( D=B^{-1} C\), c'est à dire \( C=BD\). Nous remplaçons donc \( C\) par \( BD\) dans \eqref{EQooRRJHooJrFBLn} :
    \begin{equation}
        A=BB^t=BD(BD)^t=BDD^tB^t.
    \end{equation}
    Donc \( DD^t=\mtu\), ce qui signifie que les éléments diagonaux de \( D\) sont \( \pm 1\). Nous montrons qu'ils sont positifs : à partir de \( C=BD\) nous déballons 
    \begin{equation}
        C_{ii}=\sum_lB_{il}D_{li},
    \end{equation}
    et donc
    \begin{equation}
        B_{ii}D_{ii}=C_{ii}.
    \end{equation}
    En sachant que les conditions de la décomposition de Cholesky demandent les éléments diagonaux positifs nous en déduisons que \( D_{ii}\) est positif et donc égal à \( 1\). Finalement \( D=\mtu\) et \( B=C\).
\end{proof}

Prenons la matrice
\begin{equation}
    A=\begin{pmatrix}
        4    &   2    &   -2    \\
        2    &   10    &   -7    \\
        -2    &   -7    &   9
    \end{pmatrix}
\end{equation}
Elle est symétrique et définie positive. Nous posons
\begin{subequations}        \label{EQooFMWUooFdTgiF}
    \begin{numcases}{}
    l_{11}=\sqrt{ a_{11} }
    l_{i1}=a_{i1}/l_{11}
    \end{numcases}
\end{subequations}
pour \( i=2,\ldots, n\). Et aussi
\begin{subequations}        \label{EQooJTVGooEkynpH}
    \begin{numcases}{}
        l_{jj}=(a_{jj}-\sum_{k=1}^{j-1}l_{jk}^2)^{1/2}\\
        l_{ij}=(a_{ij}-\sum_{k=1}^{j-1}l_{ik}l_{jk})/a_{jj}
    \end{numcases}
\end{subequations}
pour \( i=j+1,\ldots, n\).

Les formules \eqref{EQooFMWUooFdTgiF} nous disent comment remplir la première colonne. Cela donne la matrice
\begin{equation}
    L=\begin{pmatrix}
        \sqrt{ 4 }=2    &  .     &  .     \\
        2/2=1    &    .   &  .     \\
        -2/2=-1    &   .    &   .
    \end{pmatrix}
\end{equation}
Les formules \eqref{EQooJTVGooEkynpH} donnent les autres colonnes en fonction des précédentes.


Dans Sage :

\lstinputlisting{sageSnip005.sage}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Système linéaire (méthodes itératives)}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous trouvons des méthodes itératives lorsque les matrices sont grandes, ce qui arrive lorsque l'on discrétise une équation différentielle.

Nous allons chercher des méthodes de la forme \( x_{n+1}=Bx_n+q\); ce sont des méthodes stationnaires. La convergence d'une méthode est toujours liée à la matrice \( B\) et en général, la convergence ne dépend pas du choix du vecteur initial. Nous faisons donc souvent \( x_0=0\) et donc \( x_1=q\). Voila donc une itération de faire gratuitement.

Nous notons \( e_k\) le \defe{vecteur d'erreur}{erreur} qui est définit par \( e_k=x-x_k\). Et le \defe{vecteur résidu}{résidu!méthode itérative} \( r_k=b-Ax_k\). Attention : ici \( k\) n'est pas un indice mais un numéro de vecteur.

Notons que si \( x\) est solution, alors \( b-Ax=0\), ce qui motive le vecteur résidu.

Les conditions d'arrêt d'un algorithme seraient
\begin{subequations}
    \begin{numcases}{}
        \| e_k \|_{\infty}\ll \epsilon_1\\
        \| r_k \|_{\infty}<\epsilon_2
    \end{numcases}
\end{subequations}
où \( \epsilon_1\) et \( \epsilon_2\) sont des précisions décidées à l'avance par l'utilisateur.

\begin{proposition}
    Si \( A\) est une matrice inversible, alors
    \begin{equation}
        \lim_{k\to \infty} e_k=\lim_{k\to \infty} r_k=0.
    \end{equation}
\end{proposition}

Vu que \( r_k=Ae_k\), si la matrice \( A\) est mal conditionnée, il peut arriver que \( r_k\) reste grand alors que \( e_k\) est déjà petit.

\begin{remark}
    Dans les méthode stationnaires, nous avons \( x_{n+1}=Bx_n+q\) avec \( B\) et \( q\) fixés au départ de l'algorithme. Il existe des méthodes non stationnaires pour lesquelles l'itération prend la forme \( x_{n+1}=B_nx_n+q_n\) avec \( B_n\) et \( q_n\) qui changement avec les étapes.
\end{remark}

\begin{proposition}
    Pour la méthode \( x_{n+1}=Bx_n+q\) nous avons équivalence de
    \begin{enumerate}
        \item
            La méthode converge pour tout \( x_0\)
        \item
            \( B\) est une matrice convergente\footnote{C'est à dire \( \lim_{k\to \infty} B^k=0\).}
        \item
            \( \rho(B)<1\) (rayon spectral).
    \end{enumerate}
    De plus si \( \| B \|<1\) alors la méthode converge (quelle que soit la norme algébrique). 
\end{proposition}

La norme d'une matrice (en tout cas, certaines normes) est quelque chose de facile à calculer à l'ordinateur. Typiquement \( \| . \|_{\infty}\) est un simple maximum. Cependant si après avoir calculer \( \| B \|_i\) pour des dizaines de normes \( i\) différentes, nous avons toujours \( \| B\|_i\geq 1\), alors nous ne pouvons rien conclure.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{La méthode générale}
%---------------------------------------------------------------------------------------------------------------------------

Nous décomposons la matrice \( A\) sous la forme \( A=M-N\) avec \( M\) inversible. Le système \( Ax=b\) devient
\begin{equation}
    Mx-Nx=b
\end{equation}
puis \( Mx=Nx+b\) et finalement 
\begin{equation}
    x=M^{-1}Nx+M^{-1}b,
\end{equation}
et voila une méthode stationnaire avec \( B=M^{-1}N\) et \( q=M^{-1}b\).

Mais ici nous voyons que \( M\) doit être non seulement inversible, mais en plus doit être facilement calculable. En sachant que nous travaillons avec des grandes matrices, il n'est pas question d'inverser \( M\) avec une méthode de Gauss.

En bref, il faut choisir \( M\) triangulaire parce que c'est en gros la seule que nous pouvons inverser facilement\footnote{Les matrices orthogonales sont aussi facilement inversibles, mais ne se prêtent pas bien à une décomposition de type somme.}.

\begin{remark}
    La matrice \( B\) ne doit pas spécialement être inversible. Si elle ne l'est pas, ce n'est pas un problème.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Jacobi}
%---------------------------------------------------------------------------------------------------------------------------

Nous décomposons  
\begin{equation}        \label{EQooOCJYooCqsfQM}
    A=D-E-F
\end{equation}
où \( D\) est la diagonale de \( A\), \( -F\) est la partie triangulaire supérieure (sans la diagonale) et \( -E\) la triangulaire inférieure (sans la diagonale). Donc \( D\), \( E\) et \( F\) sont simplement des extraction de parties de la matrice \( A\) (et quelque changements de signes).

La méthode de Jacobi prend \( M=D\) et \( N=(E+F)\). L'inverse de \( M\) est facile à calculer parce que \( M\) est diagonale. Nous notons \( B_J\) la matrice \( B\) de la méthode de Jacobi.

\begin{remark}
    Il se peut que la matrice \( A\) ait des zéros sur la diagonale, même si elle est inversible. Et cela est un problème parce qu'alors la matrice \( D\) ici construite n'est pas inversible. Dans ce cas, avant de nous lancer dans la méthode de Jacobi, il faut permuter deux lignes de \( A\) et donc de \( b\).

    Attention cependant que l'on pourrait vouloir effectuer ces permutations en mettant sur la diagonale des nombres les plus grands possibles (parce qu'ensuite, ce qui rentre dans les calculs, c'est \( D^{-1}\) qui aura alors des petits nombres). Mais il faut toutefois faire en sorte que le rayon spectral de la matrice \(B \) résultante reste plus petit que \( 1\).

    Chaque changement dans \( A\) induit des changements dans \( B\) et donc sur la convergence de la méthode.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Gauss-Seidel}
%---------------------------------------------------------------------------------------------------------------------------

Nous partons de la même décomposition \( A=D-E-F\) que dans \eqref{EQooOCJYooCqsfQM}. La méthode de Gauss-Seidel prend \( M=(D-E)\) et \( N=F\).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Autres}
%---------------------------------------------------------------------------------------------------------------------------

Voir la méthode des gradients, et des gradients conjugués.

