% This is part of Mes notes de mathématique
% Copyright (c) 2011-2015
%   Laurent Claessens, Carlotta Donadello
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Extension du corps de base}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooAUOWooNdYTZf}

Nous avons discuté dans la section \ref{SECooLQVJooTGeqiR} de ce qui arrive au corps lorsqu'on l'étend. Dans cette sections nous allons étudier ce qui arrive aux applications linéaires entre deux \( \eK\)-espaces vectoriels lorsque nous étendons le corps \( \eK\) en un corps \( \eL\).

Soit donc un corps \( \eK\) et deux \( \eK\)-espaces vectoriels \( E\) et \( F\), et entrons dans le vif du sujet\footnote{Le sujet étant le corps étendu.}. Soit \( \eK\) un corps (commutatif) et une extension \( \eL\) de \( \eK\). Soient \( E\) et \( F\), des \( \eK\)-espaces vectoriels de dimension finie. 

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Extension des applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------


\begin{definition}[\cite{ooAFBYooYvTCCN}]
    L'espace vectoriel obtenu par \defe{extension du corps de base}{extension!corps de base} de \( E\) est l'espace vectoriel
    \begin{equation}
        E_{\eL}=\eL\otimes_{\eK}E.
    \end{equation}
    Ce dernier est le quotient \( \eL\otimes_{\eK}E=(\eL\times E)/\sim\) par la relation d'équivalence
    \begin{equation}
        (\lambda,v)\sim\big( a\lambda,\frac{1}{ a }v \big)
    \end{equation}
    pour tout \( a\in \eK\). Nous noterons \( [\lambda,v]\) ou \( \lambda\otimes v\) ou encore \( \lambda\otimes_{\eK}v\) la classe de \( (\lambda,v)\).
\end{definition}
Un élément de \( E_{\eL}\) est de la forme \( \sum_k[\lambda_k,v_k]\) avec \( \lambda_k\in \eL\) et \( v_k\in E\). Si \( f\colon E\to F\) est une applications linéaire nous définissons
\begin{equation}
    \begin{aligned}
        f_{\eL}\colon E_{\eL}&\to F_{\eL} \\
        [\lambda,v]&\mapsto [\lambda,f(v)]. 
    \end{aligned}
\end{equation}

\begin{remark}
    Si deux vecteurs de \( E_{\eL}\) sont linéairement indépendants pour \( \eK\), ils ne le sont pas spécialement pour \( \eL\). Par exemple si \( \eC\) est vu comme \( \eR\)-espace vectoriel, alors \( \{ 1,i \}\) est une partie libre. Mais dans \( \eC\) vu comme \( \eC\)-espace vectoriel, la partie \( \{ 1,i \}\) n'est pas libre.
\end{remark}

Nous définissons aussi l'injection canonique
\begin{equation}
    \begin{aligned}
        \iota\colon E&\to E_{\eL} \\
        v&\mapsto [1,v]. 
    \end{aligned}
\end{equation}

\begin{proposition}[\cite{ooEPEFooQiPESf}]      \label{PropooWECLooHPzIHw}
    Injectivité et surjectivité respectées.
    \begin{enumerate}
        \item
            L'application \( f_{\eL}\) est injective si et seulement si \( f\) est injective.    
        \item
            L'application \( f_{\eL}\) est surjective si et seulement si \( f\) est surjective.    
    \end{enumerate}
\end{proposition}

\begin{proof}
    Supposons pour commencer que \( f_{\eL}\) est injective.
    Le diagramme
    \begin{equation}
        \xymatrix{%
            E \ar[r]^-{f}\ar[d]_-{\tau}      &   F\ar[d]^{\tau}\\
            E_{\eL} \ar[r]_{f_{\eL}}  &   F_{\eL}
           }
    \end{equation}
    est un diagramme commutatif. En effet 
    \begin{equation}
        (\tau\circ f)(v)=[1,f(v)]
    \end{equation}
    tandis que 
    \begin{equation}
        (f_{\eL\circ\tau})(v)=f_{\eL}[1,v]=[1,f(v)].
    \end{equation}
    Donc si \( f(v)=0\) avec \( v\neq 0\) nous aurions \( (\tau\circ f)(v)=0\) et donc aussi \( (f_{\eL}\circ \tau)(v)=0\), alors que \( \tau(v)\neq 0\) dans \( E_{\eL}\).

    Réciproquement, nous supposons que \( f\) est injective et nous prouvons que \( f_{\eL}\) est injective. Par le lemme \ref{LEMooDAACooElDsYb}\ref{ITEMooEZEWooZGoqsZ}, nous savons qu'il existe \( g\colon F\to E\) telle que \( f\circ g=\id|_F\). Nous en déduisons que \( f_{\eL}\circ g_{\eL}=\id|_{F_{\eL}}\) parce que si \( [\lambda,v]\in F_{\eL}\) alors
    \begin{equation}
        (f_{\eL}\circ g_{\eL})[\lambda,v]=f_{\eL}[\lambda,g(v)]=[\lambda,(f\circ g)(v)]=[\lambda,v].
    \end{equation}
    Notons que \( g\) est injective, donc \( g_{\eL}\) est injective et l'égalité \( f_{\eL}\circ g_{\eL}=\id|_{F_{\eL}} \) implique que \( f_{\eL}\) est également injective.
\end{proof}

\begin{proposition}[\cite{ooYVQCooFBVEXo}\quext{Attention : dans ce texte, l'auteur se place dans un cadre nettement plus général. J'ai tenté de suivre et d'adapter. Jene suis même pas tout à fait certain que l'énoncé soit correct (ici). Si vous êtes expert en extension de corps, vous devriez m'écrire pour me dire ce que vous pensez de ce que vous aller lire ici.}]   \label{PROPooMHARooUycAts}
    Soit \( \{ e_i \}_{i=1,\ldots, p}\) une base de \( E\). Alors \( \{ 1\otimes e_i \}_i\) est une base de \( E_{\eL}=\eL\otimes_{\eK}E\).
\end{proposition}

\begin{proof}
    L'espace vectoriel \( E\) peut être écrit comme somme directe \( E=\bigoplus_i\eK e_i\). Si \( \lambda\in \eL\) et \( k\in \eK\) nous avons
    \begin{equation}
        \lambda\otimes ke_i=\frac{ \lambda }{ k }\otimes e_i=\frac{ \lambda }{ k }(1\otimes e_i).
    \end{equation}
    Cela pour introduire que l'application
    \begin{equation}
        \begin{aligned}
            \psi\colon \eL\otimes_{\eK}E&\to \bigoplus_i\eL(1\otimes e_i) \\
            \sum_k \lambda_k\otimes v_k&\mapsto \oplus_i \sum_k(\lambda_k v_{ik})(1\otimes e_i)
        \end{aligned}
    \end{equation}
    où \( v_k=\sum_i v_{ik}e_i\) avec \( v_{ik}\in \eK\) est un isomorphisme de \( \eL\)-espaces vectoriels. La surjectivité est facile. En ce qui concerne l'injectivité, si
    \begin{equation}
        \sum_i\sum_k(\lambda_kv_{ik})(1\otimes e_i)=0
    \end{equation}
    alors les choses suivantes sont nulles également :
    \begin{equation}
        \sum_i\sum_k(\lambda_kv_{ik})(1\otimes e_i)=\sum_{ik}(\lambda_k\otimes v_{ik}e_i)=\sum_k(\lambda_k\otimes \sum_iv_{ik}e_i)=\sum_k(\lambda_k\otimes v_k).
    \end{equation}
    Le dernier est l'argument de \( \psi\). Le fait que ce soit nul implique que \( \psi\) est injective.
\end{proof}

\begin{remark}
    Nous n'avons pas dû prouver que chacun des \( \lambda_k\otimes v_k\) était nul. Et encore heureux, parce que cela pouvait très bien être faux, vu qu'il y a plusieurs façons de noter un élément de \( E_{\eL}\) sous la forme de tels termes.
\end{remark}

\begin{corollary}       \label{CORooTQGHooIKhNtr}
    La \( \eL\)-dimension de \( E_{\eL}\) est égale à la \( \eK\)-dimension de \( E\).
\end{corollary}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Projections}
%---------------------------------------------------------------------------------------------------------------------------

\begin{probleme}
    Nous allons définir \( \pr\colon \aL(E_{\eL},F_{\eL})\to \aL(E,F)\) en faisant appel à des bases et en prouvant que les choses définies ne dépendent pas des bases choisies. Il y a sûrement une façon plus «intrinsèque» de faire.
\end{probleme}


Nous savons que \( \eL\) est un \( \eK\)-espace vectoriel dans lequel nous pouvons voir \( \eK\) comme un sous-espace (lemme \ref{LemooOLIIooXzdppM}). Dans cette optique nous choisissons dans \( \eL\) un supplémentaire de \( \eK\), c'est à dire un sous-espace vectoriel de \( \eL\) tel que
\begin{equation}
    \eL=\eK\oplus V.
\end{equation}
Nous avons alors naturellement une projection \( \pr\colon \eL\to \eK\).

Soit \( \{ e_i \}\) une base de \( E \) et \(\{ e_a \}\) une  de\( F\). Nous noterons également \( e_i\) et \( e_a\) les éléments \( \tau e_i\) et \( \tau e_a\) correspondants. Grâce à la proposition \ref{PROPooMHARooUycAts}, ce sont des bases de \( E_{\eL}\) et \( F_{\eL}\). Si la fonction \( f\colon E_{\eL}\to F_{\eL}\) s'écrit dans ce ces bases comme
\begin{equation}
    f(e_i)=\sum_af_{ai}e_a
\end{equation}
alors nous définissons \( \pr(f)\) par 
\begin{equation}        \label{EQooSAFRooJnfkLO}
    (\pr f)e_i=\sum_a\pr(f_{ai})e_a.
\end{equation}

\begin{proposition}[\cite{MonCerveau}]
    L'application \( \pr\) définie en \eqref{EQooSAFRooJnfkLO} est indépendante du choix des bases.
\end{proposition}

\begin{proof}
    Notons que dans ce qui suit, les sommes sur \( a\) ou \( b\) et celles sur \( i\) ou \( j\) ne vont pas jusqu'au même indice (dimensions de \( E\) et \( F\)). De plus nous manipulons deux choses qui se notent \( \pr\). La première est la projection \( \pr\colon \eL\to \eK\) qui ne dépend que d'un choix de supplémentaire et que nous supposons fixée ici. D'autre part il y a \( \pr\colon E_{\eL}\to E\) qui dépend a priori des bases choisies.

    Nous choisissons de nouvelles bases qui sont liées aux anciennes bases par
    \begin{subequations}
        \begin{numcases}{}
            e'_b=\sum_aB_{ab}e_a\\
            e'_i=\sum_jA_{ji}e_j.
        \end{numcases}
    \end{subequations}
    Les matrices \( A\) et \( B\) sont dans \( \GL(\eK)\). Nous allons écrire l'opérateur \( \pr'\) qui correspond à ces bases et montrer que pour toute application linéaire \( f\colon E_{\eL}\to F_{\eL} \) nous avons \( \pr(f)=\pr'(f)\). Nous avons :
    \begin{subequations}
        \begin{align}
            f(e'_j)&=\sum_iA_{ji}f(e_i)\\
            &=\sum_a\sum_b\sum_iA_{ji}f_{ai}(B^{-1})_{ba}e'b\\
            &=\sum_b\Big( \sum_{ai}A_{ji}f_{ai}(B^{-1})_{ba} \Big)e'b,
        \end{align}
    \end{subequations}
    ce qui fait que
    \begin{equation}        \label{EQooUQNBooMWHRbD}
        (\pr'f)e'_j=\sum_b\Big( \pr\big( A_{ji}f_{ai}(B^{-1})_{ba} \big) \Big)e'_b.
    \end{equation}
    Nous calculons maintenant \( (\pr'f)e_j\) en substituant \( e_j=\sum_l(A^{-1})_{lj}e'_l\) et en utilisant \eqref{EQooUQNBooMWHRbD} et la linéarité de \( \pr'\) et la \( \eK\)-linéarité de \( \pr\colon \eL\to \eK\) :
    \begin{subequations}
        \begin{align}
            (\pr'f)\Big( \sum_l(A^{-1})_{lj}e'_l \Big)
            &=\sum_l(A^{-1})_{lj}\sum_b\sum_{ai}\pr\big(A_{li}f_{ai}(B^{-1})_{ba}\big)e_b\\
            &=\sum_a\pr(f_{aj})e_a\\
            &=(\pr f)e_j.
        \end{align}
    \end{subequations}
    Donc \( \pr=\pr'\).
\end{proof}

Note au passage comme toujours : il y a un abus systématique de notation entre \( e_i\in E\) et \( \tau(e_i)=1\otimes e_i\in E_{\eL}\).

\begin{remark}[\cite{MonCerveau}]       \label{REMooBEXGooLgpHzg}
    L'opération \( \pr\colon \aL(E_{\eL},F_{\eL})\to \aL(E,F)\) ne dépend pas des bases choisies un peu partout. Mais elle dépend de l'application \( pr\colon \eL\to \eK\) déjà construite. Et celle-là dépend du choix d'un supplémentaire $V$ qui fournit \( \eL=\eK\oplus V\).

    Si \( \pr(\lambda)=0\) pour un de ces choix, cela n'implique nullement que \( \lambda=0\). Penser à \( i\in \eC\) si la projection \( \pr\colon \eC\to \eR\) est l'application \( (x+iy)\mapsto x\) parallèle à l'axe des imaginaires.

    Par contre si \( \pr(\lambda)=0\) pour tout choix de \( V\), alors nous avons bien \( \lambda=0\). Dans la suit nous «fixons» un choix de \( V\) générique, et lorsque nous rencontrerons l'égalité \( \pr(\lambda)=0\) nous en déduirons \( \lambda=0\).
\end{remark}

\begin{proposition} \label{PROPooPWDKooFNFWRI}
    Si \( f\colon E\to F\) et si \( f_{\eL}e_j=\sum_a(f_{\eL})_{aj}e_a\) et si \( f(e_j)=\sum_af_{aj}e_a\) alors
    \begin{enumerate}
        \item
            \( \pr f_{\eL}=f\),
        \item       \label{ITEMooNMPYooXosGhI}
            \( (f_{\eL})_{ja}=f_{ja} \in \eK\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous avons
    \begin{equation}
        f_{\eL}(e_i)=\sum_a f_{ai}(1\otimes e_a)=\sum_a f_{ai}\tau(e_a),
    \end{equation}
    donc
    \begin{equation}
        (\pr f_{\eL})e_i=\sum_a\pr(f_{ai})e_a=\sum_af_{ai}e_a=f(e_i).
    \end{equation}
    Cela prouve que \( \pr f_{\eL}=f\).

    Par ailleurs, 
    \begin{equation}        \label{EQooIOTFooNAdkit}
        f_{\eL}(\tau e_i)=f_{\eL}(1\otimes e_i)=1\otimes f(e_i)=\tau\big( f(e_i) \big)=\sum_af_{ai}\tau(e_a)
    \end{equation}
    alors que par définition,
    \begin{equation}        \label{EQooMYSCooPFWATG}
        f_{\eL}(\tau e_i)=\sum_a(f_{\eL})_{ai}\tau(e_a).
    \end{equation}
    Les éléments \( \tau(e_a)\) formant une base\footnote{Encore la proposition \ref{PROPooMHARooUycAts}.}, la comparaison de \eqref{EQooIOTFooNAdkit} avec \eqref{EQooMYSCooPFWATG} donne \( (f_{\eL})_{ai}=f_{ai}\in \eK\).
\end{proof}

\begin{lemma}       \label{LEMooWZGSooONEnjZ}
    Soient
    \begin{enumerate}
        \item
            Une base \( \{ e_i \}\) de \( E\) et une application linéaire \( f\colon E\to F\);
        \item
            une base \( \{ e_a \}\) de \( F\) et une application linéaire \( g\colon G\to F\);
        \item
            une base \( \{ e_{\alpha} \} \) de \( G\) et une application linéaire \( \tilde h\colon G_{\eL}\to E_{\eL}\).
    \end{enumerate}
    Alors nous avons
    \begin{equation}
        \pr(f_{\eL}\circ \tilde h)=\pr(f_{\eL})\circ\pr(\tilde h).
    \end{equation}
\end{lemma}

\begin{proof}
    Pour écrire \( \pr(f_{\eL}\circ \tilde h)\) à partir de la définition \eqref{EQooSAFRooJnfkLO} nous commençons par écrire
    \begin{equation}
        (f_{\eL}\circ \tilde h)e_{\alpha}=\sum_a(f_{\eL}\circ \tilde h)_{a\alpha}e_a=\sum_{ai}(f_{\eL})_{ai}(\tilde h)_{i\alpha}e_a=\sum_a\Big( \sum_{i}f_{ai}(\tilde h)_{i\alpha} \Big)e_a
    \end{equation}
    où nous avons utilisé le fait que \( (f_{\eL})_{ai}=f_{ai}\). Donc, en utilisant la \( \eK\)-linéarité de \( \pr\),
    \begin{equation}        \label{EQooZGCGooQsCBQH}
        \pr(f_{\eL}\circ \tilde h)e_{\alpha}=\sum_a\sum_i\pr\Big( f_{ai}(\tilde h)_{i\alpha} \Big)e_a=\sum_a\sum_if_{ai}\pr\Big( (\tilde h)_{i\alpha} \Big)e_a.
    \end{equation}
    D'autre part,
    \begin{equation}
        \begin{aligned}[]
            \pr(f_{\eL})\circ \pr(\tilde h)e_{\alpha}&=\pr(f_{\eL})\sum_i\pr\Big( (\tilde h)_{i\alpha} \Big)e_i\\
            &=\sum_i\pr\Big( (\tilde h)_{i\alpha} \Big)\sum_af_{ai}e_a\\
            &=\sum_{ai}\pr\Big( (\tilde h)_{i\alpha} \Big)f_{ai}e_a,
        \end{aligned}
    \end{equation}
    et c'est égal à \eqref{EQooZGCGooQsCBQH}.
\end{proof}

\begin{remark}
    Nous n'avons en général pas \( \pr(xy)=\pr(x)\pr(y)\) pour tout \( x,y\in \eL\). Par exemple si \( \eK=\eR\) et \( \eL=\eC\) avec la projection canonique,
    \begin{equation}
        \pr(i\cdot i)=\pr(-1)=-1
    \end{equation}
    alors que \( \pr(i)=0\).
\end{remark}

\begin{proposition}
    Soit \( f\in\aL(E,F)\) et \( g\in\aL(F,E)\). Alors il existe \( h\colon G\to E\) tel que \( f\circ h=g\) si et seulement si il existe \( \tilde g\colon G_{\eL}\to E_{\eL}\) tel que \( f_{\eL}\circ \tilde g=g_{\eL}\).
\end{proposition}

\begin{proof}
    Dans le sens direct, il suffit de poser \( \tilde h=h_{\eL}\).

    Dans le sens inverse, si nous avons \( \tilde h\colon G_{\eL}\to E_{\eL}\) tel que \( f_{\eL}\circ\tilde h=g_{\eL}\) alors en appliquant \( \pr\) des deux côtés et en utilisant le lemme \ref{LEMooWZGSooONEnjZ},
    \begin{equation}
        \pr(f_{\eL})\circ\pr(\tilde h)=\pr(g_{\eL})
    \end{equation}
    c'est à dire
    \begin{equation}
        f\circ\pr(\tilde h)=g,
    \end{equation}
    c'est à dire que l'application \( \pr\tilde h\colon G\to E\) est la réponse à la proposition.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Rang, polynôme minimal, polynôme caractéristique}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[Stabilité du rang par extension des scalaires\cite{ooEPEFooQiPESf}]     \label{PROPooJFQDooZSsxMf}
    Si \( f\colon E\to F\) est linéaire alors nous avons
    \begin{equation}
        \rang(f)=\rang(f_{\eL}).
    \end{equation}
    où à droite nous considérons le rang de l'application \( \eL\)-linéaire \( f_{\eL}\colon E_{\eL}\to F_{\eL}\).
\end{proposition}

\begin{proof}
    Il existe un supplémentaire \( V\) tel que \( E=\ker(f)\oplus V\) avec \( \dim(V)=\rang(f)\). Nous pouvons factoriser \( f\) en 
    \begin{equation}
        f=f_2\circ f_1
    \end{equation}
    avec \( f_1\colon E\to V\) est la projection parallèle à \( \ker(f)\) et est surjective (vers \( V\)) parce que \( \dim(V)=\rang(f)=\dim\big( \Image(f) \big)\). De plus \( f_2\colon V\to F\) est injective parce que si \( v\in V\) est tel que \( f_2(v)=0\) alors on aurait
    \begin{equation}
        f(v)=(f_2\circ f_1)(v)=f_2(v)=0.
    \end{equation}
    Cela donne \( v\in\ker(f)\cap V=\{ 0 \}\). Par la proposition \ref{PropooWECLooHPzIHw}, les applications \( (f_1)_{\eL}\) et \( (f_2)_{\eL}\) sont respectivement surjective et injective.

    L'application \( (f_2)_{\eL}\colon V_{\eL}\to F_{\eL}\) est forcément surjective sur son image, donc
    \begin{equation}
        (f_2)_{\eL}\colon V_{\eL}\to \Image(f_{\eL}) 
    \end{equation}
    est un isomorphisme de \( \eL\)-espaces vectoriels. Nous avons alors les égalités
    \begin{equation}        \label{EQooWLOIooKlYWTL}
        \dim_{\eL}(V_{\eL})=\dim_{\eL}\big( \Image(f_{\eL}) \big)=\rang(f_{\eL}).
    \end{equation}
    Mais aussi, par les définitions posées plus haut,
    \begin{equation}        \label{EQooEVCGooAGjmoU}
        \dim(V)=\rang(f)=\dim\big( \Image(f) \big).
    \end{equation}
    Mais le corollaire \ref{CORooTQGHooIKhNtr} nous dit que \( \dim_{\eL}(V_{\eL})=\dim_{\eK}(V)\). Donc il y a égalité des deux lignes \eqref{EQooWLOIooKlYWTL} et \eqref{EQooEVCGooAGjmoU} donne \( \rang(f)=\rang(f_{\eL})\).
\end{proof}

\begin{proposition}     \label{PROPooZAZFooUFdCUv}
    Nous avons
    \begin{enumerate}
        \item
            \( \det(f)=\det(f_{\eL})\)
        \item
            \( \chi_f=\chi_{f_{\eL}}\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    Dès que l'on a des bases nous avons \( (f_{\eL})_{ai}=f_{ai}\) par la proposition \ref{PROPooPWDKooFNFWRI}\ref{ITEMooNMPYooXosGhI}. Le nombre \( \det(f)\in \eK\) est un polynôme en les \( f_{ai}\). Entendons nous : il existe un polynôme indépendant de \( f\) et de \( \eK\) et de \( \eL\) donnant le déterminant de n'importe quelle matrice. Donc \( \det(f)=\det(f_{\eL})\).

    Même chose pour le polynôme caractéristique (définition \ref{DefOWQooXbybYD}) : les coefficient de ce polynôme sont des polynôme en les \( f_{ai}\) qui sont indépendants de \( \eL\), de \( \eK\) et de \( f\).

    Notons que \( \chi_{f_{\eL}}\) est un polynôme à coefficients dans \( \eK\).
\end{proof}

La situation est très différente avec le polynôme minimal\footnote{Définition \ref{DefCVMooFGSAgL}.}. Autant il existe une «recette» pour créer le polynôme caractéristique, il n'en n'existe pas pour le polynôme minimal (ou en tout cas, il ne suffit pas d'appliquer des polynôme en les coefficients de la matrice). La proposition suivante montre que le polynôme minimal est conservé par extension de corps, mais que pour le voir, il faut travailler plus.

\begin{proposition}[\cite{ooEPEFooQiPESf,MonCerveau}]      \label{PROPooXVZMooXcJrsJ}
    Soit \( \eL\) une extension du corps \( \eK\) et une application linéaire \( f\colon E\to F\) entre deux \( \eK\)-espaces vectoriels. Alors \( \mu_f=\mu_{f_{\eL}}\).
\end{proposition}

\begin{proof}
    Nous allons montrer que l'application
    \begin{equation}
        \begin{aligned}
            \tilde g\colon \frac{ \eL[X] }{ (\mu) }&\to \End(E_{\eL}) \\
            \bar P&\mapsto P(f_{\eL}) 
        \end{aligned}
    \end{equation}
    est bien définie et injective. La proposition \ref{PROPooVUJPooMzxzjE} nous dira alors que \( \mu\) est le polynôme minimal de \( f_{\eL}\).

    Pour prouver que l'application \( \tilde g\) est bien définie, nous commençons par prouver que  \( P(f_{\eL})=P(f)_{\eL}\) :
    \begin{subequations}
        \begin{align}
            P(f_{\eL})\lambda\otimes v&=\sum_ka_kf_{\eL}^k\lambda\otimes v\\
            &=\lambda\otimes \sum_ka_kf^k(v)\\
            &=\lambda\otimes P(f)v\\
            &=P(f)_{\eL}\lambda\otimes v.
        \end{align}
    \end{subequations}
    Par conséquent \( \mu(f_{\eL})=0\) et l'application est bien définie.

    Sur \( \eL[X]/(\mu)\) nous considérons la base \( \{ 1,\bar X,\ldots, \bar X^{\deg(\mu)-1} \}\), et \( \End(E_{\eL})\) nous considérons une base qui commence\footnote{Théorème de la base incomplète \ref{ThonmnWKs}\ref{ITEMooFVJXooGzzpOu}.} par \( \{ f_{\eL}^k \}_{k=0,\ldots, \deg(\mu)-1}\). Montrons tout de même que cette partie est libre (sinon le théorème de la base incomplète ne s'applique pas) : si \( \sum_k\lambda_kf_{\eL}^k=0\) alors
    \begin{equation}        \label{EQooSFHVooLxqUEl}
        \sum_k\pr\big( \lambda_k f_{\eL}^k\big)=0.
    \end{equation}
    Pour détailler ce que cela implique, nous calculons ceci :
    \begin{equation}
        (\lambda f_{\eL})(\tau e_i)=\lambda f_{\eL}(\tau e_i)=\sum_a \lambda f_{ia}e_a,
    \end{equation}
    par conséquent \( \pr(\lambda f_{\eL})e_i=\sum_a\pr(\lambda f_{ia})e_a\), et comme \( \pr\) est \( \eK\)-linéaire et que \( f_{ai}\in \eK\),
    \begin{equation}
        \pr(\lambda f_{\eL})e_i=\pr(\lambda)\sum_a f_{ai}e_a=\pr(\lambda)\pr(f_\eL)e_i=\pr(\lambda)f(e_i).
    \end{equation}
    Appliquer la projection \( \pr\) à l'équation \eqref{EQooSFHVooLxqUEl} donne alors \( \sum_k\pr(\lambda)_kf^k=0\). Mais comme les \( f^k\) sont linéairement indépendantes sur \( \eK\) nous avons pour tout \( k\) : \( \pr(\lambda_k)=0\) (égalité dans \( \eK\)). En nous souvenant de la remarque \ref{REMooBEXGooLgpHzg} nous en déduisons \( \lambda_k=0\) dans \( \eL\).

    Dans les choix de bases faits, l'application \( \tilde g\) a la forme
    \begin{equation}
        \tilde g=\begin{pmatrix}
            \begin{matrix}
                1    &       &       \\
                    &   1    &       \\
                    &       &   1
            \end{matrix}\\ 
            \begin{matrix}
                *    &   *    &   *    \\
                *    &   *    &   *    \\
                *    &   *    &   *    
            \end{matrix}
        \end{pmatrix},
    \end{equation}
    qui est injective.

    Vu que \( \tilde g\) est injective, \( \mu\) est le polynôme minimal de \( f_{\eL}\) et donc \( \mu=\mu_{\eL}\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Frobenius et Jordan}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{InternalLinks}
    À propos des invariants de similitude du théorème \ref{THOooDOWUooOzxzxm}.
    \begin{enumerate}
        \item
            Pour prouver que la similitude d'applications linéaires résiste à l'extension du corps de base, théorème \ref{THOooHUFBooReKZWG}.
        \item
            Pour prouver que la dimension du commutant d'un endomorphisme de \( E\) est de dimension au moins \( \dim(E)\), lemme \ref{LEMooDFFDooJTQkRu}.
    \end{enumerate}
\end{InternalLinks}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice compagnon}
%---------------------------------------------------------------------------------------------------------------------------

\begin{InternalLinks}
    Nous verrons dans la remarque \ref{REMooPVLEooYDRXQI} à propos des invariants de similitude que toute matrice est semblable à la matrice bloc-diagonale constituées des matrices compagnon de la suite des polynômes minimals.
\end{InternalLinks}

\begin{definition}      \label{DEFooOSVAooGevsda}
    Soit le polynôme \( P=X^n-a_{n-1}X^{n-1}-\ldots-a_1X-a_0\) dans \( \eK[X]\). La \defe{matrice compagnon}{matrice!compagnon} de \( P\) est la matrice\nomenclature[A]{\( C(P)\)}{matrice compagnon} donnée par
    \begin{equation}
        C(P)=\begin{pmatrix}
            0    &   \cdots    &   \cdots    &   0    &   a_0\\  
            1    &   0    &       &   \vdots    &   a_1\\  
            0    &   \ddots    &   \ddots    &   \vdots    &   \vdots\\  
            \vdots    &   \ddots    &   \ddots    &   0    &   a_{n-2}\\  
            0    &   \cdots    &   0    &   1    &   a_{n-1}    
        \end{pmatrix}
    \end{equation}
    si \( n\geq 2\) et par \( (a_0)\) si \( n=1\). 

    Une matrice est dite compagnon si elle a cette forme.
\end{definition}

\begin{proposition}
    Si \( f\) est l'endomorphisme associé à la matrice \( C(P)\) nous avons
    \begin{equation}
        f(e_i)=\begin{cases}
            e_{i+1}    &   \text{si \( i<n\)}\\
            (a_0,\ldots, a_{n-1})    &    \text{si \( i=n\)}.
        \end{cases}
    \end{equation}
    De plus l'endomorphisme \( f\) vérifie \( P(f)e_1=0\).
\end{proposition}

\begin{lemma}[\cite{RapportArgreg2011}] \label{LemkVNisk}
    Un polynôme sur un corps commutatif est le polynôme caractéristique de sa matrice compagnon. En d'autres termes nous avons \( \chi_{C(P)}=P\).
\end{lemma}

\begin{proof}
    Nous notons \( f\) l'endomorphisme associé à \( C(P)\). La propriété \( P(f)e_1=0\) nous indique que le polynôme minimal ponctuel de \( f\) en \( e_1\) divise \( P\). L'ensemble des puissances de \( f\) appliquées à \( e_1\), \( \big( f^i(e_1) \big)_{i=1,\ldots, n-1}\) est libre, donc le polynôme minimal ponctuel en \( e_1\) est de degré \( n\) au minimum. En reprenant les notations du théorème \ref{ThoCCHkoU}, nous avons \( I_{e_1}=(P)\) parce que \( P\) est de degré minimum dans \( I_{e_1}\) et \( \chi_f\in I_{e_1}\).

    Donc \( P\) divise \( \chi_f\) et est de degré égal à celui de \( \chi_f\). Étant donné qu'ils sont tous deux unitaires, ils sont égaux.
\end{proof}

\begin{remark}  \label{RemmQjZOA}
    Les matrices compagnons ne sont pas les seules dont le polynôme caractéristique est égal au polynôme minimal. En fait les matrices dont le polynôme caractéristique est égale au polynôme minimal sont denses dans les matrices. En effet une matrice dont le polynôme minimal n'est pas égal au polynôme caractéristique a un polynôme caractéristique avec une racine double. Il est possible, en modifiant arbitrairement peu la matrice de séparer la racine double en deux racines distinctes.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Réduction de Frobenius}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LEMooKUQDooKFeIYq}
    Soit un endomorphisme \( f\colon E\to E\) sur l'espace vectoriel de dimension finie \( n\). Nous notons \( \mu\) et \( \chi\) les polynômes minimal et caractéristique. Si \( f\) est cyclique, alors \( \mu=\chi\).
\end{lemma}
Le théorème \ref{THOooGLMSooYewNxW} donnera une version plus complète de ce lemme.

\begin{proof}
    Soit \( v\) un vecteur cyclique de \( f\), c'est à dire que \( \{ f^k(v) \}_{k=0,\ldots, n-1}\) est libre. Donc si \( P\) est un polynôme de degré jusqu'à \( n-1\) nous ne pouvons pas avoir \( P(f)=0\) parce que, appliqué à \( v\), ce serait une combinaisons nulle non triviale des \( f^k(v)\). Donc le polynôme minimal est au minimum de degré \( n\). Mais le polynôme caractéristique est annulateur de degré \( n\) (Cayley-Hamilton \ref{ThoCalYWLbJQ}), donc il est le polynôme minimal.
\end{proof}

\begin{theorem}[Réduction de Frobenius \cite{AutourFrobCompa,Vialivs,MoncetIVS}]        \label{THOooDOWUooOzxzxm}
    Soit \( E\), un \( \eK\)-espace vectoriel, et \( f\in \End(E)\). Alors il existe une suite de sous-espaces \( E_1,\ldots, E_r\) stables par \( f\) tels que
    \begin{enumerate}
        \item   \label{ItemmpwjnSs}
            \( E=\bigoplus_{i=1}^rE_i\);
        \item
            pour chaque \( E_i\), l'endomorphisme restreint \( f_i=f|_{E_i}\) est cyclique;
        \item
            si \( \mu_i\) est le polynôme minimal de \( f_i\) alors \( \mu_{i+1}\) divise \( \mu_i\);
    \end{enumerate}
    Une telle décomposition vérifie automatiquement \( \mu_1=\mu_f\) et \( \mu_1\cdots \mu_r=\chi_f\), et la suite \( (\mu_i)_{i=1,\ldots, r}\) ne dépend que de \( f\) et non du choix de la décomposition du point \ref{ItemmpwjnSs}.
\end{theorem}
   \index{réduction!Frobénius}
   \index{Frobénius!réduction}

Les polynômes \( \mu_i\) sont les \defe{invariants de similitude}{invariant!de similitude} de l'endomorphisme \( f\).

\begin{proof}
    Nous commençons par montrer que si une telle décomposition existe, alors
    \begin{subequations}    \label{subEqzcGouz}
        \begin{align}
            \chi_f=\prod_{i=1}^r\mu_i  \label{EqTaxsvb}\\
            \mu_f=\mu_1
        \end{align}
    \end{subequations}
    où \( \chi_f\) est le polynôme caractéristique de \( f\) et \( \mu_f\) est le polynôme minimal. D'abord le polynôme caractéristique de \( f\) devra être égal au produit des polynômes caractéristique des \( f|_{E_i}\), mais ces derniers endomorphismes étant cycliques\footnote{Définition \ref{DEFooFEIFooNSGhQE}.}, leurs polynôme caractéristiques sont égaux à leurs polynômes minimaux (lemme \ref{LEMooKUQDooKFeIYq}). Cela prouve l'égalité \eqref{EqTaxsvb}. Ensuite tous les \( \mu_i\) doivent diviser le polynôme minimal, donc \( \ppcm(\mu_1,\ldots, \mu_r)\) divise \(\mu_f\). Cependant le polynôme minimal doit contenir une et une seule fois chacun des facteurs irréductibles du polynôme caractéristique, et chacun de ces facteurs sont dans les polynômes \( \mu_i\). Par conséquent \( \ppcm(\mu_1,\ldots, \mu_r)=\mu_f\). Mais par ailleurs \( \mu_1=\ppcm(\mu_1,\ldots, \mu_r)\) parce qu'on a supposé \( \mu_{i+1}\divides \mu_i\), donc \( \mu_1=\mu_f\).
    
    Soit \( d\), le degré du polynôme minimal de \( f\) et \( y\in E\) tel que \( \mu_f=\mu_{f,y}\) (voir lemme \ref{LemSYsJJj}). Le plus petit espace stable sous \( f\) contenant \( y\) est
    \begin{equation}
        E_y=\Span\{ y,f(y),\ldots, f^{d-1}(y) \}.
    \end{equation}
    Nous notons \( e_i=f^{i-1}(y)\). Notons que les vecteurs donnés forment bien une base de \( E_y\) parce que si les \( e_i\) n'était pas linéairement indépendants, alors nous aurions des \( a_k\) tels que \( \sum_ka_ke_k=0\) et avec lesquels
    \begin{equation}
        \big( \sum_ka_kX^k \big)(f)y=0,
    \end{equation}
    ce qui contredirait la minimalité de \( \mu_{f,y}\).

    La difficulté du théorème est de trouver un complément de \( E_y\) qui soit également stable sous \( f\). Nous commençons par étendre\quext{Pour autant que j'aie compris, cette extension manque dans \cite{AutourFrobCompa}. Corrigez moi si je me trompe.} \( \{ e_1,\ldots, e_d \}\) en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Ensuite nous allons montrer que
    \begin{equation}
        E=E_y\oplus F
    \end{equation}
    avec
    \begin{equation}
        F=\{ x\in E\tq  e^*_d\big( f^k(x) \big)=0\forall k\in \eN \}.
    \end{equation}
    Par construction, \( F\) est invariant sous \( f\). Montrons pour commencer que \( E_y\cap F=\{ 0 \}\). Un élément de \( E_y\) s'écrit
    \begin{equation}
        z=a_1e_1+\ldots +a_ke_k
    \end{equation}
    avec \( k\leq d\). Étant donné que \( f\) décale les vecteurs de base, nous avons \( e^*_d\big( f^{d-k}(z) \big)=a_k\). Du coup \( z\in F\) si et seulement si \( a_1=\ldots=a_d=0\), c'est à dire que \( E_y\cap F=\{ 0 \}\).

    Nous montrons maintenant que \( \dim F=n-d\). Pour cela nous considérons l'application
    \begin{equation}
        \begin{aligned}
            T\colon \eK[F]&\to E^* \\
            g&\mapsto e^*_d\circ g. 
        \end{aligned}
    \end{equation}
    Cette application est injective. En effet un élément général de \( \eK[f]\) est
    \begin{equation}
        g=a_1\id+a_2f+\ldots +a_pf^{p-1}
    \end{equation}
    avec \( p\leq d\). Si \( T(g)=0\), alors nous avons en particulier
    \begin{equation}
        0=T(g)e_{_d-p+1}=e^*_d(a_1e_{d-p+1}+a_2e_{d-p+2}+\ldots +a_pe_d)=a_p.
    \end{equation}
    Donc \( a_p=0\) et en appliquant maintenant \( T(g)\) à \( e_{d-p}\) nous obtenons \( a_{p-1}=0\). Au final nous trouvons que \( g=0\) et donc que \( T\) est injective.

    Étant donné que \( \dim\eK[f]=d\) et que \( T\) est injective, \( \dim\Image(T)=d\). Nous regardons l'orthogonal de l'image :
    \begin{subequations}
        \begin{align}
            (\Image(T))^{\perp}&=\{ x\in E\tq T(g)x=0\forall g\in\eK[f] \}\\
            &=\{ x\in E\tq e^*_d\big( g(x) \big)=0\forall g\in \eK[f] \}\\
            &=F.
        \end{align}
    \end{subequations}
    Par conséquent \( F^{\perp}=\Image(T)\). Vu que \( \dim\Image(T)=d\), nous avons donc \( \dim F=n-d\) et il est établi que \( E=E_y\oplus F\). 

    Nous avons donc trouvé \( F\), stable par \( f\) et tel que \( E=E_y\oplus F\). Nous devons maintenant nous assurer que cette décomposition tombe bien pour les polynômes minimaux. Si \( P_1\) est le polynôme minimal de \( f|_{E_yj}\), alors par le lemme \ref{LemAGZNNa} nous avons \( P_1=\mu_{f,y}=\mu_f\) parce que \( f|_{E_y}\) est cyclique sur \( E_y\). Mettons \( P_2\), le polynôme minimal de \( f|_F\). Étant attendu que \( F\) est stable par \( f\), le polynôme \( P_2\) divise \( P_1\). En recommençant la construction sur \( F\), nous construisons un nouvel espace \( F'\) stable sous \( F\) et vérifiant \( \mu_{f|_{F'}}=P_2\), etc.

    Nous passons maintenant à la partie unicité du théorème. Soient deux suites \( F_1,\ldots, F_r\) et \( G_1,\ldots, G_s\) de sous-espaces stables par \( f\) et vérifiant
    \begin{enumerate}
        \item
            \( E=\bigoplus_{i=1}^rF_i\),
        \item
            \( f|_{F_i}\) est cyclique,
        \item
            \( \mu_{f|_{F_{i+1}}}\) divise \( \mu_{f|_{F_i}}\),
    \end{enumerate}
    et, \emph{mutatis mutandis}, les mêmes conditions pour la famille \( \{ G_i \}\). Nous posons \( P_i=\mu_{f_{F_i}}\) et \( Q_i=\mu_{f|_{G_i}}\). Nous allons montrer par récurrence que \( P_i=Q_i\) et \( \dim F_i=\dim G_i\). Il ne sera cependant pas garanti que \( F_i=G_i\). D'abord, \( P_1=Q_1\) parce qu'ils sont tous deux égaux à \( \mu_f\) par les relations \eqref{subEqzcGouz}. Nous supposons que \( P_i=Q_i\) pour \( i\leq 1\leq j-1\) et nous tentons de montrer que \( P_j=Q_j\).

    Nous avons 
    \begin{equation}    \label{EqMrCtZO}
        P_j(f)=P_j(f)|_{F_1}\oplus\ldots\oplus P_j(f)|_{F_{j-1}}.
    \end{equation}
    En effet étant donné que \( P_{j+k}\) divise \( P_j\), nous avons\footnote{En vertu du lemme \ref{LemQWvhYb}.} \( P_{j}(f)=A(f)\circ P_{j+k}(f)\), mais \( P_{j+k}(f)F_{j+k}=0\), donc \( P_j(f)F_{j+k}=0\). Les espaces \( G_i\) n'ayant a priori aucun rapport avec les polynômes \( P_i\), nous écrivons
    \begin{equation}    \label{EqJreLiO}
        P_j(f)=P_j(f)|_{G_1}\oplus\ldots\oplus P_j(f)|_{G_{j-1}}\oplus P_j(f)|_{G_j}\oplus\ldots\oplus P_j(f)|_{G_s}.
    \end{equation}
    Pour \( 1\leq i\leq j-1\), nous avons supposé \( P_i=Q_i\). Étant donné que \( f|_{F_i}\) est semblable à \( C_{_i}\) et \( f|_{G_i}\) est semblable à \( C_{Q_i}\), la matrice de \( f|_{E_i}\) est semblable à la matrice de \( f|_{G_i}\). En particulier,
    \begin{equation}
        \dim P_j(f)F_i=\dim P_j(f)G_i.
    \end{equation}
    En prenant les dimensions des images dans les égalités \eqref{EqMrCtZO} et \eqref{EqJreLiO}, nous trouvons que
    \begin{equation}
        P_j(f)|_{G_j}=\ldots=P_j(f)|_{G_s}=0.
    \end{equation}
    Par conséquent \( P_j\in I_{f|G_j}\) et donc \( P_j\) divise \( Q_j\), qui est générateur de \( I_{f|_{G_j}}\). La situation étant symétrique entre \( P\) et \( Q\), nous montrons de même que \( Q_j\) divise \( P_j\) et donc que \( P_j=Q_j\).

    Ceci achève la démonstration du théorème de réduction de Frobenius.

\end{proof}

\begin{remark}      \label{REMooPVLEooYDRXQI}
    Sous forme matricielle, ce théorème dit que toute matrice est semblable à une matrice de la forme bloc-diagonale
    \begin{equation}
        f=\begin{pmatrix}
            C_{\mu_1}    &       &       \\
                &   \ddots    &       \\
                &       &   C_{\mu_r}
        \end{pmatrix}
    \end{equation}
    où les \( C_{\mu_i}\) sont les matrices compagnon (définition \ref{DEFooOSVAooGevsda}). 

    En particulier, et ceci est très important, deux applications sont semblables si et seulement si elles ont même suite d'invariants de similitude.
\end{remark}


\begin{remark}
    Si nous travaillons sur \( \eR\), la réduite de Frobenius restera une matrice réelle, même si les valeurs propres sont complexes. En effet le procédé de Frobenius ne regarde absolument pas les valeurs propres, mais seulement les facteurs irréductibles du polynôme caractéristique. La réduite de Frobenius ne tente pas de résoudre ces polynômes, mais se contente d'en utiliser les matrices compagnon.

    La situation sera différente dans le cas de la forme normale de Jordan.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Forme normale de Jordan}
%---------------------------------------------------------------------------------------------------------------------------

Il existe une preuve directe de la réduction de Jordan ne nécessitant pas la réduction de Frobenius\cite{LecLinAlgAllen}. Cette dernière passe par les espaces caractéristiques\footnote{Aussi appelés «espaces propres généralisés».} et est à mon avis plus compliquée que la démonstration de Frobenius elle-même. Nous allons donc nous contenter de donner la réduction de Jordan comme un cas particulier de Frobenius.

\begin{theorem}[Réduction de Jordan]        \label{ThoGGMYooPzMVpe}
    Soit \( E\) un espace vectoriel sur \( \eK\), et \( f\in\End(E)\) un endomorphisme dont le polynôme caractéristique \( \chi_f\) est scindé\footnote{C'est pour cette hypothèse que \( \eK=\eR\) n'est pas le bon cadre.}. Il existe une base de \( E\) dans laquelle la matrice de \( f\) s'écrit sous la forme
    \begin{equation}
        M=\begin{pmatrix}
            J_{n_1}(\lambda_1)    &       &       \\
                &   \ddots    &       \\
                &       &   J_{n_k}(\lambda_k)
        \end{pmatrix}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( f\) (avec éventuelle répétitions) et \( J_n(\lambda)\) représente le bloc \( n\times n\)
    \begin{equation}
        J_n(\lambda)=\begin{pmatrix}
            \lambda    &   1    &       &       &   \\  
                &   \lambda    &   1    &       &   \\  
                &       &   \lambda    &       &   \\  
                &       &       &   \ddots    &   1\\  
                &       &       &       &   \lambda    
        \end{pmatrix}.
    \end{equation}
    En d'autres termes, \( J_n(\lambda)_{ii}=\lambda\) et \( J_n(\lambda)_{i-1,i}=1\).    
\end{theorem}
\index{réduction!Jordan}
\index{Jordan!réduction}

\begin{proof}
    Nous commençons par le cas où \( f\) est nilpotente; nous notons \( M\) sa matrice. Dans ce cas la seule valeur propre est zéro et le polynôme caractéristique est \( X^m\) pour un certain \( m\). Nous savons par le lemme \ref{LemkVNisk} que (la matrice de) \( f\) est semblable à sa matrice compagnon. En l'occurrence pour \( f\) nous avons
    \begin{equation}
        C_{X^m}=\begin{pmatrix}
             0   &       &       &  0     \\
             1   &   \ddots    &       &   \vdots    \\
                &   \ddots    &   \ddots    &    \vdots   \\ 
                &       &   1    &   0     
         \end{pmatrix}.
    \end{equation}
    Ensuite le changement de base (qui est une similitude) \( (e_1,\ldots, e_n)\mapsto(e_n,\ldots, e_1)\) montre que \( C_{X^m}\) est semblable à un bloc de Jordan \( J_m(0)\).

    Supposons à présent que \( f\) ne soit pas nilpotente. Par l'hypothèse de polynôme caractéristique scindé, nous supposons que \( f\) a \( m\) valeurs propres distinctes et que son polynôme caractéristique est
    \begin{equation}
        \chi_f=(X-\lambda_1)^{l_1}\ldots (X-\lambda_m)^{l_m}.
    \end{equation}
    Le lemme des noyaux (théorème \ref{ThoDecompNoyayzzMWod}) nous enseigne que
    \begin{equation}
        E=\bigoplus_{i=1}^m\underbrace{\ker(f-\mu_i\mtu)^{l_i}}_{F_i}.
    \end{equation}
    La restriction de \( f-\lambda_i\mtu\) à \( F_i\) est par construction un endomorphisme nilpotent, et donc peut s'écrire comme un bloc de Jordan avec des zéros sur la diagonale. En utilisant la décomposition
    \begin{equation}
        f|_{F_i}=(f-\lambda_i\mtu)|_{F_i}+\lambda_i\mtu_{F_i},
    \end{equation}
    nous voyons que \( f|_{F_i}\) s'écrit comme un bloc de Jordan avec \( \lambda_i\) sur la diagonale.
\end{proof}

\begin{remark}
    Nous pouvons calculer la forme normale de Jordan pour une matrice complexe ou réelle, mais dans les deux cas nous devons nous attendre à obtenir une matrice complexe parce que les valeurs propres d'une matrice réelle peuvent être complexes. Cependant nous demandons que le polynôme caractéristique de \( f\) soit scindé sur \( \eK\). En pratique, la décomposition de Jordan n'est garantie que sur les corps algébriquement clos, c'est à dire sur \( \eC\).

    La suite des invariants de similitude sur laquelle repose Frobenius, elle, est disponible sur tout corps, y compris \( \eR\).
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Commutant et endomorphismes cycliques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Endomorphisme cyclique}
%---------------------------------------------------------------------------------------------------------------------------

\begin{InternalLinks}
    La notion d'endomorphisme cyclique arrive par-ci par là.
    \begin{enumerate}
        \item
            La définition est la définition \ref{DEFooFEIFooNSGhQE}.
        \item
            Son lien avec le commutant donné dans la proposition \ref{PropooQALUooTluDif} et le théorème \ref{THOooGLMSooYewNxW}.
        \item
            Utilisation dans le théorème de Frobenius (invariants de similitude), théorème \ref{THOooDOWUooOzxzxm}.
    \end{enumerate}
\end{InternalLinks}

\begin{lemma}\label{LemSGmdnE}
    Si \( A\) est la matrice de l'endomorphisme \( f\) alors nous avons équivalence des propriétés suivantes :
    \begin{enumerate}
        \item
            La matrice \( A\) est cyclique.
        \item
            L'endomorphisme \( f\) est cyclique.
    \end{enumerate}
\end{lemma}

Si \( f\) est un endomorphisme de l'espace vectoriel \( E\) et si \( x\in E\), nous notons 
\begin{equation}
    E_{f,x}=\Span\{ f^k(x)\tq k\in \eN \}.
\end{equation}

\begin{definition}
    Soit \( E\) un espace vectoriel de dimension finie sur un corps \( \eK\) et un endomorphisme \( f\colon E\to E\). Le \defe{commutant}{commutant} de \( f\) est l'ensemble des endomorphismes de \( E\) qui commutent avec \( f\) :
    \begin{equation}
        \comC(f)=\{ g\in\aL(E,E)\tq g\circ f=f\circ g \}.
    \end{equation}
\end{definition}
Il n'est pas très compliqué de vérifier que \( \comC(f)\) est un sous-espace vectoriel de \( \aL(E,E)\).

Notons l'inclusion évidente \( \eK[f]\subset \comC(f)\). L'inclusion inverse va un peu nous occuper durant les prochaines pages.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Commutant : cas diagonalisable}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}[\cite{ooKPTNooMmncYA}]      \label{PROPooRHHEooIRGmtl}
    Si \( f\) est diagonalisable, alors
    \begin{equation}        \label{EQooOTFLooPUKAos}
        \dim\big( \comC(f) \big)=\sum_{\lambda\in\Spec(f)}\dim(E_{\lambda})^2.
    \end{equation}
    où les \( E_{\lambda} \) sont les espaces propres de \( f\).
\end{proposition}

\begin{proof}
    D'abord su \( g\in\comC(f)\) alors \( E_{\lambda}\) est stable par \( g\). En effet si \( v\in E_{\lambda}\) alors \( f\big( g(v) \big)=g\big( f(v) \big)=g(\lambda v)=\lambda g(v)\), ce qui montre que \( g(v)\) est un vecteur propre de \( f\) pour la valeur propre \( \lambda\), et donc que \( g(v)\in E_{\lambda}\).

    Nous considérons ensuite l'application 
    \begin{equation}
        \begin{aligned}
            \psi\colon \comC(f)&\to \End(E_1)\times \ldots\times \End(E_r) \\
            g&\mapsto  g|_{E_1}\times \ldots\times g|_{E_r}
        \end{aligned}
    \end{equation}
    qui est bien définie parce que \( g\) se restreint aux espaces propres de \( f\). Nous allons noter \( \psi(g)_{\lambda}\) la restriction de \( g\) à \( E_{\lambda}\).
    \begin{subproof}
    \item[\( \psi\) est injective]

        Supposons que \( g,h\in\comC(f)\) tels que \( \psi(g)=\psi(h)\). Vu que \( f\) est diagonalisable nous pouvons décomposer \( x\in  E\) en ses composantes sur les espaces propres\footnote{Théorème \ref{ThoDigLEQEXR}\ref{ITEMooZNJFooEiqDYp}.} :
        \begin{equation}
            x=\sum_{\lambda\in\Spec(f)}x_{\lambda}
        \end{equation}
        avec \( x_{\lambda}\in E_{\lambda}\).  Nous avons alors
        \begin{equation}
            g(x)=\sum_{\lambda}g(x_{\lambda})=\sum_{\lambda}\psi(g)_{\lambda}(x_{\lambda}).
        \end{equation}
        Vu que nous avons \( \psi(g)_{\lambda}=\psi(h)_{\lambda}\), nous avons aussi
        \begin{equation}
            g(x)=\sum_{\lambda}\psi(g)_{\lambda}(x_{\lambda})=\sum_{\lambda}\psi(h)_{\lambda}(x_{\lambda})=\sum_{\lambda}h(x_{\lambda})=h(x).
        \end{equation}
        Cela prouve \( g=h\) et donc que \( \psi\) est injective.
    \item[\( \psi\) est surjective]
        Si nous avons pour chaque \( \lambda\in\Spec(f)\) un endomorphisme \( g_{\lambda}\) de \( E_{\lambda}\) alors en posant
        \begin{equation}
            g(x)=\sum_{\lambda\in\Spec(f)}g_{\lambda}(x_{\lambda})
        \end{equation}
        alors nous avons bien
        \begin{equation}
            \psi(g)=\big( g_{\lambda_1},\ldots, g_{\lambda_r} \big).
        \end{equation}
    \end{subproof}
    Nous pouvons donc conclure en écrivant
    \begin{equation}
        \dim\big( \comC(f) \big)=\sum_{\lambda\in\Spec(f)}\dim\big( \End(E_{\lambda}) \big)= \sum_{\lambda\in\Spec(f)}\dim(E_{\lambda})^2.
    \end{equation}
\end{proof}

\begin{remark}      \label{REMooUGFQooVzCOvV}
    Nous avons alors immédiatement
    \begin{equation}
        \dim\big( \comC(f) \big)\geq\dim(E)
    \end{equation}
    lorsque \( f\) est diagonalisable.
\end{remark}

En suivant la notation \eqref{EqooOAYDooEpZELo}, un endomorphisme est cyclique lorsqu'il existe \( x\in E\) tel que \( E_x=E\). 

\begin{proposition}[\cite{ooKPTNooMmncYA}]      \label{PropooQALUooTluDif}
    Si \( f\) est un endomorphisme diagonalisable d'un espace vectoriel \( E\) de dimension \( n\). Nous avons équivalence entre les faits suivants.
    \begin{enumerate}
        \item\label{ITEMooSOYYooZVibjrii}
            Le polynôme minimal est égal au polynôme caractéristique : \( \mu_f=\chi_f\)
        \item\label{ITEMooSOYYooZVibjrvi}
            L'endomorphisme \( f\) est cyclique.
        \item\label{ITEMooSOYYooZVibjrv}
            \( \comC(f)=\eK[f]\).
        \item\label{ITEMooSOYYooZVibjriv}
            \( \dim\big( \comC(f) \big)=n\)
        \item\label{ITEMooSOYYooZVibjriii}
            L'endomorphisme \( f\) possède \( n\) valeurs propres distinctes.
        \item   \label{ITEMooSOYYooZVibjri}
            \( \dim\big( \eK[f] \big)=n\)
    \end{enumerate}
\end{proposition}

\begin{proof}
    Le point important de cette proposition sont les équivalences \ref{ITEMooSOYYooZVibjrii}-\ref{ITEMooSOYYooZVibjrv}. Les autres sont des intermédiaires. En particulier, dans le cas diagonalisable, nous allons voir que le point \ref{ITEMooSOYYooZVibjriii} est essentiellement une reformulation de \ref{ITEMooSOYYooZVibjrii}.
    \begin{subproof}
        \item[\ref{ITEMooSOYYooZVibjriv} implique \ref{ITEMooSOYYooZVibjriii}]
            Par la formule \eqref{EQooOTFLooPUKAos}, les espaces propres de \( f\) ont dimension \( 1\). Par conséquent \( f\) possède \( n\) valeurs propres distinctes.
        \item[\ref{ITEMooSOYYooZVibjriii} implique \ref{ITEMooSOYYooZVibjri}]
            Le théorème \ref{ThoDigLEQEXR} nous dit que le polynôme minimal est scindé à racines simples. Vu que \( f\) possède \( n\) valeurs propres distinctes, \( \mu\) est de degré \( n\).  Par l'isomorphisme \( \eK[f]=\eK[X]/(\mu)\) de la proposition \ref{PropooCFZDooROVlaA} nous avons \(\dim\big( \eK[f] \big)= \deg(\mu)=n\) par la proposition \ref{CorsLGiEN}.
        \item[\ref{ITEMooSOYYooZVibjri} implique \ref{ITEMooSOYYooZVibjrii}]
            Par l'isomorphisme \( \eK[f]=\eK[X]/(\mu)\) de la proposition \ref{PropooCFZDooROVlaA} et la proposition \ref{CorsLGiEN} nous avons \(n=\dim\big( \eK[f] \big)= \deg(\mu)\). Vu que \( \chi\) est un polynôme annulateur (Caley-Hamilton \ref{ThoCalYWLbJQ}), il est divisé par \( \mu\). Maintenant \( \mu\) et \( \chi\) sont des polynômes unitaires de degré \( n\) et \( \mu\) divise \( \chi\). Ils sont donc égaux.
        \item[\ref{ITEMooSOYYooZVibjrii} implique \ref{ITEMooSOYYooZVibjrvi}]
            Le fait que $f$ soit diagonalisable permet d'utiliser le théorème \ref{ThoDigLEQEXR} pour dire que \( \mu\) est scindé à racines simples. L'égalisation avec \( \chi \) nous permet de dire que \( f\) possède \( n\) valeurs propres distinctes. Soient \( \{ e_1,\ldots, e_n \}\) une base de diagonalisation, et prouvons que le vecteur \( v=e_1+\ldots +e_n\) est cyclique. Nous avons
            \begin{equation}
                f^k(v)=\sum_{i=1}^n\lambda_i^ke_i.
            \end{equation}
            Pour prouver que cette famille (avec \( k=0,\ldots, n-1\)) est libre\footnote{Ce sera alors une base parce que \( n\) vecteurs libres dans un espace de dimension \( n\) est toujours une base, théorème \ref{ThoMGQZooIgrXjy}\ref{ItemHIVAooPnTlsBi}.} nous en prenons une combinaison linéaire nulle et nous prouvons que les coefficients sont tous nuls. Soit donc
            \begin{equation}
                    0=\sum_{l=0}^{n-1}a_lf^l(v)=\sum_{l=0}^{n-1}a_l\sum_{i=1}^n\lambda_i^le_i=\sum_{i=1}^n\Big( \sum_{l=0}^{n-1}a_l\lambda_i^l \Big)e_i.
            \end{equation}
            Vu que cela est nul, nous avons pour tout \( i\) :
            \begin{equation}
                \sum_{l=0}^{n-1}a_l\lambda_i^l=0.
            \end{equation}
            En posant la matrice \( A_{ij}=\lambda_i^j\), cela revient à étudier le système \( \sum_j A_{ij}a_j=0\). Ce système n'a des solutions non nulles que si \( \det(A)= 0\); sinon il possède une unique solution et elle est \( a_j=0\) pour tout \( j\). Nous devons donc calculer le déterminant
            \begin{equation}
                \det\begin{pmatrix}
                    1&\lambda_1&\lambda_1^2&\cdots&\lambda_1^{n-1}\\  
                    \vdots&\vdots&\vdots&&\vdots\\  
                    1&\lambda_n&\lambda_n^2&\cdots&\lambda_n^{n-1}  
                \end{pmatrix}.
            \end{equation}
            Il s'agit du déterminant de Vandermonde déjà étudié par la proposition \ref{PropnuUvtj}. Nous avons \( \det(A)=\prod_{1\leq i<j\leq n}(\lambda_j-\lambda_i)\). Cela est bien non nul du fait que toutes les valeurs propres soient distinctes.
        \item[\ref{ITEMooSOYYooZVibjrvi} implique \ref{ITEMooSOYYooZVibjrv}]
            Soit \( v\) un vecteur cyclique de \( f\). Un endomorphisme \( g\) donne lieu à un polynôme par le fait suivant : il existe des uniques \( a_k\) (\( k=0,\ldots, n-1\)) tels que
            \begin{equation}
                g(v)=\sum_{k=0}^{n-1}a_kf^k(v).
            \end{equation}
            Cela donne une application linéaire
            \begin{equation}
                \begin{aligned}
                    \psi\colon \comC(f)&\to \eK[f] \\
                    g&\mapsto P\tq P(f)v=g(v). 
                \end{aligned}
            \end{equation}
            C'est une application injective parce que si \( \psi(g)=0\) alors \( g(v)=0\) et pour tout \( k\) nous avons \( g\big( f^k(v) \big)=f^k\big( g(v) \big)=0\). L'endomorphisme \( g\) s'annulant sur une base, est nul.
        \item[\ref{ITEMooSOYYooZVibjrv} implique \ref{ITEMooSOYYooZVibjriv}]
            Si \( n_1,\ldots, n_r\) sont les dimensions des différents espaces propres de \( f\), nous avons les inégalités
            \begin{equation}
                \dim\big( \eK[f] \big)=\deg(\mu)\leq n=n_1+\ldots +n_r\leq n_1^2+\ldots +n_r^2=\dim\big( \comC(f) \big).
            \end{equation}
            Par hypothèse d'égalité entre le premier et le dernier terme de cette suite d'inégalités, toutes les inégalités sont des égalités et en particulier \( \dim\big( \comC(f) \big)=n\).
        \end{subproof}
            Nous avons fini de prouver toutes les équivalences demandées.
\end{proof}

\begin{example}
    Pour mieux comprendre pourquoi le fait d'avoir \( n\) valeurs propres distinctes est équivalent à être cyclique, notons que si deux valeurs propres sont identiques, alors un morceau de la matrice de \( f\) serait par exemple \( \begin{pmatrix}
          2  &   0    \\ 
        0    &   2    
    \end{pmatrix}\), et dans ce cas n'importe quelle combinaison \( ae_i+be_j\) reste proportionnelle à elle-même après application de \( f\). Si nous avons des valeurs propres différentes par contre, nous avons par exemples dans \( \eR^2\) la matrice \( \begin{pmatrix}
        1    &   0    \\ 
        0    &   2    
    \end{pmatrix}\) qui donne \( f(e_1+e_2)=e_1+2e_2\). La partie \( \{ e_1+e_2,e_1+2e_2 \}\) est une base.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Commutant : cas général}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons encore un espace vectoriel \( E\) de dimension finie \( n\) et un endomorphisme \( f\colon E\to E\). Nous notons \( \mu\) sont polynôme minimal et \( \mu_x\) le polynôme minimal ponctuel en \( x\).

\begin{lemma}[\cite{ooEFHLooXpAOFz,AutourFrobCompa,ooEPEFooQiPESf}]       \label{LEMooDFFDooJTQkRu}
    Nous avons
    \begin{equation}
        \dim\big( \comC(f) \big)\geq \dim(E)
    \end{equation}
\end{lemma}

\begin{proof}
    Si \( f\) est donnée, l'espace \( \comC(f)\) est l'espace des solutions de \( fg=gf\). Supposons avoir choisit une base de \( E\) et notons \( A\) la matrice de \( f\) et \( X\) celle de \( g\). L'équation est \( AX-XA=0\).
    \begin{subproof}
        \item[Si \( A\) est trigonalisable]
            Nous supposons avoir choisi la base de telle sorte que \( A\) soit triangulaire supérieure, et nous allons nous contenter de chercher les solutions \( X\) qui sont également triangulaires supérieure. Si il y en a déjà plus que \( n\), a fortiori le résultat sera vrai.

            Le produit de deux matrices triangulaires supérieures étant une matrice triangulaire supérieure, l'équation \( AX-XA\) contient, pour les coefficients de \( X\), \( n(n+1)/2\) équations. Mais il se fait que les termes diagonaux ne sont pas de vraies équations parce que
            \begin{equation}
                (AX-XA)_{kk}=\sum_i\big( A_{ki}X_{ik}-X_{ki}A_{ik} \big)=\sum_{k\leq i\leq k}(A_{ki}X_{ik}-X_{ki}A_{ik})=0.
            \end{equation}
            Nous avons donc au maximum 
            \begin{equation}
                \frac{ n(x+1) }{2}-n
            \end{equation}
            équations linéairement indépendantes pour un minimum de \( n(n+1)/2\) inconnues. L'espace des solutions est donc de dimension au minimum \( n\).

            Cela a l'air d'être une majoration assez large, mais il existe des cas d'égalité.

        \item[Si \( A\) n'est pas trigonalisable]

            La preuve que nous donnons ici est valable même pour les endomorphismes trigonalisables.

            Nous considérons le résultat de Frobenius \ref{THOooDOWUooOzxzxm}. Nous avons donc la structure suivante:
            \begin{itemize}
                \item 
            une décomposition en somme directe \( E=E_1\oplus\ldots\oplus E_r\),
        \item
            les espaces \( E_i\) sont fixés par \( f\),
        \item
            les endomorphismes \( f_i=f|_{E_i}\) sont cycliques
        \item
            le polynôme minimal de \( f_i\) est \( \mu_i\) et \( \prod_{i=1}^r\mu_i=\chi_f\).
            \end{itemize}
            Les endomorphismes \( f_i^k\) commutent évidemment avec \( f_j\), et la partie \( \{ f_i^k \}_{k=0,\ldots, \deg(\mu_i)-1}\) est libre. Libre en tout cas en tant que partie de \( \End(E_i)\). Mais en prolongeant par \( 0\) sur \( E\), ça reste libre en tant que partie de \( \End(E)\).

            Bien entendu les \( f_j^k\) et les \( f_i^k\) (\( i\neq j\)) sont linéairement indépendants dans \( \End(E)\) parce qu'ils n'agissent pas sur les mêmes vecteurs. Donc les endomorphismes \( f_i^{k_i}\) avec \( k_i=0,\ldots, \deg(\mu_i)-1\) forment une partie libre de \( \End(E)\) composée d'endomorphismes qui commutent avec \( f\). Il y en a en tout
            \begin{equation}
                \sum_{i=1}^r\deg(\mu_i)=\deg(\chi_f)=n.
            \end{equation}
            Par conséquent \( \dim\big( \comC(f) \big)\geq \dim(E)\).
    \end{subproof}
\end{proof}

\begin{theorem}[\cite{ooRJDSooXpVtMD}]      \label{THOooGLMSooYewNxW}
    Soit un endomorphisme \( f\colon E\to E\) sur l'espace vectoriel de dimension finie \( n\). Nous notons \( \mu\) et \( \chi\) les polynômes minimal et caractéristique. Nous avons équivalence entre les faits suivants :
    \begin{enumerate}
        \item   \label{ITEMooLRXIooLWaYqJii}
            \( \mu=\chi\),
        \item   \label{ITEMooLRXIooLWaYqJi}
            \( f\) est cyclique,
        \item   \label{ITEMooLRXIooLWaYqJiii}
            \( \comC(f)=\eK[f]\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Plusieurs implications. Notons que \ref{ITEMooLRXIooLWaYqJii} implique \ref{ITEMooLRXIooLWaYqJii} a déjà été démontré par le lemme \ref{LEMooKUQDooKFeIYq}.
    \begin{subproof}
        \item[\ref{ITEMooLRXIooLWaYqJii} implique \ref{ITEMooLRXIooLWaYqJi}]
            Conformément à ce que nous permet le lemme \ref{LemSYsJJj} nous choisissons\footnote{Dans toute la suite,nous devrions écrire \( \mu_f\) et \( \mu_{f,a}\) mais nous omettons d'indiquer explicitement la dépendance en \( f\).} \( a\in E\) de telle sorte à avoir \( \mu_a=\mu\). De plus pour \( x\in E\) nous considérons l'application
            \begin{equation}
                \begin{aligned}
                    \varphi_x\colon \eK[X]&\to E \\
                    P&\mapsto P(f)x. 
                \end{aligned}
            \end{equation}
            Nous avons \( \varphi_a(P)=P(f)a\) et vu que \( E_{a}\) est engendré par les \( f^k(a)\) nous avons \( \varphi_a\big( \eK[X] \big)=E_a\). De plus l'application \( \varphi_a\) passe aux classes pour \( (\mu_a)\). Pour rappel, un élément de \( \eK[X]/(\mu_a)\) est de la forme
            \begin{equation}
                \bar P=\{ P+Q\mu_a \}_{Q\in \eK[X]}.
            \end{equation}
            Nous considérons donc l'application
            \begin{equation}
                \varphi_a\colon \frac{ \eK[X] }{ (\mu_a) }\to E_a
            \end{equation}
            et nous prouvons que c'est un isomorphisme d'espace vectoriel.
            \begin{subproof}
                \item[Linéaire]
                    Parce que \( (\lambda P+Q)(f)=(\lambda P)(f)+Q(f)\).
                \item[Injectif]
                    Si \( \varphi_a(\bar P)=0\) alors \( \varphi_a(P)=0\) (dans la deuxième, \( \varphi_a\) est l'application définie sur les polynômes et non sur les classes), ce qui montrer que \( P\) est annulateur de \( a\). Mais par définition \ref{DEFooUICRooBGYhqQ} du polynôme minimal ponctuel, \( \mu_a\) est générateur de \( \ker(\varphi_a)\); donc il existe \( Q\in \eK[X]\) tel que \( P=Q\mu_a\). En d'autres termes, du point de vue du quotient, \( \bar P=0\).
                \item[Surjectif]
                    Si \( x\in E_a\) alors il existe des coefficients \( x_k\in \eK\) tels que \( x=\sum_{k=0}^{\deg(\mu_a)-1}x_kf^k(a)\), c'est à dire \( x=P(f)a=\varphi_a(P)\).
            \end{subproof}
            Mais par hypothèse et par choix de \( a\) nous avons \( \mu_a=\mu=\chi\), donc en fait \( E_a=\eK[X]/(\chi)\). Mais nous savons que \( \deg(\chi)=\dim(E)\) et que \( \dim\big( \eK[X]/P \big)=\deg(P)\) par la proposition \ref{PropooCFZDooROVlaA}. Au final nous avons \( \dim(E_a)=\deg(\chi)=\dim(E)\). Et par conséquent \( E_a=E\). Cela prouver que \( a\) est un vecteur cyclique pour \( f\).

        \item[\ref{ITEMooLRXIooLWaYqJi} implique \ref{ITEMooLRXIooLWaYqJiii}]
            Soit \( g\in \comC(f)\); nous devons prouver que \( g\) est un polynôme de \( f\). Par hypothèse nous avons un vecteur cyclique que nous notons \( v\). Nous avons un polynôme \( P\) (dépendant de \( g\)) tel que \( g(v)=P(f)v\). Nous allons voir que \( g=P(f)\). Soit \( y\in E\) et un polynôme \( Q\) tel que \( y=Q(f)v\); en notant que \( g\) commute avec \( P(f)\) nous avons 
            \begin{equation}
                g(y)=g\big( Q(f)v \big)=Q(f)\big( g(v) \big)=Q(f)\big( P(f)v \big)=P(f)Q(f)v=P(f)y.
            \end{equation}
            Donc \( g=P(f)\).

        \item[\ref{ITEMooLRXIooLWaYqJiii} implique \ref{ITEMooLRXIooLWaYqJii}]

            Nous avons les inégalités :
            \begin{equation}
                n\leq \dim\big( \comC(f) \big)=\dim\big( \eK[f] \big)=\deg(\mu)\leq \deg(\chi)=n.
            \end{equation}
            La première est le lemme \ref{LEMooDFFDooJTQkRu}. Toutes les inégalités sont des égalités. En particulier \( \deg(\mu)=n\), ce qui signifie que \( \mu=\chi\) parce que \( \mu\) est un polynôme diviseur de \( \chi\), de même degré que \( \chi\) et unitaire tout comme \( \chi\).

    \end{subproof}
\end{proof}

\begin{corollary}[\cite{ooEPEFooQiPESf}]        \label{CORooAKQEooSliXPp}
    En suivant les notations sur les extensions de corps de base de la section \ref{SECooAUOWooNdYTZf}, l'endomorphisme \( f\colon E\to F\) est cyclique si et seulement si l'endomorphisme \( f_{\eL}\colon E_{\eL}\to F_{\eL}\) est cyclique.
\end{corollary}

\begin{proof}
    Nous savons par le théorème \ref{THOooGLMSooYewNxW} qu'un endomorphisme est cyclique si et seulement si son polynôme minimal est égal à son polynôme caractéristique. Or par les propositions \ref{PROPooZAZFooUFdCUv} et \ref{PROPooXVZMooXcJrsJ}, nous savons que ces polynômes sont identiques pour \( f\) et pour \( f_{\eL}\).
\end{proof}

\begin{theorem}[Similitude et extension de corps\cite{ooEPEFooQiPESf}]      \label{THOooHUFBooReKZWG}
    Les applications linéaires \( f,g\colon E\to E\) sont semblables si et seulement si \( f_{\eL}\) et \( g_{\eL}\) le sont.
\end{theorem}

\begin{proof}
    En ce qui concerne le sens direct, si il existe \( m\in\GL(E)\) tel que \( f=mgm^{-1}\) alors il suffit d'appliquer le lemme \ref{LEMooWZGSooONEnjZ} pour avoir \( f_{\eL}=m_{\eL}g_{\eL}m_{\eL}^{-1}\).

    Nous considérons les invariants de similitude de \( f\) du théorème \ref{THOooDOWUooOzxzxm}. Il existe une unique suite de polynômes unitaires \( \mu_i\) ($i=1,\ldots, s$) tels que \( \mu_i\divides \mu_{i+1}\) et pour laquelle nous avons une décomposition \( E=E_1\oplus \ldots\oplus E_s\) pour laquelle \( f|_{E_i}\colon E_1\to E_i\) est cyclique et de polynôme minimal \( \mu\).

    Nous avons aussi \( E_{\eL}=(E_1)_{\eL}\oplus\ldots \oplus (E_s)_{\eL}\) et les \( (E_i)_{\eL}\) sont stables sous \( f_{\eL}\) qui y sera également cyclique (corollaire  \ref{CORooAKQEooSliXPp}). De plus le polynôme minimal de \( f_{\eL}|_{(E_i)_{\eL}}\) est également \( \mu_i\).

    Autrement dit, la suite \( \mu_i\) est également la suite des invariants de similitude de \( f_{\eL}\). La remarque \ref{REMooPVLEooYDRXQI} nous permet de conclure que \( f\) et \( g\) sont semblables si et seulement si \( f_{\eL}\) et \( g_{\eL}\) le sont.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Endomorphismes nilpotents et trigonalisables}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Endomorphismes nilpotents}
%---------------------------------------------------------------------------------------------------------------------------

La \defe{trace}{trace!matrice} d'une matrice \( A\in \eM(n,\eK)\) est la somme de ses éléments diagonaux :
\begin{equation}
    \tr(A)=\sum_{i=1}^nA_{ii}.
\end{equation}
Une propriété importante est son invariance cyclique.
\begin{lemma}   \label{LemhbZTay}
    Si \( A\) et \( B\) sont des matrices carré, alors \( \tr(AB)=\tr(BA)\).

    La trace est un invariant de similitude.
\end{lemma}

\begin{proof}
    C'est un simple calcul :
    \begin{equation}
            \tr(AB)=\sum_{ik}A_{ik}B_{ki}
            =\sum_{ik}A_{ki}B_{ik} 
            =\sum_{ik}B_{ik}A_{ki}
            =\sum_i(BA)_{ii}
            =\tr(BA)
    \end{equation}
    où nous avons simplement renommé les indices \( i\leftrightarrow k\).

    En particulier, la trace est un invariant de similitude parce que \( \tr(ABA^{-1})=\tr(A^{-1} AB)=\tr(B)\). 
\end{proof}
La trace étant un invariant de similitude, nous pouvons donc définir la \defe{trace}{trace!endomorphisme} comme étant la trace de sa matrice dans une base quelconque. Si la matrice est diagonalisable, alors la trace est la somme des valeurs propres.

\begin{lemma}[\cite{fJhCTE}]   \label{LemzgNOjY}
    L'endomorphisme \( u\in\End(\eC^n)\) est nilpotent si et seulement si \( \tr(u^p)=0\) pour tout \( p\).
\end{lemma}

\begin{proof}
    Supposons que \( u\) est nilpotent. Alors ses valeurs propres sont toutes nulles et celles de \( u^p\) le sont également. La trace étant la somme des valeurs propres, nous avons alors tout de suite \( \tr(u^p)=0\).

    Supposons maintenant que \( \tr(u^p)=0\) pour tout \( p\). Le polynôme caractéristique \eqref{Eqkxbdfu} est
    \begin{equation}    \label{EqfnCqWq}
        \chi_u=(-1)^nX^{\alpha}(X-\lambda_1)^{\alpha_1}\ldots (X-\alpha_r)^{\alpha_r}.
    \end{equation}
    où les \( \lambda_i\) (\( i=1,\ldots, r\)) sont les valeurs propres non nulles distinctes de \( u\).

    Il est vite vu que le coefficient de \( X^{n-1}\) dans \( \chi_u\) est \( -\tr(u)\) parce que le coefficient de \( X^{n-1}\) se calcule en prenant tous les $X$ sauf une fois \( -\lambda_i\). D'autre part le polynôme caractéristique de \( u^p \) est le même que celui de \( u\), en remplaçant \( \lambda_i\) par \( \lambda_i^p\); cela est dû au fait que si \( v\) est vecteur propre de valeur propre \( \lambda\), alors \( u^pv=\lambda^pv\).

    Par l'équation \eqref{EqfnCqWq}, nous voyons que le coefficient du terme \( X^{n-1}\) dans les polynôme caractéristique est 
    \begin{equation}        \label{eqSoDSKH}
        0=\tr(u^p)=\alpha_1\lambda_1^p+\ldots +\alpha_r\lambda_r^p.
    \end{equation}
    Donc les nombres \( (\alpha_1,\ldots, \alpha_r)\) est une solution non triviale\footnote{Si \( \alpha_1=\ldots=\alpha_r=0\), alors les valeurs propres sont toutes nulles et la matrice est en réalité nulle dès le départ.} du système
    \begin{subequations}    \label{EqDpvTnu}
        \begin{numcases}{}
            \alpha_1X_1+\ldots +\lambda_rX_r=0\\
            \qquad\vdots\\
            \lambda^r_1X_1+\ldots +\lambda_r^rX_r=0.
        \end{numcases}
    \end{subequations}
    Cela sont les équations \eqref{eqSoDSKH} écrites avec \( p=1,\ldots, r\). Le déterminant de ce système est
    \begin{equation}
        \lambda_1\ldots\lambda_r\det\begin{pmatrix}
             1   &   \ldots    &   1    \\
             \lambda_1   &   \ldots    &   \lambda_1    \\
             \vdots   &       &   \vdots    \\ 
             \lambda_1^{r-1}   &   \ldots    &   \lambda_r^{r-1}
         \end{pmatrix}\neq 0,
    \end{equation}
    qui est un déterminant de Vandermonde (proposition \ref{PropnuUvtj}) valant
    \begin{equation}
        0=\lambda_1\ldots\lambda_r\prod_{1\leq i\leq j\leq r}(\lambda_i-\lambda_j).
    \end{equation}
    Étant donné que les \( \lambda_i\) sont distincts et non nuls, nous avons une contradiction et nous devons conclure que \( (\alpha_1,\ldots, \alpha_r)\) était une solution triviale du système \eqref{EqDpvTnu}.
\end{proof}

\begin{proposition}[\cite{SVSFooIOYShq}]    \label{PropMWWJooVIXdJp}
    Soit un \( \eK\)-espace vectoriel \( E\). Un endomorphisme \( u\in\End(E)\) est nilpotent si et seulement si il existe une base de \( E\) dans laquelle la matrice de \( u\) est strictement triangulaire supérieure.
\end{proposition}

\begin{proof}
    \begin{subproof}
       \item[\( \Rightarrow\)]
           Nous faisons la démonstration par récurrence sur la dimension de \( E\). Lorsque \( n=1\) nous avons \( u=(a)\) avec \( a\in \eK\). Vu que \( a^k=0\) pour un certain \( k\) nous avons \( a=0\) parce qu'un corps est toujours un anneau intègre\footnote{Lemme \ref{LemAnnCorpsnonInterdivzer}.}. 

           Lorsque \( \dim(E)=n\) nous savons que \( u\) a un noyau non réduit au vecteur nul (parce qu'il est nilpotent). Soit donc un vecteur non nul \( x\in\ker(u)\) et une base
           \begin{equation}
               \{ x,e_2,\ldots, e_n \}
           \end{equation}
           donnée par le théorème de la base incomplète \ref{ThonmnWKs}. La matrice de \( u\) dans cette base s'écrit
           \begin{equation}
               \begin{pmatrix}
                       \begin{array}[]{c|c}
                           0&\begin{matrix} 
                               * &   *    &   *    
                           \end{matrix}\\
                           \hline
                           \begin{matrix}
                               0 \\ 
                               0 \\ 
                               0 
                           \end{matrix}&
                           \begin{pmatrix}
                                &       &       \\
                                &   A    &       \\
                                &       &   
                           \end{pmatrix}
                       \end{array}
               \end{pmatrix}.
           \end{equation}
           Un tout petit peu de calcul de produit de matrice montre que la matrice de \( u^k\) est de la forme
           \begin{equation}
               \begin{pmatrix}
                       \begin{array}[]{c|c}
                           0&\begin{matrix} 
                               * &   *    &   *    
                           \end{matrix}\\
                           \hline
                           \begin{matrix}
                               0 \\ 
                               0 \\ 
                               0 
                           \end{matrix}&
                           \begin{pmatrix}
                                &       &       \\
                                &   A^k    &       \\
                                &       &   
                           \end{pmatrix}
                       \end{array}
               \end{pmatrix}.
           \end{equation}
           Étant donné que \( u\) est nilpotente, la matrice \( A\) l'est aussi. L'hypothèse de récurrence dit alors que \( A\) est strictement triangulaire supérieure (ou en tout cas peut le devenir par un changement de base adéquat).

       \item[\( \Leftarrow\)]

            Lorsqu'une matrice est triangulaire supérieure stricte, elle applique
            \begin{equation}
                \Span\{ e_1,\ldots, e_k \}\to\Span\{ e_1,\ldots, e_{k-1} \}.
            \end{equation}
            Donc tout vecteur finit sur zéro si on lui applique \( u\) assez souvent.
    \end{subproof}
\end{proof}



\begin{proposition}
    Soit \( A\in\GL(n,\eC)\). La suite \( (A^k)_{k\in \eZ}\) est bornée si et seulement si \( A\) est diagonalisable et \( \Spec(A)\subset \gS^1\).
\end{proposition}

\begin{proof}
    Si \( A\) est diagonalisable avec les valeurs propres \( \lambda_i\) de norme \( 1\) dans \( \eC\), alors \( A^k\) est la matrice diagonale avec les \( \lambda_i^k\) sur la diagonale. Cela reste borné pour toute valeur entière de \( k\).

    En ce qui concerne l'autre sens, nous supposons encore que
    \begin{equation}
        A=\begin{pmatrix}
            \lambda_1\mtu+N_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_s\mtu+N_s
        \end{pmatrix},
    \end{equation}
    et nous regardons un des blocs. Nous voulons prouver que \( N=0\) et que \( | \lambda |=1\).
    
    Nous commençons par regarder ce qu'implique le fait que \( (\lambda \mtu+N)^n\) reste borné pour \( n>0\). En notant \( r\) l'ordre de nilpotence de \( N\), nous avons le développement
    \begin{equation}
        (\lambda\mtu+N)^n=\sum_{k=0}^{r-1}\binom{ n }{ k }N^k\lambda^{n-k}.
    \end{equation}
    Par la proposition \ref{PropMWWJooVIXdJp}, une matrice nilpotente s'écrit dans une base sous la forme
    \begin{equation}
        N=\begin{pmatrix}
             0   &   1    &       &       \\
                &   0    &   1    &       \\
                & &   \ddots   &   \ddots    &      \\ 
                &&       &   0    &   1     \\
                &&       &      &   0     
         \end{pmatrix}
    \end{equation}
    et effectuer \( A^k\) revient à décaler la diagonale de \( 1\). Donc la famille
    \begin{equation}
        \{ \mtu,N,\ldots, N^{r-1} \}
    \end{equation}
    est libre. Par conséquent la suite \( (\lambda\mtu+N)^n\) restera bornée si et seulement si chacun des termes 
    \begin{equation}    \label{EqXRDVDCM}
        \binom{ n }{ k }N^k\lambda^{n-k}
    \end{equation}
    reste borné. Le premier terme étant \( \lambda^n\mtu\), nous avons obligatoirement \( | \lambda |\leq 1\). Si \( | \lambda |<1\), alors le coefficient \( \binom{ n }{ k }\lambda^{n-k}\) tend vers zéro. Si \( | \lambda |=1\) par contre ce coefficient tend vers l'infini et la seule façon pour que \eqref{EqXRDVDCM} reste borné est que \( N=0\). Nous avons donc deux possibilités :
    \begin{itemize}
        \item \( | \lambda |<1\)
        \item \( | \lambda |=1\) et \( N=0\).
    \end{itemize}

    Nous nous tournons maintenant sur la contrainte que \( (\lambda\mtu+N)^n\) doive rester borné pour \( n<0\). Nous avons
    \begin{equation}
        \lambda\mtu+N=\lambda(\mtu+\lambda^{-1}N),
    \end{equation}
    et nous pouvons appliquer la proposition \ref{PropQAjqUNp} à l'opérateur nilpotent \( -\lambda^{-1} N\) pour avoir
    \begin{equation}
        (\mtu+\lambda^{-1}N)^{-1}=\mtu+\sum_{k=1}^{\infty}(-\lambda)^{-1}N^k.
    \end{equation}
    Ceci pour dire que \( (\lambda\mtu+N)^{-1}=\lambda^{-1}(\mtu+\lambda^{-1}N')\) pour une autre matrice nilpotente \( N'\). Le travail déjà fait, appliqué à \( \lambda^{-1}\) et \( N'\), nous donne deux possibilités :
    \begin{itemize}
        \item \( | \lambda^{-1} |<1\)
        \item \( | \lambda^{-1} |=1\) et \( N'=0\).
    \end{itemize}
    La possibilité \( | \lambda^{-1} |<1\) est exclue parce qu'elle impliquerait \( | \lambda |>1\) qui avait déjà été exclu. Il ne reste donc que la possibilité \( | \lambda |=1\) et \( N=N'=0\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Endomorphismes trigonalisables}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[\cite{MQMKooPBfnZN}]
    Une matrice dans \( \eM(n,\eK)\) est \defe{trigonalisable}{matrice!trigonalisable} lorsqu'elle est semblable\footnote{Définition \ref{DefCQNFooSDhDpB}.} à une matrice triangulaire supérieure.
\end{definition}

\begin{proposition} \label{PropKNVFooQflQsJ}
    Soit \( u\) un endomorphisme d'un espace vectoriel \( E\) sur le corps \( \eK\). Les faits suivants sont équivalents.
    \begin{enumerate}
        \item   \label{ItemZKDMooOrTHkwi}
            L'endomorphisme \( u\) est trigonalisable (auquel cas les valeurs propres sont sur la diagonale).
        \item   \label{ItemZKDMooOrTHkwii}
            Le polynôme caractéristique de \( u\) est scindé\footnote{Définition \ref{DefCPLSooQaHJKQ}.}.
    \end{enumerate}
\end{proposition}
\index{trigonalisation!et polynôme caractéristique}

\begin{proof}
    \begin{subproof}
        \item[\ref{ItemZKDMooOrTHkwii}\( \Rightarrow\)\ref{ItemZKDMooOrTHkwi}]
            Nous avons par hypothèse que
            \begin{equation}
                \chi_u(X)=\prod_{i=1}^r(X-\lambda_i)^{\alpha_i}
            \end{equation}
            où les \( \lambda_i\) sont les valeurs propres de \( u\). Le théorème de Cayley-Hamilton \ref{ThoCalYWLbJQ} dit que \( \chi_u(u)=0\), ce qui permet d'utiliser le théorème de décomposition des noyaux \ref{ThoDecompNoyayzzMWod} :
            \begin{equation}
                E=\ker(X-\lambda_1)^{\alpha_1}\oplus\ldots\oplus\ker(X-\lambda_r)^{\alpha_r}.
            \end{equation}
            Les espaces \( F_{\lambda_i}(u)=\ker(X-\lambda_i)^{\alpha_i}\) sont les espaces caractéristiques de \( u\), ce qui fait que \( u-\lambda_i\mtu\) est nilpotent sur \( F_{\lambda_i}(u)\). L'endomorphisme \( u-\lambda_i\mtu\) est donc strictement trigonalisable supérieur sur son bloc\footnote{Proposition \ref{PropMWWJooVIXdJp}.}. Cela signifie que \( u\) est triangulaire supérieure avec les valeurs propres sur la diagonale.

        \item[\ref{ItemZKDMooOrTHkwi}\( \Rightarrow\)\ref{ItemZKDMooOrTHkwii}]

            C'est immédiat parce que le déterminant d'une matrice triangulaire est le produit des éléments de sa diagonale.
    \end{subproof}
\end{proof}

\begin{remark}  \label{RemXFZTooXkGzQg}
    Si \( \eK\) est algébriquement clos (comme \( \eC\) par exemple), alors tous les polynômes sont scindés et toutes les matrices sont trigonalisables\footnote{Le lemme de Schur complexe \ref{LemSchurComplHAftTq} va un peu plus loin et précise que la trigonalisation peut être faite par une matrice unitaire.}. Un exemple un peu simple de cela est la matrice
    \begin{equation}
        u=\begin{pmatrix}
            0    &   -1    \\ 
            1    &   0    
        \end{pmatrix}.
    \end{equation}
    Le polynôme caractéristique est \( \chi_u(X)=X^2+1\) et les valeurs propres sont \( \pm i\). Il est vite vu que dans la base
    \begin{equation}
        \{ \begin{pmatrix}
        i    \\ 
    1    
\end{pmatrix}, \begin{pmatrix}
1    \\ 
i    
\end{pmatrix}\}
    \end{equation}
    de \( \eC^2\), la matrice \( u\) se note \( \begin{pmatrix}
        i    &   0    \\ 
        0    &   -i    
    \end{pmatrix}\).
\end{remark}

\begin{remark}  \label{RemREOSooGEDJWX}
    Cela nous donne une autre façon de prouver qu'une matrice nilpotente de \( \eM(n,\eC)\) ou \( \eM(n,\eR)\) est trigonalisable\cite{KDUFooVxwqlC}. D'abord dans \( \eM(n,\eC)\), toutes les matrices sont trigonalisables\footnote{Parce que le polynôme caractéristique est scindé, voir remarque \ref{RemXFZTooXkGzQg}.}, et les valeurs propres arrivent sur la diagonale. Mais comme les valeurs propres d'une matrice nilpotente sont zéro, elle est triangulaire stricte. Par ailleurs son polynôme caractéristique est alors \( X^n\).

    Ensuite si \( u\in \eM(n,\eR)\) nous pouvons voir \( u\) comme une matrice dans \( \eM(n,\eC)\) et y calculer son polynôme caractéristique qui sera tout de même \( X^n\). Ce polynôme étant scindé, la proposition \ref{PropKNVFooQflQsJ} nous assure que \( u\) est trigonalisable. Une fois de plus, les valeurs propres étant sur la diagonale, elle est triangulaire supérieure stricte.
\end{remark}

\begin{remark}
    La méthode des pivots de Gauss\footnote{Le lemme \ref{LemZMxxnfM}.} certes permet de trigonaliser n'importe quoi, mais elle ne correspond pas à un changement de base. Autrement dit, les pivots de Gauss ne sont pas de similitudes.

    C'est là qu'il faut bien avoir en tête la différence entre \emph{équivalence} et \emph{similarité} (définition \ref{DefBLELooTvlHoB}). Lorsqu'on parle de changement de base, de matrice trigonalisable ou diagonalisable, nous parlons de similarité et non d'équivalence.
\end{remark}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Burnside}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}       \label{LemwXXzIt}
    Soit \( P\), un polynôme sur \( \eK\). Une racine de \( P\) est une racine simple si et seulement si elle n'est pas racine de \( P'\).
\end{lemma}

\begin{theorem}     \label{ThoBurnsideoPuCtS}
    Toute représentation d'un groupe abélien d'exposant fini sur \( \eC^n\) a une image finie.
\end{theorem}

\begin{proof}
    Dans le cas d'un groupe abélien, la démonstration est facile. Étant donné que \( G\) est d'exposant fini, il existe \( \alpha\in \eN^*\) tel que \( g^{\alpha}=e\) pour tout \( g\in G\). Le polynôme \( P(X)=X^{\alpha}-1\) est scindé à racines simples. En effet tout polynôme sur \( \eC\) est scindé. Le fait qu'il soit à racines simples provient du lemme \ref{LemwXXzIt} parce que si \( a^{\alpha}=1\), alors il n'est pas possible d'avoir \( \alpha a^{\alpha-1}=0\).

    Par ailleurs \( P(g)=0\). Le fait que nous ayons un polynôme annulateur de \( g\) scindé à racines simples implique que \( g\) est diagonalisable (théorème \ref{ThoDigLEQEXR}). Le fait que \( G\) soit abélien montre qu'il existe une base de \( \eC^n\) dans laquelle tous les éléments de \( G\) sont diagonaux. Nous devons par conséquent montrer qu'il existe un nombre fini de matrices de la forme
    \begin{equation}
        \begin{pmatrix}
            \lambda_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_n
        \end{pmatrix}.
    \end{equation}
    Nous savons que \( \lambda_i^{\alpha}=1\) parce que \( g^{\alpha}=\mtu\), par conséquent chacun des \( \lambda_i\) est une racine de l'unité dont il n'existe qu'un nombre fini.
\end{proof}

\begin{theorem}[Burnside\cite{fJhCTE,ooFBZQooXyHIWK}]\label{ThooJLTit}
    Un sous-groupe de \( \GL(n,\eC)\) est fini si et seulement si il est d'exposant fini.
\end{theorem}
\index{exposant}
\index{racine!de l'unité}
\index{endomorphisme!diagonalisable}

\begin{proof}
    Soit \( G\) un sous-groupe de \( \GL(n,\eC)\). Si \( G\) est fini, l'ordre de ses éléments divise \( | G |\) (corollaire \ref{CorpZItFX}) au théorème de Lagrange et l'exposant est le PPCM qui est donc fini également.

    Nous supposons maintenant que l'ordre de \( G\) est fini. Nous notons \( e\) l'exposant de \( G\). En particulier, tous les éléments de \( G\) sont des racines du polynôme \( X^e-1\).
    
    \begin{subproof}
        \item[Générateurs]

            Le groupe \( G\) est une partie de \( \eM(n,\eC)\) dont nous considérons l'algèbre engendrée (définition \ref{DefkAXaWY}) \( \mG\). Soit \( C_1,\ldots, C_r\) une famille génératrice de \( \mG\) constituée d'éléments de \( G\) et la fonction
            \begin{equation}
                \begin{aligned}
                    \tau\colon G&\to \eC^r \\
                    A&\mapsto \big( \tr(AC_1),\ldots, \tr(AC_r) \big). 
                \end{aligned}
            \end{equation}

        \item[\( \tau\) est injective] Supposons que \( \tau(A)=\tau(B)\). Alors pour tout générateur \( C_i\) nous avons \( \tr(AC_i)=\tr(BC_i)\) et par linéarité de la trace, nous avons
            \begin{equation}    \label{EqnCYmKW}
                \tr(AM)=\tr(BM)
            \end{equation}
            pour tout \( M\in G\). Notons par ailleurs
            \begin{equation}
                N=AB^{-1}-\mtu,
            \end{equation}
            qui est diagonalisable parce que \( AB^{-1}\in G\) et donc est annulé par le polynôme \( X^e-1\) qui est scindé à racines simples. Du coup \( AB^{-1}\) est diagonalisable; posons \( PAB^{-1}P^{-1}=D\), alors \( P\big( AB^{-1}-\mtu \big)P^{-1}=D-\mtu\) qui est encore diagonale. Donc \( N\) est diagonalisable.

            Par ailleurs nous avons
            \begin{subequations}
                \begin{align}
                    \tr\big( (AB^{-1})^p \big)&=\tr\big( AB^{-1}(AB^{-1})^{p-1} \big)\\
                    &=\tr\big( BB^{-1}(AB^{-1})^{p-1} \big) &\text{\eqref{EqnCYmKW}}\\
                    &=\tr\big( (AB^{-1})^{p-1} \big).
                \end{align}
            \end{subequations}
            En continuant nous obtenons
            \begin{equation}
                \tr\big(  (AB^{-1})^p \big)=\tr(\mtu)=n.
            \end{equation}
            
            D'autre part, 
            \begin{equation}
                N^k=(AB^{-1}-\mtu)^k=\sum_{p=0}^k{p\choose k}(-1)^{k-p}(AB^{-1})^p
            \end{equation}
            En prenant la trace, et en tenant compte du fait que \( \tr\big( (AB^{-1})^p \big)=n\),
            \begin{equation}
                \tr(N^k)=\sum_{p=0}^k{p\choose k}(-1)^{k-p}n=n(1-1)^k=0.
            \end{equation}
            Donc la trace de \( N^k\) est nulle et le lemme \ref{LemzgNOjY} nous enseigne que \( N\) est alors nilpotente. Étant donné qu'elle est aussi diagonalisable, elle est nulle. Nous en concluons que \( AB^{-1}=\mtu\) et donc que \( A=B\). La fonction \( \tau\) est donc injective.

        \item[Nombre fini de valeurs]

            Les éléments de \( G\) sont annulés par \( X^e-1\) qui est un polynôme scindé à racines simples. Dons le polynôme minimal d'un élément de \( G\) est (a fortiori) scindé à racines simples et le théorème \ref{ThoDigLEQEXR} nous assure alors que ces éléments sont diagonalisables. Du coup les valeur propres des matrices de \( G\) sont des racines \( e\)ièmes de l'unité. Par conséquent les traces des éléments de \( G\) ne peuvent prendre qu'un nombre fini de valeurs : toutes les sommes de \( n\) racines \( e\)ièmes de l'unité. Mais vu que les \( C_i\) sont dans \( G\), nous avons
            \begin{equation}
                \Image(\tau)=\{ \tr(A)\tq A\in G \}^r,
            \end{equation}
            qui est un ensemble fini. Par conséquent \( G\) est fini parce que \( \tau\) est injective.
    \end{subproof}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème de Lie-Kolchin}
%---------------------------------------------------------------------------------------------------------------------------

Contrairement à ce que l'on peut parfois croire, il n'est pas vrai que toute matrice à coefficient réel est diagonalisable, même pas sur \( \eC\). La raison est qu'une telle matrice peut très bien avoir des valeurs propres multiples.

\begin{example} \label{ExBRXUooIlUnSx}
    Le théorème \ref{ThoDigLEQEXR} nous donne une façon simple de trouver des matrices non diagonalisables sur \( \eC\) : il suffit que le polynôme minimal ne soit pas scindé à racines simples. Par exemple
    \begin{equation}
        A=\begin{pmatrix}
            1    &   1    \\ 
            0    &   1    
        \end{pmatrix},
    \end{equation}
    dont le polynôme caractéristique est \( \chi_A=(1-X)^2\). Ce polynôme n'a manifestement pas des racines simples. Nous pouvons faire le calcul explicite pour montrer que \( A\) n'est pas diagonalisable. D'abord l'unique valeur propre de \( A\) est \( 1\) et nous pouvons sans peine résoudre
    \begin{equation}
        \begin{pmatrix}
            1    &   1    \\ 
            0    &   1    
        \end{pmatrix}\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}=\begin{pmatrix}
            x    \\ 
            y    
        \end{pmatrix}
    \end{equation}
    qui revient au système
    \begin{subequations}
        \begin{numcases}{}
            x+y=x\\
            y=y.
        \end{numcases}
    \end{subequations}
    La première équation donne directement \( y=0\). Le seul espace propre est de dimension \( 1\) et est engendré par \( \begin{pmatrix}
        1    \\ 
        0    
    \end{pmatrix}\).
\end{example}

La remarque \ref{RemBOGooCLMwyb} donne un exemple un peu plus avancé, qui montre la multiplicité algébrique et géométrique d'une racine d'un polynôme caractéristique.

\begin{lemma}[Trigonalisation simultanée]   \label{LemSLGPooIghEPI}
    Une famille de matrices de \( \GL(n,\eC)\) commutant deux à deux est simultanément trigonalisable.
\end{lemma}
\index{trigonalisation!simultanée}

\begin{proof}
    Commençons par enfoncer une porte ouverte par la proposition \ref{PropKNVFooQflQsJ} : sur \( \GL(n,\eC)\) toutes les matrices sont trigonalisables parce que tous les polynômes sont scindés.

    Nous effectuons la démonstration par récurrence sur la dimension. Si \( n=1\) alors toues les matrice sont triangulaires et nous ne nous posons pas de questions. Nous supposons donc \( n>1\).

    Soit la famille \( (A_i)_{i\in I}\) dans \( \GL(n,\eC)\) et \( A_0\) un de ses éléments. Nous nommons \( \lambda_1,\ldots, \lambda_r\) les valeurs propres distinctes de \( A_0\). Le théorème de décomposition primaire \ref{ThoSpectraluRMLok} nous donne la somme directe d'espaces caractéristiques\footnote{Définition \ref{DefFBNIooCGbIix}.}
    \begin{equation}
        E=F_{\lambda_1}(A_0)\oplus\ldots\oplus F_{\lambda_r}(A_0).
    \end{equation}
    Nous pouvons supposer que cette somme n'est pas réduite à un seul terme. En effet si tel était le cas, \( A_0\) serait un multiple de l'identité parce que \( A_0\) n'aurait qu'une seule valeur propre et les sommes dans la décomposition de Dunford \ref{ThoRURcpW}\ref{ItemThoRURcpWiii} se réduisent à un seul terme (et \( p_i=\id\)). En particulier les dimensions des espaces \( F_{\lambda}(A_0)\) sont strictement plus petites que \( n\).

    Vu que tous les \( A_i\) commutent avec \( A_0\), les espaces \( F_{\lambda}(A_0)\) sont stables par les \( A_i\) et nous pouvons trigonaliser les \( A_i\) simultanément sur chacun des \( F_{\lambda}(A_0)\) en utilisant l'hypothèse de récurrence.
\end{proof}

\begin{theorem}[Lie-Kolchin\cite{PAXrsMn}]  \label{ThoUWQBooCvutTO}
    Tout sous-groupe connexe et résoluble de \( \GL(n,\eC)\) est conjugué à un groupe de matrices triangulaires.
\end{theorem}
\index{trigonalisation!simultanée}
\index{théorème!Lie-Kolchin}

\begin{proof}
    Soit \( G\) un sous-groupe connexe et résoluble de \( \GL(n,\eC)\).
    
    \begin{subproof}
        \item[Si sous-espace non trivial stable par \( G\)]

    Nous commençons par voir ce qu'il se passe si il existe un sous-espace vectoriel non trivial \( V\) de \( \eC^n\) stabilisé par \( G\). Pour cela nous considérons une base de \( \eC^n\) dont les premiers éléments forment une base de \( V\) (base incomplète, théorème \ref{ThonmnWKs}). Les éléments de \( G\) s'écrivent, dans cette base,
    \begin{equation}    \label{EqGOKTooEaGACG}
        \begin{pmatrix}
            g_1    &   *    \\ 
            0    &   g_2    
        \end{pmatrix}.
    \end{equation}
    Les matrices \( g_1\) et \( g_2\) sont carrés. Nous considérons alors l'application \( \psi\) définie par
    \begin{equation}
        \begin{aligned}
            \psi\colon G&\to \GL(V) \\
            g&\mapsto g_1.
        \end{aligned}
    \end{equation}
    Cela est un morphisme de groupes parce que
    \begin{equation}
        \begin{pmatrix}
            g_1    &   *    \\ 
            0    &   g_2    
        \end{pmatrix}\begin{pmatrix}
            h_1    &   *    \\ 
            0    &   h_2    
        \end{pmatrix}=
        \begin{pmatrix}
            g_1h_1    &   *    \\ 
            0    &   g_2h_2    
        \end{pmatrix},
    \end{equation}
    de telle sorte que \( \psi(gh)=\psi(g)\psi(h)\).

    Le groupe \( \psi(G)\) est connexe et résoluble. En effet \( \psi(G)\) est connexe en tant qu'image d'un connexe par une application continue (proposition \ref{PropGWMVzqb}). Et il est résoluble en tant qu'image d'un groupe résoluble par un homomorphisme par la proposition \ref{PropBNEZooJMDFIB}. Vu que \( \psi(G)\) est un sous-groupe résoluble et connexe de \( \GL(V)\) et que la dimension de \( V\) est strictement plis petite que celle de \( \eC^n\), une récurrence sur la dimension indique que \( \psi(G)\) est conjugué à un groupe de matrices triangulaires. C'est à dire qu'il existe une base de \( V\) dans laquelle toutes les matrices \( g_1\) (avec \( g\in G\)) sont triangulaires supérieures.

    On fait de même avec l'application \( g\mapsto g_2\), ce qui donne une base du supplémentaire de \( V\) dans laquelle les matrices \( g_2\) sont triangulaires. 

    En couplant ces deux bases, nous obtenons une base de \( \eC^n\) dans laquelle toutes les matrices \eqref{EqGOKTooEaGACG} (c'est à dire toutes les matrices de \( G\)) sont triangulaires supérieures.

    \item[Sinon]

    Nous supposons à présent que \( \eC^n\) n'a pas de sous-espaces non triviaux stables sous \( G\). Nous posons \( m=\min\{ k\tq D^k(G)=\{ e \} \}\), qui existe parce que \( G\) et résoluble et que sa suite dérivée termine sur \( {e}\) (proposition \ref{PropRWYZooTarnmm}).

\item[Si \( m=1\)]

    Si \( m=1\) alors \( G\) est abélien et il existe une base de \( G\) dans laquelle toutes les matrices de \( G\) sont triangulaires (lemme \ref{LemSLGPooIghEPI}). Le premier vecteur d'une telle base serait stable par \( G\), mais comme nous avons supposé qu'il n'y avait pas de sous-espaces non triviaux stabilisés par \( G\), il faut déduire que ce vecteur stable est à lui tout seul non trivial, c'est à dire que \( n=1\). Dans ce cas, le théorème est démontré.

\item[Si \( m>1\)]

    Nous devons maintenant traiter le cas où \( m>1\). Nous posons \( H=D^{m-1}(G)\); cela est un sous-groupe normal et abélien de \( G\). Encore une fois le résultat de trigonalisation simultanée \ref{LemSLGPooIghEPI} donne une base dans laquelle tous les éléments de \( H\) sont triangulaires. En particulier le premier élément de cette base est un vecteur propre commun à toutes les matrices de \( H\).

    Soit \( V\) le sous-espace engendré par tous les vecteurs propres communs de \( H\). Nous venons de voir que \( V\) n'est pas vide. Nous allons montrer que \( V\) est stable par \( G\). Soient \( h\in H\), \( v\in V\) et \( g\in G\) :
    \begin{equation}    \label{EqPMOBooVLIhrJ}
        h\big( g(v) \big)=g\underbrace{g^{-1}hg}_{\in H}(v)=g(\lambda v)=\lambda g(v)
    \end{equation}
    parce que \( v\) est vecteur propre de \( g^{-1} hg\). Ce que le calcul \eqref{EqPMOBooVLIhrJ} montre est que \( g(v)\) est vecteur propre de \( h\) pour la valeur propre \( \lambda\). Donc \( g(v)\in V\) et \( V\) est stabilisé par \( G\). Mais comme il n'existe pas d'espaces non triviaux stabilisés par \( G\), nous en déduisons que \( V=\eC^n\). Donc tous les vecteurs de \( \eC^n\) sont vecteurs propres communs de \( H\). Autrement dit on a une base de diagonalisation simultanée de \( H\).

\item[\( H\) est dans le centre de \( G\)]

    Montrons à présent que \( H\) est dans le centre de \( G\), c'est à dire que pour tout \( g\in G\) et \( h\in H\) il faut \( ghg^{-1}=h\). D'abord \( ghg^{-1}\) est une matrice diagonale (parce que elle est dans \( H\)) ayant les mêmes valeurs propres que \( h\). En effet si \( \lambda\) est valeur propre de \( ghg^{-1}\) pour le vecteur propre \( v\), alors
    \begin{subequations}
        \begin{align}
            (ghg^{-1})(v)&=\lambda v\\
            h\big( g^{-1} v \big)&=\lambda \big( g^{-1}v \big),
        \end{align}
    \end{subequations}
    c'est à dire que \( \lambda\) est également valeur propre de \( h\), pour le vecteur propre \( g^{-1} v\). Mais comme \( h\) a un nombre fini de valeurs propres, il n'y a qu'un nombre fini de matrices diagonales ayant les mêmes valeurs propres que \( h\). L'ensemble \( \AD(G)h\) est donc un ensemble fini. D'autre part, l'application \( g\mapsto g^{-1}hg\) est continue, et \( G\) est connexe, donc l'ensemble \( \AD(G)h\) est connexe. Un ensemble fini et connexe dans \( \GL(n,\eC)\) est nécessairement réduit à un seul point. Cela prouve que \( ghg^{-1}=h\) pour tout \( g\in G\) et \( h\in H\).

\item[Espaces propres stables pour tout \( G\)]

        Soit \( h\in H\) et \( W\) un espace propre de \( h\) (ça existe non vide parce que \( H\) est triangularisé, voir plus haut). Alors nous allons prouver que \( W\) est stable pour tous les éléments de \( G\). En effet si \( w\in W\) avec \( h(w)=\lambda w\) alors en permutant \( g\) et \( h\),
        \begin{equation}
            hg(w)=g(hw)=\lambda g(w),
        \end{equation}
        donc \( g(w)\) est aussi vecteur propre de \( h\) pour la valeurs propre \( \lambda\), c'est à dire que \( g(w)\in W\). Vu que nous supposons que \( \eC^n\) n'a pas d'espaces invariants non triviaux, nous devons conclure que \( W=\eC^n\), c'est à dire que \( H\) est composé d'homothéties. C'est à dire que pour tout \( h\in H\) nous avons \( h=\lambda_h\mtu\).

    \item[Contradiction sur la minimalité de \( m\)]

        Les éléments d'un groupe dérivé sont de déterminant \( 1\) parce que \( \det(g_1g_2g_1^{-1}g_2^{-1})=1\). Par conséquent pour tout \( h\), le nombre \( \lambda_h\) est une racine \( n\)\ieme de l'unité. Vu qu'il n'y a qu'une quantité finie de racines \( n\)\ieme de l'unité, le groupe\( H\) est fini et connexe et donc une fois de plus réduit à un élément, c'est à dire \( H=\{ e \}\). Cela contredit la minimalité de \( m\) et donc produit une contradiction. Nous devons donc avoir \( m=1\).

    \item[Conclusion]

        Nous avons vu que si \( \eC^n\) avait un sous-espace non trivial fixé par \( G\) alors le théorème était démontré. Par ailleurs si \( \eC^n\) n'a pas un tel sous-espace, soit \( m=1\) (et alors le théorème est également prouvé), soit \( m>1\) et alors on a une contradiction.

        Bref, le théorème est prouvé sous peine de contradiction.

    \end{subproof}

\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Mini introduction au produit tensoriel}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SeOOpHsn}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définitions}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E\), un espace vectoriel de dimension finie. Si \( \alpha\) et \( \beta\) sont deux formes linéaires sur un espace vectoriel \( E\), nous définissons \( \alpha\otimes \beta\) comme étant la \( 2\)-forme donnée par
\begin{equation}
    (\alpha\otimes \beta)(u,v)=\alpha(u)\beta(v).
\end{equation}
Si \( a\) et \( b\) sont des vecteurs de \( E\), ils sont vus comme des formes sur \( E\) via le produit scalaire et nous avons
\begin{equation}
    (a\otimes b)(u,v)=(a\cdot u)(b\cdot v).
\end{equation}
Cette dernière équation nous incite à pousser un peu plus loin la définition de \( a\otimes b\) et de simplement voir cela comme la matrice de composantes
\begin{equation}
    (a\otimes b)_{ij}=a_ib_j.
\end{equation}
Cette façon d'écrire a l'avantage de ne pas demander de se souvenir qui est une vecteur ligne, qui est un vecteur colonne et où il faut mettre la transposée. Évidemment \( (a\otimes b)\) est soit \( ab^t\) soit \( a^tb\) suivant que \( a\) et \( b\) soient ligne ou colonne.

\begin{lemma}   \label{LemMyKPzY}
    Soient \( x,y\in E\) et \( A,B\) deux opérateurs linéaires sur \( E\) vus comme matrices. Alors
    \begin{equation}        \label{EqXdxvSu}
        (Ax\otimes By)=A(x\otimes y)B^t.
    \end{equation}
\end{lemma}

\begin{proof}
    Calculons la composante \( ij\) de la matrice \( (Ax\otimes By)\). Nous avons
    \begin{subequations}
        \begin{align}
            (Ax\otimes By)_{ij}&=(Ax)_i(By)_j\\
            &=\sum_{kl}A_{ik}x_kB_{jl}y_l\\
            &=A_{ik}(x\otimes y)_{kl}B_{jl}\\
            &=\big( A(x\otimes y)B^t \big)_{ij}.
        \end{align}
    \end{subequations}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecTQkRXIu}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

Les applications multilinéaires ont déjà été définies en la définition \ref{DefFRHooKnPCT}; nous donnons ici une définition plus explicite dans le cas des applications bilinéaires.
\begin{definition}
    Une \defe{forme bilinéaire}{forme!bilinéaire} sur un espace vectoriel \( E\) est une application \( b\colon E\times E\to \eK\) telle que
    \begin{enumerate}
        \item
            \( b(u,v)=b(v,u)\),
        \item
            \( b(u+v,w)=b(u,w)+b(v,w)\),
        \item
            \( b(\lambda u,v)=\lambda b(u,v)\)
    \end{enumerate}
    pour tout \( u,v,w\in E\) et \( \lambda\in \eK\) où \( \eK\) est une corps commutatif.
\end{definition}

\begin{definition}[\cite{RUAoonJAym}]   \label{DefBSIoouvuKR}
    Soit un espace vectoriel \( E\) et \( \eF\) un corps de caractéristique différente de \( 2\). Une \defe{forme quadratique}{forme!quadratique} sur \( E\) est une application \( q\colon V\to \eF\) pour laquelle il existe une forme bilinéaire symétrique \( b\colon V\times V\to \eF\) satisfaisant \( q(x)=b(x,x)\) pour tout \( x\in V\).

    L'ensemble des formes quadratiques réelles sur \( E\) est noté \( Q(E)\)\nomenclature[B]{\( Q(E)\)}{formes quadratiques réelles sur \( E\)}.
\end{definition}

\begin{lemma}       \label{LEMooLKNTooSfLSHt}
    Si \( q\) est une forme quadratique, il existe une unique forme bilinéaire \( b\) telle que \( q(x)=b(x,x)\).
\end{lemma}

\begin{proof}
    L'existence n'est pas en cause : c'est la définition d'une forme quadratique. Pour l'unicité, étant donné une forme quadratique, la forme bilinéaire \( b\) doit forcément vérifier l'\defe{identités de polarisation}{identité!polarisation}\index{polarisation (identité)} :
\begin{equation}    \label{EqMrbsop}
    b(x,y)=\frac{ 1 }{2}\big( q(x)+q(y)-q(x-y) \big).
\end{equation}
Elle est donc déterminée par \( q\).
\end{proof}
Notons la division par \( 2\) qui est le pourquoi de la demande de la caractéristique différente de \( 2\) pour \( \eF\) dans la définition de forme quadratique.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Topologie}
%---------------------------------------------------------------------------------------------------------------------------

La topologie considérée sur \( Q(E)\) est celle de la norme
\begin{equation}    \label{EqZYBooZysmVh}
    N(q)=\sup_{\| x \|_E=1}| q(x) |,
\end{equation}
qui du point de vue de \( S_n(\eR)\) est
\begin{equation}    
    N(A)=\sup_{\| x \|_E=1}| x^tAx |.
\end{equation}
Notons que à droite, c'est la valeur absolue usuelle sur \( \eR\).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrice associée}
%---------------------------------------------------------------------------------------------------------------------------

Si une base \( \{ e_i \}_{i=1,\ldots, n}\) de l'espace vectoriel \( E\) est donnée, la \defe{matrice associée}{matrice!associée à une forme quadratique}\index{forme!quadratique!matrice associée} à la forme bilinéaire \( b\) sur \( E\) est la matrice d'éléments
\begin{equation}
    B_{ij}=b(e_i,e_j).
\end{equation}
Notons que la matrice associée à une forme bilinéaire (ou quadratique associée) est uniquement valable pour une base donnée. Si nous changeons de base, la matrice change. Cependant lorsque nous travaillons sur \( \eR^n\), la base canonique est tellement canonique que nous allons nous permettre de parler de «la» matrice associée à une forme bilinéaire. 

Si \( B_{ij}\) est la matrice associée à la forme bilinéaire \( b\) alors la valeur de \( b(u,v)\) se calcule avec la formule
\begin{equation}
    b(x,y)=\sum_{i,j}B_{ij}x_iy_j
\end{equation}
lorsque \( x_i\) et \( y_j\) sont les coordonnées de \( x\) et \( y\) dans la base choisie.

\begin{proposition} \label{PropFSXooRUMzdb}
    Soit \( \{ e_i \}\) une base de \( E\). L'application
    \begin{equation}
        \begin{aligned}
            \phi\colon Q(E)&\to S(n,\eR) \\
            q&\mapsto \big(   b(e_i,e_j)   \big)_{i,j}
        \end{aligned}
    \end{equation}
    où \( b\) est forme bilinéaire associée à \( q\) est une bijection linéaire et continue.
\end{proposition}

\begin{proof}
    Si \( \phi(q)=\phi(q')\); alors
    \begin{equation}
        q(x)=\sum_{i,j}\phi(q)_{ij}x_ix_j=\sum_{i,j}\phi(q')_{ij}x_ix_j=q'(x).
    \end{equation}
    Donc \( q=q'\). L'application \( \phi\) est donc injective

    De plus elle est surjective parce que si \( B\in S(n,\eR)\) alors la forme quadratique
    \begin{equation}
        q(x)=\sum_{i,j}B_{ij}x_ix_j
    \end{equation}
    a évidemment \( B\) comme matrice associée. L'application \( \phi\) est donc surjective.

    Notre application \( \phi\) est de plus linéaire parce que l'association d'une forme quadratique à la forme bilinéaire associée est linéaire.

    En ce qui concerne la continuité, nous la prouvons en zéro en considérant une suite convergente \( q_n\stackrel{Q(E)}{\longrightarrow}0\). C'est à dire que
    \begin{equation}
        \sup_{\| x \|=1}| q_n(x) |\to 0.
    \end{equation}
    Nous rappelons l'identité de polarisation : 
    \begin{equation}
        b_n(x,y)=\frac{ 1 }{2}\big( q_n(x-y)-q(x)-q(y) \big).
    \end{equation}
    En ce qui concerne deux des trois termes, il n'y a pas de problèmes :
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|=\big| b_n(e_i,e_j) \big|\leq\frac{ 1 }{2}\big| b_n(e_i-e_j) \big|+\frac{ 1 }{2}\big| q_n(e_i) \big|+\frac{ 1 }{2}\big| q_n(e_j) \big|.
    \end{equation}
    Si \( n\) est assez grand, nous avons tout de suite
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|\leq \frac{ 1 }{2}\big| q_n(e_i-e_j) \big|+\epsilon.
    \end{equation}
    Nous définissons \( e_{ij}\) et \( \alpha_{ij}\) de telle sorte que \( e_i-e_j=\alpha_{ij}e_{ij}\) avec \( \| e_{ij} \|=1\). Si \( \alpha=\max\{ \alpha_{ij},1 \}\) alors nous avons
    \begin{equation}
        q_n(e_i-e_j)=\alpha_{ij}^2q_n(e_{ij})\leq \alpha^2q_n(e_{ij}).
    \end{equation}
    Il suffit maintenant de prendre \( n\) assez grand pour avoir \( \sup_{\| x \|=1}| q_n(x) |\leq \frac{ \epsilon }{ \alpha^2 }\) pour avoir
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|\leq \frac{ \epsilon }{2}+\frac{ \epsilon }{ \alpha^2 }.
    \end{equation}
\end{proof}

\begin{proposition}\label{PropFWYooQXfcVY}
    Dans la base de diagonalisation de sa matrice associée, une forme quadratique a la forme
    \begin{equation}
        q(x)=\sum_i\lambda_ix_i^2
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de la matrice associée à \( q\).
\end{proposition}

\begin{proof}
Soit \( q\) une forme quadratique et \( b\) la forme bilinéaire associée. Si \( \{ f_i \}\) est une base de diagonalisation de la matrice de \( b\) alors dans cette base nous avons
\begin{equation}
    q(x)=b(x,x)=\sum_{ij}x_ix_jb(f_i,f_j)=\sum_i\lambda_ix_i^2
\end{equation}
où les \( \lambda_i\) sont les valeurs propres de la matrice de \( b\).
\end{proof}
Notons que si nous choisissons une autre base de diagonalisation, les \( \lambda_i\) ne changement pas (à part l'ordre éventuellement). Cela pour dire que nous nous permettrons de parler des \defe{valeurs propres}{valeur propre!d'une forme quadratique} d'une forme quadratique comme étant les valeurs propres de la matrice associée.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelques mots à propos de matrices}
%---------------------------------------------------------------------------------------------------------------------------

Si $g$ est une application bilinéaire sur $\eR^2$, nous disons qu'elle est
\begin{enumerate}
\item
\defe{définie positive}{application!définie positive} si $g(u,u)\geq 0$ pour tout $u\in\eR^2$ et $g(u,u)=0$ si et seulement si $u=0$.
\item
\defe{semi-définie positive}{application!semi-définie positive} si $g(u,u)\geq 0$ pour tout $u\in\eR^2$. Nous dirons aussi parfois qu'elle est simplement «positive».
\end{enumerate}
Cela est évidemment à lier à la définition \ref{DefAWAooCMPuVM} : une application bilinéaires est définie positive si et seulement si sa matrice symétrique associée l'est.

\begin{proposition}     \label{PropcnJyXZ}
    Soit $M$, une matrice $2\times 2$ symétrique. Nous avons
    \begin{enumerate}
        \item
        $\det M>0$ et $\tr(M)>0$ implique $M$ définie positive,
        \item
        $\det M>0$ et $\tr(M)<0$ implique $M$ définie négative,
    \item   \label{ItemluuFPN}
        $\det M<0$ implique ni semi définie positive, ni définie négative 
        \item
        $\det M=0$ implique $M$ semi-définie positive ou semi-définie négative.
    \end{enumerate}
\end{proposition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dégénérescence}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( b\), une forme bilinéaire symétrique non dégénérée  sur l'espace vectoriel \( E\) de dimension \( n\) sur \( \eK\) où \( \eK\) est un corps de caractéristique différente de \( 2\). Nous notons \( q\) la forme quadratique associée.

\begin{definition}
    Une forme bilinéaire est \defe{non dégénérée}{forme!bilinéaire!non dégénérée} \( b(x,z)=0\) pour tout \( z\) implique \( x=0\).
\end{definition}

\begin{lemma}   \label{LemyKJpVP}
    Soit \( b\) une forme bilinéaire non dégénérée. Si \( x\) et \( y\) sont tels que \( b(x,z)=b(y,z)\) pour tout \( z\), alors \( x=y\).
\end{lemma}

\begin{proof}
    C'est immédiat du fait de la linéarité en le premier argument et de la non-dégénérescence : si \( b(x,z)-b(y,z)=0\) alors
    \begin{equation}
        b(x-y,z)=0
    \end{equation}
    pour tout \( z\), ce qui implique \( x-y=0\).
\end{proof}

\begin{proposition}
    La forme bilinéaire \( b\) est non-dénénérée si et seulement si sa matrice associée est inversible.
\end{proposition}

\begin{proof}
    Nous savons que la matrice associée est symétrique et qu'elle peut donc être diagonalisée (théorème \ref{ThoeTMXla}). En nous plaçant dans une base de diagonalisation, nous devons prouver que la forme est non-dégénérée si et seulement si les éléments diagonaux de la matrice sont tous non nuls.

    Écrivons \( b(x,z)\) en choisissant pour \( z\) le vecteur de base \( e_k\) de composantes \( (e_k)_j=\delta_{kj}\) :
    \begin{equation}
            b(x,e_k)=\sum_{ij}x_i(e_k)_j
            =\sum_i b_{ik}x_i
            =b_{kk}x_k.
    \end{equation}
    Si \( b\) est dégénérée et si \( x\) est un vecteur non nul (disons que la composante \( x_i\) est non nulle) de \( E\) tel que \( b(x,z)=0\) pour tout \( z\in E\), alors \( b_{ii}=0\), ce qui montre que la matrice de \( b\) n'est pas inversible.

    Réciproquement si la matrice de \( b\) est inversible, alors tous les \( b_{kk}\) sont différents de zéro, et le seul vecteur \( x\) tel que \( b_{kk}x_k=0\) pour tout \( k\) est le vecteur nul.
\end{proof}


\begin{definition}[Isotropie]   \label{DefVKMnUEM}
    Un vecteur est \defe{isotrope}{isotrope (vecteur)} pour \( b\) si il est perpendiculaire à lui-même; en d'autres termes, \( x\) est isotrope si et seulement si \( b(x,x)=0\). Un sous-espace \( W\subset E\) est \defe{totalement isotrope}{isotrope!totalement} si pour tout \( x,y\in W\), nous avons \( b(x,y)=0\).

    Le \defe{cône isotrope}{isotrope!cône} de \( b\) est l'ensemble de ses vecteurs isotropes :
    \begin{equation}
        C(b)=\{ x\in E\tq b(x,x)=0 \}.
    \end{equation}
\end{definition}
Nous introduisons quelque notations. D'abord pour \( y\in E\) nous notons
\begin{equation}
    \begin{aligned}
        \Phi_y\colon E&\to \eR \\
        x&\mapsto b(x,y) 
    \end{aligned}
\end{equation}
et ensuite
\begin{equation}
    \begin{aligned}
        \Phi\colon E&\to E^* \\
        y&\mapsto \Phi_y. 
    \end{aligned}
\end{equation}
\begin{definition}
    Le fait pour une forme bilinéaire \( b\) d'être dégénérée signifie que l'application \( \Phi\) n'est pas injective. Le \defe{noyau}{noyau!d'une forme bilinéaire} de la forme bilinéaire est celui de \( \Phi\), c'est à dire
    \begin{equation}
        \ker(b)=\{ z\in E\tq b(z,y)=0\,\forall y\in E \}.
    \end{equation}
    Autrement dit, \( \ker(b)=E^{\perp}\) où le perpendiculaire est pris par rapport à \( b\).
\end{definition}
Notons tout de même que nous utilisons la notation \( \perp\) même si \( b\) est dégénérée et éventuellement pas positive; c'est à dire même si la formule \( (x,y)\mapsto b(x,y)\) ne fournit pas un produit scalaire.

\begin{proposition}[\cite{RTzQrdx}]     \label{PropHIWjdMX}
    Soit \( b\) une forme bilinéaire et symétrique. Alors
    \begin{enumerate}
        \item
            \( \ker(b)\subset C(b)\) (cône d'isotropie, définition \ref{DefVKMnUEM})
        \item
            si \( b\) est positive alors \( \ker(b)=C(b)\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Si \( z\in\ker(b)\) alors pour tout \( y\in E\) nous avons \( b(z,y)=0\). En particulier pour \( y=z\) nous avons \( b(z,z,)=0\) et donc \( z\in C(b)\).
        \item
            Soit \( b\) positive et \( x\in C(b)\). Par l'inégalité de Cauchy-Schwarz (proposition \ref{ThoAYfEHG}) nous avons
            \begin{equation}
                | b(x,y) |\leq \sqrt{   b(x,x)b(y,y) }=0.
            \end{equation}
            Donc pour tout \( y\) nous avons \( b(x,y)=0\).
    \end{enumerate}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Inégalité de Minkowski}
%---------------------------------------------------------------------------------------------------------------------------

Ce qui est couramment nommé «inégalité de Minkowski» est la proposition \ref{PropInegMinkKUpRHg} dans les espaces \( L^p\). Nous allons en donner ici un cas très particulier.

\begin{proposition} \label{PropACHooLtsMUL}
    Si \( q\) est une forme quadratique sur \( \eR^n\) et si \( x,y\in \eR^n\) alors
    \begin{equation}
        \sqrt{q(x+y)}\leq\sqrt{q(x)}+\sqrt{q(y)}.
    \end{equation}
\end{proposition}

\begin{proof}
    La proposition \ref{PropFWYooQXfcVY} nous permet de «diagonaliser» la forme quadratique \( q\). Quitte à ne plus avoir une base orthonormale, nous pouvons renormaliser les vecteurs de base pour avoir
    \begin{equation}
        q(x)=\sum_ix_i^2.
    \end{equation}
    Le résultat n'est donc rien d'autre que l'inégalité triangulaire pour la norme euclidienne usuelle, laquelle est démontrée dans la proposition \ref{PropEQRooQXazLz}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Ellipsoïde}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}   \label{LemYVWoohcjIX}
    Toute matrice peut être décomposée de façon unique en une partie symétrique et une partie antisymétrique. Cette décomposition est donnée par
\begin{equation}\label{subEqHIQooyhiWM}
    \begin{aligned}[]
            S&=\frac{ M+M^t }{ 2 },&A&=\frac{ M-M^t }{ 2 }
    \end{aligned}
\end{equation}
\end{lemma}

\begin{proof}
    L'existence est une vérification immédiate de \( S+A=M\) en utilisant \eqref{subEqHIQooyhiWM}. Pour l'unicité, si \( S+A=S'+A'\) alors \( S-S'=A-A'\). Mais \( S-S'\) est symétrique et \( A-A'\) est antisymétrique; l'égalité implique l'annulation des deux membres, c'est à dire \( S=S'\) et \( A=A'\).
\end{proof}

\begin{definition}  \label{DefOEPooqfXsE}
    Un \defe{ellipsoïde}{ellipsoïde} dans \( \eR^n\) centré en \( v\) est le lieu des points \( x\) vérifiant l'équation
    \begin{equation}\label{EqSNWooXfbTH}
        (x-v)^t M(x-v)=1
    \end{equation}
    où \( M\) est une matrice symétrique strictement définie positive\footnote{Définition \ref{DefAWAooCMPuVM}.}.

    Lorsque nous parlons d'ellipsoïde \emph{plein}, il suffit de changer l'égalité en une inégalité.
\end{definition}
Une autre façon d'écrire la relation \eqref{EqSNWooXfbTH} est d'écrire \( \langle (x-v),M(x,v)\rangle\) en utilisant le produit scalaire.

\begin{remark}
    Le fait que \( M\) soit symétrique n'est pas tout à fait obligatoire; la chose important est que toutes les valeurs propres soient strictement positives. En effet si \( M\) a toutes ses valeurs propres strictement positives, nous nommons \( S\) la partie symétrique de \( M\) et \( A\) la partie antisymétrique (lemme \ref{LemYVWoohcjIX}). Alors pour tout \( x\in \eR^n\) nous avons
    \begin{equation}
        x^tAx=\langle x, Ax\rangle =\langle A^tx,x \rangle =-\langle Ax, x\rangle =-\langle x,Ax\rangle ,
    \end{equation}
    donc \( x^tAx=0\). L'équation \( x^tMx=1\) est donc équivalente à \( x^tSx=1\) (elles ont les mêmes solutions).
    
    De plus \( S\) reste strictement définie positive parce que pour tout \( x\in \eR^n\) nous avons 
    \begin{equation}
        0<x^tMx=x^tSx.
    \end{equation}
\end{remark}

\begin{proposition}\label{PropWDRooQdJiIr}
    Si \( \ellE\) est un ellipsoïde centrée à l'origine, il existe une base de \( \eR^n\) dans laquelle son équation est :
    \begin{equation}
        \sum_{i=1}^n\frac{ x_i^2 }{ a_i^2 }=1.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous avons une matrice symétrique strictement définie positive \( S\) telle que l'équation soit \( \langle x, Sx\rangle =1\). Le théorème spectral \ref{ThoeTMXla} nous fournit une base orthonormale \( \{ e_i \}\) dans laquelle \( Se_i=\lambda_ie_i\) avec \( \lambda_i>0\). En substituant dans l'équation \( \langle x, Sx\rangle =1\) nous trouvons l'équation
    \begin{equation}
        \sum_i\lambda_ix_i^2=1.
    \end{equation}
    En posant \( a_i=\frac{1}{ \sqrt{\lambda_i} }\), nous trouvons le résultat.  Cette définition des \( a_i\) est toujours possible parce que \( \lambda_i>0\).
\end{proof}

\begin{corollary}   \label{CorKGJooOmcBzh}
    Un ellipsoïde plein centré en l'origine admet une équation de la forme \( q(x)\leq 1\) où \( q\) est une forme quadratique strictement définie positive.
\end{corollary}
Pour rappel de notation, l'ensemble des formes quadratiques strictement définies positives sur l'espace vectoriel \( E\) est noté \( Q^{++}(E)\).

\begin{proof}
    Soit \( \{ e_i \}\) une base de \( \eR^n\) telle que l'ellipsoïde \( \ellE\) ait pour équation
    \begin{equation}
        \sum_{i=1}^n\frac{ x_i^2 }{ a_i^2 }\leq 1.
    \end{equation}
    Nous considérons la forme quadratique
    \begin{equation}
        \begin{aligned}
            q\colon \eR^n&\to \eR \\
            x&\mapsto \sum_{i=1}^n\frac{ \langle x, e_i\rangle^2 }{ a_i^2 }. 
        \end{aligned}
    \end{equation}
    Nous avons évidemment \( \ellE=\{ x\in \eR^n\tq q(x)\leq 1 \}\). De plus la forme \( q\) est strictement définie positive parce que dès que \( x\neq 0\), au moins un des produits scalaires \( \langle x, e_i\rangle \) est non nul et \( q(x)> 0\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème spectral auto-adjoint}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Si \( E\) est un espace euclidien, un endomorphisme \( f\colon E\to E\) est \defe{auto-adjoint}{endomorphisme!auto-adjoint} si pour tout \( x,y\in E\) nous avons \( \langle x, f(y)\rangle=\langle f(x), Y\rangle  \). 
\end{definition}
L'ensemble des opérateurs auto-adjoints de \( E\) est noté \( \gS(E)\)\nomenclature[A]{\( \gS(E)\)}{Les opérateurs auto-adjoints de $E$}. Cette notation provient du fait que dans \( \eR^n\) muni du produit scalaire usuel, les opérateurs auto-adjoints sont les matrices symétriques.

\begin{theorem}[Théorème spectral auto-adjoint] \label{ThoRSBahHH}
    Un endomorphisme auto-adjoint d'un espace euclidien
    \begin{enumerate}
        \item
            est diagonalisable dans une base orthonormée,
        \item
            a son spectre réel.
    \end{enumerate}
\end{theorem}
\index{théorème!spectral!autoadjoint}
\index{diagonalisation!endomorphisme auto-adjoint}

\begin{proof}
    Nous procédons par récurrence sur la dimension de \( E\), et nous commençons par \( n=1\)\footnote{Dans \cite{KXjFWKA}, l'auteur commence avec \( n=0\) mais moi je n'en ai \wikipedia{en}{Vacuous_truth}{pas le courage.}.}. Soit donc \( f\colon E\to E\) avec \( \langle f(x), y\rangle =\langle x, f(y)\rangle \). Étant donné que \( f\) est également linéaire, il existe \( \lambda\in \eR\) tel que \( f(x)=\lambda x\) pour tout \( x\in E\). Tous les vecteurs de \( E\) sont donc vecteurs propres de \( f\).

    Passons à la récurrence. Nous considérons \( \dim(E)=n+1\) et \( f\in\gS(E)\). Nous considérons la forme bilinéaire symétrique \( \Phi_f\) et la forme quadratique associée \( \phi_f\). Pour rappel,
    \begin{subequations}
        \begin{align}
        \Phi_f(x,y)=\langle x, f(y)\rangle \\
        \phi_f(x)=\Phi_f(x,x).
        \end{align}
    \end{subequations}
    Et nous allons laisser tomber les indices \( f\) pour noter simplement \( \Phi\) et \( \phi\). Étant donné que \( \overline{ B(0,1) }\) est compacte et que \( \phi\) est continue, il existe \( x_0\in\overline{ B(0,1) }\) tel que 
    \begin{equation}
        \lambda=\phi(x_0)=\sup_{x\in\overline{ B(0,1) }}\phi(x).
    \end{equation}
    Notons aussi que \( \| x_0 \|=1\) : le maximum est pris sur le bord. Nous posons
    \begin{equation}
        g=\lambda\id-f
    \end{equation}
    ainsi que 
    \begin{equation}
        \Phi_1(x,y)=\langle x, g(y)\rangle .
    \end{equation}
    Cela est une forme bilinéaire et symétrique parce que
    \begin{equation}
        \Phi_1(y,x)=\langle y, g(x)\rangle =\langle g(y), x\rangle =\langle x, g(y)\rangle =\Phi_1(x,y)
    \end{equation}
    où nous avons utilisé le fait que \( g\) était auto-adjoint et la symétrie du produit scalaire. De plus \( \Phi_1\) est semi-définie positive parce que
    \begin{equation}
        \Phi_1(x,x)=\langle x, \lambda x-f(x)\rangle =\lambda\| x \|^2-\phi(x).
    \end{equation}
    Vu que \( \lambda\) est le maximum, nous avons tout de suite \( \Phi_1(x)\geq 0\) tant que \( \| x \|=1\). Et si \( x\) n'est pas de norme \( 1\), c'est le même prix parce qu'on se ramène à \( \| x \|=1\) en multipliant par un nombre positif. Attention cependant : 
    \begin{equation}
        \Phi_1(x_0,x_0)=\lambda\| x_0 \|^2-\phi(x_0)=0.
    \end{equation}
    Donc \( \Phi_1\) a un noyau contenant \( x_0\) par la proposition \ref{PropHIWjdMX}. Nous en déduisons que \( \Image(g)\neq E\) en effet, \( x_0\in\Image(g)^{\perp}\), mais nous avons la proposition \ref{PropXrTDIi} sur les dimensions : 
    \begin{equation}
        \dim E=\dim(\Image(g))+\dim( \Image(g)^{\perp}).
    \end{equation}
    Vu que \( \Image(g)^{\perp}\) est un espace vectoriel non réduit à \( \{ 0 \}\), la dimension de \( \Image(g)\) ne peut pas être celle de \( E\). L'endomorphisme \( g\) n'étant pas surjectif, il ne peut pas être injectif non plus parce que nous sommes en dimension finie; il existe donc \( e_1\in E\) tel que \( g(e_1)=0\) et tant qu'à faire nous choisissons \( \| e_1 \|=1\) (ici la norme est bien celle de l'espace euclidien considéré). Par définition,
    \begin{equation}
        f(e_1)=\lambda e_1,
    \end{equation}
    c'est à dire que \( \lambda\in\Spec(f)\). Et \( \phi\) étant une forme quadratique réelle nous avons \( \lambda\in \eR\).

    Nous posons à présent \( H=\Span\{ e_1 \}^{\perp}\). C'est un sous-espace stable par \( f\) parce que si \( x\in H\) alors
    \begin{equation}
        \langle e_1, f(x)\rangle =\langle f(e_1j),x\rangle =\lambda\langle e_1, x\rangle =0.
    \end{equation}
    Nous pouvons donc considérer la restriction de \( f\) à \( H\) : \( f_H\colon H\to H\). Cet endomorphisme est bilinéaire et symétrique sur l'espace \( H\) de dimension inférieure à celle de \( E\), donc la récurrence nous donne une base orthonormée
    \begin{equation}
        \{ e_2,\ldots, e_n \}
    \end{equation}
    de vecteurs propres de \( f_H\). De plus les valeurs propres sont réelles, toujours par récurrence. Donc
    \begin{equation}
        \Spec(f)=\{ \lambda \}\cup\Spec(f_H)\subset \eR.
    \end{equation}
    Notons pour être complet que si \( i\geq 2\) alors
    \begin{equation}
        \langle e_1, e_i\rangle =0
    \end{equation}
    parce que le vecteur \( e_i\) est par construction choisit dans l'espace \( H=e_1^{\perp}\). Nous avons donc bien une base orthonormée de \( E\) construite sur des vecteurs propres de \( f\).
\end{proof}

\begin{corollary}   \label{CorSMHpoVK}
    Soit \( E\) un espace vectoriel ainsi que \( \phi\) et \( \psi\) des formes quadratiques sur \( E\) avec \( \psi\) définie positive. Alors il existe une base \( \psi\)-orthonormale dans laquelle \( \phi\) est diagonale.
\end{corollary}

\begin{proof}
    Il suffit de considérer l'espace euclidien \( E\) muni du produit scalaire \( \langle x, y\rangle =\psi(x,y)\). Ensuite nous diagonalisons la matrice (symétrique) de \( \phi\) pour ce produit scalaire à l'aide du théorème \ref{ThoRSBahHH}.
\end{proof}

\begin{definition}      \label{DefYNWUFc}
    Dans le cas de \( V=\eR^m\) nous avons un produit scalaire canonique. Soient $u$ et $v$, deux vecteurs de $\eR^m$. Le \defe{produit scalaire}{produit!scalaire} de $u$ et $v$, noté $\langle u, v\rangle $ ou $u\cdot v$ est le réel
	\begin{equation}		\label{EqDefProdScalsumii}
		\langle u, v\rangle =\sum_{k=1}^m u_kv_k=u_1v_1+u_2v_2+\ldots+u_mv_n.
	\end{equation}
\end{definition}

Calculons par exemple le produit scalaire de deux vecteurs de la base canonique : $\langle e_i, e_j\rangle $. En utilisant la formule de définition et le fait que $(e_i)_k=\delta_{ik}$, nous avons
\begin{equation}
	\langle e_i, e_j\rangle =\sum_{k=1}^m\delta_{ik}\delta_{jk}.
\end{equation}
Nous pouvons effectuer la somme sur $k$ en remarquant qu'à cause du $\delta_{ik}$, seul le terme avec $k=i$ n'est pas nul. Effectuer la somme revient donc à remplacer tous les $k$ par des $i$ :
\begin{equation}
	\langle e_i, e_j\rangle =\delta_{ii}\delta_{ji}=\delta_{ji}.
\end{equation}

Une des propriétés intéressantes du produit scalaire est qu'il permet de décomposer un vecteur dans une base, comme nous le montre la proposition suivante.

\begin{proposition}		\label{PropScalCompDec}
	Si nous notons $v_i$ les composantes du vecteur $v$, c'est à dire si $v=\sum_{i=1}^m v_ie_i$, alors nous avons $v_j=\langle v, e_j\rangle $.
\end{proposition}

\begin{proof}
	\begin{equation}		\label{Eqvejscalcomp}
		v\cdot e_j=\sum_{i=1}^m\langle v_ie_i, e_j\rangle =\sum_{i=1}^mv_i\langle e_i, e_j\rangle =\sum_{i=1}^mv_i\delta_{ij}
	\end{equation}
	En effectuant la somme sur $i$ dans le membre de droite de l'équation \eqref{Eqvejscalcomp}, tous les termes sont nuls sauf celui où $i=j$; il reste donc
	\begin{equation}
		v\cdot e_j=v_j.
	\end{equation}
\end{proof}

Le produit scalaire ne dépend en réalité pas de la base orthogonale choisie. 

\begin{lemma}
	Si $\{ e_i \}$ est la base canonique, et si $\{ f_i \}$ est une autre base orthonormale, alors si $u$ et $v$ sont deux vecteurs de $\eR^m$, nous avons
	\begin{equation}
		\sum_i u_iv_j=\sum_iu'_iv'_j
	\end{equation}
	où $u_i$ sont les composantes de $u$ dans la base $\{ e_i \}$ et $u'_i$ sont celles dans la base $\{ f_i \}$.
\end{lemma}

\begin{proof}
	La preuve demande un peu d'algèbre linéaire. Étant donné que $\{ f_i \}$ est une base orthonormale, il existe une matrice $A$ orthogonale ($AA^t=\mtu$) telle que $u'_i=\sum_jA_{ij}u_j$ et idem pour $v$. Nous avons alors
	\begin{equation}
		\begin{aligned}[]
			\sum_iu'_iv'_j&=\sum_i\left( \sum_jA_{ij} u_j\right)\left( \sum_k A_{ik}v_k \right)\\
			&=\sum_{ijk}A_{ij}A_{ik}u_jv_k\\
			&=\sum_{jk}\underbrace{\sum_i(A^t)_{ji}A_{ik}}_{=\delta_{jk}}u_jv_k\\
			&=\sum_{jk}\delta_{jk}u_jv_k\\
			&=\sum_ku_jv_k.
		\end{aligned}
	\end{equation}	
\end{proof}

Cette proposition nous permet de réellement parler du produit scalaire entre deux vecteurs de façon intrinsèque sans nous soucier de la base dans laquelle nous regardons les vecteurs.

Nous dirons que deux vecteurs sont \defe{orthogonaux}{orthogonal} lorsque leur produit scalaire est nul. Nous écrivons que $u\perp v$ lorsque $\langle u, v\rangle =0$.
\begin{definition}	\label{DefNormeEucleApp}
	La \defe{norme euclidienne}{norme!euclidienne!dans $\eR^m$} d'un élément de $\eR^m$ est définie par $\| u \|=\sqrt{u\cdot u}$.
\end{definition}

Cette définition est motivée par le fait que le produit scalaire $u\cdot u$ donne exactement la norme usuelle donnée par le théorème de Pythagore :
\begin{equation}
	u\cdot u=\sum_{i=1}^mu_iu_i=\sum_{i=1}^m u_i^2=u_1^2+u_2^2+\ldots+u_m^2.
\end{equation}

Le fait que $e_i\cdot e_j=\delta_{ij}$ signifie que la base canonique est \defe{orthonormée}{orthonormé}, c'est à dire que les vecteurs de la base canonique sont orthogonaux deux à deux et qu'ils ont tout $1$ comme norme.

\begin{lemma}\label{LemSclNormeXi}
	Pour tout $u\in\eR^m$, il existe un $\xi\in\eR^m$ tel que $\| u \|=\xi\cdot u$ et $\| \xi \|=1$.
\end{lemma}

\begin{proof}
	Vérifions que le vecteur $\xi=u/\| u \|$ ait les propriétés requises. D'abord $\| \xi \|=1$ parce que $u\cdot u=\| u \|^2$. Ensuite
	\begin{equation}
		\xi\cdot u=\frac{ u\cdot u }{ \| u \| }=\frac{ \| u \|^2 }{ \| u \| }=\| u \|.
	\end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces hermitiens}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}  \label{DefMZQxmQ}
Si \( E\) est un espace vectoriel sur \( \eC\), nous disons qu'une application \( \langle ., .\rangle \colon E\times E\to \eC\) est un \defe{produit scalaire hermitien}{produit!scalaire!hermitien}\index{hermitien!produit scalaire} si pour tout \( u,v\in E\) nous avons
\begin{enumerate}
    \item
        \( \langle u, v\rangle =\overline{ \langle v, u\rangle  }\)
    \item
        \( \lambda\langle u, v\rangle =\langle \lambda u, v\rangle =\langle u, \bar \lambda v\rangle \)
    \item
        \( \langle u, u\rangle \in \eR^+\) et \( \langle u, u\rangle =0\) si et seulement si \( u=0\).
\end{enumerate}
\end{definition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Coordonnées cylindriques et sphériques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Les \defe{coordonnées cylindriques}{coordonnées!cylindrique} sont un perfectionnement des coordonnées polaires. Il s'agit simplement de donner le point $(x,y,z)$ en faisant la conversion $(x,y)\mapsto(r,\theta)$ et en gardant le $z$. Les formules de passage sont
\begin{subequations}
	\begin{numcases}{}
		x=r\cos(\theta)\\
		y=r\sin(\theta)\\
		z=z.
	\end{numcases}
\end{subequations}

Les \defe{coordonnées sphériques}{coordonnées!sphériques} sont ce qu'on appelle les «méridiens» et «longitudes» en géographie. Les formules de transformation sont 
\begin{subequations}		\label{SubEqsCoordSphe}
	\begin{numcases}{}
		x=\rho\sin(\theta)\cos(\varphi)\\
		y=\rho\sin(\theta)\sin(\varphi)\\
		z=\rho\cos(\theta)
	\end{numcases}
\end{subequations}
avec $0\leq\theta\leq\pi$ et $0\leq\varphi<2\pi$.

\begin{remark}
	Attention : d'un livre à l'autre les conventions sur les noms des angles changent. N'essayez donc pas d'étudier par cœur des formules concernant les coordonnées sphériques trouvées autre part. Par exemple sur le premier dessin de \href{http://fr.wikipedia.org/wiki/Coordonnées_sphériques}{wikipédia}, l'angle $\varphi$ est noté $\theta$ et l'angle $\theta$ est noté $\Phi$. Mais vous noterez que sur cette même page, les convention de noms de ces angles changent plusieurs fois.
\end{remark}

Trouvons le changement inverse, c'est à dire trouvons $\rho$, $\theta$ et $\varphi$ en termes de $x$, $y$ et $z$. D'abord nous avons
\begin{equation}
	\rho=\sqrt{x^2+y^2+z^2}.
\end{equation}
Ensuite nous savons que
\begin{equation}
	\cos(\theta)=\frac{ z }{ \rho }
\end{equation}
détermine de façon unique\footnote{Le problème $\rho=0$ ne se pose pas; pourquoi ?} un angle $\theta\in\mathopen[ 0 , \pi \mathclose]$. Dès que $\rho$ et $\theta$ sont connus, nous pouvons poser $r=\rho\sin\theta$ et alors nous nous trouvons avec les équations
\begin{subequations}
	\begin{numcases}{}
		x=r\cos(\varphi)\\
		y=r\sin(\varphi),
	\end{numcases}
\end{subequations}
qui sont similaires à celles déjà étudiées dans le cas des coordonnées polaires.

% TODO: Ajouter un texte sur les équations de plan, et pourquoi ax+by+cz+d=0 est perpendiculaire au vecteur (a,b,c).

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Méthode de Gauss pour résoudre des systèmes d'équations linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Pour résoudre un système d'équations linéaires, on procède comme suit:
\begin{enumerate}
\item Écrire le système sous forme matricielle. \[\text{p.ex. } \begin{cases} 2x+3y &= 5 \\ x+2y &= 4 \end{cases} \Leftrightarrow \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right) \]
\item Se ramener à une matrice avec un maximum de $0$ dans la partie de gauche en utilisant les transformations admissibles:
\begin{enumerate}
\item Remplacer une ligne par elle-même + un multiple d'une autre;
\[\text{p.ex. } \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{L_1  - 2. L_2 \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right) \]
\item Remplacer une ligne par un multiple d'elle-même;
\[\text{p.ex. } \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{-L_1  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 2 & 4 \end{array}\right) \]
\item Permuter des lignes.
\[\text{p.ex. } \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 0 & -2 \end{array}\right)  \stackrel{L_1  \mapsto L_2' \text{ et } L_2  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \]
\end{enumerate}
\item Retransformer la matrice obtenue en système d'équations.
\[\text{p.ex. }  \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \Leftrightarrow \begin{cases} x &= -2 \\ y &= 3 \end{cases}  \]
\end{enumerate}

\begin{remark}
\begin{itemize}
\item Si on obtient une ligne de zéros, on peut l'enlever:
\[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right) \Leftrightarrow  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \end{array}\right) \]
\item Si on obtient une ligne de zéros suivie d'un nombre non-nul, le système d'équations n'a pas de solution:
\[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 7 \end{array}\right) \Leftrightarrow  \begin{cases} \cdots \\ \cdots \\ 0x + 0y + 0z = 7 \end{cases} \Rightarrow \textbf{Impossible} \]
\item Si on moins d'équations que d'inconnues, alors il y a une infinité de solutions qui dépendent d'un ou plusieurs paramètres:
\[\text{p.ex. }  \left(\begin{array}{ccc|c} 1 & 0 & -2 & 2 \\ 0 & 1 & 3 & 0 \end{array}\right) \Leftrightarrow  \begin{cases} x - 2z = 2 \\ y + 3z = 0 \end{cases} \Leftrightarrow  \begin{cases} x = 2 + 2\lambda \\ y = -3\lambda \\ z = \lambda \end{cases} \]
\end{itemize}
\end{remark}
