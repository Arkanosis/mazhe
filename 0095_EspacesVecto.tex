% This is part of Mes notes de mathématique
% Copyright (c) 2011-2015
%   Laurent Claessens, Carlotta Donadello
% See the file fdl-1.3.txt for copying conditions.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Matrice compagnon et endomorphismes cycliques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Matrice compagnon}
%---------------------------------------------------------------------------------------------------------------------------

Soit le polynôme \( P=X^n-a_{n-1}X^{n-1}-\ldots-a_1X-a_0\) dans \( \eK[X]\). La \defe{matrice compagnon}{matrice!compagnon} de \( P\) est la matrice\nomenclature[A]{\( C(P)\)}{matrice compagnon} donnée par
\begin{equation}
    C(P)=\begin{pmatrix}
        0    &   \cdots    &   \cdots    &   0    &   a_0\\  
        1    &   0    &       &   \vdots    &   a_1\\  
        0    &   \ddots    &   \ddots    &   \vdots    &   \vdots\\  
        \vdots    &   \ddots    &   \ddots    &   0    &   a_{n-2}\\  
        0    &   \cdots    &   0    &   1    &   a_{n-1}    
    \end{pmatrix}
\end{equation}
si \( n\geq 2\) et par \( (a_0)\) si \( n=1\). Si \( f\) est l'endomorphisme associé à la matrice \( C(P)\) nous avons
\begin{equation}
    f(e_i)=\begin{cases}
        e_{i+1}    &   \text{si \( i<n\)}\\
        (a_0,\ldots, a_{n-1})    &    \text{si \( i=n\)}.
    \end{cases}
\end{equation}
Cet endomorphisme est conçu pour vérifier \( P(f)e_1=0\).

\begin{lemma}[\cite{RapportArgreg2011}] \label{LemkVNisk}
    Un polynôme sur un corps commutatif est le polynôme caractéristique de sa matrice compagnon. En d'autres termes nous avons \( \chi_{C(P)}=P\).
\end{lemma}

\begin{proof}
    Nous notons \( f\) l'endomorphisme associé à \( C(P)\). La propriété \( P(f)e_1=0\) nous indique que le polynôme minimal ponctuel de \( f\) en \( e_1\) divise \( P\). L'ensemble des puissances de \( f\) appliquées à \( e_1\), \( \big( f^i(e_1) \big)_{i=1,\ldots, n-1}\) est libre, donc le polynôme minimal ponctuel en \( e_1\) est de degré \( n\) au minimum. En reprenant les notations du théorème \ref{ThoCCHkoU}, nous avons \( I_{e_1}=(P)\) parce que \( P\) est de degré minimum dans \( I_{e_1}\) et \( \chi_f\in I_{e_1}\).

    Donc \( P\) divise \( \chi_f\) et est de degré égal à celui de \( \chi_f\). Étant donné qu'ils sont tous deux unitaires, ils sont égaux.
\end{proof}

\begin{remark}  \label{RemmQjZOA}
    Les matrices compagnons ne sont pas les seules dont le polynôme caractéristique est égal au polynôme minimal. En fait les matrices dont le polynôme caractéristique est égale au polynôme minimal sont denses dans les matrices. En effet une matrice dont le polynôme minimal n'est pas égal au polynôme caractéristique a un polynôme caractéristique avec une racine double. Il est possible, en modifiant arbitrairement peu la matrice de séparer la racine double en deux racines distinctes.
\end{remark}

\begin{definition}[Matrices, endomorphismes et vecteurs cycliques]
    Une matrice est \defe{cyclique}{cyclique!matrice}\index{matrice!cyclique} si elle est semblable à une matrice compagnon. Un endomorphisme \( f\colon E\to E\) est \defe{cyclique}{cyclique!endomorphisme}\index{endomorphisme!cyclique} si il existe un vecteur \( x\in E\) tel que \( \{ f^k(x)\tq k=1,\ldots, n-1 \}\) est une base de \( E\). Un vecteur ayant cette propriété est un \defe{vecteur cyclique}{vecteur!cyclique} pour \( f\).
\end{definition}

\begin{lemma}\label{LemSGmdnE}
    Si \( A\) est la matrice de l'endomorphisme \( f\) alors nous avons équivalence des propriétés suivantes :
    \begin{enumerate}
        \item
            La matrice \( A\) est cyclique.
        \item
            L'endomorphisme \( f\) est cyclique.
        \item
            Le polynôme caractéristique de \( A\) est égal à son polynôme caractéristique.
    \end{enumerate}
\end{lemma}

\begin{lemma}   \label{LemAGZNNa}
    Si \( f\colon E\to E\) est un endomorphisme cyclique et si \( y\) est un vecteur cyclique de \( f\), alors le polynôme minimal de \( f\) est égal au polynôme minimal de \( f\) au point \( y\) : \( \mu_{f}=\mu_{f,y}\).
\end{lemma}

\begin{proof}
    Montrons que \( \mu_{f,y}\) est un polynôme annulateur de \( f\), ce qui prouvera que \( \mu(f)\) divise \( \mu_{f,y}\). Étant donné que \( y\) est cyclique, tout élément de \( E\) s'écrit sous la forme \( x=Q(f)y\). Prenons un polynôme \( P\) annulateur de \( f\) en \( y\) : \( P(f)y=0\). Nous montrons que \( P\) est alors un polynôme annulateur de \( f\). En effet, nous avons
    \begin{equation}
        P(f)x=\big( P(f)\circ Q(f) \big)y=\big( Q(f)\circ P(f) \big)y=0
    \end{equation}
    où nous avons utilisé le lemme \ref{LemQWvhYb}.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Réduction de Frobenius}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Réduction de Frobenius \cite{AutourFrobCompa,Vialivs,MoncetIVS}]      \index{réduction!Frobénius}\index{Frobénius!réduction}
    Soit \( E\), un \( \eK\)-espace vectoriel où \( \eK\) est \( \eR\) ou \( \eC\), et \( f\in \End(E)\). Alors il existe une suite de sous-espaces \( E_1,\ldots, E_r\) stables par \( f\) tels que
    \begin{enumerate}
        \item   \label{ItemmpwjnSs}
            \( E=\bigoplus_{i=1}^rE_i\);
        \item
            pour chaque \( E_i\), l'endomorphisme restreint \( f_i=f|_{E_i}\) est cyclique;
        \item
            si \( \mu_i\) est le polynôme minimal de \( f_i\) alors \( \mu_{i+1}\) divise \( \mu_i\);
    \end{enumerate}
    Une telle décomposition vérifie automatiquement \( \mu_1=\mu_f\) et \( \mu_1\cdots \mu_r=\chi_f\), et la suite \( (\mu_i)_{i=1,\ldots, r}\) ne dépend que de \( f\) et non du choix de la décomposition du point \ref{ItemmpwjnSs}.
\end{theorem}

Les polynômes \( \mu_i\) sont les \defe{invariants de similitude}{invariant!de similitude} de l'endomorphisme \( f\).

\begin{proof}
    Nous commençons par montrer que si une telle décomposition existe, alors
    \begin{subequations}    \label{subEqzcGouz}
        \begin{align}
            \chi_f=\prod_{i=1}^r\mu_i  \label{EqTaxsvb}\\
            \mu_f=\mu_1
        \end{align}
    \end{subequations}
    où \( \chi_f\) est le polynôme caractéristique de \( f\) et \( \mu_f\) est le polynôme minimal. D'abord le polynôme caractéristique de \( f\) devra être égal au produit des polynômes caractéristique des \( f|_{E_i}\), mais ces derniers endomorphismes étant cycliques, leurs polynôme caractéristiques sont égaux à leurs polynômes minimaux (lemme \ref{LemSGmdnE}). Cela prouve l'égalité \eqref{EqTaxsvb}. Ensuite tous les \( \mu_i\) doivent diviser le polynôme minimal, donc \( \ppcm(\mu_1,\ldots, \mu_r)\) divise \(\mu_f\). Cependant le polynôme minimal doit contenir une et une seule fois chacun des facteurs irréductibles du polynôme caractéristique, et chacun de ces facteurs sont dans les polynômes \( \mu_i\). Par conséquent \( \ppcm(\mu_1,\ldots, \mu_r)=\mu_f\). Mais par ailleurs \( \mu_1=\ppcm(\mu_1,\ldots, \mu_r)\) parce qu'on a supposé \( \mu_{i+1}\divides \mu_i\), donc \( \mu_1=\mu_f\).
    
    Soit \( d\), le degré du polynôme minimal de \( f\) et \( y\in E\) tel que \( \mu_f=\mu_{f,y}\) (voir lemme \ref{LemSYsJJj}). Le plus petit espace stable sous \( f\) contenant \( y\) est
    \begin{equation}
        E_y=\Span\{ y,f(y),\ldots, f^{d-1}(y) \}.
    \end{equation}
    Nous notons \( e_i=f^{i-1}(y)\). Notons que les vecteurs donnés forment bien une base de \( E_y\) parce que si les \( e_i\) n'était pas linéairement indépendants, alors nous aurions des \( a_k\) tels que \( \sum_ka_ke_k=0\) et avec lesquels
    \begin{equation}
        \big( \sum_ka_kX^k \big)(f)y=0,
    \end{equation}
    ce qui contredirait la minimalité de \( \mu_{f,y}\).

    La difficulté du théorème est de trouver un complément de \( E_y\) qui soit également stable sous \( f\). Nous commençons par étendre\quext{Pour autant que j'aie compris, cette extension manque dans \cite{AutourFrobCompa}. Corrigez moi si je me trompe.} \( \{ e_1,\ldots, e_d \}\) en une base \( \{ e_1,\ldots, e_n \}\) de \( E\). Ensuite nous allons montrer que
    \begin{equation}
        E=E_y\oplus F
    \end{equation}
    avec
    \begin{equation}
        F=\{ x\in E\tq  e^*_d\big( f^k(x) \big)=0\forall k\in \eN \}.
    \end{equation}
    Par construction, \( F\) est invariant sous \( f\). Montrons pour commencer que \( E_y\cap F=\{ 0 \}\). Un élément de \( E_y\) s'écrit
    \begin{equation}
        z=a_1e_1+\ldots +a_ke_k
    \end{equation}
    avec \( k\leq d\). Étant donné que \( f\) décale les vecteurs de base, nous avons \( e^*_d\big( f^{d-k}(z) \big)=a_k\). Du coup \( z\in F\) si et seulement si \( a_1=\ldots=a_d=0\), c'est à dire que \( E_y\cap F=\{ 0 \}\).

    Nous montrons maintenant que \( \dim F=n-d\). Pour cela nous considérons l'application
    \begin{equation}
        \begin{aligned}
            T\colon \eK[F]&\to E^* \\
            g&\mapsto e^*_d\circ g. 
        \end{aligned}
    \end{equation}
    Cette application est injective. En effet un élément général de \( \eK[f]\) est
    \begin{equation}
        g=a_1\id+a_2f+\ldots +a_pf^{p-1}
    \end{equation}
    avec \( p\leq d\). Si \( T(g)=0\), alors nous avons en particulier
    \begin{equation}
        0=T(g)e_{_d-p+1}=e^*_d(a_1e_{d-p+1}+a_2e_{d-p+2}+\ldots +a_pe_d)=a_p.
    \end{equation}
    Donc \( a_p=0\) et en appliquant maintenant \( T(g)\) à \( e_{d-p}\) nous obtenons \( a_{p-1}=0\). Au final nous trouvons que \( g=0\) et donc que \( T\) est injective.

    Étant donné que \( \dim\eK[f]=d\) et que \( T\) est injective, \( \dim\Image(T)=d\). Nous regardons l'orthogonal de l'image :
    \begin{subequations}
        \begin{align}
            (\Image(T))^{\perp}&=\{ x\in E\tq T(g)x=0\forall g\in\eK[f] \}\\
            &=\{ x\in E\tq e^*_d\big( g(x) \big)=0\forall g\in \eK[f] \}\\
            &=F.
        \end{align}
    \end{subequations}
    Par conséquent \( F^{\perp}=\Image(T)\). Vu que \( \dim\Image(T)=d\), nous avons donc \( \dim F=n-d\) et il est établi que \( E=E_y\oplus F\). 

    Nous avons donc trouvé \( F\), stable par \( f\) et tel que \( E=E_y\oplus F\). Nous devons maintenant nous assurer que cette décomposition tombe bien pour les polynômes minimaux. Si \( P_1\) est le polynôme minimal de \( f|_{E_yj}\), alors par le lemme \ref{LemAGZNNa} nous avons \( P_1=\mu_{f,y}=\mu_f\) parce que \( f|_{E_y}\) est cyclique sur \( E_y\). Mettons \( P_2\), le polynôme minimal de \( f|_F\). Étant attendu que \( F\) est stable par \( f\), le polynôme \( P_2\) divise \( P_1\). En recommençant la construction sur \( F\), nous construisons un nouvel espace \( F'\) stable sous \( F\) et vérifiant \( \mu_{f|_{F'}}=P_2\), etc.

    Nous passons maintenant à la partie unicité du théorème. Soient deux suites \( F_1,\ldots, F_r\) et \( G_1,\ldots, G_s\) de sous-espaces stables par \( f\) et vérifiant
    \begin{enumerate}
        \item
            \( E=\bigoplus_{i=1}^rF_i\),
        \item
            \( f|_{F_i}\) est cyclique,
        \item
            \( \mu_{f|_{F_{i+1}}}\) divise \( \mu_{f|_{F_i}}\),
    \end{enumerate}
    et, \emph{mutatis mutandis}, les mêmes conditions pour la famille \( \{ G_i \}\). Nous posons \( P_i=\mu_{f_{F_i}}\) et \( Q_i=\mu_{f|_{G_i}}\). Nous allons montrer par récurrence que \( P_i=Q_i\) et \( \dim F_i=\dim G_i\). Il ne sera cependant pas garanti que \( F_i=G_i\). D'abord, \( P_1=Q_1\) parce qu'ils sont tous deux égaux à \( \mu_f\) par les relations \eqref{subEqzcGouz}. Nous supposons que \( P_i=Q_i\) pour \( i\leq 1\leq j-1\) et nous tentons de montrer que \( P_j=Q_j\).

    Nous avons 
    \begin{equation}    \label{EqMrCtZO}
        P_j(f)=P_j(f)|_{F_1}\oplus\ldots\oplus P_j(f)|_{F_{j-1}}.
    \end{equation}
    En effet étant donné que \( P_{j+k}\) divise \( P_j\), nous avons\footnote{En vertu du lemme \ref{LemQWvhYb}.} \( P_{j}(f)=A(f)\circ P_{j+k}(f)\), mais \( P_{j+k}(f)F_{j+k}=0\), donc \( P_j(f)F_{j+k}=0\). Les espaces \( G_i\) n'ayant a priori aucun rapport avec les polynômes \( P_i\), nous écrivons
    \begin{equation}    \label{EqJreLiO}
        P_j(f)=P_j(f)|_{G_1}\oplus\ldots\oplus P_j(f)|_{G_{j-1}}\oplus P_j(f)|_{G_j}\oplus\ldots\oplus P_j(f)|_{G_s}.
    \end{equation}
    Pour \( 1\leq i\leq j-1\), nous avons supposé \( P_i=Q_i\). Étant donné que \( f|_{F_i}\) est semblable à \( C_{_i}\) et \( f|_{G_i}\) est semblable à \( C_{Q_i}\), la matrice de \( f|_{E_i}\) est semblable à la matrice de \( f|_{G_i}\). En particulier,
    \begin{equation}
        \dim P_j(f)F_i=\dim P_j(f)G_i.
    \end{equation}
    En prenant les dimensions des images dans les égalités \eqref{EqMrCtZO} et \eqref{EqJreLiO}, nous trouvons que
    \begin{equation}
        P_j(f)|_{G_j}=\ldots=P_j(f)|_{G_s}=0.
    \end{equation}
    Par conséquent \( P_j\in I_{f|G_j}\) et donc \( P_j\) divise \( Q_j\), qui est générateur de \( I_{f|_{G_j}}\). La situation étant symétrique entre \( P\) et \( Q\), nous montrons de même que \( Q_j\) divise \( P_j\) et donc que \( P_j=Q_j\).

    Ceci achève la démonstration du théorème de réduction de Frobenius.

\end{proof}


Sous forme matricielle, ce théorème dit que toute matrice est semblable à une matrice de la forme bloc-diagonale
\begin{equation}
    f=\begin{pmatrix}
        C_{\mu_1}    &       &       \\
            &   \ddots    &       \\
            &       &   C_{\mu_r}
    \end{pmatrix}
\end{equation}

\begin{remark}
    Si nous travaillons sur \( \eR\), la réduite de Frobenius restera une matrice réelle, même si les valeurs propres sont complexes. En effet le procédé de Frobenius ne regarde absolument pas les valeurs propres, mais seulement les facteurs irréductibles du polynôme caractéristique. La réduite de Frobenius ne tente pas de résoudre ces polynômes, mais se contente d'en utiliser les matrices compagnon.

    La situation sera différente dans le cas de la forme normale de Jordan.
\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Forme normale de Jordan}
%---------------------------------------------------------------------------------------------------------------------------

Il existe une preuve directe de la réduction de Jordan ne nécessitant pas la réduction de Frobenius\cite{LecLinAlgAllen}. Cette dernière passe par les espaces caractéristiques\footnote{Aussi appelés «espaces propres généralisés».} et est à mon avis plus compliquée que la démonstration de Frobenius elle-même. Nous allons donc nous contenter de donner la réduction de Jordan comme un cas particulier de Frobenius.

\begin{theorem}[Réduction de Jordan]        \label{ThoGGMYooPzMVpe}
    Soit \( E\) un espace vectoriel sur \( \eK\), et \( f\in\End(E)\) un endomorphisme dont le polynôme caractéristique \( \chi_f\) est scindé\footnote{C'est pour cette hypothèse que \( \eK=\eR\) n'est pas le bon cadre.}. Il existe une base de \( E\) dans laquelle la matrice de \( f\) s'écrit sous la forme
    \begin{equation}
        M=\begin{pmatrix}
            J_{n_1}(\lambda_1)    &       &       \\
                &   \ddots    &       \\
                &       &   J_{n_k}(\lambda_k)
        \end{pmatrix}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( f\) (avec éventuelle répétitions) et \( J_n(\lambda)\) représente le bloc \( n\times n\)
    \begin{equation}
        J_n(\lambda)=\begin{pmatrix}
            \lambda    &   1    &       &       &   \\  
                &   \lambda    &   1    &       &   \\  
                &       &   \lambda    &       &   \\  
                &       &       &   \ddots    &   1\\  
                &       &       &       &   \lambda    
        \end{pmatrix}.
    \end{equation}
    En d'autres termes, \( J_n(\lambda)_{ii}=\lambda\) et \( J_n(\lambda)_{i-1,i}=1\).    
\end{theorem}
\index{réduction!Jordan}
\index{Jordan!réduction}

\begin{proof}
    Nous commençons par le cas où \( f\) est nilpotente; nous notons \( M\) sa matrice. Dans ce cas la seule valeur propre est zéro et le polynôme caractéristique est \( X^m\) pour un certain \( m\). Nous savons par le lemme \ref{LemkVNisk} que (la matrice de) \( f\) est semblable à sa matrice compagnon. En l'occurrence pour \( f\) nous avons
    \begin{equation}
        C_{X^m}=\begin{pmatrix}
             0   &       &       &  0     \\
             1   &   \ddots    &       &   \vdots    \\
                &   \ddots    &   \ddots    &    \vdots   \\ 
                &       &   1    &   0     
         \end{pmatrix}.
    \end{equation}
    Ensuite le changement de base (qui est une similitude) \( (e_1,\ldots, e_n)\mapsto(e_n,\ldots, e_1)\) montre que \( C_{X^m}\) est semblable à un bloc de Jordan \( J_m(0)\).

    Supposons à présent que \( f\) ne soit pas nilpotente. Par l'hypothèse de polynôme caractéristique scindé, nous supposons que \( f\) a \( m\) valeurs propres distinctes et que son polynôme caractéristique est
    \begin{equation}
        \chi_f=(X-\lambda_1)^{l_1}\ldots (X-\lambda_m)^{l_m}.
    \end{equation}
    Le lemme des noyaux (théorème \ref{ThoDecompNoyayzzMWod}) nous enseigne que
    \begin{equation}
        E=\bigoplus_{i=1}^m\underbrace{\ker(f-\mu_i\mtu)^{l_i}}_{F_i}.
    \end{equation}
    La restriction de \( f-\lambda_i\mtu\) à \( F_i\) est par construction un endomorphisme nilpotent, et donc peut s'écrire comme un bloc de Jordan avec des zéros sur la diagonale. En utilisant la décomposition
    \begin{equation}
        f|_{F_i}=(f-\lambda_i\mtu)|_{F_i}+\lambda_i\mtu_{F_i},
    \end{equation}
    nous voyons que \( f|_{F_i}\) s'écrit comme un bloc de Jordan avec \( \lambda_i\) sur la diagonale.
\end{proof}

\begin{remark}
    Nous pouvons calculer la forme normale de Jordan pour une matrice complexe ou réelle, mais dans les deux cas nous devons nous attendre à obtenir une matrice complexe parce que les valeurs propres d'une matrice réelle peuvent être complexes. Cependant nous demandons que le polynôme caractéristique de \( f\) soit scindé sur \( \eK\). En pratique, la décomposition de Jordan n'est garantie que sur les corps algébriquement clos, c'est à dire sur \( \eC\).

    La suite des invariants de similitude sur laquelle repose Frobenius, elle, est disponible sur tout corps, y compris \( \eR\).
\end{remark}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Calcul différentiel dans un espace vectoriel normé}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecLStKEmc}

Nous développons dans cette section le concept de différentielle de fonction de et vers des espaces vectoriels normés au lieu de \( \eR^n\).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Différentielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}  \label{DefKZXtcIT}
    Soit une application \( f\colon E\to F\) entre deux espaces de Banach. Nous disons que \( f\) est \defe{différentiable}{différentiable!dans un Banach} en \( a\in E\) si il existe une application linéaire continue\footnote{Nous demandons bien que le candidat différentielle soit continue; en dimension infinie ce n'est pas le cas de toutes les fonctions linéaires, comme le montre l'exemple \ref{ExHKsIelG}.} \( T\colon E\to F\) telle que
    \begin{equation}\label{EqIQuRGmO}
        \lim_{h\to 0} \frac{ f(a+h)-f(a)-T(h) }{ \| h \| }=0.
    \end{equation}
\end{definition}

L'application \( a\mapsto T\) est la \defe{différentielle}{différentielle} de \( f\) au point \( a\) et est notée \( df_a\). L'application différentielle
\begin{equation}
    \begin{aligned}
        df\colon E&\to \aL(E,F) \\
        a&\mapsto df_a 
    \end{aligned}
\end{equation}
est également très importante. 

\begin{definition}      \label{DefJYBZooPTsfZx}
Une application \( f\colon E\to F\) est de \defe{classe \( C^1\)}{classe $C^1$} lorsque l'application différentielle \( df\colon E\to \aL(E,F)\) est continue. Voir aussi les définitions \ref{DefPNjMGqy} pour les applications de classe \( C^k\).
\end{definition}

\begin{remark}      \label{RemATQVooDnZBbs}
    L'application norme étant continue, le critère du théorème \ref{ThoWeirstrassRn} est en réalité assez général. Par exemple à partir d'une application différentiable\footnote{Définition \ref{DefKZXtcIT}.} \( f\colon X\to Y\)  nous pouvons considérer la fonction réelle
    \begin{equation}
        a\mapsto \|  df_a   \|
    \end{equation}
    où la norme est la norme opérateur\footnote{Définition \ref{DefNFYUooBZCPTr}.}. Si \( f\) est de classe \( C^1\) alors cette application est continue et donc bornée sur un compact \( K\) de \( X\).
\end{remark}


%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{(non ?) Différentiabilité des applications linéaires}
%---------------------------------------------------------------------------------------------------------------------------

Si \( E\) et \( F\) sont deux espaces vectoriels nous notons \( \aL(E,F)\)\nomenclature[Y]{\( \aL(E,F)\)}{Les applications linéaires de \( E\) vers \( F\)} l'ensemble des applications linéaires de \( E\) vers \( F\) et \( \cL(E,F)\)\nomenclature[Y]{\( \cL\)}{Les applications linéaires continues de \( E\) vers \( F\)} l'ensemble des applications linéaires continues de \( E\) vers \( F\). Ces espaces seront bien entendu, sauf mention du contraire, toujours munis de la norme opérateur de l'exemple \ref{ExemdefnormpMrt}. 

\begin{example}[Une application linéaire non continue]  \label{ExHKsIelG}
    Soit \( V\) l'espace vectoriel normé des suites \emph{finies} de réels muni de la norme usuelle $\| c \|=\sqrt{\sum_{i=0}^{\infty}| c_i |^2}$ où la somme est finie. Nous nommons \( \{ e_k \}_{k\in \eN}\) la base usuelle de cet espace, et nous considérons l'opérateur \( f\colon V\to V\) donnée par \( f(e_k)=ke_k\). C'est évidemment linéaire, mais ce n'est pas continu en zéro. En effet la suite \( u_k=e_k/k\) converge vers \( 0\) alors que \( f(u_k)=e_k\) ne converge pas.
\end{example}

Cet exemple aurait pu également être donnée dans un espace de Hilbert, mais il aurait fallu parler de domaine.
%TODO : le faire, et regarder si Hilbet n'est pas la complétion de cet espace. Référencer à l'endroit qui définit l'espace vectoriel librement engendré. Ici ce serait par N.

%TODO : dire qu'une application bilinéaire sur RxR n'est pas une application linéaire sur R^2

\begin{example}[Une autre application linéaire non continue\cite{GTkeGni}]
    En dimension infinie, une application linéaire n'est pas toujours continue. Soit \( E\) l'espace des polynômes à coefficients réels sur \( \mathopen[ 0 , 1 \mathclose]\) muni de la norme uniforme. L'application de dérivation \( \varphi\colon E\to E\), \( \varphi(P)=P'\) n'est pas continue.

    Pour la voir nous considérons la suite \( P_n=\frac{1}{ n }X^n\). D'une part nous avons \( P_n\to 0\) dans \( E\) parce que \( P_n(x)=\frac{ x^n }{ n }\) avec \( x\in \mathopen[ 0 , 1 \mathclose]\). Mais en même temps nous avons \( \varphi(P_n)=X^{n-1}\) et donc \( \| \varphi(P_n) \|=1\).

    Nous n'avons donc pas \( \lim_{n\to \infty} \varphi(P_n)=\varphi(\lim_{n\to \infty} P_n)\) et l'application \( \varphi\) n'est pas continue en \( 0\). Elle n'est donc continue nulle part par linéarité.

    Nous avons utilisé le critère séquentiel de la continuité, voir la définition \ref{DefENioICV} et la proposition \ref{PropFnContParSuite}.
\end{example}

Nous avons cependant le résultat suivant.
\begin{proposition}[\cite{GKPYTMb} Continue si et seulement si bornée] \label{PropmEJjLE}
    Soient \( E\) et \( F\) des espaces vectoriels normés, et \( u\colon E\to F\) une application linéaire. Alors \( u\) est bornée\footnote{Au sens où \( \| u \|<\infty\) pour la norme opérateur.} si et seulement si elle est continue.
\end{proposition}
\index{opérateur!linéaire!borné}
\index{application!linéaire!bornée}

\begin{proof}
    Nous commençons par supposer que \( u\) est bornée. Pour tout \( x,y\in E\) nous avons
    \begin{equation}
        \| u(x)-u(y) \|=\| u(x-y) \|\leq \| u \|\| x-y \|.
    \end{equation}
    En particulier si \( x_n\stackrel{E}{\longrightarrow}x\) alors
    \begin{equation}
        0\leq \| u(x_n)-u(x) \|\leq \| u \|\| x-x_n \|\to 0
    \end{equation}
    et \( u\) est continue en vertu de la caractérisation séquentielle de la continuité, proposition \ref{PropFnContParSuite}.

Supposons maintenant que \( \| u \|\) ne soit pas borné, c'est à dire que l'ensemble \( \{ \| u(x) \|\tq \| x \|=1 \}\) ne soit pas borné. Alors pour tout \( k\geq 1\) il existe \( x_k\in B(0,1)\) tel que \( \| u(x_k) \|>k\). La suite \( x_k/k\) tend vers zéro parce que \( \| x_k \|=1\), mais \( \| u(x_k) \|\geq 1\) pour tout \( k\). Cela montre que \( u\) n'est pas continue.
\end{proof}

\begin{remark}  \label{RemOAXNooSMTDuN}
Cette proposition permet de retrouver l'exemple \ref{ExHKsIelG} plus simplement. Si \( \{ e_k \}_{k\in \eN}\) est une base d'un espace vectoriel normé formée de vecteurs de norme \( 1\), alors l'opérateur linéaire donné par \( u(e_k)=ke_k\) n'est pas borné et donc pas continu.
\end{remark}

C'est également ce résultat qui montre que le produit scalaire est continu sur un espace de Hilbert par exemple.

\begin{lemma}
    Si \( f\) est linéaire et différentiable alors \( df_a(u)=f(u)\).
\end{lemma}

\begin{proof}
    En effet la linéarité de \( f\) donne
    \begin{equation}
        f(a+h)-f(a)-f(h)=0
    \end{equation}
    pour tout \( h\). Donc la limite \eqref{EqIQuRGmO} est nulle. Les applications linéaires non continues ne sont donc pas différentiables.
\end{proof}

\begin{lemma}   \label{LemLLvgPQW}
    Une application linéaire continue est de classe \(  C^{\infty}\).
\end{lemma}

\begin{proof}
    Soit \( a\in E\). Étant donné que \( f\) est linéaire et continue, elle est différentiable et
    \begin{equation}
        \begin{aligned}
            df\colon E&\to \cL(E,F) \\
            a&\mapsto f 
        \end{aligned}
    \end{equation}
    est une fonction constante et en particulier continue; nous avons donc \( f\in C^1\). Pour la différentielle seconde nous avons \( d(df)_a=0\) parce que \( df(a+h)-df(a)=f-f=0\). Toutes les différentielles suivantes sont nulles.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dérivation en chaine et formule de Leibnitz}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition} \label{PropOYtgIua}
    Soient \( f_i\colon U\to F_i\), des fonctions de classe \( C^r\) où \( U\) est ouvert dans l'espace vectoriel normé \( E\) et les \( F_i\) sont des espaces vectoriels normés. Alors l'application
    \begin{equation}
        \begin{aligned}
        f=f_1\times \cdots\times f_n\colon U&\to F_1\times \cdots\times F_n \\
    x&\mapsto \big( f_1(x),\ldots, f_n(x) \big) 
        \end{aligned}
    \end{equation}
    est de classe \( C^r\) et
    \begin{equation}
    d^rf=d^rf_1\times\ldots d^rf_n.
    \end{equation}
\end{proposition}

\begin{proof}
    Soit \( x\in U\) et \( h\in E\). La différentiabilité des fonctions \( f_i\) donne
    \begin{equation}
        f_i(x+h)=f_i(x)+(df_i)_x(h)+\alpha_i(h)
    \end{equation}
    avec \( \lim_{h\to 0} \alpha_i(h)/\| h \|=0\). Par conséquent
    \begin{equation}
        f(x+h)=\big( \ldots, f_i(x)+(df_i)_x(h)+\alpha_i(h),\ldots \big)= \big( \ldots,f_i(x),\ldots \big)+ \big( \ldots,(df_i)_x(h),\ldots \big)+ \big( \ldots,\alpha_i(h),\ldots \big).
    \end{equation}
    Mais la définition \ref{DefFAJgTCE} de la norme dans un espace produit donne
    \begin{equation}
        \lim_{h\to 0} \frac{ \| \big( \alpha_1(h),\ldots, \alpha_n(h) \big) \| }{ \| h \| }=0,
    \end{equation}
    ce qui nous permet de noter \( \alpha(h)=\big( \alpha_1(h),\ldots, \alpha_n(h) \big)\) et avoir \( \lim_{h\to 0} \alpha(h)/\| h \|=0\). Avec tout ça nous avons bien
    \begin{equation}
        f(x+h)=f(x)+\big( (df_1)_x(h)+\ldots +(df_n)_x(h) \big)+\alpha(h),
    \end{equation}
    ce qui signifie que \( f\) est différentiable et
    \begin{equation}
        df_x=\big( df_1,\ldots, df_n \big).
    \end{equation}
\end{proof}

\begin{theorem}[Différentielle de fonctions composées\cite{SNPdukn}]    \label{ThoAGXGuEt}
    Soient \( E\), \( F\) et \( G\) des espaces vectoriels normés, \( U\) ouvert dans \( E\) et \( V\) ouvert dans \( F\). Soient des applications de classe \( C^r\) (\( r\geq 1\))
    \begin{subequations}
        \begin{align}
            f\colon U\to V\\
            g\colon V\to G.
        \end{align}
    \end{subequations}
    Alors l'application \( g\circ f\colon V\to G\) est de classe \( C^r\) et
    \begin{equation}\label{EqHFmezmr}
        d(g\circ f)_x=dg_{f(x)}\circ df_x.
    \end{equation}
\end{theorem}

\begin{proof}
    Nous nous fixons \( x\in U\). La fonction \( f\) est différentiable en \( x\in U\) et \( g\) en \( f(x)\), donc nous pouvons écrire
    \begin{equation}
        f(x+h)=f(x)+df_x(h)+\alpha(h)
    \end{equation}
    et
    \begin{equation}
        g\big( f(x)+u \big)=g\big( f(x) \big)+dg_{f(x)}(u)+\beta(u)
    \end{equation}
    où la fonction \( \alpha\) a la propriété que
    \begin{equation}
        \lim_{h\to 0} \frac{ \| \alpha(h) \| }{ \| h \| }=0;
    \end{equation}
    et la même chose pour \( \beta\). La fonction composée en \( x+h\) s'écrit donc
    \begin{equation}    \label{EqCXcfhfH}
        (g\circ f)(x+h)=g\big( f(x)+df_x(h)+\alpha(h) \big)=g\big( f(x) \big)+dg_{f(x)}\big( df_x(h)+\alpha(h) \big)+\beta\big( df_x(h)+\alpha(h) \big).
    \end{equation}
    Nous montrons que tous les «petits» termes de cette formule peuvent être groupés. D'abord si \( h\) est proche de \( 0\), nous avons
    \begin{equation}
        \frac{ \| df_x(h)+\alpha(h) \| }{ \| h \| }\leq\frac{ \| df_x \|\| h \| }{ \| h \| }+\frac{ \| \alpha(h) \| }{ \| h \| }.
    \end{equation}
    Si \( h\) est petit, le second terme est arbitrairement petit, donc en prenant n'importe que \( M>\| df_x \|\) nous avons
    \begin{equation}
        \frac{ \| df_x(h)+\alpha(h) \| }{ \| h \| }\leq M.
    \end{equation}
    Par ailleurs, nous avons
    \begin{equation}
        \frac{ \| \beta\big( df_x(h)+\alpha(h) \big) \| }{ \| h \| }=\frac{  \| \beta\big( df_x(h)+\alpha(h) \big) \|  }{ \| df_x(h)+\alpha(h) \| }\frac{  \| df_x(h)+\alpha(h) \|  }{ \| h \| }\leq M\frac{  \| \beta\big( df_x(h)+\alpha(h) \big) \|  }{   \| df_x(h)+\alpha(h) \| }.
    \end{equation}
    Vu que la fraction est du type \( \frac{ \beta( f(h)) }{ f(h) }\) avec \( \lim_{h\to 0} f(h)=0\), la fraction tend vers zéro lorsque \( h\to 0\). En posant
    \begin{equation}
        \gamma_1(h)=\beta\big( df_x(h)+\alpha(h) \big)
    \end{equation}
    nous avons \( \lim_{h\to 0} \gamma_1(h)/\| h \|=0\).

    L'autre candidat à être un petit terme dans \eqref{EqCXcfhfH} est traité en utilisant la proposition \ref{PropEDvSQsA} :
    \begin{equation}
        \| dg_{f(x)}\big( \alpha(h) \big) \|\leq \| dg_{f(x)} \|\| \alpha(h) \|.
    \end{equation}
    Donc
    \begin{equation}
        \frac{ \| dg_{f(x)}\big( \alpha(h) \big) \| }{ \| h \| }\leq \| dg_{f(x)} \|\frac{ \| \alpha(h) \| }{ \| h \| },
    \end{equation}
    ce qui nous permet de poser
    \begin{equation}
        \gamma_2(h)=dg_{f(x)}\big( \alpha(h) \big)
    \end{equation}
    avec \( \gamma_2\) qui a la même propriété que \( \gamma_1\). Avec tout cela, en posant \( \gamma=\gamma_1+\gamma_2\) nous récrivons
    \begin{equation}
        (g\circ f)(x+h)=g\big( f(x) \big)+dg_{f(x)}\big( df_x(h) \big)+\gamma(h)
    \end{equation}
    avec \( \lim_{h\to 0} \frac{ \gamma(h) }{ \| h \| }=0\). Tout cela pour dire que
    \begin{equation}
        \lim_{h\to 0} \frac{ (g\circ f)(x+h)-(g\circ f)(x)-\big( dg_{f(x)}\circ df_x \big)(h) }{ \| h \| }=0,
    \end{equation}
    ce qui signifie que 
    \begin{equation}
        d(g\circ f)_x=dg_{f(x)}\circ df_x.
    \end{equation}
    Nous avons donc montré que si \( f\) et \( g\) sont différentiables, alors \( g\circ f\) est différentiable avec différentielle donnée par \eqref{EqHFmezmr}.

    Nous passons à la régularité. Nous supposons maintenant que \( f\) et \( g\) sont de classe \( C^r\) et nous considérons l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon L(F,G)\times L(E,F)&\to L(E,G) \\
            (A,B)&\mapsto A\circ B. 
        \end{aligned}
    \end{equation}
    Montrons que l'application \( \varphi\) est continue en montrant qu'elle est bornée\footnote{Proposition \ref{PropmEJjLE}.}. Pour cela nous écrivons la norme opérateur
    \begin{equation}
        \| \varphi \|=\sup_{\| (A,B) \|=1}\| \varphi(A,B) \|=\sup_{\| (A,B) \|=1}\| A\circ B \|\leq\sup_{\| (A,B) \|=1}\| A \|\| B \|\leq 1.
    \end{equation}
    Pour ce calcul nous avons utilisé le fait que la norme opérateur soit une norme algébrique (proposition \ref{PropEDvSQsA}) ainsi que la définition \ref{DefFAJgTCE} de la norme sur un espace produit pour la dernière majoration. L'application \( \varphi\) est donc continue et donc \(  C^{\infty}\) par le lemme \ref{LemLLvgPQW}. Nous considérons également l'application
    \begin{equation}
        \begin{aligned}
        \psi\colon U&\to L(F,G)\times L(E,F) \\
        x&\mapsto \big( dg_{f(x)},df_x \big). 
        \end{aligned}
    \end{equation}
    Vu que \( f\) et \( g\) sont \( C^1\), l'application \( \psi\) est continue. Ces deux applications \( \varphi\) et \( \psi\) sont choisies pour avoir
    \begin{equation}
        (\varphi\circ\psi)(x)=\varphi\big( dg_{f(x)},df_x \big)=dg_{f(x)}\circ df_x,
    \end{equation}
    c'est à dire \( \varphi\circ\psi=d(g\circ f)\). Les applications \( \varphi\) et \( \psi\) étant continues, l'application \( d(g\circ f)\) est continue, ce qui prouve que \( g\circ f\) est \( C^1\).

    Si \( f\) et \( g\) sont \( C^r\) alors \( dg\in C^{r-1}\) et \( dg\circ f\in C^{r-1}\) où il ne faut pas se tromper : \( dg\colon F\to L(F,G)\) et \( f\colon U\to F\); la composée est \( dg\circ f\colon x\mapsto dg_{f(x)}\in L(F,G)\). 
    
    Pour la récurrence nous supposons que \( f,g\in C^{r-1}\) implique \( g\circ f\in C^{r-1}\) pour un certain \( r\geq 2\) (parce que nous venons de prouver cela avec \( r=1\) et \( r=2\)). Soient \( f,g\in C^r\) et montrons que \( g\circ f\in C^r\). Par la proposition \ref{PropOYtgIua} nous avons
    \begin{equation}
        \psi=dg\circ f\times df\in C^{r-1},
    \end{equation}
    et donc \( d(g\circ f)=\varphi\circ\psi\in C^{r-1}\), ce qui signifie que \( g\circ f\in C^r\).
\end{proof}

\begin{lemma}[Leibnitz pour les formes bilinéaires\cite{SNPdukn}]\label{LemFRdNDCd}
    Si \( B\colon E\times F\to G\) est bilinéaire et continue, elle est \(  C^{\infty}\) et
    \begin{equation}    \label{EqXYJgDBt}
        dB_{(x,y)}(u,v)=B(x,v)+B(u,y).
    \end{equation}
\end{lemma}

\begin{proof}
    D'abord le membre de droite de \eqref{EqXYJgDBt} est une application linéaire et continue, donc c'est un bon candidat à être différentielle. Nous allons prouver que ça l'est, ce qui prouvera la différentiabilité de \( B\). Avec ce candidat, le numérateur de la définition \eqref{EqIQuRGmO} s'écrit dans notre cas
    \begin{equation}
        B\big( (x,y)+(u,v) \big)-B(x,y)-B(x,v)-B(u,y)=B(u,v).
    \end{equation}
    Il reste à voir que 
    \begin{equation}
        \lim_{ (u,v)\to (0,0) } \frac{ B(u,v) }{ \| (u,v) \| }=0
    \end{equation}
    Par l'équation \eqref{EqYLnbRbC} nous avons
    \begin{equation}
        \frac{ \| B(u,v) \| }{ \| (u,v) \| }\leq \frac{ \| B \|\| u \|\| v \| }{ \| u \| }=\| B \|\| v \|
    \end{equation}
    parce que \( \| (u,v) \|\geq \| u \|\). À partir de là il est maintenant clair que
    \begin{equation}
        \lim_{(u,v)\to (0,0)}\frac{ \| B(u,v) \| }{ \| (u,v) \| }=0,
    \end{equation}
    ce qu'il fallait.
\end{proof}

\begin{proposition}[Règle de Leibnitz\cite{SNPdukn}]
    Soient \( E,F_1,F_2\) des espaces vectoriels normés, \( U\) ouvert dans \( E\) et des applications de classe \( C^r\) (\( r\geq 1\))
    \begin{subequations}
        \begin{align}
            f_1\colon U\to F_1\\
            f_2\colon U\to F_2\\
        \end{align}
    \end{subequations}
    et \( B\in\cL(F_1\times F_2,G)\). Alors l'application
    \begin{equation}
        \begin{aligned}
            \varphi\colon U&\to G \\
            x&\mapsto B\big( f_1(x),f_2(x) \big) 
        \end{aligned}
    \end{equation}
    est de classe \( C^r\) et
    \begin{equation}    \label{EqMNGBXWc}
        d\varphi_x(u)=\varphi\big( (df_1)_x(u),f_2(x) \big)+\varphi\big( f_1(x),(df_2)_x(u) \big).
    \end{equation}
\end{proposition}
\index{Leibnitz!applications entre espaces vectoriels normés}

\begin{proof}
    Par hypothèse \( B\) est continue (c'est la définition de l'espace \( \cL\)), et donc \(  C^{\infty}\) par le lemme \ref{LemFRdNDCd}. Par ailleurs la fonction \( f_1\times f_2\) est de classe \( C^r\) parce que \( f_1\) et \( f_2\) le sont et parce que la proposition \ref{PropOYtgIua} le dit. L'application composée \( B\circ(f_1\times f_2)\) est donc également de classe \( C^r\) par le théorème \ref{ThoAGXGuEt}.

    Il ne nous reste donc qu'à prouver la formule \ref{EqMNGBXWc}. En utilisant la différentielle du produit cartésien\footnote{Proposition \ref{PropOYtgIua}.} nous avons
    \begin{equation}
        f\big( B\circ(f_1\times f_2) \big)_x(h)=dB_{(f_1\times f_2)(x)}\big( (df_1)_x(h),(df_2)_x(h) \big).
    \end{equation}
    Nous développons cela en utilisant le lemme \ref{LemFRdNDCd} :
    \begin{subequations}
        \begin{align}
        d\big( B\circ(f_1\times f_2) \big)_x(h)&=dB_{\big( f_1(x),f_2(x) \big)}\big( (df_1)_x(h),(df_2)_x(h) \big)\\
        &=B\big( f_1(x),(df_2)_x(h) \big)+B\big( (df_1)_x(h),f_2(x) \big),
        \end{align}
    \end{subequations}
    comme souhaité.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Différentielle partielle}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}[Différentielle partielle]    \label{VJM_CtSKT}
    Soient \( E\), \( F\) et \( G\) des espaces vectoriels normés et une fonction \( f\colon E\times F\to G\). Nous définissons sa \defe{différentielle partielle}{différentielle!partielle} sur l'espace \( E\) par
    \begin{equation}
        \begin{aligned}
            d_1f_{(x_0,y_0)}\colon E&\to G \\
            u&\mapsto \Dsdd{ f(x_0+tu,y_0 }{t}{0} .
        \end{aligned}
    \end{equation}
    La différentielle \( d_2\) se définit de la même façon.
\end{definition}

\begin{proposition}[\cite{SNPdukn}] \label{PropLDN_nHWDF}
    Soient \( E_1\), \( E_2\) et \( F\) des espaces vectoriels normés, soit un ouvert \( U\subset E_1\times E_2\) et une fonction \( f\colon U\to F\).
    \begin{enumerate}
        \item   \label{ItemRDD_oPmXVi}
            Si \( f\) est différentiable alors les différentielles partielles existent et
            \begin{subequations}
                \begin{align}
                    d_1f_{(x_0,y_0)}(u)=df_{(x_0,y_0)}(u,0)\\
                    d_2f_{(x_0,y_0)}(v)=df_{(x_0,y_0)}(0,v)
                \end{align}
            \end{subequations}
            où \( u\in E_1\) et \( v\in E_2\).
        \item
            Si \( f\) est différentiable alors
            \begin{equation}
                df_{(x_0,y_0)}(u,v)=d_1f_{(x_,y_0)}(u)+d_2f_{(x_0,y_0)}(v).
            \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    Nous posons \( \alpha=(x_0,y_0)\in U\) et
    \begin{equation}
        \begin{aligned}
            j_{\alpha}^{(1)}\colon E_1&\to E_1\times E_2 \\
            x&\mapsto (x,y_0). 
        \end{aligned}
    \end{equation}
    C'est une fonction de classe \(  C^{\infty}\) et 
    \begin{equation}
        (dj_{\alpha}^{(1)})_{x_0}(u)=\Dsdd{ j_{\alpha}^{(1)}(x_0+tu) }{t}{0}=\Dsdd{ (x_0+tu,y_0) }{t}{0}=(u,0).
    \end{equation}
    D'autre part 
    \begin{subequations}
        \begin{align}
            (d_1f)_{\alpha}(u)&=\Dsdd{ f(x_0+tu,y_0) }{t}{0}\\
            &=\Dsdd{ (f\circ j_{\alpha}^{(1)})(x_0+tu) }{t}{0}\\
            &=\big( d(f\circ j_{\alpha}^{(1)}) \big)_{x_0}(u).
        \end{align}
    \end{subequations}
    À ce moment nous utilisons la règle des différentielles composées \ref{ThoAGXGuEt} pour dire que
    \begin{equation}
        (d_1f)_{\alpha}(u)=df_{j_{\alpha}^{(1)}(x_0)}\circ (dj_{\alpha}^{(1)})_{x_0}(u)=df_{\alpha}(u,0).
    \end{equation}
    Voila qui prouve déjà le point \ref{ItemRDD_oPmXVi}.

    Pour la suite nous considérons les fonctions 
    \begin{equation}
        \begin{aligned}[]
            P_1(x,y)&=x,&&&J_1(u)&=(u,0),\\
            P_2(x,y)&=y,&&&J_2(v)&=(0,v)
        \end{aligned}
    \end{equation}
    et nous avons l'égalité évidente
    \begin{equation}
        J_1\circ P_1+J_2\circ P_2=\mtu
    \end{equation}
    sur \( E_1\times E_2\). En appliquant \( df_{\alpha}\) à cette dernière égalité, en appliquant à \( (u,v)\) et en utilisant la linéarité de \( df_{\alpha}\) nous trouvons
    \begin{subequations}
        \begin{align}
            df_{\alpha}(u,v)&=df_{\alpha}\big( (J_1\circ P_1)(u,v) \big)+df_{\alpha}\big( (J_2\circ P_2)(u,v) \big)\\
            &=df_{\alpha}(u,0)+df_{\alpha}(0,v)\\
            &=(d_1f)_{\alpha}(u)+(d_2f)_{\alpha}(v)
        \end{align}
    \end{subequations}
    où nous avons utilisé le point \ref{ItemRDD_oPmXVi} pour la dernière égalité.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Formule des accroissements finis}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition} \label{PropDQLhSoy}
    Soit \( E\) un espace vectoriel normé. Soient \( a<b\) dans \( \eR\) et deux fonctions
    \begin{subequations}
        \begin{align}
            f\colon \mathopen[ a , b \mathclose]\to E\\
            g\colon \mathopen[ a , b \mathclose]\to \eR
        \end{align}
    \end{subequations}
    continues sur \( \mathopen[ a , b \mathclose]\) et dérivables sur \( \mathopen] a , b \mathclose[\). Si pour tout \( t\in\mathopen] a , b \mathclose[\) nous avons \( \| f'(t) \|\leq g'(t)\) alors
        \begin{equation}
            \| f(b)-f(a) \|\leq g(b)-g(a).
        \end{equation}
\end{proposition}

\begin{proof}
    Soit \( \epsilon>0\) et la fonction
    \begin{equation}
        \begin{aligned}
            \varphi_{\epsilon}\colon \mathopen[ a , b \mathclose]&\to \eR \\
            t&\mapsto \| f(t)-f(a) \|-g(t)-\epsilon t. 
        \end{aligned}
    \end{equation}
    Cela est une fonction continue réelle à variable réelle. En particulier pour tout \( u\in\mathopen] a , b \mathclose[\) la fonction \( \varphi_{\epsilon}\) est continue sur le compact \( \mathopen[ u , b \mathclose]\) et donc y atteint son minimum en un certain point \( c\in\mathopen[ u , b \mathclose]\); c'est le bon vieux théorème de Weierstrass \ref{ThoWeirstrassRn}. Nous commençons par montrer que pour tout \( u\), ledit minimum ne peut être que \( b\). Pour cela nous allons montrer que si \( t\in\mathopen[ u , b [\), alors \( \varphi_{\epsilon}(s)<\varphi_{\epsilon}(t)\) pour un certain \( s>t\). Par continuité si \( s\) est proche de \( t\) nous avons
        \begin{equation}
            \left\|  \frac{ f(s)-f(t) }{ s-t }  \right\|-\frac{ \epsilon }{2}<\| f'(t) \|<g'(t)+\frac{ \epsilon }{2}=\frac{ g(s)-g(t) }{ s-t }+\frac{ \epsilon }{2}.
        \end{equation}
        Ces inégalités proviennent de la limite
        \begin{equation}
            \lim_{s\to t} \frac{ f(s)-f(t) }{ s-t }=f'(t),
        \end{equation}
        donc si \( s\) et \( t\) sont proches,
        \begin{equation}
            \left\| \frac{ f(s)-f(t) }{ s-t }-f'(t) \right\|
        \end{equation}
        est petit. Si \( s>t\) nous pouvons oublier des valeurs absolues et transformer l'inégalité en
        \begin{equation}
            \| f(s)-f(t) \|<g(s)-g(t)+\epsilon(s-t).
        \end{equation}
        Utilisant cela et l'inégalité triangulaire,
        \begin{subequations}
            \begin{align}
                \varphi_{\epsilon}(s)&\leq\| f(s)-f(t) \|+\| f(t)-f(a) \|-g(s)-\epsilon s\\
                &\leq g(s)-g(t)+\epsilon s-\epsilon t+\| f(t)-f(a) \|-g(s)-\epsilon s\\
                &=\varphi_{\epsilon}(t).
            \end{align}
        \end{subequations}
        Donc nous avons bien \( \varphi_{\epsilon}(s)<\varphi_{\epsilon}(t)\) avec l'inégalité stricte. Par conséquent pour tout \( u\in\mathopen] a , b \mathclose[\) nous avons \( \varphi_{\epsilon}(b)<\varphi_{\epsilon}(u)\) et en prenant la limite \( u\to a\) nous avons
        \begin{equation}
            \varphi_{\epsilon}(b)\leq \varphi_{\epsilon}(a).
        \end{equation}
        Cette inégalité donne immédiatement
        \begin{equation}
            \| f(b)-f(a) \|\leq g(b)-g(a)+\epsilon(b-a)
        \end{equation}
         pour tout \( \epsilon>0\) et donc
         \begin{equation}
            \| f(b)-f(a) \|\leq g(b)-g(a).
         \end{equation}
\end{proof}

\begin{theorem}[Théorème des accroissements finis]\label{ThoNAKKght}
    Soient \( E\) et \( F\) des espaces vectoriels normés, \( U \) ouvert dans \( E\) et une application différentiable \( f\colon U\to F\). Pour tout segment \( \mathopen[ a , b \mathclose]\subset U\) nous avons
    \begin{equation}
        \| f(b)-f(a) \|\leq\left( \sup_{x\in\mathopen[ a , b \mathclose]}\| df_x \| \right)\| b-a \|.
    \end{equation}
\end{theorem}
\index{théorème!accroissements finis}
Une version de ce théorème adaptée aux espaces de dimension finie est le théorème \ref{val_medio_2}.

\begin{proof}
    Nous prenons les applications
    \begin{equation}
        \begin{aligned}
            k\colon \mathopen[ 0 , 1 \mathclose]&\to E \\
            t&\mapsto f\big( (1-t)a+tb \big) 
        \end{aligned}
    \end{equation}
    et
    \begin{equation}
        \begin{aligned}
            g\colon \mathopen[ 0 , 1 \mathclose]&\to \eR \\
            t&\mapsto t\sup_{x\in\mathopen[ a , b \mathclose]}\| df_x \|\| b-a \|.
        \end{aligned}
    \end{equation}
    Pour tout \( t\) nous avons \( g'(t)=M\| b-a \|\) où il n'est besoin de dire ce qu'est \( M\). D'un autre côté nous avons aussi
    \begin{equation}
        \begin{aligned}[]
            k'(t)&=\lim_{\epsilon\to 0}\frac{ f\big( (1-t-\epsilon)a+(t+\epsilon)b \big)-f\big( (1-t)a+tb \big) }{ \epsilon }\\
            &=\Dsdd{ f\big( (1-t)a+tb+\epsilon(b-a) \big)  }{\epsilon}{0}\\
            &=df_{(1-t)a+tb}(b-a)
        \end{aligned}
    \end{equation}
    où nous avons utilisé l'hypothèse de différentiabilité de \( f\) sur \( \mathopen[ a , b \mathclose]\) et donc en \( (1-t)a+tb\). Nous avons donc
    \begin{equation}
        \| k'(t) \|\leq \| b-a \|\| df_{(1-t)a+tb} \|\leq M\| b-a \|=g'(t)
    \end{equation}
    La proposition \ref{PropDQLhSoy} est donc utilisable et
    \begin{equation}
        \| k(1)-k(0) \|=g(1)-g(0),
    \end{equation}
    c'est à dire
    \begin{equation}
        \| f(b)-f(a) \|=M\| b-a \|
    \end{equation}
    comme il se doit.
\end{proof}

\begin{proposition} \label{ProFSjmBAt}
    Soient \( E\) et \( F\) des espaces vectoriels normés, \( U \) ouvert dans \( E\) et une application \( f\colon U\to F\). Soient \( a,b\in U\) tels que \( \mathopen[ a , b \mathclose]\subset U\). Nous posons \( u=(b-a)/\| b-a \|\) et nous supposons que pour tout \( x\in\mathopen[ a , b \mathclose]\), la dérivée directionnelle
    \begin{equation}
        \frac{ \partial f }{ \partial u }(x)=\Dsdd{ f(x+tu) }{t}{0}
    \end{equation}
    existe. Nous supposons de plus que \( \frac{ \partial f }{ \partial u }(x)\) est continue en \( x=a\). Alors
    \begin{equation}
        \| f(b)-f(a) \|\leq\left( \sup_{x\in\mathopen[ a , b \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \| \right)\| b-a \|.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous posons évidemment 
    \begin{equation}
        M=\sup_{x\in\mathopen[ a , b \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \| 
    \end{equation}
    et nous considérons les fonctions
    \begin{equation}
        k(t)=f\big( (1-t)a+tb \big)
    \end{equation}
    et
    \begin{equation}
        g(t)=tM\| b-a \|.
    \end{equation}
    Pour alléger les notations nous posons \( x=(1-t)a+tb\) et nous calculons avec un petit changement de variables dans la limite :
    \begin{equation}
        k'(t)=\Dsdd{  f\big( x+\epsilon(b-a) \big)  }{\epsilon}{0}=\| b-a \|\Dsdd{ f\big( x+\frac{ \epsilon }{ \| b-a \| }(b-a) \big) }{\epsilon}{0}=\| b-a \|\frac{ \partial f }{ \partial u }(x),
    \end{equation}
    et donc encore une fois nous avons
    \begin{equation}
        \| k'(t) \|\leq g'(t),
    \end{equation}
    ce qui donne
    \begin{equation}
        \| k(1)-k(0) \|=g(1)-g(0),
    \end{equation}
    c'est à dire
    \begin{equation}
        \| f(b)-f(a) \|\leq \sup_{x\in\mathopen[ a , b \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \|\| b-a \|.
    \end{equation}
\end{proof}

\begin{theorem} \label{ThoOYwdeVt}
    Soient \( E,V\) deux espaces vectoriels normés, une application \( f\colon E\to V\), un point \( a\in E\) tel que pour tout \( u\in E\), la dérivée
    \begin{equation}
        \Dsdd{ f(x+tu) }{t}{0}
    \end{equation}
    existe pour tout \( x\in B(a,r)\) et est continue (par rapport à \( x\)) en \( x=a\). Nous supposons de plus que\quext{Je ne suis pas certain que cette hypothèse soit nécessaire, voir la question \ref{ItemLPrIWZhPg} de la page \pageref{ItemLPrIWZhPg}.}
    \begin{equation}
        \frac{ \partial f }{ \partial u }(a)=0
    \end{equation}
    pour tout \( u\in E\). Alors \( f\) est différentiable en \( a\) et
    \begin{equation}
        df_a=0
    \end{equation}
\end{theorem}

\begin{proof}
    Soit \( \epsilon>0\). Pourvu que \( \| h \|\) soit assez petit pour que \( a+h\in B(a,r)\), la proposition \ref{ProFSjmBAt} nous donne
    \begin{equation}
        \| f(a+h)-f(a) \|\leq \sup_{x\in\mathopen[ a , a+h \mathclose]}\| \frac{ \partial f }{ \partial u }(x) \|  |h |
    \end{equation}
    où \( u=h/\| h \|\). Par continuité de \( \partial_uf(x)\) en \( x=a\) et par le fait que cela vaut \( 0\) en \( x=a\), il existe un \( \delta>0\) tel que si \( \| h \|<\delta\) alors
    \begin{equation}
        \| \frac{ \partial f }{ \partial u }(a+h) \|\leq \epsilon.
    \end{equation}
    Pour de tels \( h\) nous avons
    \begin{equation}
        \| f(a+h)-f(a) \|\leq \epsilon\| h \|,
    \end{equation}
    ce qui prouve que l'application linéaire \( T(u)=0\) convient parfaitement pour faire fonctionner la définition \ref{DefKZXtcIT}.
%
%    Nous ne supposons plus que les dérivées directionnelles de \( f\) sont nulles en \( x=a\). Alors nous posons, pour \( x\in U\),
%    \begin{equation}    \label{EqCUgHXHy}
%        g(x)=f(x)-\Dsdd{ f(a+s(x-a)) }{s}{0}.
%    \end{equation}
%    Le fait que cette fonction soit bien définie est encore un coup de hypothèses sur les dérivées directionnelles de \( f\) qui sont bien définies autour de \( a\). Cette nouvelle fonction \( g\) satisfait à \( \frac{ \partial g }{ \partial v }(a)=0\) pour tout \( v\in E\) parce que
%    \begin{subequations}
%        \begin{align}
%            \frac{ \partial g }{ \partial v }(a)&=\Dsdd{ g(a+tv) }{t}{0}\\
%            &=\Dsdd{ f(a+tv)-\Dsdd{ f\big( a+s(tv) \big) }{s}{0} }{t}{0}\\
%            &=\frac{ \partial f }{ \partial v }(a)-\Dsdd{ t\frac{ \partial f }{ \partial v }(a) }{t}{0}\\
%            &=0.
%        \end{align}
%    \end{subequations}
%    Pour la dérivée par rapport à \( s\) nous avons effectué le changement de variables \( s\to ts\), ce qui explique la présence d'un \( t\) en facteur. La fonction \( g\) est donc différentiable en \( a\).
%
%
% Position 229262367
    % Attention : ce qui suit est faux. Mais il y a peut-être moyen d'adapter.
%\item[Dérivées non nulles]
%
%    Nous allons montrer que la fonction 
%    \begin{equation}
%        l(x)=\Dsdd{ f\big( a+s(x-a) \big) }{t}{0}
%    \end{equation}
%    est différentiable en \( x=a\), de différentielle \( T(u)=l(u+a)\). Cela fournira la différentiabilité de \( f\) parce que \eqref{EqCUgHXHy} donnerait alors \( f\) comme somme de deux fonctions différentiables.
%
%    En premier lieu nous devons montrer que \( T\) ainsi définie est linéaire.
%    
%    Notre but est donc de prouver que
%    \begin{equation}
%        \lim_{h \to 0}\frac{ \| l(x+h)-l(x)-l(h) \| }{ \| h \| }=0.
%    \end{equation}
%    Un premier pas est de calculer
%    \begin{subequations}
%        \begin{align}
%            l(x+h)-l(x)-l(h)&=\lim_{s\to 0}\frac{ f\big( s(x+h) \big)-f(0)-f(sx)+f(0)-f(sh)+f(0) }{ s }\\
%            &=\lim_{s\to 0}\frac{ f\big( s(x+h) \big)-f(sx)-f(sh)+f(0) }{ s }.
%        \end{align}
%    \end{subequations}
%    Ensuite nous étudions le numérateur en utilisant la proposition \ref{ProFSjmBAt}:
%    \begin{subequations}
%        \begin{align}
%            \| f\big( s(x+h) \big)-f(sx)-f(sh)+f(0) \|&\leq  \| f\big( s(x+h) \big)-f(sx)\| + \|f(sh)-f(0) \|  \\
%            &\leq \sup_{z\in\mathopen[ sx , sx+sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| sh \|\\
%            &\quad +\sup_{z\in\mathopen[ 0 , sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| sh \|.
%        \end{align}
%    \end{subequations}
%    La division par \( s\) se passe bien et nous avons
%    \begin{subequations}
%        \begin{align}
%            \| l(x+h)-l(x)-l(h) \|&\leq \lim_{s\to 0}  \sup_{z\in\mathopen[ sx , sx+sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| h \|+ \sup_{z\in\mathopen[ 0 , sh \mathclose]}\| \frac{ \partial f }{ \partial h }(z) \|\| h \|\\
%            &=2\| h \|\| \frac{ \partial f }{ \partial h }(0) \|        \label{SubeqVMMoSDH}\\
%            &=2\| h \|^2\| \frac{ \partial f }{ \partial u }(0) \|
%        \end{align}
%    \end{subequations}
%    où nous avons posé \( u=h/\| h \|\). Pour l'égalité \eqref{SubeqVMMoSDH} nous avons utilisé la continuité de \( \frac{ \partial f }{ \partial h }(z)\) en \( z=0\). Du coup
%    \begin{equation}
%        \lim_{y\to 0} \frac{ \| f(x+h)-f(x)-f(h) \| }{ \| h \| }=\lim_{h\to 0} 2\| h \|\| \frac{ \partial f }{ \partial u }(0) \|=0.
%    \end{equation}
%    Cela prouve que \( l\) est bien différentiable en \( x=0\).
%
%    \end{subproof}
%
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{L'inverse, sa différentielle}
%---------------------------------------------------------------------------------------------------------------------------

Si \( E\) est un espace de Banach, nous sommes intéressé à l'espace \( \GL(E)\) des endomorphismes inversibles de \( E\) sur \( E\). Cet ensemble est métrique par la formule usuelle
\begin{equation}
    \| T \|=\sup_{\| x \|=1}\| T(x) \|_E.
\end{equation}

\begin{theorem}[Inverse dans \( \GL(E)\)\cite{laudenbach2000calcul,SNPdukn}]    \label{ThoCINVBTJ}
    Soient \( E\) et \( F\) des espaces vectoriels normés.
    \begin{enumerate}
        \item
        L'ensemble \( \GL(E)\) est ouvert dans \( \End(E)\).
    \item
        L'application inverse
    \begin{equation}
        \begin{aligned}
        i\colon \GL(E,F)&\to \GL(F,E) \\
        u&\mapsto u^{-1} 
        \end{aligned}
    \end{equation}
    est de classe \( C^{\infty}\) et
    \begin{equation}
        di_{u_0}(h)=-u_0^{-1}\circ h\circ u_0^{-1}
    \end{equation}
    pour tout \( h\in\End(E)\)
    \end{enumerate}
\end{theorem}
\index{différentielle!de $u\mapsto u^{-1}$}

\begin{proof}
Nous supposons que \( \GL(E,F)\) n'est pas vide, sinon ce n'est pas du jeu.
        \begin{subproof}

        \item[Cas de dimension finie]

            Si la dimension de \( E\) et \( F\) est finie, elles doivent être égales, sinon il n'y a pas de fonctions inversibles \( E\to F\). L'ensemble \( \GL(E,F)\) est donc naturellement \( \GL(n,\eR)\). Un élément de \( \eM(n,\eR)\) est dans \( \GL(n,\eR)\) si et seulement si son déterminant est non nul. Le déterminant étant une fonction continue (polynomiale) en les entrées de la matrice, l'ensemble \( \GL(n,\eR)\) est ouvert dans \( \eM(n,\eR)\).

            Même idée pour la régularité de la fonction \( i\colon \GL(n,\eR)\to \GL(n,\eR)\), \( X\mapsto X^{-1}\). Les entrées de \( X^{-1}\) sont les cofacteurs de \( X\) divisé par \( \det(X)\), et donc des polynômes en les entrées de \( X\) divisés par un polynôme qui ne s'annule pas sur \( \GL(n,\eR)\), et donc sur un ouvert autour de \( X\) et de \( X^{-1}\). Bref, tout est \(  C^{\infty}\).

            Le reste de la preuve parle de la dimension infinie.

        \item[Ouvert autour de l'identité]
            
        Nous commençons par prouver que \( B(\mtu,1)\subset \GL(E)\). Pour cela il suffit de remarquer que si \( \| u \|<1\) alors le lemme \ref{PropQAjqUNp} nous donne un inverse de \( (1+u)\) en la personne de \( \sum_{k=0}^{\infty}(-u)^k\).

    \item[Ouvert en général]

        Soit maintenant \( u_0\in\GL(E)\). Si \( \| u \|<\frac{1}{ \| u_0^{-1} \| }\) alors \( \| u_0^{-1}u \|<1\), ce qui signifie que
        \begin{equation}
            \mtu+u_0^{-1}u
        \end{equation}
    est inversible. Mais \( u_0+u=u_0(\mtu+u_0^{-1}u)\), donc \( u_0+u\in\GL(E)\) ce qui signifie que
    \begin{equation}
    B\left( u_0,\frac{1}{ \| u_0^{-1} \| } \right)\subset \GL(E).
    \end{equation}

    \item[Différentielle en l'identité]

    Nous commençons par prouver que \( di_{\mtu}(u)=-u\). Pour cela nous posons 
    \begin{equation}
        \alpha(h)=\sum_{k=2}^{\infty}(-1)^kh^k
    \end{equation}
    et nous calculons
    \begin{equation}
    di_{\mtu}(u)=\Dsdd{ i(\mtu+tu) }{t}{0}=\Dsdd{ \mtu-tu+\alpha(tu) }{t}{0}.
    \end{equation}
    Il suffit de prouver que \( \Dsdd{ \alpha(tu) }{t}{0}=0\) pour conclure que \( di_{\mtu}(u)=-u\). Pour cela, nous remarquons que \( \alpha(0)=0\) et donc que
    \begin{subequations}
        \begin{align}
        \Dsdd{ \alpha(tu) }{t}{0}&=\lim_{t\to 0} \frac{ \alpha(tu)-\alpha(0) }{ t }\\
        &=\lim_{t\to 0} \sum_{k=2}^{\infty}(-1)^k\frac{ (tu)^k }{ t }\\
        &=-\lim_{t\to 0} u\sum_{k=1}^{\infty}(-1)^kt^ku^k.
        \end{align}
    \end{subequations}
    La norme de ce qui est dans la limite est majorée par
    \begin{equation}
    \| u \|\sum_{k=1}^{\infty}\| tu \|^k=\| u \|\left( \frac{1}{ 1-\| tu \| }-1 \right),
    \end{equation}
    et cela tend vers zéro lorsque \( t\to\infty\). Nous avons utilisé la somme \ref{EqRGkBhrX} de la série géométrique. Nous avons bien prouvé que \( di_{\mtu}(u)=-u\).

    \item[Différentielle en général]
    Soit maintenant \( u_0\in\GL(E)\) et \( h\in\End(E)\) tel que \( u_0+h\in \GL(E)\); par le premier point, il suffit de prendre \( \| h \|\) suffisamment petit. Vu que \( u_0+h=u_0(\mtu+u_0^{-1}h)\) nous avons
    \begin{equation}
        (u_0+h)^{-1}=(\mtu+u_0^{-1}h)^{-1}u_0^{-1}.
    \end{equation}
    Nous pouvons donc calculer
    \begin{equation}
        (u_0+h)^{-1}=\big( \mtu-u_0^{-1}h+\alpha(u_0^{-1}h) \big)u_0^{-1}=u_0^{-1}-u_0^{-1}hu_0^{-1}+\alpha(u_0^{-1}h)u_0^{-1},
    \end{equation}
    et ensuite
    \begin{equation}
        di_{u_0}(h)=\Dsdd{ i(u_0+th) }{t}{0}=\Dsdd{ u_0^{-1}-tu_0^{-1}hu_0^{-1}+\alpha(tu_0^{-1}h)u_0^{-1} }{t}{0},
    \end{equation}
    mais nous avons déjà vu que
    \begin{equation}
        \Dsdd{ \alpha(th) }{t}{0}=0,
    \end{equation}
    donc
    \begin{equation}
        di_{u_0}(h)=-u_0^{-1}hu_0^{-1}
    \end{equation}
    Cela donne la différentielle de l'application inverse.

    \item[Continuité de l'inverse]

        L'application \( i\) est continue parce que différentiable.
    \item[L'inverse est \(  C^{\infty}\)]

        Nous allons écrire la fonction inverse comme une composée. Soient les applications
        \begin{equation}
            \begin{aligned}
                B\colon \cL(F,E)\times \cL(F,E)&\to \cL\big( \cL(E,F),\cL(F,E) \big) \\
                B(\psi_1,\psi_2)(A)&= -\psi_1\circ A\circ\psi_2
            \end{aligned}
        \end{equation}
        et
        \begin{equation}
            \begin{aligned}
                \Delta\colon \cL(F,E)&\to \cL(F,E)\times \cL(F,E) \\
                \varphi&\mapsto (\varphi,\varphi) 
            \end{aligned}
        \end{equation}
        Nous avons alors 
        \begin{equation}
            di=B\circ\Delta\circ i.
        \end{equation}
        L'application \( \Delta\) est de classe \(  C^{\infty}\). Nous devons voir que \( B\) l'est aussi. Pour le voir nous commençons par prouver qu'elle est bornée :
        \begin{equation}
            \begin{aligned}[]
                \| B \|&=\sup_{\| \psi_1 \|,\| \psi_2 \|=1}\| B(\psi_1,\psi_2) \|_{\aL\big( L(E,F),L(F,E) \big)}\\
                &=\sup_{  \| \psi_1 \|,\| \psi_2 \|=1 }\sup_{\| A \|=1}\| \psi_1\circ A\circ\psi_2 \|_{L(F,E)}\\
                &\leq \sup_{\| \psi_1 \|,\| \psi_2 \|=1}\sup_{\| A \|=1}\| \psi_1 \|\| A \|\| \psi_2 \|\\
                &\leq 1.
            \end{aligned}
        \end{equation}
        Donc \( B\) est bien bornée et par conséquent continue. Une application bilinéaire continue est \(  C^{\infty}\) par le lemme \ref{LemFRdNDCd}. La décomposition \( di=B\circ \Delta\circ i\) nous donne donc que \( i\in C^{\infty}\) dès que \( i\) est continue, ce que nous avions déjà montré.
        \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Exponentielle de matrice}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
%\label{subsecAOnIwQM}
\label{secAOnIwQM}

\begin{enumerate}
    \item
        En ce qui concerne la continuité, nous aurons évidemment besoin de théorie à propos de l'inversion de limites et de sommes. Nous en parlerons donc en \ref{subsecXNcaQfZ}.
    \item 
        Les séries entières de matrices seront traitées plus en détail autour de la proposition \ref{PropFIPooSSmJDQ}.
\end{enumerate}

\begin{proposition}     \label{PropPEDSooAvSXmY}
    Soit \( V\) un espace vectoriel de dimension finie et \( A\in\End(V)\). La série
    \begin{equation}
        \exp(A)=\mtu+A+\frac{ A^2 }{ 2 }+\frac{ A^3 }{ 3 }+\ldots =\sum_{k=1}^{\infty}\frac{ A^k }{ k! }.
    \end{equation}
    converge normalement dans \( \big( \End(V),\| . \|_{op} \big)\).  L'\defe{exponentielle}{exponentielle!de matrice} de la matrice \( A\) est cette matrice.
\end{proposition}

\begin{proof}
    Vu que la norme opérateur est une norme d'algèbre par la proposition \ref{PropEDvSQsA}, nous avons pour tout \( k\) la majoration \( \| A^k \|\leq \| A \|^k\). Nous avons donc
    \begin{equation}
        \sum_{k=0}^{\infty}\frac{ \| A^k \| }{ k! }\leq \sum_k\frac{ \| A \|^k }{ k! }.
    \end{equation}
    La dernière somme converge en vertu de la convergence de la série exponentielle donnée en exemple \ref{ExIJMHooOEUKfj}.
\end{proof}

Étant donné que c'est une limite, il y a une question de convergence et donc de topologie. C'est pour cela que nous ne pouvions pas introduire l'exponentielle de matrice avant d'avoir introduit la norme des matrices. La convergence de la série pour toute matrice sera prouvée au passage dans la proposition \ref{PropFMqsIE}.


La fonction exponentielle \(  x\mapsto e^{x}\) n'est pas un polynôme en \( x\), mais nous avons les résultat marrant suivant.
\begin{proposition} \label{PropFMqsIE}
    Si \( u\) est un endomorphisme, alors \( \exp(u)\) est un polynôme en \( u\)\footnote{Nan, mais j'te jure : \( \exp\) n'est pas un polynôme, mais $\exp(u)$ est un polynôme de \( u\).}.
\end{proposition}

\begin{proof}
    Nous considérons l'application
    \begin{equation}
        \begin{aligned}
            \varphi_u\colon \eK[X]&\to \End(E) \\
            P&\mapsto P(u)
        \end{aligned}
    \end{equation}
    déjà introduite en \eqref{EqOVKooeMJuv}. Étant donné que l'image de \( \varphi_u\) est un fermé dans \( \End(E)\), il suffit de montrer que la série
    \begin{equation}
        \sum_{k=0}^{\infty}\frac{ \varphi_u(X)^k }{ k! }
    \end{equation}
    converge dans \( \End(E)\) pour qu'elle converge dans \( \Image(\varphi_u)\). Pour ce faire nous nous rappelons de la norme opérateur \eqref{ExemdefnormpMrt} et de la propriété fondamentale \( \| A^k \|\leq \| A \|^k\). En notant \( A=\varphi_u(X)\) et en utilisant l'inégalité \eqref{EqFwTvwI},
    \begin{equation}
        \left\| \sum_{k=n}^m\frac{ A^k }{ k! } \right\|\leq \sum_{k=n}^m\frac{ \| A^k \| }{ k! }\leq \sum_{k=n}^m\frac{ \| A \|^k }{ k! },
    \end{equation}
    ce qui est une morceau du développement de \(  e^{\| A \|}\). La limite \( n\to\infty\) est donc zéro par la convergence de l'exponentielle réelle. La suite des sommes partielles de  $e^{A}$ est donc de Cauchy. La série converge donc parce que nous sommes dans un espace vectoriel réel de dimension finie (\( \End(E)\)).
\end{proof}
% TODO : et tant qu'on y est, justifier la convergence de la série de l'exponentielle réelle.

\begin{remark}
    Pourquoi \( \exp(u)\) est-il un polynôme d'endomorphisme alors que \( \exp\) n'est pas un polynôme ? Lorsque nous disons que la fonction \( x\mapsto \exp(x)\) n'est pas un polynôme, nous sommes en train de localiser la fonction \( \exp\) à l'intérieur de l'espace de toutes les fonctions \( \eR\to \eR\), c'est à dire à l'intérieur d'un espace de dimension infinie. Au contraire lorsqu'on parle de \( \exp(u)\) et qu'on le compare aux endomorphismes \( P(u)\), nous sommes en train de repérer \( \exp(u)\) à l'intérieur de l'espace des matrices qui est de dimension finie. Il n'est donc pas étonnant que l'on parvienne moins à faire la distinction.

    Si par contre nous considérons \( \exp\) en tant qu'application \( \exp\colon \End(E)\to \End(E)\), ce n'est pas un polynôme.

    Si \( u\) et \( v\) sont des endomorphismes, nous aurons des polynômes \( P\) et \( Q\) tels que \( e^u=P(u)\) et \( e^v=Q(v)\); mais nous n'aurons en général évidemment pas \( P=Q\). En cela, \( \exp\) n'est pas un polynôme.
\end{remark}

Nous reprenons l'exemple de \cite{MneimneReduct}. Soit \( A\) une matrice dont le polynôme minimum s'écrit
\begin{equation}
    P(X)=(X-1)^2(X-2).
\end{equation}
Par le théorème \ref{ThoDecompNoyayzzMWod} de décomposition des noyaux nous avons
\index{théorème!décomposition des noyaux!et exponentielle de matrice}
\begin{equation}
    E=\ker(A-1)^2\oplus\ker(A-2).
\end{equation}
En suivant les notations de ce théorème nous avons \( P_1(X)=(X-1)^2\), \( P_2(X)=X-2\) et
\begin{subequations}
    \begin{align}
        Q_1(X)&=X-2\\
        Q_2(X)&=(X-1)^2.
    \end{align}
\end{subequations}
Les polynômes \( R_i\) dont l'existence est assurée par le théorème de Bézout sont
\begin{equation}
    \begin{aligned}[]
        R_1(X)&=-X\\
        R_2(X)&=1.
    \end{aligned}
\end{equation}
Nous avons
\begin{equation}
    R_1Q_1+R_2Q_2=1.
\end{equation}
Le projecteur \( p_i\) sur \( \ker P_i\) est \( R_iQ_i\) :
\begin{equation}
    \begin{aligned}[]
        p_1&=-A(A-2)=\pr_{\ker(u-1)^2}\\
        p_2&=(A-1)^2=\pr_{\ker(u-2)}.
    \end{aligned}
\end{equation}
Passons maintenant au calcul de l'exponentielle. Nous avons évidemment
\begin{equation}
    e^A=e^Ap_1+e^Ap_2.
\end{equation}
Étant donné que \( p_1\) est le projecteur sur le noyau de \( (A-1)^2\), nous avons
\begin{equation}
    e^Ap_1=ee^{A-1}p_1=ep_1+e(u-1)1=ep_1=-Ae(A-2).
\end{equation}
En effet \( e^{A-1}p_1=\sum_{k=0}^{\infty}(A-1)^k\circ p_1\). De la même façon nous avons
\begin{equation}
    e^Ap_2=e^2e^{A-2}p_2=e^2p_2=e^2(A-1)^2.
\end{equation}
Au final,
\begin{equation}
    e^A=-Ae(A-2)+e^2(A-1)^2.
\end{equation}

\begin{theorem}
    Soit une matrice \( A\in \eM(n,\eC)\). On a que la suite \( (A^kx)\) tends vers zéro pour tout \( x\) si et seulement si \( \rho(A)<1\) où \( \rho(A)\)\index{rayon!spectral} est le rayon spectral de $A$
\end{theorem}
\index{décomposition!Dunford!exponentielle de matrice}

\begin{proof}
    Dans le sens direct, il suffit de prendre comme \( x\), un vecteur propre de \( A\). Dans ce cas nous avons \( A^kx=\lambda^kx\). Mais \( \lambda^kx\) ne tend vers zéro que si \( \lambda<1\). Donc toute les valeurs propres de \( A\) doivent être plus petite que \( 1\) et \( \rho(A)<1\).

    Pour l'autre sens nous utilisons la décomposition de Dunford (théorème \ref{ThoRURcpW}) : il existe une matrice inversible \( P\) telle que
    \begin{equation}
        A=P^{-1}(D+N)P
    \end{equation}
    où \( D\) est diagonale, \( N\) est nilpotente et \( [D,N]=0\). Étant donné que \( D+N\) est triangulaire, son polynôme caractéristique que
    \begin{equation}
        \chi_{D+N}(\lambda)=\prod_i D_{ii}-\lambda.
    \end{equation}
    Par similitude, c'est le même polynôme caractéristique que celui de \( A\) et nous savons alors que la diagonale de \( D\) contient les valeurs propres de \( A\).

    Par ailleurs nous avons
    \begin{subequations}
        \begin{align}
            A^k&=P^{-1}(D+N)^kP\\
            &=P^{-1}\sum_{j=0}^k{j\choose k}D^{j-k}N^jP\\
            &=P^{-1}\sum_{j=0}^{n-1}{j\choose k}D^{j-k}N^jP
        \end{align}
    \end{subequations}
    où nous avons utilité le fait que \( D\) et \( N\) commutent ainsi que \( N^{n-1}=0\) parce que \( N\) est nilpotente. Nous utilisons la norme matricielle usuelle, pour laquelle \( \| D \|=\rho(D)=\rho(A)\). Nous avons alors
    \begin{equation}
        \| (D+N)^k \|\leq \sum_{j=0}^k{j\choose k}\rho(D)^{k-j}\| N \|^j.
    \end{equation}
    Du coup si \( \rho(D)<1\) alors \( \| (D+N)^k \|\to 0\) (et c'est même un si et seulement si).
\end{proof}

Une application de la décomposition de Jordan est l'existence d'un logarithme pour les matrices. La proposition suivant va d'une certaine manière donner un logarithme pour les matrices inversibles complexes. Dans le cas des matrices réelles \( m\) telles que \( \| m-\mtu \|<1\), nous donnerons au lemme \ref{LemQZIQxaB} une formule pour le logarithme sous forme d'une série; ce logarithme sera réel.
\begin{proposition} \label{PropKKdmnkD}
    Toute matrice inversible complexe est une exponentielle.
\end{proposition}
\index{exponentielle!de matrice}
\index{décomposition!Jordan!et exponentielle de matrice}

\begin{proof}
    Soit \( A\in \GL(n,\eC)\); nous allons donner une matrice \( B\in \eM(n,\eC)\) telle que \( A=\exp(B)\). D'abord remarquons qu'il suffit de prouver le résultat pour une matrice par classe de similitude. En effet si \( A=\exp(B)\) et si \( M\) est inversible alors 
    \begin{subequations}    \label{EqqACuGK}
        \begin{align}
            \exp(MBM^{-1})&=\sum_k\frac{1}{ k! }(MBM^{-1})^k\\
            &=\sum_k\frac{1}{ k! }MB^kM^{-1}\\
            &=M\exp(B)M^{-1}.
        \end{align}
    \end{subequations}
    Donc \( MAM^{-1}=\exp(MBM^{-1})\). Nous pouvons donc nous contenter de trouver un logarithme pour les blocs de Jordan. Nous supposons donc que \( A=(\mtu+N)\) avec \( N^m=0\). En nous inspirant de \eqref{EqweEZnV}, nous posons
    \begin{equation}
        D(t)=tN-\frac{ t^2 }{ 2 }N^2+\ldots +(-1)^m\frac{ t^{m-1} }{ m-1 }N^{m-1}
    \end{equation}
    et nous allons prouver que \(  e^{D(1)}=\mtu+N\). Notons que \( N\) étant nilpotente, cette somme ainsi que toutes celles qui viennent sont finies. Il n'y a donc pas de problèmes de convergences dans cette preuve (si ce n'est les passages des équations \eqref{EqqACuGK}).

    Nous posons \( S(t)= e^{D(t)}\) (la somme est finie), et nous avons
    \begin{equation}
        S'(t)=D'(t) e^{D(t)}
    \end{equation}
    Afin d'obtenir une expression qui donne \( S'\) en termes de \( S\), nous multiplions par \( (\mtu+tN)\) en remarquant que \( (\mtu+tN)D'(t)=N\) nous avons
    \begin{equation}
        (\mtu+tN)S'(t)=NS(t).
    \end{equation}
    En dérivant à nouveau,
    \begin{equation}    \label{EqKjccqP}
        (\mtu+tN)S''(t)=0.
    \end{equation}
    La matrice \( (\mtu+tN)\) est inversible parce que son noyau est réduit à \( \{ 0 \}\). En effet si \( (\mtu+tN)x=0\), alors \( Nx=-\frac{1}{ t }x\), ce qui est impossible parce que \( N\) est nilpotente. Ce que dit l'équation \eqref{EqKjccqP} est alors que \( S''(t)=0\). Si nous développons \( S(t)\) en puissances de \( t\) nous nous arrêtons au terme d'ordre \( 1\) et nous avons
    \begin{equation}
        S(t)=S(0)+tS'(0)=\mtu+tD'(0)=1+tN.
    \end{equation}
    En \( t=1\) nous trouvons \( S(1)=\mtu+N\). La matrice \( D(1)\) donnée est donc bien un logarithme de $\mtu+N$.
\end{proof}

\begin{proposition}[\cite{fJhCTE}]
    Si \( A\in \eM(n,\eR)\) a un polynôme caractéristique scindé, alors \( A\) est diagonalisable si et seulement si \( e^A\) est diagonalisable.
\end{proposition}
\index{décomposition!Dunford!application}
\index{exponentielle!de matrice}
\index{diagonalisable!exponentielle}

\begin{proof}
    Si \( A\) est diagonalisable, alors il existe une matrice inversible \( M\) telle que \( D=M^{-1}AM\) soit diagonale (c'est la définition \ref{DefCNJqsmo}). Dans ce cas nous avons aussi \( (M^{-1}AM)^k=M^{-1}A^kM\) et donc \( M^{-1}e^AM=e^{M^{-1}AM}=e^D\) qui est diagonale.

    La partie difficile est donc le contraire. 
    
    \begin{subproof}
        \item[Qui est diagonalisable et comment ?]
            Nous supposons que \( e^A\) est diagonalisable et nous écrivons la décomposition de Dunford (théorème \ref{ThoRURcpW}) :
            \begin{equation}
                A=S+N
            \end{equation}
            où \( S\) est diagonalisable, \( N\) est nilpotente, \( [S,N]=0\). Nous avons besoin de prouver que \( N=0\).
    
            Les matrices \( A\) est \( S\) commutent; en passant au développement nous en déduisons que \( A\) et \( e^S\) commutent, puis encore en passant au développement que \( e^A\) et \( e^S\) commutent. Vu que \( S\) est diagonalisable, \( e^S\) l'est et par hypothèse \( e^A\) est également diagonalisable. Donc \( e^A\) et \( e^{-S}\) sont simultanément diagonalisables par la proposition \ref{PropGqhAMei}.

            Étant donné que \( A\) et \( S\) commutent, nous avons \( e^N=e^{A-S}=e^Ae^{-S}\), et nous en déduisons que \( e^N\) est diagonalisable vu que les deux facteurs \( e^A\) et \( e^{-S}\) sont simultanément diagonalisables.

        \item[Unipotence]

            Si \( r\) est le degré de nilpotence de \( N\), nous avons
            \begin{equation}    \label{EqQHjvLZQ}
                e^N-\mtu=N+\frac{ N^2 }{2}+\ldots +\frac{ N^{r-1} }{ (r-1)! }.
            \end{equation}
            Donc
            \begin{equation}
                (e^N-\mtu)^k=\left( N+\frac{ N^2 }{2}+\ldots +\frac{ N^{r-1} }{ (r-1)! } \right)^k
            \end{equation}
            où le membre de droite est un polynôme en \( N\) dont le terme de plus bas degré est de degré \( k\). Donc \( (e^N-\mtu)\) est nilpotente et \( e^N\) est unipotente.

            Si \( M\) est la matrice qui diagonalise \( e^N\), alors la matrice diagonale \( M^{-1}e^NM\) est tout autant unipotente que \( e^N\) elle-même. En effet,
            \begin{subequations}
                \begin{align}
                    (M^{-1}e^NM-\mtu)^r&=\sum_{k=0}^r\binom{ r }{ k }(-1)^{r-k}M^{-1}(e^N)^kM\\
                    &=M^{-1}\left( \sum_{k=0}^r\binom{ r }{ k }(-1)^{r-k}(e^N)^k \right)M\\
                    &=M^{-1}(e^N-\mtu)^rM\\
                    &=0.
                \end{align}
            \end{subequations}

            La matrice \( M^{-1}e^NM\) est donc une matrice diagonale et unipotente; donc \( M^{-1}e^NM=\mtu\), ce qui donne immédiatement que \( e^N=\mtu\).

        \item[Polynômes annulateurs]

            En reprenant le développement \eqref{EqQHjvLZQ} sachant que \( e^N=\mtu\), nous savons que
            \begin{equation}
                N+\frac{ N^2 }{2}+\ldots +\frac{ N^{r-1} }{ (r-1)! }=0.
            \end{equation}
            Dit en termes pompeux (mais non moins porteurs de sens), le polynôme
            \begin{equation}
                Q(X)=X+\frac{ X^2 }{2}+\ldots +\frac{ X^{r-1} }{ (r-1)! }
            \end{equation}
            est un polynôme annulateur de \( N\).
            
            La proposition \ref{PropAnnncEcCxj} stipule que le polynôme minimal d'un endomorphisme divise tous les polynômes annulateurs. Dans notre cas, \( X^r\) est un polynôme annulateur et donc le polynôme minimal de \( N\) est de la forme \( X^k\). Donc il est \( X^r\) lui-même.
            
            Nous avons donc \( X^r\divides Q\). Mais \( Q\) est un polynôme contenant le monôme \( X\) donc \( X^r\) ne peut diviser \( Q\) que si \( r=1\). Nous en concluons que \( X\) est un polynôme annulateur de \( N\). C'est à dire que \( N=0\).

        \item[Conclusion]

            Vu que Dunford dit que \( A=S+N\) et que nous venons de prouver que \( N=0\), nous concluons que \( A=S\) avec \( S\) diagonalisable.

    \end{subproof}
\end{proof}

\begin{proposition}[\cite{fJhCTE}]
    Si \( A\in \eM(n,\eC)\) est telle que \( \rho(A)<1\), alors \( A^n\to 0\).
\end{proposition}

\begin{proof}
    Nous nous plaçons dans une base des espaces caractéristiques de \( A\), c'est à dire que nous supposons que la matrice \( A\) a la forme
    \begin{equation}        \label{EqWMvkgLo}
        A=\begin{pmatrix}
            \lambda_1\mtu+N_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_s\mtu+N_s
        \end{pmatrix}
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de \( A\) et les \( N_i\) sont nilpotentes. En effet nous savons que l'espace caractéristique \( F_{\lambda_i}\) est l'espace de nilpolence de \( A-\lambda_i\mtu\). Si nous notons \( A_i\) la restriction de \( A\) à cet espace, la matrice \( N_i=A_i-\lambda_i\mtu\) est nilpotente. Du coup \( A_i=\lambda_I\mtu+N_i\) et nous avons bien la décomposition \eqref{EqWMvkgLo}.

    Nous avons donc \( A^n\to 0\) si et seulement si \( (N_i+\lambda_i\mtu)^n\to 0\) pour tout \( i\). Soit donc \( N\) nilpotente et \( \lambda<1\) (parce que nous savons que toutes les valeurs propres de \( A\) sont inférieures à un). Nous avons
    \begin{equation}
            (\lambda\mtu+N)^n=\sum_{k=0}^n\binom{ n }{ k }\lambda^{n-k}N^{k}
            =\sum_{k=0}^{r-1}\binom{ n }{ k }\lambda^{n-k}N^{k}.
    \end{equation}
    Nous voyons que le nombre de termes dans la somme ne dépend pas de \( n\). De plus pour chacun de termes, la puissance de \( N\) ne dépend pas non plus de \( n\). Le terme
    \begin{equation}
        \binom{ n }{ k }\lambda^{n-k}\leq P(n)\lambda^{n-k}
    \end{equation}
    où \( P\) est un polynôme tend vers zéro lorsque \( n\) devient grand parce que c'est une cas polynôme fois exponentielle.
\end{proof}

\begin{proposition}
    Soit \( A\in\GL(n,\eC)\). La suite \( (A^k)_{k\in \eZ}\) est bornée si et seulement si \( A\) est diagonalisable et \( \Spec(A)\subset \gS^1\).
\end{proposition}

\begin{proof}
    Si \( A\) est diagonalisable avec les valeurs propres \( \lambda_i\) de norme \( 1\) dans \( \eC\), alors \( A^k\) est la matrice diagonale avec les \( \lambda_i^k\) sur la diagonale. Cela reste borné pour toute valeur entière de \( k\).

    En ce qui concerne l'autre sens, nous supposons encore que
    \begin{equation}
        A=\begin{pmatrix}
            \lambda_1\mtu+N_1    &       &       \\
                &   \ddots    &       \\
                &       &   \lambda_s\mtu+N_s
        \end{pmatrix},
    \end{equation}
    et nous regardons un des blocs. Nous voulons prouver que \( N=0\) et que \( | \lambda |=1\).
    
    Nous commençons par regarder ce qu'implique le fait que \( (\lambda \mtu+N)^n\) reste borné pour \( n>0\). En notant \( r\) l'ordre de nilpotence de \( N\), nous avons le développement
    \begin{equation}
        (\lambda\mtu+N)^n=\sum_{k=0}^{r-1}\binom{ n }{ k }N^k\lambda^{n-k}.
    \end{equation}
    Par la proposition \ref{PropMWWJooVIXdJp}, une matrice nilpotente s'écrit dans une base sous la forme
    \begin{equation}
        N=\begin{pmatrix}
             0   &   1    &       &       \\
                &   0    &   1    &       \\
                & &   \ddots   &   \ddots    &      \\ 
                &&       &   0    &   1     \\
                &&       &      &   0     
         \end{pmatrix}
    \end{equation}
    et effectuer \( A^k\) revient à décaler la diagonale de \( 1\). Donc la famille
    \begin{equation}
        \{ \mtu,N,\ldots, N^{r-1} \}
    \end{equation}
    est libre. Par conséquent la suite \( (\lambda\mtu+N)^n\) restera bornée si et seulement si chacun des termes 
    \begin{equation}    \label{EqXRDVDCM}
        \binom{ n }{ k }N^k\lambda^{n-k}
    \end{equation}
    reste borné. Le premier terme étant \( \lambda^n\mtu\), nous avons obligatoirement \( | \lambda |\leq 1\). Si \( | \lambda |<1\), alors le coefficient \( \binom{ n }{ k }\lambda^{n-k}\) tend vers zéro. Si \( | \lambda |=1\) par contre ce coefficient tend vers l'infini et la seule façon pour que \eqref{EqXRDVDCM} reste borné est que \( N=0\). Nous avons donc deux possibilités :
    \begin{itemize}
        \item \( | \lambda |<1\)
        \item \( | \lambda |=1\) et \( N=0\).
    \end{itemize}

    Nous nous tournons maintenant sur la contrainte que \( (\lambda\mtu+N)^n\) doive rester borné pour \( n<0\). Nous avons
    \begin{equation}
        \lambda\mtu+N=\lambda(\mtu+\lambda^{-1}N),
    \end{equation}
    et nous pouvons appliquer la proposition \ref{PropQAjqUNp} à l'opérateur nilpotent \( -\lambda^{-1} N\) pour avoir
    \begin{equation}
        (\mtu+\lambda^{-1}N)^{-1}=\mtu+\sum_{k=1}^{\infty}(-\lambda)^{-1}N^k.
    \end{equation}
    Ceci pour dire que \( (\lambda\mtu+N)^{-1}=\lambda^{-1}(\mtu+\lambda^{-1}N')\) pour une autre matrice nilpotente \( N'\). Le travail déjà fait, appliqué à \( \lambda^{-1}\) et \( N'\), nous donne deux possibilités :
    \begin{itemize}
        \item \( | \lambda^{-1} |<1\)
        \item \( | \lambda^{-1} |=1\) et \( N'=0\).
    \end{itemize}
    La possibilité \( | \lambda^{-1} |<1\) est exclue parce qu'elle impliquerait \( | \lambda |>1\) qui avait déjà été exclu. Il ne reste donc que la possibilité \( | \lambda |=1\) et \( N=N'=0\).
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Mini introduction au produit tensoriel}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SeOOpHsn}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Définitions}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( E\), un espace vectoriel de dimension finie. Si \( \alpha\) et \( \beta\) sont deux formes linéaires sur un espace vectoriel \( E\), nous définissons \( \alpha\otimes \beta\) comme étant la \( 2\)-forme donnée par
\begin{equation}
    (\alpha\otimes \beta)(u,v)=\alpha(u)\beta(v).
\end{equation}
Si \( a\) et \( b\) sont des vecteurs de \( E\), ils sont vus comme des formes sur \( E\) via le produit scalaire et nous avons
\begin{equation}
    (a\otimes b)(u,v)=(a\cdot u)(b\cdot v).
\end{equation}
Cette dernière équation nous incite à pousser un peu plus loin la définition de \( a\otimes b\) et de simplement voir cela comme la matrice de composantes
\begin{equation}
    (a\otimes b)_{ij}=a_ib_j.
\end{equation}
Cette façon d'écrire a l'avantage de ne pas demander de se souvenir qui est une vecteur ligne, qui est un vecteur colonne et où il faut mettre la transposée. Évidemment \( (a\otimes b)\) est soit \( ab^t\) soit \( a^tb\) suivant que \( a\) et \( b\) soient ligne ou colonne.

\begin{lemma}   \label{LemMyKPzY}
    Soient \( x,y\in E\) et \( A,B\) deux opérateurs linéaires sur \( E\) vus comme matrices. Alors
    \begin{equation}        \label{EqXdxvSu}
        (Ax\otimes By)=A(x\otimes y)B^t.
    \end{equation}
\end{lemma}

\begin{proof}
    Calculons la composante \( ij\) de la matrice \( (Ax\otimes By)\). Nous avons
    \begin{subequations}
        \begin{align}
            (Ax\otimes By)_{ij}&=(Ax)_i(By)_j\\
            &=\sum_{kl}A_{ik}x_kB_{jl}y_l\\
            &=A_{ik}(x\otimes y)_{kl}B_{jl}\\
            &=\big( A(x\otimes y)B^t \big)_{ij}.
        \end{align}
    \end{subequations}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Formes bilinéaires et quadratiques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecTQkRXIu}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Généralités}
%---------------------------------------------------------------------------------------------------------------------------

Les applications multilinéaires ont déjà été définies en la définition \ref{DefFRHooKnPCT}; nous donnons ici une définition plus explicite dans le cas des applications bilinéaires.
\begin{definition}
    Une \defe{forme bilinéaire}{forme!bilinéaire} sur un espace vectoriel \( E\) est une application \( b\colon E\times E\to \eK\) telle que
    \begin{enumerate}
        \item
            \( b(u,v)=b(v,u)\),
        \item
            \( b(u+v,w)=b(u,w)+b(v,w)\),
        \item
            \( b(\lambda u,v)=\lambda b(u,v)\)
    \end{enumerate}
    pour tout \( u,v,w\in E\) et \( \lambda\in \eK\) où \( \eK\) est une corps commutatif.
\end{definition}

\begin{definition}[\cite{RUAoonJAym}]   \label{DefBSIoouvuKR}
    Soit un espace vectoriel \( E\) et \( \eF\) un corps de caractéristique différente de \( 2\). Une \defe{forme quadratique}{forme!quadratique} sur \( E\) est une application \( q\colon V\to \eF\) pour laquelle il existe une forme bilinéaire symétrique \( b\colon V\times V\to \eF\) satisfaisant \( q(x)=b(x,x)\) pour tout \( x\in V\).

    L'ensemble des formes quadratiques réelles sur \( E\) est noté \( Q(E)\)\nomenclature[B]{\( Q(E)\)}{formes quadratiques réelles sur \( E\)}.
\end{definition}

\begin{lemma}
    Si \( q\) est une forme quadratique, il existe une unique forme bilinéaire \( b\) telle que \( q(x)=b(x,x)\).
\end{lemma}

\begin{proof}
    L'existence n'est pas en cause : c'est la définition d'une forme quadratique. Pour l'unicité, étant donné une forme quadratique, la forme bilinéaire \( b\) doit forcément vérifier l'\defe{identités de polarisation}{identité!polarisation}\index{polarisation (identité)} :
\begin{equation}    \label{EqMrbsop}
    b(x,y)=\frac{ 1 }{2}\big( q(x)+q(y)-q(x-y) \big).
\end{equation}
Elle est donc déterminée par \( q\).
\end{proof}
Notons la division par \( 2\) qui est le pourquoi de la demande de la caractéristique différente de \( 2\) pour \( \eF\) dans la définition de forme quadratique.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Topologie}
%---------------------------------------------------------------------------------------------------------------------------

La topologie considérée sur \( Q(E)\) est celle de la norme
\begin{equation}    \label{EqZYBooZysmVh}
    N(q)=\sup_{\| x \|_E=1}| q(x) |,
\end{equation}
qui du point de vue de \( S_n(\eR)\) est
\begin{equation}    
    N(A)=\sup_{\| x \|_E=1}| x^tAx |.
\end{equation}
Notons que à droite, c'est la valeur absolue usuelle sur \( \eR\).

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Matrice associée}
%---------------------------------------------------------------------------------------------------------------------------

Si une base \( \{ e_i \}_{i=1,\ldots, n}\) de l'espace vectoriel \( E\) est donnée, la \defe{matrice associée}{matrice!associée à une forme quadratique}\index{forme!quadratique!matrice associée} à la forme bilinéaire \( b\) sur \( E\) est la matrice d'éléments
\begin{equation}
    B_{ij}=b(e_i,e_j).
\end{equation}
Notons que la matrice associée à une forme bilinéaire (ou quadratique associée) est uniquement valable pour une base donnée. Si nous changeons de base, la matrice change. Cependant lorsque nous travaillons sur \( \eR^n\), la base canonique est tellement canonique que nous allons nous permettre de parler de «la» matrice associée à une forme bilinéaire. 

Si \( B_{ij}\) est la matrice associée à la forme bilinéaire \( b\) alors la valeur de \( b(u,v)\) se calcule avec la formule
\begin{equation}
    b(x,y)=\sum_{i,j}B_{ij}x_iy_j
\end{equation}
lorsque \( x_i\) et \( y_j\) sont les coordonnées de \( x\) et \( y\) dans la base choisie.

\begin{proposition} \label{PropFSXooRUMzdb}
    Soit \( \{ e_i \}\) une base de \( E\). L'application
    \begin{equation}
        \begin{aligned}
            \phi\colon Q(E)&\to S(n,\eR) \\
            q&\mapsto \big(   b(e_i,e_j)   \big)_{i,j}
        \end{aligned}
    \end{equation}
    où \( b\) est forme bilinéaire associée à \( q\) est une bijection linéaire et continue.
\end{proposition}

\begin{proof}
    Si \( \phi(q)=\phi(q')\); alors
    \begin{equation}
        q(x)=\sum_{i,j}\phi(q)_{ij}x_ix_j=\sum_{i,j}\phi(q')_{ij}x_ix_j=q'(x).
    \end{equation}
    Donc \( q=q'\). L'application \( \phi\) est donc injective

    De plus elle est surjective parce que si \( B\in S(n,\eR)\) alors la forme quadratique
    \begin{equation}
        q(x)=\sum_{i,j}B_{ij}x_ix_j
    \end{equation}
    a évidemment \( B\) comme matrice associée. L'application \( \phi\) est donc surjective.

    Notre application \( \phi\) est de plus linéaire parce que l'association d'une forme quadratique à la forme bilinéaire associée est linéaire.

    En ce qui concerne la continuité, nous la prouvons en zéro en considérant une suite convergente \( q_n\stackrel{Q(E)}{\longrightarrow}0\). C'est à dire que
    \begin{equation}
        \sup_{\| x \|=1}| q_n(x) |\to 0.
    \end{equation}
    Nous rappelons l'identité de polarisation : 
    \begin{equation}
        b_n(x,y)=\frac{ 1 }{2}\big( q_n(x-y)-q(x)-q(y) \big).
    \end{equation}
    En ce qui concerne deux des trois termes, il n'y a pas de problèmes :
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|=\big| b_n(e_i,e_j) \big|\leq\frac{ 1 }{2}\big| b_n(e_i-e_j) \big|+\frac{ 1 }{2}\big| q_n(e_i) \big|+\frac{ 1 }{2}\big| q_n(e_j) \big|.
    \end{equation}
    Si \( n\) est assez grand, nous avons tout de suite
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|\leq \frac{ 1 }{2}\big| q_n(e_i-e_j) \big|+\epsilon.
    \end{equation}
    Nous définissons \( e_{ij}\) et \( \alpha_{ij}\) de telle sorte que \( e_i-e_j=\alpha_{ij}e_{ij}\) avec \( \| e_{ij} \|=1\). Si \( \alpha=\max\{ \alpha_{ij},1 \}\) alors nous avons
    \begin{equation}
        q_n(e_i-e_j)=\alpha_{ij}^2q_n(e_{ij})\leq \alpha^2q_n(e_{ij}).
    \end{equation}
    Il suffit maintenant de prendre \( n\) assez grand pour avoir \( \sup_{\| x \|=1}| q_n(x) |\leq \frac{ \epsilon }{ \alpha^2 }\) pour avoir
    \begin{equation}
        \big| \phi(q_n)_{ij} \big|\leq \frac{ \epsilon }{2}+\frac{ \epsilon }{ \alpha^2 }.
    \end{equation}
\end{proof}

\begin{proposition}\label{PropFWYooQXfcVY}
    Dans la base de diagonalisation de sa matrice associée, une forme quadratique a la forme
    \begin{equation}
        q(x)=\sum_i\lambda_ix_i^2
    \end{equation}
    où les \( \lambda_i\) sont les valeurs propres de la matrice associée à \( q\).
\end{proposition}

\begin{proof}
Soit \( q\) une forme quadratique et \( b\) la forme bilinéaire associée. Si \( \{ f_i \}\) est une base de diagonalisation de la matrice de \( b\) alors dans cette base nous avons
\begin{equation}
    q(x)=b(x,x)=\sum_{ij}x_ix_jb(f_i,f_j)=\sum_i\lambda_ix_i^2
\end{equation}
où les \( \lambda_i\) sont les valeurs propres de la matrice de \( b\).
\end{proof}
Notons que si nous choisissons une autre base de diagonalisation, les \( \lambda_i\) ne changement pas (à part l'ordre éventuellement). Cela pour dire que nous nous permettrons de parler des \defe{valeurs propres}{valeur propre!d'une forme quadratique} d'une forme quadratique comme étant les valeurs propres de la matrice associée.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelque mots à propos de matrices}
%---------------------------------------------------------------------------------------------------------------------------

Si $g$ est une application bilinéaire sur $\eR^2$, nous disons qu'elle est
\begin{enumerate}
\item
\defe{définie positive}{application!définie positive} si $g(u,u)\geq 0$ pour tout $u\in\eR^2$ et $g(u,u)=0$ si et seulement si $u=0$.
\item
\defe{semi-définie positive}{application!semi-définie positive} si $g(u,u)\geq 0$ pour tout $u\in\eR^2$. Nous dirons aussi parfois qu'elle est simplement «positive».
\end{enumerate}
Cela est évidemment à lier à la définition \ref{DefAWAooCMPuVM} : une application bilinéaires est définie positive si et seulement si sa matrice symétrique associée l'est.

\begin{proposition}     \label{PropcnJyXZ}
    Soit $M$, une matrice $2\times 2$ symétrique. Nous avons
    \begin{enumerate}
        \item
        $\det M>0$ et $\tr(M)>0$ implique $M$ définie positive,
        \item
        $\det M>0$ et $\tr(M)<0$ implique $M$ définie négative,
    \item   \label{ItemluuFPN}
        $\det M<0$ implique ni semi définie positive, ni définie négative 
        \item
        $\det M=0$ implique $M$ semi-définie positive ou semi-définie négative.
    \end{enumerate}
\end{proposition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Dégénérescence}
%---------------------------------------------------------------------------------------------------------------------------

Soit \( b\), une forme bilinéaire symétrique non dégénérée  sur l'espace vectoriel \( E\) de dimension \( n\) sur \( \eK\) où \( \eK\) est un corps de caractéristique différente de \( 2\). Nous notons \( q\) la forme quadratique associée.

\begin{definition}
    Une forme bilinéaire est \defe{non dégénérée}{forme!bilinéaire!non dégénérée} \( b(x,z)=0\) pour tout \( z\) implique \( x=0\).
\end{definition}

\begin{lemma}   \label{LemyKJpVP}
    Soit \( b\) une forme bilinéaire non dégénérée. Si \( x\) et \( y\) sont tels que \( b(x,z)=b(y,z)\) pour tout \( z\), alors \( x=y\).
\end{lemma}

\begin{proof}
    C'est immédiat du fait de la linéarité en le premier argument et de la non-dégénérescence : si \( b(x,z)-b(y,z)=0\) alors
    \begin{equation}
        b(x-y,z)=0
    \end{equation}
    pour tout \( z\), ce qui implique \( x-y=0\).
\end{proof}

\begin{proposition}
    La forme bilinéaire \( b\) est non-dénénérée si et seulement si sa matrice associée est inversible.
\end{proposition}

\begin{proof}
    Nous savons que la matrice associée est symétrique et qu'elle peut donc être diagonalisée (théorème \ref{ThoeTMXla}). En nous plaçant dans une base de diagonalisation, nous devons prouver que la forme est non-dégénérée si et seulement si les éléments diagonaux de la matrice sont tous non nuls.

    Écrivons \( b(x,z)\) en choisissant pour \( z\) le vecteur de base \( e_k\) de composantes \( (e_k)_j=\delta_{kj}\) :
    \begin{equation}
            b(x,e_k)=\sum_{ij}x_i(e_k)_j
            =\sum_i b_{ik}x_i
            =b_{kk}x_k.
    \end{equation}
    Si \( b\) est dégénérée et si \( x\) est un vecteur non nul (disons que la composante \( x_i\) est non nulle) de \( E\) tel que \( b(x,z)=0\) pour tout \( z\in E\), alors \( b_{ii}=0\), ce qui montre que la matrice de \( b\) n'est pas inversible.

    Réciproquement si la matrice de \( b\) est inversible, alors tous les \( b_{kk}\) sont différents de zéro, et le seul vecteur \( x\) tel que \( b_{kk}x_k=0\) pour tout \( k\) est le vecteur nul.
\end{proof}


\begin{definition}[Isotropie]   \label{DefVKMnUEM}
    Un vecteur est \defe{isotrope}{isotrope (vecteur)} pour \( b\) si il est perpendiculaire à lui-même; en d'autres termes, \( x\) est isotrope si et seulement si \( b(x,x)=0\). Un sous-espace \( W\subset E\) est \defe{totalement isotrope}{isotrope!totalement} si pour tout \( x,y\in W\), nous avons \( b(x,y)=0\).

    Le \defe{cône isotrope}{isotrope!cône} de \( b\) est l'ensemble de ses vecteurs isotropes :
    \begin{equation}
        C(b)=\{ x\in E\tq b(x,x)=0 \}.
    \end{equation}
\end{definition}
Nous introduisons quelque notations. D'abord pour \( y\in E\) nous notons
\begin{equation}
    \begin{aligned}
        \Phi_y\colon E&\to \eR \\
        x&\mapsto b(x,y) 
    \end{aligned}
\end{equation}
et ensuite
\begin{equation}
    \begin{aligned}
        \Phi\colon E&\to E^* \\
        y&\mapsto \Phi_y. 
    \end{aligned}
\end{equation}
\begin{definition}
    Le fait pour une forme bilinéaire \( b\) d'être dégénérée signifie que l'application \( \Phi\) n'est pas injective. Le \defe{noyau}{noyau!d'une forme bilinéaire} de la forme bilinéaire est celui de \( \Phi\), c'est à dire
    \begin{equation}
        \ker(b)=\{ z\in E\tq b(z,y)=0\,\forall y\in E \}.
    \end{equation}
    Autrement dit, \( \ker(b)=E^{\perp}\) où le perpendiculaire est pris par rapport à \( b\).
\end{definition}
Notons tout de même que nous utilisons la notation \( \perp\) même si \( b\) est dégénérée et éventuellement pas positive; c'est à dire même si la formule \( (x,y)\mapsto b(x,y)\) ne fournit pas un produit scalaire.

\begin{proposition}[\cite{RTzQrdx}]     \label{PropHIWjdMX}
    Soit \( b\) une forme bilinéaire et symétrique. Alors
    \begin{enumerate}
        \item
            \( \ker(b)\subset C(b)\) (cône d'isotropie, définition \ref{DefVKMnUEM})
        \item
            si \( b\) est positive alors \( \ker(b)=C(b)\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Si \( z\in\ker(b)\) alors pour tout \( y\in E\) nous avons \( b(z,y)=0\). En particulier pour \( y=z\) nous avons \( b(z,z,)=0\) et donc \( z\in C(b)\).
        \item
            Soit \( b\) positive et \( x\in C(b)\). Par l'inégalité de Cauchy-Schwarz (proposition \ref{ThoAYfEHG}) nous avons
            \begin{equation}
                | b(x,y) |\leq \sqrt{   b(x,x)b(y,y) }=0.
            \end{equation}
            Donc pour tout \( y\) nous avons \( b(x,y)=0\).
    \end{enumerate}
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Inégalité de Minkowski}
%---------------------------------------------------------------------------------------------------------------------------

Ce qui est couramment nommé «inégalité de Minkowski» est la proposition \ref{PropInegMinkKUpRHg} dans les espaces \( L^p\). Nous allons en donner ici un cas très particulier.

\begin{proposition} \label{PropACHooLtsMUL}
    Si \( q\) est une forme quadratique sur \( \eR^n\) et si \( x,y\in \eR^n\) alors
    \begin{equation}
        \sqrt{q(x+y)}\leq\sqrt{q(x)}+\sqrt{q(y)}.
    \end{equation}
\end{proposition}

\begin{proof}
    La proposition \ref{PropFWYooQXfcVY} nous permet de «diagonaliser» la forme quadratique \( q\). Quitte à ne plus avoir une base orthonormale, nous pouvons renormaliser les vecteurs de base pour avoir
    \begin{equation}
        q(x)=\sum_ix_i^2.
    \end{equation}
    Le résultat n'est donc rien d'autre que l'inégalité triangulaire pour la norme euclidienne usuelle, laquelle est démontrée dans la proposition \ref{PropEQRooQXazLz}.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Ellipsoïde}
%---------------------------------------------------------------------------------------------------------------------------

\begin{lemma}   \label{LemYVWoohcjIX}
    Toute matrice peut être décomposée de façon unique en une partie symétrique et une partie antisymétrique. Cette décomposition est donnée par
\begin{equation}\label{subEqHIQooyhiWM}
    \begin{aligned}[]
            S&=\frac{ M+M^t }{ 2 },&A&=\frac{ M-M^t }{ 2 }
    \end{aligned}
\end{equation}
\end{lemma}

\begin{proof}
    L'existence est une vérification immédiate de \( S+A=M\) en utilisant \eqref{subEqHIQooyhiWM}. Pour l'unicité, si \( S+A=S'+A'\) alors \( S-S'=A-A'\). Mais \( S-S'\) est symétrique et \( A-A'\) est antisymétrique; l'égalité implique l'annulation des deux membres, c'est à dire \( S=S'\) et \( A=A'\).
\end{proof}

\begin{definition}  \label{DefOEPooqfXsE}
    Un \defe{ellipsoïde}{ellipsoïde} dans \( \eR^n\) centré en \( v\) est le lieu des points \( x\) vérifiant l'équation
    \begin{equation}\label{EqSNWooXfbTH}
        (x-v)^t M(x-v)=1
    \end{equation}
    où \( M\) est une matrice symétrique strictement définie positive\footnote{Définition \ref{DefAWAooCMPuVM}.}.

    Lorsque nous parlons d'ellipsoïde \emph{plein}, il suffit de changer l'égalité en une inégalité.
\end{definition}
Une autre façon d'écrire la relation \eqref{EqSNWooXfbTH} est d'écrire \( \langle (x-v),M(x,v)\rangle\) en utilisant le produit scalaire.

\begin{remark}
    Le fait que \( M\) soit symétrique n'est pas tout à fait obligatoire; la chose important est que toutes les valeurs propres soient strictement positives. En effet si \( M\) a toutes ses valeurs propres strictement positives, nous nommons \( S\) la partie symétrique de \( M\) et \( A\) la partie antisymétrique (lemme \ref{LemYVWoohcjIX}). Alors pour tout \( x\in \eR^n\) nous avons
    \begin{equation}
        x^tAx=\langle x, Ax\rangle =\langle A^tx,x \rangle =-\langle Ax, x\rangle =-\langle x,Ax\rangle ,
    \end{equation}
    donc \( x^tAx=0\). L'équation \( x^tMx=1\) est donc équivalente à \( x^tSx=1\) (elles ont les mêmes solutions).
    
    De plus \( S\) reste strictement définie positive parce que pour tout \( x\in \eR^n\) nous avons 
    \begin{equation}
        0<x^tMx=x^tSx.
    \end{equation}
\end{remark}

\begin{proposition}\label{PropWDRooQdJiIr}
    Si \( \ellE\) est un ellipsoïde centrée à l'origine, il existe une base de \( \eR^n\) dans laquelle son équation est :
    \begin{equation}
        \sum_{i=1}^n\frac{ x_i^2 }{ a_i^2 }=1.
    \end{equation}
\end{proposition}

\begin{proof}
    Nous avons une matrice symétrique strictement définie positive \( S\) telle que l'équation soit \( \langle x, Sx\rangle =1\). Le théorème spectral \ref{ThoeTMXla} nous fournit une base orthonormale \( \{ e_i \}\) dans laquelle \( Se_i=\lambda_ie_i\) avec \( \lambda_i>0\). En substituant dans l'équation \( \langle x, Sx\rangle =1\) nous trouvons l'équation
    \begin{equation}
        \sum_i\lambda_ix_i^2=1.
    \end{equation}
    En posant \( a_i=\frac{1}{ \sqrt{\lambda_i} }\), nous trouvons le résultat.  Cette définition des \( a_i\) est toujours possible parce que \( \lambda_i>0\).
\end{proof}

\begin{corollary}   \label{CorKGJooOmcBzh}
    Un ellipsoïde plein centré en l'origine admet une équation de la forme \( q(x)\leq 1\) où \( q\) est une forme quadratique strictement définie positive.
\end{corollary}
Pour rappel de notation, l'ensemble des formes quadratiques strictement définies positives sur l'espace vectoriel \( E\) est noté \( Q^{++}(E)\).

\begin{proof}
    Soit \( \{ e_i \}\) une base de \( \eR^n\) telle que l'ellipsoïde \( \ellE\) ait pour équation
    \begin{equation}
        \sum_{i=1}^n\frac{ x_i^2 }{ a_i^2 }\leq 1.
    \end{equation}
    Nous considérons la forme quadratique
    \begin{equation}
        \begin{aligned}
            q\colon \eR^n&\to \eR \\
            x&\mapsto \sum_{i=1}^n\frac{ \langle x, e_i\rangle^2 }{ a_i^2 }. 
        \end{aligned}
    \end{equation}
    Nous avons évidemment \( \ellE=\{ x\in \eR^n\tq q(x)\leq 1 \}\). De plus la forme \( q\) est strictement définie positive parce que dès que \( x\neq 0\), au moins un des produits scalaires \( \langle x, e_i\rangle \) est non nul et \( q(x)> 0\).
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Théorème spectral auto-adjoint}
%---------------------------------------------------------------------------------------------------------------------------

\begin{definition}
    Si \( E\) est un espace euclidien, un endomorphisme \( f\colon E\to E\) est \defe{auto-adjoint}{endomorphisme!auto-adjoint} si pour tout \( x,y\in E\) nous avons \( \langle x, f(y)\rangle=\langle f(x), Y\rangle  \). 
\end{definition}
L'ensemble des opérateurs auto-adjoints de \( E\) est noté \( \gS(E)\)\nomenclature[A]{\( \gS(E)\)}{Les opérateurs auto-adjoints de $E$}. Cette notation provient du fait que dans \( \eR^n\) muni du produit scalaire usuel, les opérateurs auto-adjoints sont les matrices symétriques.

\begin{theorem}[Théorème spectral auto-adjoint] \label{ThoRSBahHH}
    Un endomorphisme auto-adjoint d'un espace euclidien
    \begin{enumerate}
        \item
            est diagonalisable dans une base orthonormée,
        \item
            a son spectre réel.
    \end{enumerate}
\end{theorem}
\index{théorème!spectral!autoadjoint}
\index{diagonalisation!endomorphisme auto-adjoint}

\begin{proof}
    Nous procédons par récurrence sur la dimension de \( E\), et nous commençons par \( n=1\)\footnote{Dans \cite{KXjFWKA}, l'auteur commence avec \( n=0\) mais moi je n'en ai \wikipedia{en}{Vacuous_truth}{pas le courage.}.}. Soit donc \( f\colon E\to E\) avec \( \langle f(x), y\rangle =\langle x, f(y)\rangle \). Étant donné que \( f\) est également linéaire, il existe \( \lambda\in \eR\) tel que \( f(x)=\lambda x\) pour tout \( x\in E\). Tous les vecteurs de \( E\) sont donc vecteurs propres de \( f\).

    Passons à la récurrence. Nous considérons \( \dim(E)=n+1\) et \( f\in\gS(E)\). Nous considérons la forme bilinéaire symétrique \( \Phi_f\) et la forme quadratique associée \( \phi_f\). Pour rappel,
    \begin{subequations}
        \begin{align}
        \Phi_f(x,y)=\langle x, f(y)\rangle \\
        \phi_f(x)=\Phi_f(x,x).
        \end{align}
    \end{subequations}
    Et nous allons laisser tomber les indices \( f\) pour noter simplement \( \Phi\) et \( \phi\). Étant donné que \( \overline{ B(0,1) }\) est compacte et que \( \phi\) est continue, il existe \( x_0\in\overline{ B(0,1) }\) tel que 
    \begin{equation}
        \lambda=\phi(x_0)=\sup_{x\in\overline{ B(0,1) }}\phi(x).
    \end{equation}
    Notons aussi que \( \| x_0 \|=1\) : le maximum est pris sur le bord. Nous posons
    \begin{equation}
        g=\lambda\id-f
    \end{equation}
    ainsi que 
    \begin{equation}
        \Phi_1(x,y)=\langle x, g(y)\rangle .
    \end{equation}
    Cela est une forme bilinéaire et symétrique parce que
    \begin{equation}
        \Phi_1(y,x)=\langle y, g(x)\rangle =\langle g(y), x\rangle =\langle x, g(y)\rangle =\Phi_1(x,y)
    \end{equation}
    où nous avons utilisé le fait que \( g\) était auto-adjoint et la symétrie du produit scalaire. De plus \( \Phi_1\) est semi-définie positive parce que
    \begin{equation}
        \Phi_1(x,x)=\langle x, \lambda x-f(x)\rangle =\lambda\| x \|^2-\phi(x).
    \end{equation}
    Vu que \( \lambda\) est le maximum, nous avons tout de suite \( \Phi_1(x)\geq 0\) tant que \( \| x \|=1\). Et si \( x\) n'est pas de norme \( 1\), c'est le même prix parce qu'on se ramène à \( \| x \|=1\) en multipliant par un nombre positif. Attention cependant : 
    \begin{equation}
        \Phi_1(x_0,x_0)=\lambda\| x_0 \|^2-\phi(x_0)=0.
    \end{equation}
    Donc \( \Phi_1\) a un noyau contenant \( x_0\) par la proposition \ref{PropHIWjdMX}. Nous en déduisons que \( \Image(g)\neq E\) en effet, \( x_0\in\Image(g)^{\perp}\), mais nous avons la proposition \ref{PropXrTDIi} sur les dimensions : 
    \begin{equation}
        \dim E=\dim(\Image(g))+\dim( \Image(g)^{\perp}).
    \end{equation}
    Vu que \( \Image(g)^{\perp}\) est un espace vectoriel non réduit à \( \{ 0 \}\), la dimension de \( \Image(g)\) ne peut pas être celle de \( E\). L'endomorphisme \( g\) n'étant pas surjectif, il ne peut pas être injectif non plus parce que nous sommes en dimension finie; il existe donc \( e_1\in E\) tel que \( g(e_1)=0\) et tant qu'à faire nous choisissons \( \| e_1 \|=1\) (ici la norme est bien celle de l'espace euclidien considéré). Par définition,
    \begin{equation}
        f(e_1)=\lambda e_1,
    \end{equation}
    c'est à dire que \( \lambda\in\Spec(f)\). Et \( \phi\) étant une forme quadratique réelle nous avons \( \lambda\in \eR\).

    Nous posons à présent \( H=\Span\{ e_1 \}^{\perp}\). C'est un sous-espace stable par \( f\) parce que si \( x\in H\) alors
    \begin{equation}
        \langle e_1, f(x)\rangle =\langle f(e_1j),x\rangle =\lambda\langle e_1, x\rangle =0.
    \end{equation}
    Nous pouvons donc considérer la restriction de \( f\) à \( H\) : \( f_H\colon H\to H\). Cet endomorphisme est bilinéaire et symétrique sur l'espace \( H\) de dimension inférieure à celle de \( E\), donc la récurrence nous donne une base orthonormée
    \begin{equation}
        \{ e_2,\ldots, e_n \}
    \end{equation}
    de vecteurs propres de \( f_H\). De plus les valeurs propres sont réelles, toujours par récurrence. Donc
    \begin{equation}
        \Spec(f)=\{ \lambda \}\cup\Spec(f_H)\subset \eR.
    \end{equation}
    Notons pour être complet que si \( i\geq 2\) alors
    \begin{equation}
        \langle e_1, e_i\rangle =0
    \end{equation}
    parce que le vecteur \( e_i\) est par construction choisit dans l'espace \( H=e_1^{\perp}\). Nous avons donc bien une base orthonormée de \( E\) construite sur des vecteurs propres de \( f\).
\end{proof}

\begin{corollary}   \label{CorSMHpoVK}
    Soit \( E\) un espace vectoriel ainsi que \( \phi\) et \( \psi\) des formes quadratiques sur \( E\) avec \( \psi\) définie positive. Alors il existe une base \( \psi\)-orthonormale dans laquelle \( \phi\) est diagonale.
\end{corollary}

\begin{proof}
    Il suffit de considérer l'espace euclidien \( E\) muni du produit scalaire \( \langle x, y\rangle =\psi(x,y)\). Ensuite nous diagonalisons la matrice (symétrique) de \( \phi\) pour ce produit scalaire à l'aide du théorème \ref{ThoRSBahHH}.
\end{proof}

\begin{definition}      \label{DefYNWUFc}
    Dans le cas de \( V=\eR^m\) nous avons un produit scalaire canonique. Soient $u$ et $v$, deux vecteurs de $\eR^m$. Le \defe{produit scalaire}{produit!scalaire} de $u$ et $v$, noté $\langle u, v\rangle $ ou $u\cdot v$ est le réel
	\begin{equation}		\label{EqDefProdScalsumii}
		\langle u, v\rangle =\sum_{k=1}^m u_kv_k=u_1v_1+u_2v_2+\ldots+u_mv_n.
	\end{equation}
\end{definition}

Calculons par exemple le produit scalaire de deux vecteurs de la base canonique : $\langle e_i, e_j\rangle $. En utilisant la formule de définition et le fait que $(e_i)_k=\delta_{ik}$, nous avons
\begin{equation}
	\langle e_i, e_j\rangle =\sum_{k=1}^m\delta_{ik}\delta_{jk}.
\end{equation}
Nous pouvons effectuer la somme sur $k$ en remarquant qu'à cause du $\delta_{ik}$, seul le terme avec $k=i$ n'est pas nul. Effectuer la somme revient donc à remplacer tous les $k$ par des $i$ :
\begin{equation}
	\langle e_i, e_j\rangle =\delta_{ii}\delta_{ji}=\delta_{ji}.
\end{equation}

Une des propriétés intéressantes du produit scalaire est qu'il permet de décomposer un vecteur dans une base, comme nous le montre la proposition suivante.

\begin{proposition}		\label{PropScalCompDec}
	Si nous notons $v_i$ les composantes du vecteur $v$, c'est à dire si $v=\sum_{i=1}^m v_ie_i$, alors nous avons $v_j=\langle v, e_j\rangle $.
\end{proposition}

\begin{proof}
	\begin{equation}		\label{Eqvejscalcomp}
		v\cdot e_j=\sum_{i=1}^m\langle v_ie_i, e_j\rangle =\sum_{i=1}^mv_i\langle e_i, e_j\rangle =\sum_{i=1}^mv_i\delta_{ij}
	\end{equation}
	En effectuant la somme sur $i$ dans le membre de droite de l'équation \eqref{Eqvejscalcomp}, tous les termes sont nuls sauf celui où $i=j$; il reste donc
	\begin{equation}
		v\cdot e_j=v_j.
	\end{equation}
\end{proof}

Le produit scalaire ne dépend en réalité pas de la base orthogonale choisie. 

\begin{lemma}
	Si $\{ e_i \}$ est la base canonique, et si $\{ f_i \}$ est une autre base orthonormale, alors si $u$ et $v$ sont deux vecteurs de $\eR^m$, nous avons
	\begin{equation}
		\sum_i u_iv_j=\sum_iu'_iv'_j
	\end{equation}
	où $u_i$ sont les composantes de $u$ dans la base $\{ e_i \}$ et $u'_i$ sont celles dans la base $\{ f_i \}$.
\end{lemma}

\begin{proof}
	La preuve demande un peu d'algèbre linéaire. Étant donné que $\{ f_i \}$ est une base orthonormale, il existe une matrice $A$ orthogonale ($AA^t=\mtu$) telle que $u'_i=\sum_jA_{ij}u_j$ et idem pour $v$. Nous avons alors
	\begin{equation}
		\begin{aligned}[]
			\sum_iu'_iv'_j&=\sum_i\left( \sum_jA_{ij} u_j\right)\left( \sum_k A_{ik}v_k \right)\\
			&=\sum_{ijk}A_{ij}A_{ik}u_jv_k\\
			&=\sum_{jk}\underbrace{\sum_i(A^t)_{ji}A_{ik}}_{=\delta_{jk}}u_jv_k\\
			&=\sum_{jk}\delta_{jk}u_jv_k\\
			&=\sum_ku_jv_k.
		\end{aligned}
	\end{equation}	
\end{proof}

Cette proposition nous permet de réellement parler du produit scalaire entre deux vecteurs de façon intrinsèque sans nous soucier de la base dans laquelle nous regardons les vecteurs.

Nous dirons que deux vecteurs sont \defe{orthogonaux}{orthogonal} lorsque leur produit scalaire est nul. Nous écrivons que $u\perp v$ lorsque $\langle u, v\rangle =0$.
\begin{definition}	\label{DefNormeEucleApp}
	La \defe{norme euclidienne}{norme!euclidienne!dans $\eR^m$} d'un élément de $\eR^m$ est définie par $\| u \|=\sqrt{u\cdot u}$.
\end{definition}

Cette définition est motivée par le fait que le produit scalaire $u\cdot u$ donne exactement la norme usuelle donnée par le théorème de Pythagore :
\begin{equation}
	u\cdot u=\sum_{i=1}^mu_iu_i=\sum_{i=1}^m u_i^2=u_1^2+u_2^2+\ldots+u_m^2.
\end{equation}

Le fait que $e_i\cdot e_j=\delta_{ij}$ signifie que la base canonique est \defe{orthonormée}{orthonormé}, c'est à dire que les vecteurs de la base canonique sont orthogonaux deux à deux et qu'ils ont tout $1$ comme norme.

\begin{lemma}\label{LemSclNormeXi}
	Pour tout $u\in\eR^m$, il existe un $\xi\in\eR^m$ tel que $\| u \|=\xi\cdot u$ et $\| \xi \|=1$.
\end{lemma}

\begin{proof}
	Vérifions que le vecteur $\xi=u/\| u \|$ ait les propriétés requises. D'abord $\| \xi \|=1$ parce que $u\cdot u=\| u \|^2$. Ensuite
	\begin{equation}
		\xi\cdot u=\frac{ u\cdot u }{ \| u \| }=\frac{ \| u \|^2 }{ \| u \| }=\| u \|.
	\end{equation}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Espaces hermitiens}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}  \label{DefMZQxmQ}
Si \( E\) est un espace vectoriel sur \( \eC\), nous disons qu'une application \( \langle ., .\rangle \colon E\times E\to \eC\) est un \defe{produit scalaire hermitien}{produit!scalaire!hermitien}\index{hermitien!produit scalaire} si pour tout \( u,v\in E\) nous avons
\begin{enumerate}
    \item
        \( \langle u, v\rangle =\overline{ \langle v, u\rangle  }\)
    \item
        \( \lambda\langle u, v\rangle =\langle \lambda u, v\rangle =\langle u, \bar \lambda v\rangle \)
    \item
        \( \langle u, u\rangle \in \eR^+\) et \( \langle u, u\rangle =0\) si et seulement si \( u=0\).
\end{enumerate}
\end{definition}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Coordonnées cylindriques et sphériques}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Les \defe{coordonnées cylindriques}{coordonnées!cylindrique} sont un perfectionnement des coordonnées polaires. Il s'agit simplement de donner le point $(x,y,z)$ en faisant la conversion $(x,y)\mapsto(r,\theta)$ et en gardant le $z$. Les formules de passage sont
\begin{subequations}
	\begin{numcases}{}
		x=r\cos(\theta)\\
		y=r\sin(\theta)\\
		z=z.
	\end{numcases}
\end{subequations}

Les \defe{coordonnées sphériques}{coordonnées!sphériques} sont ce qu'on appelle les «méridiens» et «longitudes» en géographie. Les formules de transformation sont 
\begin{subequations}		\label{SubEqsCoordSphe}
	\begin{numcases}{}
		x=\rho\sin(\theta)\cos(\varphi)\\
		y=\rho\sin(\theta)\sin(\varphi)\\
		z=\rho\cos(\theta)
	\end{numcases}
\end{subequations}
avec $0\leq\theta\leq\pi$ et $0\leq\varphi<2\pi$.

\begin{remark}
	Attention : d'un livre à l'autre les conventions sur les noms des angles changent. N'essayez donc pas d'étudier par cœur des formules concernant les coordonnées sphériques trouvées autre part. Par exemple sur le premier dessin de \href{http://fr.wikipedia.org/wiki/Coordonnées_sphériques}{wikipédia}, l'angle $\varphi$ est noté $\theta$ et l'angle $\theta$ est noté $\Phi$. Mais vous noterez que sur cette même page, les convention de noms de ces angles changent plusieurs fois.
\end{remark}

Trouvons le changement inverse, c'est à dire trouvons $\rho$, $\theta$ et $\varphi$ en termes de $x$, $y$ et $z$. D'abord nous avons
\begin{equation}
	\rho=\sqrt{x^2+y^2+z^2}.
\end{equation}
Ensuite nous savons que
\begin{equation}
	\cos(\theta)=\frac{ z }{ \rho }
\end{equation}
détermine de façon unique\footnote{Le problème $\rho=0$ ne se pose pas; pourquoi ?} un angle $\theta\in\mathopen[ 0 , \pi \mathclose]$. Dès que $\rho$ et $\theta$ sont connus, nous pouvons poser $r=\rho\sin\theta$ et alors nous nous trouvons avec les équations
\begin{subequations}
	\begin{numcases}{}
		x=r\cos(\varphi)\\
		y=r\sin(\varphi),
	\end{numcases}
\end{subequations}
qui sont similaires à celles déjà étudiées dans le cas des coordonnées polaires.

% TODO: Ajouter un texte sur les équations de plan, et pourquoi ax+by+cz+d=0 est perpendiculaire au vecteur (a,b,c).

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Déterminant et produit vectoriel}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Quelque propriétés du déterminant}
%---------------------------------------------------------------------------------------------------------------------------

Une \defe{matrice}{matrice} $2\times 2$ est un tableau de nombres
\begin{equation}
    \begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}.
\end{equation}
Le \defe{déterminant}{déterminant} de cette matrice est le nombre
\begin{equation}
    \begin{vmatrix}
          a  &   b    \\ 
        c    &   d    
    \end{vmatrix}=ad-cb.
\end{equation}
Ce nombre détermine entre autres le nombre de solutions que va avoir le système d'équations linéaires associé à la matrice.

Pour une matrice $3\times 3$, nous avons le même concept, mais un peu plus compliqué. Le déterminant de la matrice
\begin{equation}
    \begin{pmatrix}
        a_{11}    &   a_{12}    &   a_{13}    \\
        a_{21}    &   a_{22}    &   a_{23}    \\
        a_{31}    &   a_{32}    &   a_{33}    
    \end{pmatrix}
\end{equation}
est le nombre
\begin{equation}
    \begin{vmatrix}
        a_{11}    &   a_{12}    &   a_{13}    \\
        a_{21}    &   a_{22}    &   a_{23}    \\
        a_{31}    &   a_{32}    &   a_{33}    
    \end{vmatrix}=
    a_{11}\begin{vmatrix}
        a_{22}  &   a_{23}    \\ 
        a_{32}    &   a_{33}    
    \end{vmatrix}+
    a_{12}\begin{vmatrix}
        a_{21}  &   a_{23}    \\ 
        a_{31}    &   a_{33}
    \end{vmatrix}+
    a_{13}\begin{vmatrix}
        a_{21}  &   a_{22}    \\ 
        a_{31}    &   a_{32}
    \end{vmatrix}.
\end{equation}


\begin{proposition}
    Si on permute deux lignes ou deux colonnes d'une matrice, alors le déterminant change de signe.
\end{proposition}

\begin{proposition}
    Si on multiplie une ligne ou une colonne d'une matrice par un nombre $\lambda$, alors le déterminant est multiplié par $\lambda$.
\end{proposition}

\begin{proposition}
    Si deux lignes ou deux colonnes sont proportionnelles, alors le déterminant est nul.
\end{proposition}

\begin{proposition}
    Si on ajoute à une ligne une combinaison linéaire des autres lignes, alors le déterminant ne change pas (idem pour les colonnes).
\end{proposition}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit vectoriel}
%---------------------------------------------------------------------------------------------------------------------------

Une application importante du déterminant $3\times 3$ est qu'il détermine le \defe{produit vectoriel}{produit!vectoriel} entre deux vecteurs. Pour cela nous introduisons les vecteurs de base
\begin{equation}
    \begin{aligned}[]
        e_x&=\begin{pmatrix}
            1    \\ 
            0    \\ 
            0    
        \end{pmatrix}
        ,&e_y=\begin{pmatrix}
            0    \\ 
            1    \\ 
            0    
        \end{pmatrix},&e_z&=\begin{pmatrix}
            0    \\ 
            0    \\ 
            1    
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Ensuite, si $v$ et $w$ sont des vecteurs dans $\eR^3$, nous définissons
\begin{equation}
    \begin{aligned}[]
        \begin{pmatrix}
            v_x    \\ 
            v_y    \\ 
            v_z    
        \end{pmatrix}\times\begin{pmatrix}
            w_x    \\ 
            w_y    \\ 
            w_z    
        \end{pmatrix}=
        \begin{vmatrix}
              e_x  &   e_y    &   e_z    \\
              v_x  &   v_y    &   v_z    \\
              w_x  &   w_y    &   w_z    \\
        \end{vmatrix}&=
        (v_yw_z-w_yvz)e_x\\
        &-(v_xw_z-w_xvz)e_y\\
        &+(v_xw_y-w_xvy)e_z\in\eR^3
    \end{aligned}
\end{equation}

Ce produit vectoriel peut aussi être écrit sous la forme
\begin{equation}        \label{EqProdVectEspilonijk}
    v\times w=\sum_{i,j,k}\epsilon_{ijk}v_iw_j1_k
\end{equation}
où $\epsilon_{ijk}$ est défini par $\epsilon_{xyz}=1$ et ensuite $\epsilon_{ijk}$ est $1$ ou $-1$ suivant que la permutation des $x$, $y$ et $z$ est paire ou impaire.

Un grand intérêt du produit vectoriel est qu'il fournit un vecteur qui est simultanément perpendiculaire aux deux vecteurs donnés.
\begin{proposition}
    Le vecteur $v\times w$ est perpendiculaire à $v$ et à $w$.
\end{proposition}

\begin{proposition}
    Le produit vectoriel est une opération antisymétrique, c'est à dire
    \begin{equation}
        v\times w=-w\times v.
    \end{equation}
    En particulier $v\times v=0$ pour tout vecteur $v\in\eR^3$.
\end{proposition}

\begin{proposition}
    Le produit vectoriel est linéaire. Pour tout vecteurs $a$, $b$, $c$ et pour tout nombre $\alpha$ et $\beta$ nous avons
    \begin{equation}
        \begin{aligned}[]
            a\times (\alpha b +\beta c)&=\alpha(a\times b)+\beta(a\times c)\\
            (\alpha a+\beta b)\times c&=\alpha(a\times c)+\beta(b\times c).
        \end{aligned}
    \end{equation}
\end{proposition}

Les trois vecteurs de base $e_x$, $e_y$ et $e_y$ ont des produits vectoriels faciles à retenir :
\begin{equation}
    \begin{aligned}[]
        e_x\times e_y&=e_z\\
        e_y\times e_z&=e_x\\
        e_z\times e_x&=e_y
    \end{aligned}
\end{equation}

\begin{example}
    Calculons le produit vectoriel $v\times w$ avec
    \begin{equation}
        \begin{aligned}[]
            v&=\begin{pmatrix}
                3    \\ 
                -1    \\ 
                1    
            \end{pmatrix}&w=\begin{pmatrix}
                1    \\ 
                2    \\ 
                -1    
            \end{pmatrix}.
        \end{aligned}
    \end{equation}
    Les vecteurs s'écrivent sous la forme $v=3e_x-e_y+e_z$ et $w=e_x+2e_y-e_z$. Le produit vectoriel s'écrit
    \begin{equation}
        \begin{aligned}[]
            (3e_x-e_y+e_z)\times (e_x+2e_y-e_z)&=6e_x\times e_y-3e_x\times e_z\\
                                &\quad -e_y\times e_x + e_y\times e_z\\
                                &\quad + e_z\times e_x + 2e_z\times e_y\\
                                &=6e_z+3e_y+e_z+e_x+e_y-2e_x\\
                                &=-e_x+4e_y+7e_z.
        \end{aligned}
    \end{equation}
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Produit mixte}
%---------------------------------------------------------------------------------------------------------------------------

Si $a$, $b$ et $c$ sont trois vecteurs, leur \defe{produit mixte}{produit!mixte} est le nombre $a\cdot(b\times c)$. En écrivant le produit vectoriel sous forme de somme de trois déterminants $2\times 2$, nous avons
\begin{equation}
    \begin{aligned}[]
        a\cdot& (b\times c)\\&=(a_1e_x+a_2e_y+a_3e_z)\cdot\left(
        \begin{vmatrix}
            b_2    &   b_3    \\ 
            c_2    &   c_3    
        \end{vmatrix}e_x-\begin{vmatrix}
            b_1    &   b_3    \\ 
            c_1    &   c_3    
        \end{vmatrix}e_y+\begin{vmatrix}
            b_1    &   b_2    \\ 
            c_1    &   c_2    
        \end{vmatrix}\right)\\
        &=a_1\begin{vmatrix}
            b_2    &   b_3    \\ 
            c_2    &   c_3    
        \end{vmatrix}-a_2\begin{vmatrix}
            b_1    &   b_3    \\ 
            c_1    &   c_3    
        \end{vmatrix}+a_3\begin{vmatrix}
            b_1    &   b_2    \\ 
            c_1    &   c_2    
        \end{vmatrix}\\
        &=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3    \\
            c_1    &   c_2    &   c_3
        \end{vmatrix}.
    \end{aligned}
\end{equation}
Le produit mixte s'écrit donc sous forme d'un déterminant. Nous retenons cette formule:
\begin{equation}        \label{EqProduitMixteDet}
    a\cdot (b\times c)=\begin{vmatrix}
        a_1    &   a_2    &   a_3    \\
        b_1    &   b_2    &   b_3    \\
        c_1    &   c_2    &   c_3
    \end{vmatrix}.
\end{equation}


\begin{proposition}
    Le produit vectoriel $a\times b$ est un vecteur orthogonal à $a$ et $b$.
\end{proposition}

\begin{proof}
    Vérifions que $a\perp (a\times b)$. Pour cela, nous calculons $a\cdot (a\times b)$, c'est à dire le produit mixte
    \begin{equation}
        a\cdot(a\times b)=\begin{vmatrix}
            a_1    &   a_2    &   a_3    \\
            a_1    &   a_2    &   a_3    \\
            b_1    &   b_2    &   b_3
        \end{vmatrix}=0.
    \end{equation}
    L'annulation de ce déterminant est due au fait que deux de ses lignes sont égales.
\end{proof}

\begin{proposition}     \label{PropNormeProdVectoabsint}
    Nous avons
    \begin{equation}
        \| a\times b \|=\| a \|\| b \|\sin(\theta)
    \end{equation}
    où $\theta\in\mathopen[ 0.\pi \mathclose]$ est l'angle formé par $a$ et $b$.
\end{proposition}

\begin{proof}
    En utilisant la décomposition du produit vectoriel, nous avons
    \begin{equation}
        \begin{aligned}[]
            \| a\times b \|^2&=\begin{vmatrix}
                a_2    &   a_3    \\ 
                b_2    &   b_3    
            \end{vmatrix}^2+\begin{vmatrix}
                a_1    &   a_3    \\ 
                b_1    &   b_3    
            \end{vmatrix}^2+\begin{vmatrix}
                a_1    &   a_2    \\ 
                b_1    &   b_2    
            \end{vmatrix}^2\\
            &=(a_2b_3-b_2a_3)^2+(a_1b_3-a_3b_1)^2+(a_1b_2-a_2b_1)^2\\
            &=(a_1^2+a_2^2+a_3^2)(b_1^2+b_2^2+b_3^2)-(a_1b_1+a_2b_2+a_3b_3)^2\\
            &=\| a \|^2\| b \|^2-(a\cdot b)^2\\
            &=\| a \|^2\| b \|^2-\| a \|^2\| b \|^2\cos^2(\theta)\\
            &=\| a \|^2\| b \|^2\big( 1-\cos^2(\theta) \big)\\
            &=\| a \|^2\| b \|^2\sin^2(\theta).
        \end{aligned}
    \end{equation}
    D'où le résultat.
\end{proof}

\begin{remark}      \label{RemaAireParalProdVect}
    Le nombre $\| a \|\| b \|\sin(\theta)$ est l'aire du parallélogramme formé par les vecteurs $a$ et $b$, comme cela se voit sur la figure \ref{LabelFigBNHLooLDxdPA}. % From file BNHLooLDxdPA
\newcommand{\CaptionFigBNHLooLDxdPA}{Calculer l'aire d'un parallélogramme.}
\input{Fig_BNHLooLDxdPA.pstricks}

\end{remark}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Interprétation géométrique du déterminant}
%---------------------------------------------------------------------------------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Déterminant de dimension deux}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

La valeur absolue du déterminant 
\begin{equation}        \label{EqDeratb}
    \begin{vmatrix}
        a_1    &   a_2    \\ 
        b_1    &   b_2    
    \end{vmatrix}
\end{equation}
est l'aire du parallélogramme déterminé par les vecteurs $\begin{pmatrix}
    a_1    \\ 
    a_2    
\end{pmatrix}$ et $\begin{pmatrix}
    b_1    \\ 
    b_2    
\end{pmatrix}$. En effet, d'après la remarque \ref{RemaAireParalProdVect}, l'aire de ce parallélogramme est donnée par la norme du produit vectoriel
\begin{equation}
    \begin{pmatrix}
        a_1    \\ 
        a_2    \\ 
        0    
    \end{pmatrix}\times
    \begin{pmatrix}
          b_1  \\ 
        b_2    \\ 
        0    
    \end{pmatrix}=\begin{vmatrix}
        e_x    &   e_y    &   e_z    \\
        a_1    &   a_2    &   0    \\
        b_1    &   b_2    &   0
    \end{vmatrix}=
    \begin{vmatrix}
        a_1    &   a_2    \\ 
        b_1    &   b_2    
    \end{vmatrix}e_z,
\end{equation}
donc la norme $\| a\times b \|$ est bien donnée par la valeur absolue du déterminant \eqref{EqDeratb}.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Déterminant de dimension trois}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Si les vecteurs $a$, $b$ et $c$ ne sont pas coplanaires, alors la valeur absolue du produit mixte (voir équation \eqref{EqProduitMixteDet}) $a\cdot(b\times c)$ donne le volume du parallélépipède construit sur les vecteurs $a$, $b$ et $c$.

En effet si $\varphi$ est l'angle entre $b\times c$ et $a$, alors la hauteur du parallélépipède vaut $\| a \|\cos(\varphi)$. En effet la direction verticale est donnée par $b\times c$, et la hauteur est alors la «composante verticale» de $a$. Par conséquent, étant donné que $\| b\times c \|$ est la surface de la base, le volume du parallélépipède vaut
\begin{equation}
    V=\| b\times c\|  \| a \|\cos(\varphi).
\end{equation}
Or cette formule est le produit scalaire de $a$ par $b \times c$; ce dernier étant donné par le déterminant de la matrice formée des composantes de $a$, $b$ et $c$ grâce à la formule \eqref{EqProduitMixteDet}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Méthode de Gauss pour résoudre des systèmes d'équations linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Pour résoudre un système d'équations linéaires, on procède comme suit:
\begin{enumerate}
\item Écrire le système sous forme matricielle. \[\text{p.ex. } \begin{cases} 2x+3y &= 5 \\ x+2y &= 4 \end{cases} \Leftrightarrow \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right) \]
\item Se ramener à une matrice avec un maximum de $0$ dans la partie de gauche en utilisant les transformations admissibles:
\begin{enumerate}
\item Remplacer une ligne par elle-même + un multiple d'une autre;
\[\text{p.ex. } \left(\begin{array}{cc|c} 2 & 3 & 5 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{L_1  - 2. L_2 \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right) \]
\item Remplacer une ligne par un multiple d'elle-même;
\[\text{p.ex. } \left(\begin{array}{cc|c} 0 & -1 & -3 \\ 1 & 2 & 4 \end{array}\right)  \stackrel{-L_1  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 2 & 4 \end{array}\right) \]
\item Permuter des lignes.
\[\text{p.ex. } \left(\begin{array}{cc|c} 0 & 1 & 3 \\ 1 & 0 & -2 \end{array}\right)  \stackrel{L_1  \mapsto L_2' \text{ et } L_2  \mapsto L_1'}{\Longrightarrow} \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \]
\end{enumerate}
\item Retransformer la matrice obtenue en système d'équations.
\[\text{p.ex. }  \left(\begin{array}{cc|c} 1 & 0 & -2 \\ 0 & 1 & 3  \end{array}\right) \Leftrightarrow \begin{cases} x &= -2 \\ y &= 3 \end{cases}  \]
\end{enumerate}

\begin{remark}
\begin{itemize}
\item Si on obtient une ligne de zéros, on peut l'enlever:
\[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right) \Leftrightarrow  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \end{array}\right) \]
\item Si on obtient une ligne de zéros suivie d'un nombre non-nul, le système d'équations n'a pas de solution:
\[\text{p.ex. }  \left(\begin{array}{ccc|c} 3 & 4 & -2 & 2 \\ 4 & -1 & 3 & 0 \\ 0 & 0 & 0 & 7 \end{array}\right) \Leftrightarrow  \begin{cases} \cdots \\ \cdots \\ 0x + 0y + 0z = 7 \end{cases} \Rightarrow \textbf{Impossible} \]
\item Si on moins d'équations que d'inconnues, alors il y a une infinité de solutions qui dépendent d'un ou plusieurs paramètres:
\[\text{p.ex. }  \left(\begin{array}{ccc|c} 1 & 0 & -2 & 2 \\ 0 & 1 & 3 & 0 \end{array}\right) \Leftrightarrow  \begin{cases} x - 2z = 2 \\ y + 3z = 0 \end{cases} \Leftrightarrow  \begin{cases} x = 2 + 2\lambda \\ y = -3\lambda \\ z = \lambda \end{cases} \]
\end{itemize}
\end{remark}
