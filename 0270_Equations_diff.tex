% This is part of Mes notes de mathématique
% Copyright (c) 2008-2009,2011-2013
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.

Une équation différentielle ordinaire est la recherche de toutes les fonctions définie sur une partie de $\eR$ satisfaisant à une certaine égalité, faisant intervenir les dérivées de la fonction recherchée.

Dans la suite, $I$ désignera un intervalle de $\eR$. Une fonction sera \defe{dérivable sur $I$}{dérivable!fonction} si elle est dérivable au sens usuel sur l'intérieur de $I$, et si elle est dérivable à droite (resp. à gauche) sur l'éventuel bord gauche (resp. droit) de $I$.

\begin{definition}
  Une \defe{équation différentielle ordinaire d'ordre $n$ sur $I$}{équation!différentielle!ordinaire d'ordre 1} est la recherche d'une fonction $y : I \to \eR$ dérivable $n$ fois, satisfaisant à une équation du type
  \begin{equation}\label{eqequadiff}
    F(t, y(t), y^\prime(t), \ldots, y^{n\prime}(t)) = 0 \quad \text{pour tout $t \in I$}
  \end{equation}
  où $I$ est un intervalle de $\eR$ et \begin{math}F : (I \times D) \subset (\eR\times\eR^{n+1})\to \eR\end{math} est une fonction donnée.
\end{definition}

\begin{remark}
L'équation différentielle~(\ref{eqequadiff}) sera raccourcie sous la forme
  \begin{equation}
    F(t, y, y^\prime, \ldots, y^{n\prime}) = 0
  \end{equation}
  où la dépendance en $t$ est sous-entendue.
\end{remark}

\begin{example}
	Soit $f : I \to \eR$ une fonction continue fixée. L'équation différentielle
	\begin{equation}
		y^\prime = f(t)
	\end{equation}
	se ramène à la recherche des primitives de $f$ sur l'intervalle $I$.
\end{example}

Le lemme suivant sert de temps en temps.
\begin{lemma}[Lemme de Grönwall]\index{Grönwall (lemme)}\index{lemme!Grönwall} \label{LemuBVozy}
    Soient \( \phi\) et \( \psi\) deux fonctions telles que pour tout \( t\in\mathopen[ t_0 , t_1 \mathclose]\), \( \phi(t)\geq 0\), \( \psi(t)\geq 0\) et
    \begin{equation}
        \phi(t)\leq +L\int_{t_0}^f\psi(s)\phi(s)ds
    \end{equation}
    où \( K\) et \( L\) sont des constantes positives. Alors
    \begin{equation}
        \phi(t)\leq K\exp\big( L\int_{t_0}^t\psi \big).
    \end{equation}
\end{lemma}
%TODO : la preuve.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Équations linéaires du premier ordre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Une \defe{équation différentielle linéaire}{équation!différentielle!linéaire} est une équation de la forme
\begin{equation}
	y'+u(t)y=v(t).
\end{equation}

\begin{example}
Tant qu'il n'y a pas de second membre, c'est facile. Prenons l'exemple suivant :
\begin{equation}
	y'+2ty=0.
\end{equation}
Nous mettons tous les $t$ d'un côté et tous les $y$ et $y'$ de l'autre :
\begin{equation}
	\frac{ y' }{ y }=-2t,
\end{equation}
et puis on intègre sans oublier la constante d'intégration :
\begin{equation}
	\ln(y)=-t^2+C,
\end{equation}
et donc $y(t)=K e^{-t^2}$.
\end{example}

Lorsqu'il y a un second membre, il y a une astuce. Prenons par exemple
\begin{equation}		\label{EqDiffExLin}
	y'+2ty=4t.
\end{equation}
L'astuce est de commencer par résoudre l'équation sans le second membre (l'équation homogène associée). Nous notons $y_H$ la solution. Ici, la réponse est
\begin{equation}
	y_H(t)=K e^{-t^2}.
\end{equation}
Ensuite le truc est d'essayer de trouver la solution de l'équation \eqref{EqDiffExLin} sous la forme
\begin{equation}		\label{EqEssaiLin}
	y(t)=K(t) e^{t^2}.
\end{equation}
L'idée est de prendre la même que la solution de l'équation homogène (sans second membre), mais en disant que $K$ est une fonction. Afin de trouver la fonction $K$ qui donne la solution, il suffit de remettre l'essai \eqref{EqEssaiLin} dans l'équation \eqref{EqDiffExLin} :
\begin{equation}
	\underbrace{K' e^{-t^2}-2tK e^{-t^2}}_{y'(t)}+\underbrace{2tK e^{-t^2}}_{2ty(t)}=4t
\end{equation}
Les deux termes avec $K$ se simplifient et il reste
\begin{equation}
	K'(t)=4t e^{t^2},
\end{equation}
ce qui signifie $K(t)=2 e^{t^2+C}$. Nous avons donc déterminé la fonction qui fait fonctionner l'essai, et la solution à l'équation est
\begin{equation}
	y(t)=\big( 2 e^{t^2}+C \big) e^{-t^2}=2+C e^{-t^2}.
\end{equation}


La technique pour résoudre cette équation est de commencer par résoudre l'équation homogène associée. Si $U(t)$ est une primitive de $u(t)$, nous avons
\begin{equation}
	\begin{aligned}[]
		y'_H(t)+u(t)y_H(t)&=0\\
		\frac{ y'_H }{ y_H }&=-u(t)\\
		\ln(y_H)&=-U(t)+C\\
		y_H(t)&= e^{-U(t)+C}=K e^{-U(t)}
	\end{aligned}
\end{equation}
où $K= e^{C}$.

Cela fournit la solution générale de l'équation homogène. Il existe un truc génial qui permet d'en tirer la solution générale du système non homogène. Lorsque nous avons trouvé $y_H(t)=K e^{-U(t)}$, le symbole $K$ désigne une constante. La méthode de \defe{variation des constantes}{variation des constantes} consiste à essayer la solution
\begin{equation}		\label{EqEssayVarSctr}
	y(t)=K(t) e^{-U(t)},
\end{equation}
c'est à dire à dire que la constante est en réalité une fonction. Afin de trouver quelle fonction $K(t)$ fait en sorte que l'essai \eqref{EqEssayVarSctr} soit une solution, nous la remplaçons dans l'équation de départ $y'+uy=v$. Maintenant,
\begin{equation}
	y'(t)=K'(t) e^{-U(t)}-K(t)u(t) e^{-U(t)}.
\end{equation}
En remettant dans l'équation,
\begin{equation}
	y'+uy=K' e^{-U}-Ku e^{-U}+uK e^{-U}=K' e^{-U}=v.
\end{equation}
Notez que les termes en $K$ se sont miraculeusement simplifiés. Cela est directement dû au fait que $ e^{-U}$ est solution de l'équation homogène. Nous restons avec l'équation
\begin{equation}
	K'=\frac{ v }{  e^{-U} }
\end{equation}
pour $K(t)$. La solution générale du problème non homogène est donc finalement donnée par
\begin{equation}
	y(t)=\big( W(t)+C \big) e^{-U(t)}
\end{equation}
si $W(t)$ est une primitive de $v(t)e^{U(t)}$.

Tout ceci est un peu heuristique. La proposition suivante dit dans quels cas ça fonctionne.
\begin{proposition}
Soient $u$ et $v$ continues sur $I$ et $U$, une primitive de $u$ sur $I$ et $W$ une primitive de $v e^{-U}$ sur $I$. Une fonction $y\colon I\to \eR$ est solution de $y'+u(t)y=v(t)$ si et seulement si il existe une constante $C\in \eR$ telle que
\begin{equation}
	y(t)=\big( W(t)+C \big) e^{U(t)}
\end{equation}
pour tout $t\in I$.
\end{proposition}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Pourquoi la variation des constantes fonctionne toujours ?}
%---------------------------------------------------------------------------------------------------------------------------

Prenons une équation non homogène 
\begin{equation}        \label{EqAstNNHomo}
    z'(t)=f(t)z(t)+g(t),
\end{equation}
et supposons avoir une solution de l'homogène associée sous la forme $z_H(t)=Ch(t)$. Le coup de la variation des constates consiste à essayer une solution pour l'équation non homogène sous la forme\footnote{Je ne sais plus qui a eu l'idée de changer le nom de la constante de $C$ vers $K$ au moment de la transformer en fonction, mais c'est une bonne idée.}
\begin{equation}
    z(t)=K(t)h(t).
\end{equation}
Nous injectons cette solution dans l'équation de départ en utilisant le fait que $z'(t)=K'(t)h(t)+K(t)h'(t)$ :
\begin{equation}
    K'(t)h(t)+K(t)h'(t)=f(t)K(t)h(t)+g(t).
\end{equation}
Le terme $K(t)h'(t)$ se récrit en utilisant la propriété de définition de $h$, c'est à dire que $h'(t)=f(t)h(t)$. Nous voyons que les termes ne contenant pas de $K'$ se simplifient; il reste
\begin{equation}
    K'h=g.
\end{equation}
Cette équation a comme solution
\begin{equation}
    K=\int \frac{ f }{ h }+C.
\end{equation}
J'insiste sur la constante d'intégration ! En réalité, celles et ceux qui auront compris l'équation \eqref{Eqttzint} sauront que $K$ est donné par
\begin{equation}
    K(t)=\int_{\xi_0}^{t}\frac{ f(\xi) }{ g(\xi) }d\xi
\end{equation}
où $\xi_0$ joue le rôle de la constante d'intégration.

Quoi qu'il en soit, la solution générale de l'équation non homogène est
\begin{equation}        \label{EqSolVarCosntCool}
    z(t)=K(t)h(t)=\left( \int\frac{ g }{ h }+C \right)h.
\end{equation}
Cette solution comprend deux termes : $Ch$ qui est solution de l'homogène, et $\left( \int \frac{ g }{ h } \right)h$ qui est une particulière de l'équation non homogène.

Quelque conclusions :

\begin{enumerate}
\item
Si vous avez encore du $K$ (et pas que du $K'$) dans votre équation qui donne $K$, c'est que vous n'être pas dans le cadre d'une équation de type \eqref{EqAstNNHomo}. Le plus souvent, c'est que vous avez fait une faute de calcul quelque part.

\item
La méthode des variations des constantes n'est pas en contradiction avec le principe de \og SGEH+SPENH\fg. En effet, la SGEP et la SPENH sont toutes deux dans la solution \eqref{EqSolVarCosntCool}.

\item
La variation des constantes peut être vue comme une façon cool de trouver une solution particulière de l'équation non homogène.

\item
    La simplification ne se fait que après avoir remplacé $Kh'$ par $Kfh$, c'est à dire après avoir utilisé le fait que $z_H$ est solution de l'homogène. Sinon, la simplification n'est pas du tout évidente a priori. Il se peut même que, visuellement, les termes $Kh'$ et $Kfh$ ne se ressemblent pas du tout. Un exemple de cela arrivera par exemple dans l'exemple \ref{ExYCPtxgZ}, pour arriver à l'équation \eqref{EDEqFracII107exoVVprb}.

\end{enumerate}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Équations à variables séparées}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{Secvarsep}

Une \defe{équation à variables séparées}{équation!différentielle!variables séparées} est une équation  de la forme
\begin{equation}		\label{EqDiffSeparee}
	y'=u(t)f(y)
\end{equation}
où $u\colon I\to \eR$ et $f\colon J\to \eR$ sont deux fonctions continues données. Les propositions \ref{ProJLykrK} et \ref{PropOkmXmC} résolvent ce cas, mais avant de voir cela, nous allons donner quelque indication «pratiques».

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{La méthode rapide}
%---------------------------------------------------------------------------------------------------------------------------

On peut évidement mettre tous les $y$ et $y'$ d'un côté :
\begin{equation}
	\frac{ y' }{ f(y) }=u(x).
\end{equation}
Une fois que cela est fait, on écrit $y'=\frac{ dy }{ dx }$, et on envoie le $dx$ du côté des $x$ :
\begin{equation}
	\frac{ dy }{ f(y) }=u(x)dx.
\end{equation}
Maintenant il suffit de prendre l'intégrale des deux côtés : comme la position des $dx$ et $dy$ l'indiquent, il faut intégrer par rapport à $y$ d'un côté et par rapport à $dx$ de l'autre côté.

L'intégrale à gauche est facile : c'est $\ln(y)$. À droite, par contre, ça dépend tout à fait de $u$.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{La méthode plus propre}
%---------------------------------------------------------------------------------------------------------------------------

\begin{equation}
	y'(t)=u(t)f\big( y(t) \big).
\end{equation}
Nous considérons $U$, une primitive de $u$ sut $I$ et $G$, une primitive de $1/f$ sur $J$. Si $I'\subseteq I$ et $y\colon I'\to J$, alors $y$ est solution de \eqref{EqDiffSeparee} si et seulement si il existe une constante $C$ telle que
\begin{equation}		\label{EqSolSepThe}
	G\big( y(t) \big)=U(t)+C.
\end{equation}
La recherche des solutions de l'équation différentielle se ramène donc à la recherche de primitives et de solutions d'une équation algébrique (il faut isoler $y(t)$ dans \eqref{EqSolSepThe}). Réciproquement toute solution régulière de cette dernière relation est solution de l'équation différentielle.

Remarque : lorsque nous cherchons $U$ et $G$, nous ne cherchons que \emph{une} primitive. Il ne faut pas considérer des constantes d'intégration à ce niveau.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Les théorèmes}
%---------------------------------------------------------------------------------------------------------------------------

\begin{proposition}     \label{ProJLykrK}
Nous considérons l'équation \eqref{EqDiffSeparee} avec $u(t)$ continue sur $I$ et $f$ continue sur $J$ avec $f(\eta)\neq 0$ pour tout $\eta\in J$. Soit $U$, une primitive de $u$ sur $I$, et $G$, une primitive de $1/f$ sur $J$.

Si $y\colon Y'\to J$ est une fonction sur un intervalle $I'\subset I$, alors $y$ est solution de l'équation \eqref{EqDiffSeparee} si et seulement si il existe $C\in\eR$ tel que
\begin{equation}		\label{EqSoluceEqDiffSep}
	G\big( y(t) \big)=U(t)+C.
\end{equation}
\end{proposition}
Cette proposition dit que toutes les solutions qui ne s'annulent jamais sur un intervalle ont la forme $G\big( y(t) \big)=U(t)+C$ et peuvent donc être trouvées en calculant des primitives.

La formule \eqref{EqSoluceEqDiffSep} peut être obtenue de la façon heuristique suivante, en écrivant $y'=dy/dt$, et en passant le $dt$ à droite. Nous trouvons successivement
\begin{equation}
	\begin{aligned}[]
		y'&=u(t)f(y)\\
		dy&=u(t)f(y)dt\\
		\frac{ dy }{ f(y) }&=u(t)dt\\
		\int\frac{ dy }{ f(y) }&=\int u(t)dt\\
		G(y)&=U(t)+C.
	\end{aligned}
\end{equation}

\begin{proposition} \label{PropOkmXmC}
Soient $u$ continue sur $I$ et $f$ continue sur $J$, et $f(\eta)\neq 0$ sur $J$. Soient $t_0\in I$ et $y_0\in J$. Alors il existe $I'\subset I$ avec $t_0\in I'$ et $f\in C^1(I'\to J)$ tels que
\begin{enumerate}

\item
$y$ est solution de \eqref{EqDiffSeparee} sur $I'$ et vérifie $y(t_0)=y_0$,
\item
si $z$ est une solution de \eqref{EqDiffSeparee} sur $I''\subset I'$ avec $t_0\in I''$ et $z(t_0)=y_0$, alors $I''\subset I'$ et $z(t)=y(t)$ pour tout $t\in I''$.

\end{enumerate}
\end{proposition}

\begin{example} \label{ExYCPtxgZ}
    Résoudre l'équation différentielle
    \begin{equation}
        y-\cos(t)y'=\cos(t)\big(1-\sin(t)\big)y^2.
    \end{equation}

La fonction $y=0$ est solution. En posant $z=1/y$, nous trouvons l'équation
\begin{equation}		\label{EDEqII107EqpourZ}
	z+\cos(t)z'=\cos(t)\big(1-\sin(t)\big)
\end{equation}
à laquelle $z$ doit satisfaire. L'équation homogène est
\begin{equation}
	z_H'=-\frac{ z_H }{ \cos(t) }.
\end{equation}
Ceci est une équation à variables séparées que nous résolvons en suivant les méthodes données plus haut : nous posons
\begin{equation}		\label{EqEDufUGII107}
	\begin{aligned}[]
		u(t)	&=\frac{1}{ \cos(t) }, \\
		f(z)	&=-z,\\
		U(t)	&=\ln\left[ \tan\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right) \right]	&\text{(voir formulaire)},\\
		G(z)	&=\ln\left( \frac{1}{ z } \right).
	\end{aligned}
\end{equation}
La solution $z_H$ est donnée par l'équation
\begin{equation}
	\ln\left( \frac{1}{ z } \right)=\ln\left[ K\tan\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right) \right],
\end{equation}
c'est à dire
\begin{equation}
	z_H(t)=\frac{ K }{ \tan\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right) }.
\end{equation}
Nous appliquons maintenant la méthode de variation des constantes sur cette solution afin de trouver la solution générale de l'équation \eqref{EDEqII107EqpourZ}. En utilisant la règle de Leibnitz, $z'=K'z_H+Kz'_H$, nous trouvons
\begin{equation}
	\frac{ K }{ \tan\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right) }+\cos(t)\left( \frac{ K' }{  \tan\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right) }-\frac{ K }{ 2\sin^2 \left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right)  } \right)=\cos(t)\big( 1-\sin(t) \big).
\end{equation}
Malgré leurs apparences, les deux termes en $K$ se simplifient. En effet, en vertu de l'équation $z_H'=\frac{ -z_H }{ \cos(t) }$, nous avons
\begin{equation}
	\frac{ -K }{ 2\sin^2\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right)}=\frac{ -K }{ \cos(t)\tan\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right) }.
\end{equation}
Le travail de voir quel est le lien entre $\sin^2\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right)$, $\tan\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right)$ et $\cos(t)$ est en réalité fait dans votre formulaire au moment où vous l'avez utilisé pour intégrer $u$ pour obtenir le $U(t)$ de \eqref{EqEDufUGII107}.

Après cette simplification durement méritée, nous trouvons l'équation suivante pour $K(t)$ :
\begin{equation}		\label{EDEqFracII107exoVVprb}
	\frac{ K' }{ \tan\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right) }=1-\sin(t).
\end{equation}
Résoudre cela revient à trouver la primitive de
\begin{equation}
\big( 1-\sin(t) \big) \tan\left( \frac{ \pi }{ 4 }+\frac{ t }{ 2 } \right),
\end{equation}
ce qui est relativement compliqué. La réponse est
\begin{equation}
	\begin{aligned}[]
		K(t)	&=\ln \left(\sin \left({{2\,x+\pi}\over{4}}\right)+1\right)+\ln  \left(\sin \left({{2\,x+\pi}\over{4}}\right)-1\right)\\
			&\quad+2\,\ln \sec  \left({{2\,x+\pi}\over{4}}\right)+2\,\sin ^2\left({{2\,x+\pi}\over{4 }}\right)
	\end{aligned}
\end{equation}
Nous pouvons un peu simplifier en utilisant le fait que $\ln(a+b)+\ln(a-b)=\ln(a^2-b^2)$ :
\begin{equation}
	\begin{aligned}[]
		K(t)	=\ln\left(-\cos^2 \left({{2\,x+\pi}\over{4}}\right)\right)
			+2\,\ln \sec  \left({{2\,x+\pi}\over{4}}\right)+2\,\sin ^2\left({{2\,x+\pi}\over{4 }}\right).
	\end{aligned}
\end{equation}
Il me semble toutefois qu'il faudrait prendre des valeurs absolues pour les logarithmes.

\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Équations linéaires d'ordre supérieur}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équations et systèmes linéaire à coefficients constants}
%---------------------------------------------------------------------------------------------------------------------------

Nous regardons l'équation
\begin{equation}	\label{EqLinConstantRappels}
	y^{(n)} + a_1 y^{(n-1)} + \ldots + a_{n-1} y^\prime + a_n y = v(t)
\end{equation}
où les coefficients $a_k$ sont maintenant des constantes. La méthode est donnée à la page $311$ du cours. Il faut commencer par résoudre le polynôme caractéristique
\begin{equation}
	r^n+a_1 r^{n-1}+\ldots +a_n=0.
\end{equation}
Si $\lambda_1,\ldots,\lambda_k$ sont les solutions avec multiplicité $\mu_1,\ldots,\mu_k$, alors le \defe{système fondamental}{système!fondamental} de solutions linéairement indépendantes est l'ensemble suivant de solutions à l'équation homogène :
\begin{equation}
	\begin{aligned}[]
		 e^{\lambda_1 t},t e^{\lambda_1 t},	&	\ldots,t^{\mu_1-1} e^{\lambda_1  t}\\
							&\vdots\\
		 e^{\lambda_k t},t e^{\lambda_k t},	&\ldots,t^{\mu_k-1} e^{\lambda_k  t}.
	\end{aligned}
\end{equation}
Nous notons $y_i$ ces solutions. La solution générale de l'équation homogène est donc donnée par
\begin{equation}
	y_H=\sum_i c_i y_i.
\end{equation}
Afin de trouver la solution générale de l'équation non homogène, nous appliquons la méthode de variation des constantes, en imposant les $n-1$ conditions
\begin{equation}		\label{EqVarCstSubtil}
	\sum_{i=1}^n c'_i(t)y_i^{(l)}(t)=0
\end{equation}
avec $l=0,\ldots,n-2$. Ces condition plus l'équation de départ \eqref{EqLinConstantRappels} forment un système de $n$ équations différentielles pour les $n$ fonctions inconnues $c_i(t)$.

Cette condition peut paraître mystérieuse. Elle est posée à la page 337 du cours de première année. Il est cependant encore possible de travailler sans poser la condition \eqref{EqVarCstSubtil} en suivant la recette, en calculant des déterminants de Wronskien. Des exemples sont donnés dans les exercices sur le second ordre.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Si les coefficients ne sont pas constants ?}
%---------------------------------------------------------------------------------------------------------------------------

Une équation différentielle linéaire d'ordre $n$ sur $I$ est une équation de la forme
\begin{equation}	\label{EqLinRappels}
	y^{(n)} + u_1(t) y^{(n-1)} + \ldots + u_{n-1}(t) y^\prime + u_n(t) y = v(t)
\end{equation}
où $v$ et $u_k$ sont des fonctions continues fixées de $I$ vers $\eR$.

Pour résoudre cette équation, il faut commencer par résoudre l'équation homogène correspondante (c'est à dire celle que l'on obtient en posant $v(t)=0$). Ensuite, nous trouvons la solution de l'équation \eqref{EqLinRappels} en appliquant la méthode de la \defe{variation des constantes}{variation des constantes}.

Donnons un exemple du pourquoi la méthode de variations des constantes est efficace. Soit l'équation 
\begin{equation}		\label{EqDiffExempleVarCst}
	u'+f(t)u=g(t),
\end{equation}
 et disons que $u_H$ est une solution de l'équation homogène. La méthode de variations des constantes consiste à poser $u(t)=K(t)u_H(t)$, et donc $u'(t)=K'u_H+Ku_H'$. En remettant dans l'équation de départ,
\begin{equation}
	K'u_H+Ku_H'+fKu_H=g.
\end{equation}
La somme $Ku_H'+fKu_H$ est nulle, par définition de $u_H$. Par conséquent, il ne reste que
\begin{equation}
	K'=\frac{ g(t) }{ u_H }.
\end{equation}
Lorsqu'on utilise la méthode de variation des constantes, nous trouvons toujours une simplification \og miraculeuse\fg.

Dans l'immédiat, nous ne considérons que le cas où les \( u_i\) sont des constantes. Le cas où les \( u_i\) deviennent des fonctions de \( t\) sera vu plus tard.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Système d'équations linéaires}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{La magie de l'exponentielle\ldots}
%---------------------------------------------------------------------------------------------------------------------------

Prenons l'équation différentielle très simple
\begin{equation}
	y'=ay.
\end{equation}
La solution est $y(t)=A e^{at}$. Et si on a la donnée que Cauchy $y(t_0)=y_0$, alors
\begin{equation}		\label{EqytexposimpleProp}
	y(t)=A e^{at} e^{-at_0} e^{at_0}= e^{a(t-t_0)}y(t_0).
\end{equation}
Donc on a le facteur multiplicatif $ e^{a(t-t_0)}$ qui sert à faire passer de $y(0)$ à $y(t)$. C'est un peu un opérateur d'évolution. Ce qui fait la magie  de l'exponentielle, c'est son développement en série
\begin{equation}		\label{EqDevExpoMag}
	e^x=1+x+\frac{ x^2 }{ 2 }+\frac{ x^3 }{ 3! }+\frac{ x^4 }{ 4! }+\ldots
\end{equation}
qui est tel que chaque terme est la dérivée du terme suivant.

Maintenant, si on a un système
\begin{equation}
	\bar y'=A\bar y,
\end{equation}
il n'est pas du tout étonnant d'avoir comme solution $\bar y(t)= e^{At}$ où l'exponentielle de la matrice est définie exactement par la série \eqref{EqDevExpoMag}. C'est un peu longuet, mais dans le cours, c'est effectivement ce qui est prouvé. La matrice résolvante $R(t,t_0)\colon \bar y_0\to \bar y(t;t_0,y_0)$ est donné par
\begin{equation}
	R(t,t_0)= e^{(t-t_0)A},
\end{equation}
exactement comme dans l'équation \eqref{EqytexposimpleProp}.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{\ldots mais la difficulté}
%---------------------------------------------------------------------------------------------------------------------------

Maintenant, il est suffisant de calculer des exponentielles de matrices pour résoudre des systèmes. Hélas, il est en général très difficile de calculer des exponentielles. Tu peux essayer de prouver les deux suivantes :
\begin{equation}
	\begin{aligned}[]
		A=\begin{pmatrix}
	0	&	a	\\ 
	-a	&	0	
\end{pmatrix}	&\leadsto  e^{A}=\begin{pmatrix}
	\cos(a)	&	\sin(a)	\\ 
	-\sin(a)	&	\cos(a)	
\end{pmatrix}\\
		S=\begin{pmatrix}
	0	&	a	\\ 
	a	&	0	
\end{pmatrix}	&\leadsto  e^{S}=\begin{pmatrix}
	\cosh(a)	&	\sinh(a)	\\ 
	\sinh(a)	&	\cosh(a)	
\end{pmatrix}.
	\end{aligned}
\end{equation}
La première, tu vas la revoir si tu fais de la géométrie différentielle ou de la mécanique quantique : l'algèbre de Lie du groupe des matrices orthogonales de déterminant $1$ est l'algèbre des matrices antisymétriques.

La seconde se retrouve en relativité parce que $e^S$ est la matrice qui préserve $x^2-y^2$, tout comme $e^A$ préserve $x^2+y^2$. Si tu as besoin d'une rafraîchissure de mémoire sur les fonctions hyperboliques, ou bien si leur utilité en relativité t'intéresse, tu peux aller voir la partie sur la relativité \href{http://student.ulb.ac.be/~lclaesse/physique-math.pdf}{ici}\footnote{ \url{http://student.ulb.ac.be/~lclaesse/physique-math.pdf}}.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{La recette}
%---------------------------------------------------------------------------------------------------------------------------

Afin d'éviter de devoir calculer explicitement des exponentielles de matrices, nous faisons appel à toutes sortes de trucs, dont la forme de Jordan. Le résultat final est la méthode suivante. Soit le système homogène
\begin{equation}
	\bar y'=A\bar y.
\end{equation}

\let\oldTheEnumi\theenumi
\renewcommand{\theenumi}{\arabic{enumi}.}
\begin{enumerate}

\item 
D'abord, nous calculons les valeurs propres de $A$.

\item
Ensuite les vecteurs propres.

\item\label{ItemRapSystDc}
Une bonne valeur propre, c'est une valeur propre dont l'espace propre a une dimension égale à sa multiplicité. C'est à dire que si $\lambda$ est de multiplicité $m$, alors on a, dans les bon cas,  $m$ vecteur propres linéairement indépendants.

Dans ce cas, si $v_1,\ldots,v_m$ sont les vecteurs, alors on a les solutions linéairement indépendantes suivantes :
\begin{equation}
	\begin{pmatrix}
	\vdots	\\ 
	v_1	\\ 
		\vdots	
\end{pmatrix} e^{\lambda t},\ldots,
\begin{pmatrix}
	\vdots		\\ 
	v_m	\\ 
	\vdots		
\end{pmatrix} e^{\lambda t}.
\end{equation}
Pour chaque bonne valeur propre, ça nous fait un tel paquet de solutions linéairement indépendantes.

\item
Si $\lambda$ n'est pas une bonne valeur propre, alors les choses se compliquent. Mettons que $\lambda$ ait $k$ vecteurs propres en moins que sa multiplicité. Dans ce cas, il faut chercher des solutions sous la forme
\begin{equation}		\label{EqEqRapAsTestPolk}
	 \begin{pmatrix}
	a^{(k)}_1t^k+\ldots+a_1^{(0)}	\\ 
	\vdots	\\ 
	a^{(k)}_1t^k+\ldots+a_n^{(0)}		
\end{pmatrix} e^{\lambda t}.
\end{equation}
C'est à dire qu'on prend comme coefficient de $ e^{\lambda t}$, un vecteur de polynômes de degré $k$. Il faut mettre cela dans l'équation de départ pour voir quelles sont les contraintes sur les constantes $a_i^{(j)}$ introduites.

\item\label{ItemRapSystDe}
Nous avons un cas particulier du cas précédent. Si $\lambda$ est une valeur propre de multiplicité $m$ qui n'a que un seul vecteur propre $v$, alors il faut chercher des polynômes de degré $m-1$, et on peut directement fixer le coefficient de $t^{m-1}$, ce sera l'unique vecteur propres :
\begin{equation}
\left[
	\begin{pmatrix}
	\vdots	\\ 
	v	\\ 
	\vdots	
\end{pmatrix}+
\begin{pmatrix}
	a_1^{(m-2)}	\\ 
	\vdots	\\ 
	a_n^{(m-2)}	
\end{pmatrix}t^{m-2}+\ldots
\right] e^{\lambda t}.
\end{equation}
Cela économise quelque calculs par rapport à poser brutalement \eqref{EqEqRapAsTestPolk}.

\end{enumerate}
\let\theenumi\oldTheEnumi

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Système d'équations linéaires avec matrice constante}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons l'équation différentielle
\begin{equation}    \label{EqOOsXZJ}
    y'(t)=Ay(t)
\end{equation}
pour la fonction \( y\colon \eR\to \eR^n\) et \( A\) est une matrice ne dépendant pas de \( t\). Nous supposons que \( A\) est diagonalisable pour les vecteurs propres \( v_i\) et les valeurs propres \( \lambda_i\) correspondantes.

La matrice 
\begin{equation}
    R(t)=\big[  e^{\lambda_1t}v_1\, \ldots  e^{\lambda_nt}v_n \big]
\end{equation}
est la \defe{matrice résolvante}{résolvante} du système. Alors la solution du système \eqref{EqOOsXZJ} pour la condition initiale \( y(0)=y_0\) est 
\begin{equation}
    y(t)=R(t)y_0.
\end{equation}
En effet
\begin{equation}
    AR(t)=\left[  A\begin{pmatrix}
        \uparrow    \\ 
        e^{\lambda_1t}v_1    \\ 
        \downarrow    
    \end{pmatrix}\,\ldots\,A\begin{pmatrix}
        \uparrow    \\ 
        e^{\lambda_nt}v_n    \\ 
            \downarrow
    \end{pmatrix}\right]=R'(t).
\end{equation}
Par conséquent \( y'(t)=R'(t)y_0=AR(t)y_0=Ay(t)\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Système d'équations linéaires avec matrice non constante}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[\cite{WNxwuWc}]\label{ThoNYEXqxO}
    Soit \( I\) un intervalle de \( \eR\) et une fonction \( M\colon \eR\to \aL(\eR^n,\eR^n)\). Si les composantes \( M_{ij}\) sont des fonctions continues sur \( I\) alors :
    \begin{enumerate}
        \item
    pour tout \( t_0\in I\) et pour tout \( y_0\in R^n\) le système
    \begin{equation}    \label{EqKYDrMgu}
        y'(t)=M(t)y(t)
    \end{equation}
    admet une unique solution maximale définie sur \( I\) telle que \( y(t_0)=y_0\);
\item 
    l'ensemble des solutions de l'équation \eqref{EqKYDrMgu} sur \( I\) est un espace vectoriel de dimension~\( n\).
    \end{enumerate}
\end{theorem}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Réduction de l'ordre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecWGdleRM}

Afin de diminuer l'ordre d'une équation dans laquelle le paramètre n'apparaît pas, il y a deux changements de variables très utiles. Le premier, le plus simple, est simplement de poser $z(t)=y'(t)$, ce qui donne $z'(t)=y''(t)$. Le second, \emph{qui n'est pas le même}, est $z\big( y(t) \big)=y'(t)$, qui entraîne $y''(t)=z'\big( y(t) \big)z(t)$. Dans ce second cas, il faut également changer de variable, et utiliser $y(t)$ comme variable au lieu de $t$.


Si ça ne marche pas, il faut suivre la procédure ci-après.

Nous supposons avoir une équation différentielle d'ordre \( p\) dans laquelle \( y^{(p)}\) est isolée des autres dérivées : 
\begin{equation}    \label{EqHDeVQgn}
 y^{(p)}(t)=f\big( t,y(t),y'(t),\ldots, y^{(p-1)}(t) \big)  
\end{equation}
où \( f\) est une fonction \( f\colon \eR\times \eR^p\to \eR\), et la fonction cherchée est \( y\colon \eR\to \eR\).

La méthode proposée ici consiste à transformer cette équation d'ordre \( p\) en un système d'équations d'ordre \( 1\). Pour cela nous posons
\begin{equation}
    \begin{aligned}
        F\colon \eR\times \eR^p&\to \eR^p \\
        (x,X)&\mapsto \begin{pmatrix}
            X_2    \\ 
            \vdots    \\ 
            X_p    \\ 
            f(x,X_1,\ldots, X_p)    
        \end{pmatrix},
    \end{aligned}
\end{equation}
et à une fonction \( y\colon \eR\to \eR\) nous associons la fonction
\begin{equation}
    \begin{aligned}
        Y\colon \eR&\to \eR^p \\
        x&\mapsto \begin{pmatrix}
            y(x)    \\ 
            y'(x)    \\ 
            \vdots    \\ 
            y^{(p-1)}(x)    
        \end{pmatrix}.
    \end{aligned}
\end{equation}
La fonction \( y\) résout l'équation \eqref{EqHDeVQgn} si et seulement si la fonction \( Y\) résout l'équation
\begin{equation}    \label{EqDVFdMNi}
    Y'(t)=F\big( t,Y(t) \big).
\end{equation}

De plus si l'équation \eqref{EqHDeVQgn} est donnée avec les conditions initiales \( y^{(k)}=a_k\) (\( k=0,\ldots, p-1\)) alors l'équation \eqref{EqDVFdMNi} vient avec les conditions initiales
\begin{equation}
    Y(t_0)=\begin{pmatrix}
        a_0    \\ 
        \vdots    \\ 
        a_{p-1}    
    \end{pmatrix},
\end{equation}
c'est à dire \( Y(t_0)=A_0\) avec \( A_0\in \eR^p\).

Le théorème de Cauchy-Lipschitz \ref{ThokUUlgU} nous donne existence et unicité locale de la solution au système \ref{EqDVFdMNi}. Lorsque le système est linéaire, c'est à dire sous la forme \( Y'(t)=M(t)Y(t)\), alors il y a mieux : le théorème \ref{ThoNYEXqxO}.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Équation du second ordre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Wronskien}
%---------------------------------------------------------------------------------------------------------------------------

Nous considérons ici une équation différentielle de la forme
\begin{equation}    \label{EqJDAAnWY}
    y''(t)+q(t)y(t)=0
\end{equation}
Dans ce point nous allons considérer la fonction \( q\) sans hypothèse de périodicité. L'équation de Hill (sous-section \ref{SubSecDWwVVPa}) sera la même équation, mais en supposant que \( q\) est périodique.

Nous commençons par argumenter que si \( q\) est continue, alors l'ensemble des solutions de l'équation \eqref{EqJDAAnWY} est un espace vectoriel de dimension deux. Pour cela il suffit d'appliquer la méthode de réduction de l'ordre (section \ref{SecWGdleRM}) puis le théorème de dimension pour les systèmes linéaires (théorème \ref{ThoNYEXqxO}). En effet si la fonction \( y_1\) est solution de \eqref{EqJDAAnWY} si et seulement si le vecteur \(Y= \begin{pmatrix}
    y_1    \\ 
    y_2    
\end{pmatrix}\) est solution du système linéaire
\begin{equation}
    Y'(t)=\begin{pmatrix}
        0    &   1    \\ 
        -q(t)    &   0    
    \end{pmatrix}Y(t).
\end{equation}

Soient deux solutions \( y_1\) et \( y_2\) de l'équation différentielle. Le \defe{Wronskien}{Wronskien} de ces deux solutions est le déterminant
\begin{equation}
    W(t)=\begin{vmatrix}
        y_1    &   y_2    \\ 
        y'_1    &   y'_2    
    \end{vmatrix}.
\end{equation}
Si nous considérons l'équation différentielle
\begin{equation}
    y''+py'+qy=0,
\end{equation}
le Wronskien peut être déterminé sans savoir explicitement \( y_1\) et \( y_2\) parce que \( W=y_1y'_2-y'_1y_2\), et en dérivant,
\begin{subequations}
    \begin{align}
        W'&=y_1y_2''+y'_1y'_2-y''_1y_2-y'_1y'_2\\
        &=y_1(-py'_2-qy_2)-(-py'_1-qy_1)y_2\\
        &=-p\begin{vmatrix}
            y_1    &   y_2    \\ 
            y'_1    &   y'_2    
        \end{vmatrix},
    \end{align}
\end{subequations}
c'est à dire
\begin{equation}    \label{EqHEMRgM}
    W'=-pW.
\end{equation}
Il suffit donc de savoir une condition initiale pour obtenir une équation différentielle pour \( W\).

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Avec second membre}
%---------------------------------------------------------------------------------------------------------------------------

Une équation différentielle du second ordre avec un second membre se présente sous la forme
\begin{equation}
	ay''(t)+by'(t)+cy(t)=v(t)
\end{equation}
où $v(t)$ est une fonction donnée. Le truc est de commencer par résoudre l'équation différentielle sans second membre, c'est à dire trouver la fonction $y_H(t)$ telle que
\begin{equation}
	ay''_H(t)+by_H'(t)+cy_H(t)=0.
\end{equation}
Cela se fait en utilisant la méthode du polynôme caractéristique.

Ensuite, il faut trouver une solution particulière $y_P(t)$ de l'équation avec le second membre. Une seule. Pour y parvenir, il faut du doigté et un peu de technique. Il faut faire des essais en fonction de ce à quoi ressemble le $v(t)$ :
\begin{enumerate}

	\item
		Si $v(t)$ est un polynôme, alors il faut essayer un polynôme,

	\item
		Si $v(t)=\cos(\omega t)$ ou bien $v(t)=\sin(\omega t)$, alors essayer $y_P(t)=A\cos(t)+B\sin(\omega t)$,

	\item
		Si $v(t)= e^{\omega t}$, alors essayer $y_P(t)=A e^{\omega t}$.

\end{enumerate}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Équation \texorpdfstring{$y''+q(t)y=0$}{y''+q(t)y=0}}
%---------------------------------------------------------------------------------------------------------------------------
\label{subsecSyTwyM}


Nous allons donner quelque propriété des solutions de l'équation
\begin{equation}
    y''+qy=0
\end{equation}
en fonction de telle ou telle hypothèse sur \( q\).

\begin{proposition}
    Si \( q\colon \eR^+\to \eR\) est continue et si
    \begin{equation}
        \int_0^{\infty}| q(t) |dt
    \end{equation}
    converge, alors
    \begin{enumerate}
        \item
            toute solution bornée de \( y''+qy=0\) vérifie \( \lim_{t\to \infty} y'(t)=0\),
        \item
            l'équation \( y''+qy=0\) admet des solutions non bornées.
    \end{enumerate}
\end{proposition}

\begin{proof}
    Soit \( y\) une solution bornée, et intégrons l'équation différentielle entre \( 0\) et \( \infty\) :
    \begin{equation}
        \int_0^{\infty}y''(t)dt=-\int_0^{\infty}q(t)y(t)dt.
    \end{equation}
    La fonction \( y\) étant bornée, l'hypothèse sur \( q\) permet de dire que l'intégrale de droite existe. Par ailleurs,
    \begin{equation}
        \int_0^{\infty}y''=\lim_{a\to \infty}\int_0^ay''=\lim_{a\to \infty}y'(a)-y'(0).
    \end{equation}
    Cela justifie que la limite \( \lim_{t\to \infty} y'(t)\) existe. Posons \( \alpha=\lim_{t\to \infty} y'(t)\) et supposons par l'absurde que \( \alpha\neq 0\). Soit \( \epsilon>0\) et \( \lambda\) assez grand pour que
    \begin{equation}
        \| y'-\alpha \|_{\mathopen[ \lambda , \infty [}<\epsilon.
    \end{equation}
    Soit aussi \( x>\lambda\). Nous avons
    \begin{subequations}
        \begin{align}
            y(x)&=y(\lambda)+\int_{\lambda}^xy'(t)dt\\
            &\geq y(\lambda)\int_{\lambda}^x(\alpha-\epsilon)\\
            &=y(\lambda)+\alpha x-\epsilon\lambda.
        \end{align}
    \end{subequations}
    En prenant la limite des deux côtés on voit que \( y(x)\to \infty\) dès que \( \alpha\neq 0\), ce qui est contraire aux hypothèses. Donc \( \alpha=0\).

    Pour la seconde partie de la proposition, nous devons prouver que l'équation \( y''+qy=0\) possède des solutions non bornées. Si l'équation a seulement des solutions bornées et si \( \{ u,v \}\) est une base de solutions, alors nous avons \( u',v'\to 0\). Si nous reprenons l'équation \eqref{EqHEMRgM} avec \( p=0\) nous savons que dans notre cas le Wronskien satisfait à \( W'=0\), c'est à dire qu'il est constant. Mais vu que \( u\) et \( v\) sont bornées et que les dérivées tendent vers zéro, nous avons \( W(t)\to 0\) et donc \( W(t)=0\).

    Or l'annulation identique du Wronskien contredit que \( \{ u,v \}\) serait une base de solutions. Donc il existe des solutions non bornées.
\end{proof}

\begin{proposition} \label{PropMYskGa}
    Soit l'équation différentielle \( y''+qy=0\). Si \( q\) est \( C^1\), strictement positive et croissante, alors toutes les solutions sont bornées.
\end{proposition}
\index{monotonie}

\begin{proof}
    Soit \( y\) une solution et multiplions l'équation par \( 2y'\) (qui est non nulle par hypothèse) :
    \begin{equation}
        2y'y''+2qy'y=0.
    \end{equation}
    Nous allons intégrer cela en nous souvenant que \( 2y'y''\) est la dérivée de \( (y')^2\). Pour tout \( t>0\) nous avons
    \begin{subequations}
        \begin{align}
            0&=y'(t)^2-y'(0)^2+2\underbrace{\int_0^tq(t)y'(t)y(t)dt}_{\text{par partie}}\\
            &=y'(t)^2-y'(0)^2+2\left( [qy^2]_0^t-\int_0^tq'y^2 \right)\\
        \end{align}
    \end{subequations}
    Le terme qui nous intéresse est celui qui contient \( y(t)\) :
    \begin{equation}
        2q(t)y(t)^2=-y'(t)^2+y'(0)^2+2q(0)y(0)^2+2\int_0^t q'y^2
    \end{equation}
    Nous pouvons majorer \( -y'(t)^2\) par zéro et remplacer toutes les constantes par \( K\) :
    \begin{equation}
        q(t)y(t)^2\leq\int_0^tq'y^2+K=\int_0^t\frac{ q' }{ q }qy^2.
    \end{equation}
    C'est le moment d'utiliser le lemme de Grönwall \ref{LemuBVozy} avec \( \phi=qy^2\) et \( \psi=q'/q\). Les hypothèses de croissance et de positivité ont été posées exprès. Bref, on a
    \begin{subequations}
        \begin{align}
            qy^2&\leq K\exp\left( \int_0^t\frac{ q'(s) }{ q(s) }ds \right)\\
            &=K\exp\left( \ln\frac{ q(t) }{ q(0) } \right)\\
            &=K\frac{ q(t) }{ q(0) }.
        \end{align}
    \end{subequations}
    Notons que \( q(0)\) est strictement positif. Nous déduisons que
    \begin{equation}
        y^2\leq \frac{ K }{ q(0) }
    \end{equation}
    et donc \( y\) est bornée.
\end{proof}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Équation de Hill}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecDWwVVPa}

L'équation \defe{de Hill}{équation!différentielle!Hill} est une équation différentielle de la forme 
\begin{equation}    \label{EqPQMvzEZ}
    y''+qy=0
\end{equation}
où
\begin{enumerate}
    \item
        \( q\in C^1(\eR,\eR)\),
    \item
        \( q\) est paire et \( \pi\)-périodique
\end{enumerate}
Nous nous intéressons aux solutions complexes de cette équation différentielle.

Nous nommons \( W\subset C^2(\eR,\eC)\) l'espace des solutions complexes de l'équation \eqref{EqPQMvzEZ}. Nous savons par ce qui a été dit en \ref{subsecSyTwyM} que cet espace est de dimension deux. De plus avec le hypothèses faites ici sur \( q\), nous savons que les solutions sont de classe $C^3$ parce que si \( y\) est une solution, alors l'équation \( y''=qy\) nous indique que \( y\) est \( C^1\) parce que \( y''\) existe (\( y'\) est dérivable et donc continue). Mais si \( y\) est de classe \( C^1\), alors le membre de droite \( qy\) est \( C^1\) et donc \( y''\) est \( C^1\), ce qui prouve que \( y\) est de classe \( C^3\). La récurrence ne va pas plus loin parce que \( q\) est seulement de classe \( C^1\).

Nous considérons l'application de translation
\begin{equation}
    \begin{aligned}
        T\colon C^2(\eR,\eC)&\to C^2(\eR,\eC) \\
        (Ty)(x)&=y(x+\pi). 
    \end{aligned}
\end{equation}
En utilisant la règle de dérivation de fonctions composées, \( (Ty)'=Ty'\) et \( (Ty)''=Ty''\), de telle sorte que si \( u\) est solution de l'équation \eqref{EqPQMvzEZ}, alors \( Tu\) est également solution. Donc \( W\) est un espace stable par \( T\).

Le théorème \ref{ThoNYEXqxO} nous permet de choisir une base de \( W\) en imposant des conditions. Nous choisissons une base \( \{ y_1,y_2 \}\) telles que
\begin{equation}
    \begin{aligned}[]
        y_1(0)&=1       &&  y_2(0)=0\\
        y'_1(0)&=0      &&  y_2'(0)=1.
    \end{aligned}
\end{equation}
Le théorème \ref{ThoNYEXqxO} nous assure que deux telles solutions existent et qu'elles forment une base de \( W\) parce que \( W\) est de dimension \( 2\).

\begin{lemma}[\cite{WNxwuWc}]   \label{IVLzNaU}
    Avec ce choix de base \( \{ y_1,y_2 \}\) la matrice de \( T\) est donnée par
    \begin{equation}
        T=\begin{pmatrix}
            y_1(\pi)    &   y_2(\pi)    \\ 
            y'_1(\pi)    &   y'_2(\pi)    
        \end{pmatrix}.
    \end{equation}
    De plus la fonction \( y_1\) est paire et la fonction \( y_2\) est impaire.
\end{lemma}

\begin{proof}

Cherchons la matrice de \( T\) dans cette base en associant \( \begin{pmatrix}
    1    \\ 
    0    
\end{pmatrix}\) à \( y_1\) et \( \begin{pmatrix}
    0    \\ 
    1    
\end{pmatrix}\) à \( y_2\). Si \( T=\begin{pmatrix}
    a    &   b    \\ 
    c    &   d    
\end{pmatrix}\), alors
\begin{equation}    \label{EqSZhBPGy}
    Ty_1=\begin{pmatrix}
        a    &   b    \\ 
        c    &   d    
    \end{pmatrix}\begin{pmatrix}
        1    \\ 
        0    
    \end{pmatrix}=\begin{pmatrix}
        a    \\ 
        c    
    \end{pmatrix}=ay_1+cy_2.
\end{equation}
En évaluant cela en \( t=0\),
\begin{equation}
    (Ty_1)(0)=ay_1(0)+cy_2(0)=a,
\end{equation}
donc \(a=(Ty_1)(0)=y_1(\pi)\). En dérivant \eqref{EqSZhBPGy}, en tenant compte du fait que \( (Ty_1)'=Ty_1'\) et en évaluant en \( t=0\), nous trouvons de même \( c=y'_1(\pi)\). Puis le même cinéma avec \( y_2\) donne
\begin{equation}
    T=\begin{pmatrix}
        y_1(\pi)    &   y_2(\pi)    \\ 
        y'_1(\pi)    &   y'_2(\pi)    
    \end{pmatrix}.
\end{equation}
    
Passons maintenant à la parité de \( y_1\) et \( y_2\). Nous posons \( \psi(t)=y_1(-t)\). Alors \( \psi'(t)=-y_1'(-t)\) et \( \psi''(t)=y_1''(t)\), tant et si bien que
\begin{equation}
    \psi''(t)+q(t)\psi(t)=y_1''(-t)+q(t)y_1(-t)=0.
\end{equation}
donc \( \psi\) est une solution de l'équation. Mais
\begin{subequations}
    \begin{numcases}{}
        \psi(0)=y_1(0)\\
       \psi'(0)=-y'_1(0)=0,
    \end{numcases}
\end{subequations}
donc \( \psi\) a les mêmes conditions initiales que \( y_1\). Par conséquent \( \psi=y_1\) (par le l'unicité donnée dans le théorème de Cauchy-Lipschitz \ref{ThokUUlgU}) et \( y_1\) est paire. Nous procédons de même en partant de \( \varphi(t)=-y_2(-t)\) pour trouver que \( \varphi=y_2\) et que donc que \( y_2\) est impaire.


\end{proof}
Remémorons nous toutefois, pour calmer toute enthousiasme excessif, que \( T\) dépend de deux solutions et donc de la fonction \( q\) donnée dans l'équation.

\begin{proposition}[\cite{KXjFWKA}] \label{PropGJCZcjR}
    Nous considérons l'équation \( y''+qy=0\) et sa base de solutions \( \{ y_1,y_2 \}\) en suivant les notations données plus haut.
    \begin{enumerate}
        \item
            Si \( | \tr(T) |<2\), alors toutes les solutions de l'équation sont bornées.
        \item
            Si \( | \tr(T) |=2\) alors nous avons une solution non bornée.
        \item
            Si \( |\tr(T)|>2\) alors toutes les solutions de l'équation sont non bornées.
        \item
            Le cas \( | \tr(T) |=2\) se présente si et seulement si \( y'_1(\pi)y_2(\pi)=0\).
    \end{enumerate}
\end{proposition}
\index{endomorphisme!sous-espace stable}
\index{endomorphisme!diagonalisable}
\index{équation!différentielle!étude qualitative}
\index{équation!différentielle!système}


\begin{proof}
    Remarquons que le déterminant de la matrice \( T\) est égal au Wronskien des solutions \( y_1\) et \( y_2\) calculé en \( t=\pi\). Calculons sa valeur :
    \begin{equation}
        W(y_1,y_2)=\det\begin{pmatrix}
            y_1    &   y_2    \\ 
            y'_1    &   y'_2    
        \end{pmatrix}=y_1y'_2-y'_1y'_2.
    \end{equation}
    En dérivant et en remplaçant \( y''_i\) par \( -qy_i\), nous trouvons tout de suite \( W(y_1,y_2)'=0\). Donc le Wronskien est constant et il est facile de le calculer en \( t=0\) :
    \begin{equation}
        W(y_1,y_2)(0)=1-0=1.
    \end{equation}
    Donc pour tout \( t\) nous avons \( W(y_1,y_2)(t)=1\). En particulier
    \begin{equation}
        \det(T)=W(y_1,y_2)(\pi)=1,
    \end{equation}
    et notons au passage que \( T\) est inversible.

    Nous écrivons le polynôme caractéristique de \( T\) sous la forme \( \chi_T=X^2-\tr(T)X+\det(T)\), c'est à dire
    \begin{equation}
        \chi_T=X^2-\tr(T)X+1,
    \end{equation}
    dont le discriminant est \( \Delta=\tr(A)^2-4\).

    Nous passons à présent aux différents points de la proposition.
    \begin{enumerate}
        \item
            Si \( | \tr(T) |<2\), alors \( \Delta<0\) et \( \chi_T\) a deux racines complexes conjuguées que nous notons \( \rho\) et \( \bar\rho\). De plus le produit des racines étant le terme indépendant, \( \rho\bar\rho=1\); en particulier \( | \rho |=| \bar \rho |=1\). Notons \( \{ u,v \}\) une base de vecteurs propres : \( Tu=\rho u\) et \( Tv=\bar \rho v\). Il est vite vu que la fonction \( | u |\) est \( \pi\)-périodique :
            \begin{equation}
                | u |(t+\pi)=| u(t+\pi) |=| (Tu)(t) |=| (\rho u)(t) |=| \rho | | u |(t)=| u |(t).
            \end{equation}
            La fonction \( | u |\) est continue\footnote{La fonction \( u\) elle-même n'est cependant pas garantie d'être périodique.} et périodique ergo bornée. La fonction \( | v |\) est bornée pour la même raison et par linéarité, toutes les fonctions de \( W\) sont bornées.

        \item

            Si \( \tr(T)=\pm 2\), alors \( \Delta=0\) et \( \chi_T\) a une racine réelle double\footnote{Ce qui n'implique pas le fait d'avoir deux vecteurs propres pour cette valeur propre, mais tout de même au moins un, voir l'exemple \ref{ExICOJcFp}.} qui doit être \( \pm 1\). Soit \( u\) un vecteur propre de \( T\) pour la valeur propre \( \pm 1\). Nous avons
            \begin{equation}
                | u |(t+\pi)=| Tu(t) |=| \pm u(t) |,
            \end{equation}
            ce qui prouve encore que \( | u |\) est périodique et donc bornée.

            Notons que nous n'avons pas d'informations sur le fait qu'une autre solution soit ou non bornée.

        \item

            Si \( | \tr(T) |>2\), alors \( \chi_T\) a deux racines réelles distinctes \( r\) et \( r'\) avec \( rr'=1\) (toujours les relations coefficients-racines). En raison de quoi \( r'=r^{-1}\) et quitte à échanger \( r\) et \( r'\) nous supposons \( | r |>1\). L'opérateur est maintenant diagonalisable et nous considérons \( \{ u,v \}\) une base de vecteurs propres pour les valeurs propres \( r\) et \( r'\). Une solution non nulle de l'équation s'écrit donc sous la forme
            \begin{equation}
                y=\alpha u+\beta v
            \end{equation}
            avec \( (\alpha,\beta)\neq (0,0)\).

            \begin{itemize}
                \item Si \( \alpha=0\), alors \( \beta\neq 0\) et nous choisissons une valeur \( t\) telle que \( v(t)\neq 0\). Dans ce cas,
                    \begin{equation}
                        y(t+n\pi)=\beta v(t+n\pi)=\beta(T^nv)(t)=\beta (r')^n v(t),
                    \end{equation}
                    et en faisant \( n\to -\infty\) nous obtenons \( \pm \infty\) suivant le signe de \( \beta\).

                \item Si \( \alpha\neq 0\), alors nous fixons\footnote{Mais pas trop hein; nous aurons encore besoin d'assigner à \( t\) d'autres valeurs dans d'autres théorèmes.} \( t\) tel que \( u(t)\neq 0\). Alors
                    \begin{equation}
                        y(t+n\pi)=\alpha r^nu(t)+\beta (r')^n(t).
                    \end{equation}
                    En faisant \( n\to \infty\), nous avons \( (r')^n\to 0\) tandis que le premier terme tend vers \( \pm\infty\) suivant le signe de \( \alpha\).
            \end{itemize}

        \item

            D'abord le théorème de Cayley-Hamilton \ref{ThoCalYWLbJQ} nous indique que \( \chi_T(T)=0\), c'est à dire que
            \begin{equation}    \label{EqFHVSsUO}
                T^2-\tr(T)T+1=0.
            \end{equation}
            Nous avons déjà mentionné le fait que \( T\) était inversible. Multiplions donc \eqref{EqFHVSsUO} par \( T^{-1}\) :
            \begin{equation}    \label{EqPNyjBOy}
                T+T^{-1}=\tr(T)\mtu_2.
            \end{equation}
            Vu que \( T^{-1}\) est l'endomorphisme \( T^{-1}u(t)=u(t-\pi)\), sa matrice est donnée par
            \begin{equation}
                T^{-1}=\begin{pmatrix}
                    y_1(-\pi)    &   y_2(-\pi)    \\ 
                    y'_1(-\pi)    &   y'_2(-\pi)    
                \end{pmatrix}=\begin{pmatrix}
                    y_1(\pi)    &   -y_2(\pi)    \\ 
                    -y'_1(\pi)    &   y'_2(\pi)    
                \end{pmatrix}
            \end{equation}
            où nous avons utilisé le fait que \( y_1\) était paire et \( y_2\) impaire (lemme \ref{IVLzNaU}). Si nous notons \( T=\begin{pmatrix}
                a    &   b    \\ 
                c    &   d    
            \end{pmatrix}\), alors \( T^{-1}=\begin{pmatrix}
                a    &   -b    \\ 
                -c    &   d    
            \end{pmatrix}\) et
            \begin{equation}
                T+T^{-1}=\begin{pmatrix}
                    2a    &     0  \\ 
                       0 &   2b    
                \end{pmatrix}.
            \end{equation}
            L'équation \eqref{EqPNyjBOy} donne alors, vu que \( \tr(T)=a+d\),
            \begin{equation}
                \begin{pmatrix}
                    2a    &   0    \\ 
                    0    &   2b    
                \end{pmatrix}=\begin{pmatrix}
                    a+d    &   0    \\ 
                    0    &   a+d    
                \end{pmatrix},
            \end{equation}
            ce qui donne immédiatement \( a=d\). La matrice de \( T\) a donc comme forme \( T=\begin{pmatrix}
                a    &   b    \\ 
                c    &   a    
            \end{pmatrix}\) et \( \tr(T)=2a\).

            Donc \( \tr(T)=\pm 2\) si et seulement si \( a=\pm 1\) et vu que \( 1=\det(T)=a^2-bc\), nous avons \( a=\pm 1\) si et seulement si \( bc=0\), ce qui signifie exactement \( y'_1(\pi)y_2(\pi)=0\).
    \end{enumerate}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Différents types d'équations différentielles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation homogène}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecEqDiffHomo}

Une équation différentielle \defe{homogène}{équation!différentielle!homogène} est une équation de la forme
\begin{equation}
	y'=f(t,y)
\end{equation}
où $f(\lambda t,\lambda y)=f(t,y)$ pour tout $\lambda\neq 0$.

Elle se présente sous la forme
\begin{equation}
	y'=\frac{ \text{degré $n$ en $t,y$} }{  \text{degré $n$ en $t,y$}  },
\end{equation}
avec pas de $y'$ à droite : juste du $y$ et du $t$.

\begin{lemma}
L'équation $y'=f(t,y)$ est homogène si et seulement si $f(t,y)$ est une fonction de $y/t$ seulement.
\end{lemma}
Pour résoudre l'équation homogène, on pose
\begin{equation}		\label{EqDiffHomoPoser}
	z(t)=\frac{ y(t) }{ t },
\end{equation}
donc $tz=y$, et 
\begin{equation}
	y'(t)=tv'(t)+v(t),
\end{equation}
à remettre dans l'équation de départ.
%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation de Bernoulli}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecBernh}

C'est une équation du type
\begin{equation}	\label{EqBerNDiffalp}
	y'=a(t)y+b(t)y^{\alpha}
\end{equation}
où $\alpha\neq 0$ ou $1$. Pour la résoudre, on divise l'équation par $y^{\alpha}$, et on pose $u=y^{1-\alpha}$, et on tombe sur une équation linéaire
\begin{equation}
	u'=(1-\alpha)\big( a(t)u+b(t) \big).
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation de \href{http://fr.wikipedia.org/wiki/Jacopo_Riccati}{Riccati}}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecRicatti}

C'est une équation de la forme 
\begin{equation}		\label{EqDiffGFeneRicatti}
	y'=a(t)y^2+b(t)y+c(t).
\end{equation}
\index{équation!de Riccati}

En général, on ne peut pas la résoudre, mais si on en connaît \emph{a priori} des solutions particulières, alors on peut s'en sortir.

\begin{enumerate}

\item 
Si on sait que $y_1(t)$ est une solution, alors on pose
\begin{equation}
	y(t)=y_1(t)+\frac{1}{ u(t) },
\end{equation}
et on obtient une équation linéaire
\begin{equation}
	u'=-\big( 2y_1(t)a(t)+b(t) \big)u-a(t).
\end{equation}

\item
Si $y_1$ et $y_2$ sont solutions, alors nous avons $y$ sous forme implicite
\begin{equation}
	\frac{ y-y_1 }{ y-y_2 }=K e^{\int a(t)\big( y_1(t)-y_2(t) \big)dt}.
\end{equation}
\end{enumerate}

Pour résoudre une équation de Ricatti, il faut donc d'abord deviner une ou deux solutions.

%---------------------------------------------------------------------------------------------------------------------------
					\subsection{Équation différentielle exacte}
%---------------------------------------------------------------------------------------------------------------------------
\label{SubSecEqDiffExacte}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Résolution lorsque tout va bien}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Avant de vous lancer dans les équations différentielles exacte, vous devez lire la section sur les formes différentielles \ref{SecFormDiffRappel}. Une équation différentielle exacte est de la forme $P(t,y)+Q(t,y)y'=0$ que nous allons écrire sous la forme
\begin{equation}		\label{EqExacteDiff}
	P(t,y)dt+Q(t,y)dy=0.
\end{equation}
Nous savons que si $\partial_yP=\partial_tQ$, alors il existe une fonction $f(t,y)$ telle que $Pdt+Qdy=df$. Pour trouver une telle fonction, nous pouvons simplement intégrer la forme $Pdt+Qdy$. En effet, si $\gamma\colon [0,1]\to \eR^2$ est un chemin tel que $\gamma(0)=(0,0)$ et $\gamma(1)=(t,y)$, alors en définissant
\begin{equation}
	f(t,y)=\int_{\gamma}[Pdt+Qdt]=\int_{0}^1\big[ (P\circ\gamma)(u)dt+(Q\circ\gamma)(u) \big]\big( \gamma'(u) \big)du,
\end{equation}
nous avons $df=Pdt+Qdy$. N'importe quel chemin fait l'affaire. Calculons avec $\gamma(u)=(tu,yu)$. La dérivée de ce chemin est donnée par
\begin{equation}
	\gamma'(u)=t\begin{pmatrix}
	1	\\ 
	0	
\end{pmatrix}+y\begin{pmatrix}
	0	\\ 
	1	
\end{pmatrix}.
\end{equation}
Étant donné que $dt\begin{pmatrix}
	a	\\ 
	b	
\end{pmatrix}=a$ et $dy\begin{pmatrix}
	a	\\ 
	b	
\end{pmatrix}=b$, nous avons
\begin{equation}
	\begin{aligned}[]
	f(t,y)&=\int_0^1[Pdt+Qdy]\big( \gamma(u) \big)\left( t\begin{pmatrix}
	1	\\ 
	0	
\end{pmatrix}+y\begin{pmatrix}
	0	\\ 
	1	
\end{pmatrix} \right)du\\
		&=\int_0^1P\big( \gamma(t) \big)tdu+\int_0^1Q\big( \gamma(t) \big)ydu\\
		&=\int_0^1\big[ tP(tu,uy)+yQ(tu,yu) \big]du.
	\end{aligned}
\end{equation}
Nous retrouvons exactement la formule \eqref{EqIMFormI33Fffdd}. Si ça t'étonne, c'est que tu n'as pas compris ;) Dans le cas où nous avons la fonction $f$ qui vérifie $P=\partial_tf$ et $Q=\partial_yf$, l'équation \eqref{EqExacteDiff} devient
\begin{equation}
	\frac{ \partial f }{ \partial t }+\frac{ \partial f }{ \partial y }\frac{ dy }{ dt }=0,
\end{equation}
c'est à dire 
\begin{equation}
	\frac{ d }{ dt }\Big[ f\big( t,y(t) \big) \Big]=0,
\end{equation}
dont la solution
\begin{equation}
	f\big( t,y(t) \big)=C
\end{equation}
donne la solution $y(t)$ sous forme implicite.

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
					\subsubsection{Facteur intégrant (quand tout ne va pas bien)}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Si la forme $Pdt+Qdy$ n'est pas exacte, il n'existe pas de fonction $f$ qui résolve l'affaire. Nous pouvons toutefois essayer de trouver un \defe{facteur intégrant}{facteur!intégrant}. Nous cherchons une fonction $M$ telle que
\begin{equation}
	(MP)dt+(MQ)dy
\end{equation}
soit exacte. Nous cherchons donc $M(t,y)$ telle que $\partial_y(MP)=\partial_t(MQ)$. En utilisant la règle de Leibnitz, nous trouvons l'équation suivante pour $M$ :
\begin{equation}		\label{EqDuFacteurIntegrant}
	M(\partial_yP-\partial_tQ)=Q(\partial_tM)-P(\partial_yM).
\end{equation}
Cette équation est en générale extrêmement difficile à résoudre, mais dans certains cas particuliers, il est possible d'en trouver une solution à tâtons.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Distributions pour les équations différentielles}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecTNgeNms}

Nous commençons par définir l'espace \(  C^{\infty}\big( \eR,\swS'(\eR^d) \big)\)\nomenclature[Y]{$C^{\infty}\big( \eR,\swS'(\eR^d) \big)$}{Fonctions à valeurs dans les distributions.} en disant que \( t\mapsto u_t\) est dans cet espace si
\begin{enumerate}
    \item
        pour tout \( t\in \eR\) nous avons \( u_t\in \swS'(\eR^d)\),
    \item
        l'application \( t\mapsto u_t\) est de classe \(  C^{\infty}\).
\end{enumerate}
Pour définir ce que nous entendons par une fonction de classe \( C^k\) à valeurs dans \( \swS'(\eR^d)\) nous nous souvenons de la proposition \ref{PropQAuJstI}.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Équation de Schrödinger}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Équation de Schrödinger\cite{KXjFWKA}]    \label{ThoLDmNnBR}
    Soit \( g\in\swS'(\eR^d)\) et le problème
    \begin{subequations}
        \begin{numcases}{}
            \partial_t\tilde u-i\Delta \tilde u=0   \label{EqIKhGuiq}\\
            u_0=g
        \end{numcases}
    \end{subequations}
    où \( \tilde u\in C^{\infty}\big( \eR,\swD'(\eR^d) \big)\) est lié à \( u\) par la remarque  \ref{RemZYVkHRT}. Alors
    \begin{enumerate}
        \item   \label{ItemVFracYji}
            Il existe une unique solution dans \( C^{\infty}\big( \eR,\swS'(\eR^d) \big)\).
        \item   \label{ItemVFracYjiii}
            Cette solution \( u\) vérifie de plus \( \tilde u\in\swS'(\eR\times \eR^d)\).
    \end{enumerate}
\end{theorem}
\index{Schrödinger}
\index{distribution!équation de Schrödinger}

\begin{proof}
    Nous allons donner explicitement une fonction \( u\in C^{\infty}\big( \eR,\swS'(\eR^d) \big)\) et nous allons vérifier l'équation \eqref{EqIKhGuiq} en testant sur une fonction \( \psi\in\swS'(\eR\times \eR^d)\). Cela prouvera le point \ref{ItemVFracYjiii} ainsi que la partie existence de \ref{ItemVFracYji}. Dans ce qui suit toutes les transformées de Fourier seront par rapport à la variable \( x\in \eR^d\) ou par rapport à \( \xi\). Jamais par rapport à \( t\in \eR\).

    \begin{subproof}
    \item[Existence]
        Pour \( t\in \eR\) nous posons\footnote{En utilisant la définition \eqref{DefTDkrqkA} du produit d'une distribution par une fonction.}
        \begin{equation}
            u_t=\TF^{-1}(f_t\hat g)
        \end{equation}
        où \( f_t\in\swS(\eR^d)\) est la fonction \( f_t(x)= e^{-it\| x \|^2}\). Pour toute fonction \( \varphi\in\swS(\eR^d)\) nous avons
        \begin{equation}
            u_t(\varphi)=(f\hat g)\big( \TF^{-1}(\varphi) \big)=\hat g\big( f\TF^{-1}(\varphi) \big)=g\Big( \TF\big( f\TF^{-1}(\varphi) \big) \Big).
        \end{equation}
        Le fait que \( \TF^{-1}(\varphi)\) soit une fonction Schwartz fait partie de la proposition \ref{PropKPsjyzT}. Pour chaque \( t\) nous avons bien \( u_t\in\swS'(\Omega)\).

        De plus la fonction \( h(t,x)= e^{-it\| x \|^2}(\TF^{-1}\varphi)(x)\) est dans \(  C^{\infty}(\eR\times \eR^d)\), et par conséquent l'application
        \begin{equation}
            t\mapsto \hat g\big( h(t,.) \big)
        \end{equation}
        est également \(  C^{\infty}\) par la proposition \ref{PropBQUOcyw}. Ceci pour dire que \( u\in C^{\infty}\big( \eR,\swS'(\eR^d) \big)\). Il faut encore vérifier que cette fonction est bien une solution de notre problème. Nous testons cette équation sur \( \psi\in\swS(\eR\times\eR^d)\). Pour alléger les notations nous posons \( \psi_t\colon x\mapsto \psi(t,x)\) et par conséquent aussi \( (\partial_t\psi_t)(x)=(\partial_t\psi)(t,x)\). Nous avons :
        \begin{subequations}
            \begin{align}
                \heartsuit&=(\partial_t\tilde u-i\Delta\tilde u)(\psi)\\
                &=-\tilde u(\partial_t\psi)-i\tilde u(\Delta\psi)\\
                &=-\int_{\eR}u_t\big( (\partial_t\psi_t)+i(\Delta\psi_t) \big)dt
            \end{align}
        \end{subequations}
        Ici nous nous souvenons du lemme \ref{LemYYjFZSa} qui nous dit que nous pouvons permuter \( \TF^{-1}\) et \( \partial_t\). Et pour l'autre terme il faut utiliser le lemme \ref{LemQPVQjCx} avec \( | \alpha |=2\) et une somme pour obtenir que
        \begin{equation}
            \widehat{\Delta\varphi}(x)=-\| x \|^2\hat\varphi(x),
        \end{equation}
        qui dans notre cas s'écrit sous la forme
        \begin{equation}
            \TF^{-1}\Big( (\Delta\psi_t) \Big)(x)=-\| x \|^2\TF^{-1}\psi(t,x).
        \end{equation}
        En remettant bout à bout,
        \begin{subequations}
            \begin{align}
                \heartsuit&=-\int_{\eR}(f_t\hat g)\Big( (\partial_t-i\| . \|^2)\TF^{-1}\psi_t \Big)dt\\
                &=-\int_{\eR}\hat g\Big( x\mapsto  e^{-it\| x \|^2}(\partial_t-i\| x \|^2)(\TF^{-1}\psi)(t,x) \Big)dt
            \end{align}
        \end{subequations}
        Pour alléger les notations nous notons \( \check{\psi_t}(x)=(\TF^{-1}\psi)(t,x)\). Nous avons
        \begin{equation}
            \partial_t\left(  e^{-it\| x \|^2}\check\psi_t(x) \right)=-i\| x \|^2 e^{-it\| x \|^2}\check{\psi_t}(x)+ e^{-it\| x \|^2}(\partial_t\check{\psi_t}),x)= e^{-it\| x \|^2}\big( \partial_t-i\| x \|^2 \big)\check{\psi_t}(x);
        \end{equation}
        cela nous permet d'un peu factoriser une dérivée dans \( \heartsuit\) :
        \begin{subequations}
            \begin{align}
                \heartsuit&=-\int_{\eR}\hat g\left( \partial_t\Big(  e^{-it\| . \|^2}\check{\psi_t}(.) \Big) \right)dt\\
                &=-\int_{\eR}\partial_t\hat g\left(  e^{-it\| . \|^2}\check{\psi_t}(.) \right)dt\\
                &=-\lim_{N\to \infty} \left[ \hat g\Big(  e^{-i\| . \|^2}\check{\psi_t}(.) \Big) \right]_{t=-N}^{t=N}.
            \end{align}
        \end{subequations}
        Histoire de bien comprendre les notations, il ne s'agit pas de calculer \( \hat g\big(  e^{-it\| . \|^2}\check\psi_t \big)\) pour un \( t\) général et de remplacer ensuite \( t\) par \( N\) et \( -N\). En effet la valeur de \( \hat g\big(  e^{-it\| . \|^2}\check\psi_t \big)\) pour un \( t\) donné est celle qu'on obtient en calculant \( \hat g(\ldots)\) après avoir remplacé \( t\) par ce que l'on veut. Par conséquent, en posant \( \varphi(t,\xi)= e^{-i\| \xi \|^2}\check\psi_t(\xi)\) nous avons :
        \begin{subequations}
            \begin{align}
                \heartsuit&=\lim_{N\to \infty} \left[ g\left( x\mapsto\int_{\eR} e^{-ix\xi}\varphi(t,\xi )d\xi \right) \right]_{t=-N}^{t=N}\\
                &=\lim_{N\to \infty} g\left( x\mapsto\int_{\eR} e^{-ix\xi}\varphi(N,\xi )d\xi \right)-\lim_{N\to \infty} g\left( x\mapsto\int_{\eR} e^{-ix\xi}\varphi(-N,\xi )d\xi \right)
            \end{align}
        \end{subequations}
        La limite commute avec \( g\) parce que cette dernière est une distribution (continue). De plus la limite commute avec l'intégrale parce que ce qui est dedans est Schwartz. La fonction \( \varphi\) étant Schwartz, la limite est nulle. Donc
        \begin{equation}
            \heartsuit=0
        \end{equation}
        et la fonction \( u\) proposée est bien une solution de l'équation de Schrödinger dans \(  C^{\infty}(\eR,\swS'(\eR^d))\).

    \item[Unicité]

        Nous considérons deux solutions \( u_1,u_2\in C^{\infty}\big( \eR,\swS'(\eR^d) \big)\) et la fonction \( u=u_1-u_2\) doit satisfaire au problème
        \begin{subequations}
            \begin{numcases}{}
                (\partial_t\tilde u-i\Delta\tilde u)(\psi)=0\\
                u_0=0.
            \end{numcases}
        \end{subequations}
        Nous allons montrer que seule la fonction \( u_t=0\) peut satisfaire à cela pour tout \( \psi\in\swS(\eR\times \eR^d)\). Nous allons même montrer qu'en imposant ces équations seulement sur la partie de \( \swS(\eR\times\eR^d)\) qui est à support compact par rapport à \( \eR\), la seule solution est \( u_t=0\). Soit donc \( \psi\in\swS(\eR\times \eR^d)\) à support compact vis-à-vis de sa variable \( t\). Alors
        \begin{equation}
            0=-\tilde u(\partial_t\psi+i\Delta\psi)=-\int_{\eR}u_t\Big( (\partial_t\psi_t)+i(\Delta\psi_t) \Big)dt
        \end{equation}
        où encore une fois \( \partial_t\psi_t\) est la fonction \( x\mapsto (\partial_t\psi)(t,x)\). Maintenant nous utilisons la proposition \ref{PropUDkgksG} pour dire que 
        \begin{equation}
            \frac{ d }{ dt }\Big( u_t(\psi_t) \Big)=u^{(1)}_t(\psi_t)+u_t\left( \frac{ \partial \psi }{ \partial t }(t,.) \right)
        \end{equation}
        pour écrire
        \begin{equation}
            0=-\int_{\eR}\frac{ d }{ dt }\big( u_t(\psi_t) \big)-u_{t}^{(1)}(\psi_t)+u_t\big( i(\Delta\psi)(t,.) \big)dt
        \end{equation}
        Le premier terme est facile :
        \begin{equation}
            \int_{\eR}\frac{ d }{ dt }\Big( u_t(\psi_t) \Big)dt=\lim_{N\to \infty} \Big[ u_t(\psi_t) \Big]_{t=-N}^{t=N}=0
        \end{equation}
        parce que \( \psi\) est à support compact par rapport à \( t\). Nous restons donc avec
        \begin{equation}
            \int_{\eR}u_{t}^{(1)}(\psi_t)-iu_t\big( (\Delta\psi)(t,.) \big)dt=0
        \end{equation}
        Nous traitons le terme en \( u_t^{(1)}\) en utilisant le fait évident \( T(\varphi)=(\TF T)(\TF^{-1}\varphi)\) et en remarquant le lemme \ref{LemWRoRPIX} :
        \begin{equation}
            u_t^{(1)}(\psi_t)=(\TF u_t^{(1)})(\TF^{-1}\psi_t)=(\TF u)_t^{(1)}(\TF^{-1}\psi_t).
        \end{equation}
        Pour l'autre terme on fait un peu la même chose en nous souvenant ce que fait la transformée de Fourier en traversant le laplacien :
        \begin{equation}
            u_t(\Delta\psi_t)=(\TF u_t)(\TF^{-1}\Delta\psi_t)=(\TF u_t)\big( x\mapsto -\| x \|^2(\TF^{-1}\psi_t)(x) \big).
        \end{equation}
        En recollant encore :
        \begin{equation}    \label{EqHOGaGpt}
            \int_{\eR}(\TF u)^{(1)}_t(\TF^{-1}\psi_t)+i(\TF u_t)\big( \| . \|^2\TF^{-1}\psi_t \big)dt=0.
        \end{equation}
        Cette équation est valable tant que \( \psi\in \swS(\eR\times\eR^d)\) avec support compact en \( t\). Nous allons nous en créer une super cool. D'abord nous choisissons \( \varphi\in\swS(\eR^d)\) et \( \chi\in\swD(\eR)\) et nous considérons\footnote{Le candidat qui parvient à effectivement présenter ça comme développement, il est fort.}
        \begin{equation}    \label{EqEVtJcnz}
            \psi(t,x)=\TF\Big( \xi\mapsto  e^{it\| \xi \|^2}\varphi(\xi)\chi(t) \Big)(x).
        \end{equation}
        Notons que la transformée de Fourier conserve le fait qu'une fonction soit Schwartz, mais pas le fait d'avoir support compact. Cependant nous ne prenons que la transformée de Fourier par rapport à \( x\). Le résultat est donc une fonction \( \psi\) qui est Schwartz par rapport à \( x\) et support compact par rapport à \( t\). Nous pouvons donc écrire \eqref{EqHOGaGpt} en utilisant la fonction \eqref{EqEVtJcnz} :
        \begin{equation}    \label{EqHPUyZFz}
            0=\int_{\eR}(\TF u)_t^{(1)}\Big( x\mapsto e^{it\| x \|^2}\varphi(x)\chi(t) \Big)+i(\TF u_t)\Big( x\mapsto\| x \|^2 e^{it\| x \|^2}\varphi(x)\chi(t) \Big)dt.
        \end{equation}
        Là dedans, \( \chi(t)\) peut sortir à la fois de la transformée de Fourier et de l'application des distributions; il doit seulement rester dans l'intégrale. Dans le second terme nous allons utiliser l'égalité (due entre autre à la proposition \ref{PropUDkgksG}) :
        \begin{subequations}    \label{EqCRGfbLU}
            \begin{align}
            \frac{ d }{ dt }\big( \hat u_t( e^{it\| . \|^2}\varphi) \big)&=\frac{ d }{ dt }\left( u_t\big( \TF e^{it\| . \|^2}\varphi \big) \right)\\
            &=u_t^{(1)}\big( \TF  e^{it\| . \|^2}\varphi \big)+u_t\left( \frac{ \partial  }{ \partial t }\TF e^{it\| . \|^2}\varphi \right)\\
            &=(\TF u_t^{(1)})\big( x\mapsto  e^{it\| x \|^2}\varphi(x) \big)+(\TF u_t)\big( x\mapsto i\| x \|^2 e^{it\| x \|^2}\varphi(x) \big)\\
            &=(\TF u)_t^{(1)}\big( x\mapsto  e^{it\| x \|^2}\varphi(x) \big)+(\TF u_t)\big( x\mapsto i\| x \|^2 e^{it\| x \|^2}\varphi(x) \big).
            \end{align}
        \end{subequations}
        Et là, magie c'est exactement ce qui est dans \eqref{EqHPUyZFz}. Donc
        \begin{equation}
            \int_{\eR}\frac{ d }{ dt }\hat u_t\big( x\mapsto  e^{it\| x \|^2}\varphi(x) \big)\chi(t)dt=0
        \end{equation}
        pour toute fonctions à support compact \( \chi\). Donc la proposition \ref{PropAAjSURG} nous dit que
        % 13107277
        \begin{equation}
            \partial_t\hat u_t\big( x\mapsto e^{it\| x \|^2}\varphi(x) \big)=0.
        \end{equation}
        C'est zéro partout et non seulement presque partout parce qu'en plus nous avons la continuité. Par conséquent pour tout \( t\in \eR\) nous avons
        \begin{equation}
            \hat u_t\big( x\mapsto e^{it\| x \|^2}\varphi(x) \big)=\hat u_0\big( x\mapsto \varphi(x)\big)=0.
        \end{equation}
        Et cela est vrai pour toute fonction \( \varphi\in\swS(\eR^d)\). Nous considérons donc \( t_0\in \eR\) et une fonction \( \theta\in\swS(\eR^d)\) pour construire
        \begin{equation}
            \varphi(x)= e^{-it_0\| x \|^2}\theta(x).
        \end{equation}
        Nous avons alors \( \hat u_{t_0}\big( x\mapsto\theta(x) \big)=0\), ce qui signifie que \( \hat u_{t_0}=0\). Du coup pour tout \( \theta\in\swS(\eR^d)\) nous avons \( u_{t_0}(\TF\theta)=0\), mais comme la transformée de Fourier est une bijection de \( \swS(\eR^d)\) (proposition \ref{PropKPsjyzT}) nous avons en fait \( u_{t_0}(\theta)=0\) pour tout \( \theta\in\swS(\eR^d)\), c'est à dire \( u_{t_0}=0\) pour tout \( t_0\in \eR\) et au final \( u=0\).
    \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Nombres de Bell}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{theorem}[Nombres de Bell\cite{KXjFWKA}]  \label{ThoYFAzwSg}
    Soit \( n\geq 1\) et \( B_n\) le nombre de partitions distinctes de l'ensemble \( \{ 1,\ldots, n \}\) avec la convention que \( B_0=0\). Alors
    \begin{enumerate}
        \item
            La série entière
            \begin{equation}    \label{EqYCMGBmP}
                \sum_{n=0}^{\infty}\frac{ B_n }{ n! }x^n
            \end{equation}
            a un rayon de convergence \( R>0\) et sa somme est donnée par
            \begin{equation}
                f(x)= e^{ e^{x}-1}
            \end{equation}
            pour tout \( x\in\mathopen] -R , R \mathclose[\).
        \item
            Pour tout \( k\in \eN\),
            \begin{equation}
                B_n=\frac{1}{ e }\sum_{k=0}^{\infty}\frac{ k^n }{ k! }.
            \end{equation}
            \item
                Le rayon de convergence de la série \eqref{EqYCMGBmP} est en réalité infini : \( R=\infty\).
    \end{enumerate}
\end{theorem}
\index{anneau!de séries formelles}
\index{dénombrement!partitions de \( \{ 1,\ldots, n \} \)}
\index{série!numérique}
\index{série!entière}
\index{limite!inversion}

\begin{proof}
    \begin{enumerate}
        \item
            Soit \( n\geq 1\) et \( 0\leq k\leq n\). Nous notons \( E_k\) l'ensemble des partitions de \( \{ 1,\ldots, n+1 \}\) pour lesquelles le «paquet» contenant \( n+1\) soit de cardinal \( n+1\). Calculons le cardinal de \( E_k\).

            Pour construire un élément de \( E_k\), il faut d'abord il faut choisir \( k\) éléments dans \( \{ 1,\ldots, n \}\), ce qui donne \( n\choose k\) possibilités. Ensuite il faut trouver une partition des \( (n+1)-(k+1)=n-k\) éléments restants, ce qui fait \( B_{n-k}\) possibilités. Donc
            \begin{equation}
                \Card(E_k)={n\choose k}B_{n-k}.
            \end{equation}
            L'intérêt des ensembles \( E_k\) est que \( \{ E_0,\ldots, E_n \}\) est une partition de l'ensemble des partitions de \( \{ 1,\ldots, n+1 \}\), c'est à dire que \( B_{n+1}=\sum_{k=0}^n\Card(E_k)\), ce qui va nous donner une relation de récurrence pour les \( B_n\) :
\begin{equation}
                    B_{n+1}=\sum_{k=0}^n\Card(E_k)
                   =\sum_{k=0}^n{n\choose k}B_{n-k}
                    =\sum_{l=0}^n{n\choose n-l}B_l 
                    =\sum_{l=0}^n{n\choose l}B_l.
\end{equation}
où nous avons utilisé un petit changement de variables \( l=n-k\). Afin d'étudier la convergence de la série \eqref{EqYCMGBmP}, nous allons montrer par récurrence que pour tout \( n\), \( B_n<n!\). D'abord pour \( n=0\) c'est bon : \( B_1=1\) parce que la seule partition de \( \{ 1 \}\) est \( \{ 1 \}\). Supposons que l'inégalité soit vraie pour une certaine valeur \( k\), et montrons qu'elle est vraie pour la valeur \( k+1\) :
\begin{equation}
                    B_{k+1}=\sum_{l=0}^n{n\choose k}B_k
                   \leq \sum_{l=0}^n{n\choose k}k!
                    =k!\sum_{l=0}^k\underbrace{\frac{1}{ (n-k)! }}_{\leq 1}
                    \leq n!(n+1)
                    =(n+1)!
\end{equation}
            où nous avons utilisé la formule \( {n\choose k}=\frac{ n! }{ k!(n-k)! }\).

            Donc pour tout \( x\in \eR\) nous avons
            \begin{equation}
                0\leq \frac{ B_n }{ n! }| x^n |\leq | x |^n,
            \end{equation}
            et donc la série a un rayon de convergence au moins aussi grand que celui de la série géométrique, c'est à dire que \( 1\). Donc \( R\geq 1\). Nous nommons \( R\) ce rayon de convergence.

        \item

            Soit \( x\in\mathopen] -R , R \mathclose[\). Pour une telle valeur de \( x\) à l'intérieur du disque de convergence, la proposition \ref{ProptzOIuG} nous permet de dériver terme à terme la série\footnote{C'est ici qu'on utilise la convention \( B_0=0\) et ça aura une influence sur le choix de la constante \( K\) plus bas.}
                \begin{equation}
                    f(x)=\sum_{k=0}^{\infty}\frac{ B_k }{ k! }x^k=1+\sum_{k=0}^{\infty}\frac{ B_{k+1} }{ (k+1)! }x^{k+1},
                \end{equation}
                pour obtenir
                \begin{equation}
                    f'(x)=\sum_{k=0}^{\infty}\frac{ B_{k+1} }{ (k+1) }(k+1)x^k=\sum_{k=0}^{\infty}\frac{ B_{k+1} }{ k! }x^k=\sum_{k=0}^{\infty}\left( \sum_{l=0}^k{k\choose l}B_l \right)\frac{ x^k }{ k! }=\sum_{k=0}^{\infty}\left( \sum_{l=0}^k\frac{ B_l }{ l!(l-k)! } \right)x^k.
                \end{equation}
                En cette expression, nous reconnaissons un produit de Cauchy (proposition\ref{ThokPTXYC}) avec \( a_l=\frac{ B_l }{ l! }\) et \( b_n=\frac{ 1 }{ n! }\). Vu que ce sont deux séries ayant un rayon de convergence plus grand que zéro, le produit a encore un rayon de convergence plus grand que zéro et nous pouvons prendre le produit des séries :
                \begin{equation}
                    f'(x)=\left( \sum_{l=0}^{\infty}\frac{ B_l }{ l! }x^l \right)\left( \sum_{k=0}^{\infty}\frac{1}{ k! }x^k \right)=f(x) e^{x}.
                \end{equation}
                Le théorème \ref{ThoNYEXqxO} (avec \( M(x)=e^x\) dans les cas \( n=1\) et \( I=\mathopen] -R , R \mathclose[\)) nous indique que l'espace vectoriel des solutions de cette équation est de dimension \( 1\). Si nous en trouvons une non nulle par n'importe quel moyen, c'est bon. Une solution étant dérivable est continue, donc l'équation \( f'=f e^{x}\) nous indique que \( f'\) est continue. Une solution non nulle va automatiquement accepter un petit voisinage sur lequel la manipulation suivante a un sens :
                    \begin{equation}
                        \frac{ f'(x) }{ f(x) }= e^{x},
                    \end{equation}
                    donc \( \ln\big( | f(x) | \big)= e^{x}+C\) et \( f(x)=K e^{ e^{x}}\) pour une certaine constante. Il est vite vérifier que cette fonction est une solution de l'équation différentielle \( y'(x)=y(x) e^{x}\) et par unicité, toutes les solutions sont de cette forme. Autrement dit, l'espace des solution est l'espace vectoriel \( \Span\{ x\mapsto e^{e^x} \}\). Étant donné que \( f(0)=0\), nous devons choisir \( K=\frac{1}{ e }\) et donc 
                    \begin{equation}
                        f(x)=\frac{1}{ e } e^{e^x}= e^{e^x-1}.
                    \end{equation}

                \item

                    Nous commençons par écrire la fonction \( f\) comme une série de puissance. La partie simple du calcul : pour \( x\in \mathopen] -R , R \mathclose[\), nous avons
                        \begin{equation}    \label{EqODjgjDN}
                        e^{e^x}=\sum_{k=0}^{\infty}\frac{ (e^x)^k }{ k! }=\sum_{k=0}^{\infty}\frac{1}{ k! }\sum_{l=0}^{\infty}\frac{ (kx)^l }{ l! }=\sum_{k=0}^{\infty}\sum_{l=0}^{\infty}\frac{k^l}{k! }\frac{ x^l }{ l! }.
                    \end{equation}
                    Notons que cela n'est pas une série de puissance en \( x\) parce qu'il y a la double somme. Nous allons inverser les sommes au moyen du théorème de Fubini sous la forme du corollaire \ref{CorTKZKwP}. Pour cela nous considérons la fonction
                    \begin{equation}
                        \begin{aligned}
                            a\colon \eN\times \eN&\to \eR \\
                            (k,l)&\mapsto \frac{ (kx)^l }{ k!l! } 
                        \end{aligned}
                    \end{equation}
                    et nous mettons la mesure de comptage\footnote{Nous passons outre les avertissements et menaces de Arnaud Girand.} sur \( \eN\) et \( \eN^2\). Nous commençons donc à vérifier l'intégrabilité variable par variable de \( | a |\) :
                    \begin{subequations}    \label{SubEqsFHsBfhk}
                        \begin{align}
                            \int_{\eN}\left( \int_{\eN}| a(k,l) |dm(l) \right)dm(k)&=\sum_{k=0}^{\infty}\frac{1}{ k! }\frac{ (k| x |)^l }{ l! }\\
                            &=\sum_{k=0}^{\infty}\frac{1}{ k! } e^{k| x |}.
                        \end{align}
                    \end{subequations}
                    Nous devons montrer que cette dernière somme va bien. Pour cela nous posons \( u_k=\frac{ e^{k| x |} }{ k! }\) et nous remarquons que \( \frac{ u_{k+1} }{ u_k }\to 0\). Donc la double intégrale \eqref{SubEqsFHsBfhk} converge, ergo \( a\in L^1(\eN\times \eN)\), ce qui nous permet d'utiliser le théorème de Fubini \ref{ThoFubinioYLtPI} pour inverser les \sout{sommes} \sout{intégrales} sommes dans l'équation \ref{EqODjgjDN} :
                    \begin{equation}
                        \frac{1}{ e }e^{e^x}=\frac{1}{ e }\sum_{k=0}^{\infty}\sum_{l=0}^{\infty}\frac{1}{ k! }\frac{1}{ l! }(kx)^l=\sum_{l=0}\frac{1}{ e }\frac{1}{ l! }\left( \sum_{k=0}^{\infty}\frac{ k^l }{ k! } \right)x^l.
                    \end{equation}
                    Cela est un développement en série entière pour la fonction \( \frac{1}{ e } e^{e^x}\), dont nous savions déjà le développement \eqref{EqYCMGBmP}; par unicité du développement nous pouvons identifier les coefficients :
                    \begin{equation}
                        B_l=\frac{1}{ e }\sum_{k=0}^{\infty}\frac{ k^l }{ k! }.
                    \end{equation}
                    
                \item

                    Le développement \eqref{EqODjgjDN} étant en réalité valable pour tout \( x\) et tous les calculs subséquents l'étant aussi, le développement
                    \begin{equation}
                        e^{e^x-1}=\sum_{n=0}^{\infty}\frac{ B_n }{ n! }x^n
                    \end{equation}
                    est en fait valable pour tout \( x\), ce qui donne à la série entière un rayon de convergence infini.
    \end{enumerate}
\end{proof}
