Les chaînes de Markov interviennent pour la description des systèmes dont l'évolution future ne dépend que de l'état présent.

\begin{definition}
    Soit \( E\) un ensemble au plus dénombrable et \( (\Omega,\tribF,P)\) un espace probabilisé. Une \defe{chaîne de Markov}{chaîne de Markov} à valeurs dans \( E\) est une famille \( (X_n)_{n\in\eN}\) de variables aléatoires telles que pour tout \( x_0,\ldots,x_{n+1}\in E\),
    \begin{equation}
        P(X_{n+1}=x_{n=1}|X_n=x_n,\ldots,X_0=x_0)=P(X_{n+1}=x_{n+1}|X_n=x_n).
    \end{equation}
\end{definition}
Pour une chaîne de Markov, il n'est pas important de savoir l'historique pour prédire la futur : \( X_{n+1}\) est seulement déterminé par \( X_n\).

\begin{remark}
    Il existe une théorie des chaînes de Markov à temps continu ou avec \( E\) non dénombrable, mais ce n'est pas au programme.
\end{remark}

Nous notons
\begin{equation}
    p_n(x,y)=P(X_{n+1}=y|X_n=x)
\end{equation}
la \defe{probabilité de transition}{transition!probabilité} de la chaîne à l'instant \( n\). Si cette probabilité ne dépend pas de \( n\), nous disons que la chaîne de Markov est \defe{homogène}{homogène!chaîne de Markov}, et nous notons \( p(x,y)\) au lieu de \( p_n(x,y)\). Nous notons \( Q\) la matrice (éventuellement infinie) de transition\index{matrice!de transition}
\begin{equation}
    Q_{xy}=p(x,y).
\end{equation}
Nous avons
\begin{equation}
    \sum_{y\in E}p(x,y)=1
\end{equation}
parce que c'est la somme de toutes les transitions possibles en partant de \( x\). Notons aussi que \( p(x,y)\geq 0\).

\begin{definition}
    Une matrice dont tous les éléments sont positifs ou nuls et donc la somme de toutes les lignes sont \( 1\) est une \defe{matrice stochastique}{matrice!stochastique}.
\end{definition}

\begin{lemma}
    Si \( U\) est une matrice stochastique, alors il existe une chaîne de Markov dont la matrice de transition soit \( U\).
\end{lemma}

\begin{remark}
    La somme \( \sum_{x\in E}p(x,y)\) ne vaut pas spécialement \( 1\). Si les états \( x_1\) et \( x_2\) arrivent tous les deux en \( y\) de façon certaine, alors nous avons \( \sum_xp(x,y)\geq 2\). Il n'y a donc pas de limites aux sommes des colonnes.
\end{remark}

\begin{example}
    Nous considérons une fourmi qui se déplace dans un appartement à trois pièces \( A\), \( B\), \( C\). Supposons qu'à chaque minute, elle a une probabilité \( 1/3\) de rester dans la pièce et une probabilité \( 2/3\) de se déplacer. Le plan de l'appartement est
    \begin{equation}
        \xymatrix{%
        A \ar[r]      &  B\ar[r]&C
           }
    \end{equation}
    De la pièce \( A\) est est donc uniquement possible d'aller vers la pièce \( B\); de la \( B\) il est possible d'aller en \( A\) et en \( C\) et de la \( C\) il est uniquement possible d'aller en \( B\).

    La matrice de transition de cette chaîne de Markov est 
    \begin{equation}
        Q=\begin{pmatrix}
            1/3    &   2/3    &   0    \\
            1/3    &   1/3    &   1/3    \\
            0    &   2/3    &   1/3
        \end{pmatrix}
    \end{equation}
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Marche aléatoire sur \texorpdfstring{$\eZ$}{\( \eZ\)}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit \( (Y_n)\) une suite de variables aléatoires indépendantes et identiquement distribuées valant \( -1\) avec une probabilité \( p\) et \( 1\) avec une probabilité \( (1-p)\). La loi est
\begin{equation}
    Y_n\sim p\delta_{-1}+(1-p)\delta_{1}.
\end{equation}
Nous considérons la variable aléatoire
\begin{equation}
    X_n=X_0+\sum_{i=1}^nY_i
\end{equation}
où \( X_0\) est une variable aléatoire indépendante des \( Y_i\) à valeurs dans \( \eZ\). Nous vérifions à présent que \( X_n\) est une chaîne de Markov avec comme espace d'états \( E=\eZ\). Nous devons montrer que
\begin{equation}        \label{EqAVoirMarkovMAZ}
    P\big( X_{n+1}=x_{n+1}| X_n=x_n,\ldots,X_0=x_0 \big)=P\big( X_{n+1}=x_{n+1}|X_n=x_n \big).
\end{equation}
Étant donné que \( X_{n+1}=X_n+Y_{n+1}\), nous pouvons remplacer \( X_{n+1}=x_{n+1}\) par \( x_n+Y_{n+1}=x_{n+1}\) dans le membre de gauche de \ref{EqAVoirMarkovMAZ}.

L'événement 
\begin{equation}
    \{ X_n=x_n,\ldots,X_1=x_1,X_0=x_0 \}\subset\Omega
\end{equation}
est égal à l'événement
\begin{equation}
    \{ X_0=x_0,Y_1=x_1-x_0,Y_2=x_2-x_1,\ldots,Y_n=x_n-x_{n-1} \}.
\end{equation}
Cet événement est l'ensemble
\begin{equation}
    X_0^{-1}(x_0)\cap Y_1^{-1}(x_1-x_0)\cap\ldots\cap Y_{n}^{-1}(x_n-x_{n-1})
\end{equation}
qui est dans la tribu engendrée par les variables aléatoires \( X_0,(Y_i)_{i=1,\ldots,n}\). Le point délicat du raisonnement est de montrer que les événements \( A\) et \( B\) donnés par
\begin{subequations}
    \begin{align}
        A&=\{ Y_{n+1}=x_{n+1}-x_n \}\\
        B&=\{ X_0=x_0 \}\cap\bigcap_{i=1}^n Y_i=x_i-x_{i-1}
    \end{align}
\end{subequations}
sont indépendants. Nous ne pouvons pas montrer directement que \( P(A\cap B)=P(A)P(B)\) parce que cela est la formule que nous voulons utiliser pour montrer que la chaîne est de Markov. Nous passons donc par les tribus :
\begin{subequations}        \label{subesqqsABtribsYYXzY}
    \begin{align}
        A&\in\sigma(Y_{n+1})\\
        B&\in\sigma(X_0,Y_1,\ldots,Y_n).
    \end{align}
\end{subequations}
Nous utilisons maintenant l'hypothèse d'indépendance des variables aléatoires \( X_0\) et \( Y_i\) pour conclure que les deux tribus des équations \eqref{subesqqsABtribsYYXzY} sont indépendantes. Les événements \( A\) et \( B\) sont par conséquent indépendants. De la même façon, l'événement \( A\) est indépendant de l'événement \( \{ X_n=x_n \}\). Nous avons donc successivement
\begin{subequations}
    \begin{align}
        P(X_{n+1}=x_{n+1}|Y_i=x_i-x_{i-1},X_0=x_0)&=P(Y_{n+1}=x_{n+1}-x_n)\\
        &=P(Y_{n+1}=x_{x+1}-x_n|X_n=x_n)\\
        &=P(Y_{n+1}=x_{n+1}-X_n|X_n=x_n)\\
        &=P(X_{n+1}=x_{n+1}|X_n=x_n).
    \end{align}
\end{subequations}
La chaîne est par conséquent de Markov.

La matrice de transition de cette chaîne de Markov est une matrice infinie «dans tous les sens» :
\begin{equation}
    p(x,y)=\begin{cases}
        p    &   \text{si y=x-1}\\
        (1-p)    &    \text{si y=x+1}\\
        0    &   \text{sinon}.
    \end{cases}
\end{equation}

\begin{proposition}
    Voici quelque propriétés des chaînes de Markov homogènes.
    \begin{enumerate}
        \item
            La probabilité d'une trajectoire donnée est
            \begin{equation}
                P(X_n=x_n,X_{n-1}=x_{n-1},\ldots,X_0=x_0)=p(x_{n-1},x_n)\dots p(x_0,x_1)P(X_0=x_0).
            \end{equation}
        \item
            La probabilité de transition «en \( n\) coups» est donnée par la puissance \( n\)ième de la matrice de transition :
            \begin{equation}
                P(X_n=x_n|X_0=x_0)=Q^n_{x_0,x_n}.
            \end{equation}
        \item
            Si l'espace des états \( E\) est fini, l'espérance d'une fonction bornée \( f\colon E\to \eR\) de l'état est donné par
            \begin{subequations}
                \begin{align}
                    E\big( f(X_{n+1})|X_n=x_n,\ldots,X_0=x_0 \big)&=E\big( f(X_{n+1})|X_n=x_n \big)\\
                    &=\sum_{y\in E}f(y)p(x_n,y).j
                \end{align}
            \end{subequations}
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Étant donné que \( P(A\cap B)=P(A|B)P(B)\), nous avons
            \begin{equation}
                P(X_n=x_n,\ldots,X_0=x_0)=P(X_n=x_n|X_{n-1}=x_{n-1},\ldots,X_0=x_0)P(X_{n-1}=x_{n-1},\ldots,X_0=x_0).
            \end{equation}
            Par la propriété de Markov, le premier facteur est
            \begin{equation}
                P(X_n=x_n|X_{n-1}=x_{n-1})=p(x_{n-1},x_n).
            \end{equation}
            Le reste est une récurrence sur \( n\).

        \item
            Montrons avec \( n=2\). En utilisant les divers points du théorème \ref{ThoBayesEtAutres}, nous avons
            \begin{subequations}
                \begin{align}
                    P(X_2=x_2|X_0=x_0)&=\sum_{y\in E}P(X_2=x_2,X_1=y|X_0=x_0)\\
                    &=\sum_{y\in E}P(X_2=x_2|X_1=y,X_0=x_0)P(X_1=y|X_0=x_0)\\
                    &=\sum_{y\in E}P(X_2=x_2|X_1=y)P(X_1=y|X_0=x_0)\\
                    &=\sum_{y\in E}p(x_2,y)p(y,x_0) \label{subEqyExdyyxz}\\
                    &=Q^2_{x_2,x_0}.
                \end{align}
            \end{subequations}
            Bien entendu ici la notion de produit matriciel doit être comprise de façon formelle lorsque \( E\) est infini.
            \begin{remark}
                Nous avons utilisé l'homogénéité de la chaîne de Markov au moment d'écrire l'expression \eqref{subEqyExdyyxz}. En principe nous aurions dû écrire \( p_2(y,x_2)p_1(x_0,y)\).
            \end{remark}
    \end{enumerate}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Graphe de transition}
%---------------------------------------------------------------------------------------------------------------------------

Le \defe{graphe de transition}{graphe de transition} d'une chaîne de Markov est le graphe dont les sommets sont les éléments de l'espace des états de la chaîne et dont les sommets sont reliés par des arrêtes pondérées par la probabilité de transition correspondante.

\begin{definition}
    Une chaîne de Markov est \defe{irréductible}{irréductible!chaîne de Markov}\index{chaîne de Markov!irréductible} si pour tout \( x,y\in E\), il existe \( n\) tel que \( p^n(x,y)>0\) où 
    \begin{equation}
        p^n(x,y)=P(X_n=y|X_0=x).
    \end{equation}
    Le nombre \( n\) peut dépendre de \( x\) et \( y\).
\end{definition}

\begin{lemma}
    Une chaîne de Markov homogène est irréductible si et seulement si son graphe de transition est connexe.
\end{lemma}

\begin{proof}
    Pour chaque couple \( (x,y)\in E^2\) nous avons
    \begin{equation}
        \begin{aligned}[]
            p^n(x,y)&=\sum_{z_i\in E}P(X_n=y,X_{n-1}=z_{n-1},\ldots,X_1=z_1,X_0=x)\\
            &=\sum_{z_i}p(z_{n-1},y)p(z_{n-2},z_{n-1})\ldots p(z_1,z_2)p(x,z_1).
        \end{aligned}
    \end{equation}
    La positivité d'un des termes de la somme signifie que le graphe est connexe tandis que la positivité de \( p^n(x,y)\) signifie que la chaîne est irréductible.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Classification des états}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Sauf mention expresse du contraire, nous considérons toujours une chaîne de Markov homogène.

\begin{definition}
    Un état \( x\in E\) est \defe{absorbant}{absorbant} pour la chaine \( (X_n)\) si \( p(x,x)=1\).
\end{definition}
Il n'est pas spécialement impossible d'arriver sur un état absorbant, mais il est impossible d'en sortir.

Si \( x\in E\), nous notons
\begin{equation}
    T(x)=\inf\{ k\geq 1\tq X_k=x \},
\end{equation}
le \defe{premier temps d'atteinte}{premier temps d'atteinte} de l'état \( x\). Si \( X_0=x\), alors \( T(x)\) est le \defe{temps de retour}{temps de retour} en \( x\). Si \( p\in \eN\) nous notons
\begin{equation}
    T_p(x)=\inf\{ k\geq 1\tq X_{k+p}=x \}.
\end{equation}
C'est le temps mis pour atteindre \( x\) à partir de l'instant \( p\).

\begin{proposition}
    La loi de la variable aléatoire \( [T_p(x)|X_p=x]\) est la même que celle de la variable aléatoire \( [T(x)|X_0=x]\).
\end{proposition}

\begin{proof}
    Nous devons montrer que 
    \begin{equation}
        P(T_p(x)=k|X_p=x)=P(T(x)=k|X_0=x).
    \end{equation}
    Cela est intuitivement évident du fait qu'une chaîne de Markov soit un processus sans mémoire. Affin de prouver, nous allons sommer sur tous les états intermédiaires possibles :
    \begin{subequations}
        \begin{align}
            P&(T_p(x)=k|X_0=x)=P(X_{p+k}=x,X_{p+k-1}\neq x,\ldots,X_{p+1}\neq x|X_p=x)\\
            &=\sum_{z_i\neq x}P(X_{p+k}=x,X_{p+k-1}=z_{k-1},\ldots,X_{p+1}=z_1|X_p=x)\\
            &=\sum_{z_i}P(X_{p+k}=x,X_{p+k-i}=z_i|X_{p+1}=z_1,X_p=x)\underbrace{P(X_{p+1}=z_1|X_p=x)}_{=p(x,z_1)}\\
            &=\sum_{z_i}P(X_{p+k}=x,X_{p+k-i}=z_i|X_{p+2}=z_2,X_{p+1}=z_1,X_p=x)\\
            &\qquad\underbrace{P(X_{p+2}=z_2|X_{p+1}=z_1,X_p=x)}_{P(X_{p+2}=z_2|X_{p+1}=z_1)=p(z_1,z_2)}p(x,z_1)\\
            &=\ldots\\
            &=\sum_{z_i}p(x,z_1)p(z_1,z_2)\ldots p(z_{k-1},z_{k-1})p(z_{k-1},x).
        \end{align}
    \end{subequations}
    À ce point ci, nous avons éliminé toute référence à \( p\) grâce à l'homogénéité de la chaîne. Nous pouvons refaire le calcul à l'envers pour reconstituer l'expression de départ sans le \( p\) :
    \begin{subequations}
        \begin{align}
         \sum_{z_i}p(x,z_1)p(z_1,z_2)\ldots &p(z_{k-1},z_{k-1})p(z_{k-1},x)\\
         &=P(x_k=x,X_{k-1}\neq x,\ldots,X_1\neq x|X_0=x)\\
         &=P(T(x)=k),
        \end{align}
    \end{subequations}
    ce qu'il fallait obtenir.
\end{proof}

\begin{definition}
    Un état \( x\) est \defe{récurrent}{récurrent!état} si \( P(T(x)=\infty|X_0=0)=0\), c'est à dire si la probabilité de ne jamais retourner en $x$ lorsqu'on y est passé est nulle. L'état \( x\) est \defe{transient}{transient!état} dans le cas contraire.
\end{definition}

Nous introduisons une variable aléatoire qui compte le nombre de fois que la chaîne de Markov passe par l'état \( x\) :
\begin{equation}
    N_x=\sum_{k=0}^{\infty}\mtu_{\{ X_k=x\}}.
\end{equation}
C'est une variable aléatoire à valeurs dans \( \eN\cup\{ \infty \}\).

\begin{proposition}
    Les deux propriétés suivantes sont équivalentes à dire que \( x\) est récurrent:
    \begin{enumerate}
        \item
            \( P(N_x<\infty|X_0=x)=0\)
        \item
            \( E(N_x|X_0=x)=\infty\).
    \end{enumerate}
    Les deux propriétés suivantes sont équivalentes à dire que \( x\) est transient :
    \begin{enumerate}
        \item
            \( P(N_x<\infty|X_0=x)=1\)
        \item
            \( E(N_x|X_0=x)<\infty\).
    \end{enumerate}
\end{proposition}

