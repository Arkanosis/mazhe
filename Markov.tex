\begin{quote}
    Mets tes deux pieds en canard, c'est la chaîne de Markov qui se prépare.
\end{quote}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Généralités}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Les chaînes de Markov interviennent pour la description des systèmes dont l'évolution future ne dépend que de l'état présent.

\begin{definition}
    Soit \( E\) un ensemble au plus dénombrable et \( (\Omega,\tribF,P)\) un espace probabilisé. Une \defe{chaîne de Markov}{chaîne de Markov} à valeurs dans \( E\) est une famille \( (X_n)_{n\in\eN}\) de variables aléatoires telles que pour tout \( x_0,\ldots,x_{n+1}\in E\),
    \begin{equation}
        P(X_{n+1}=x_{n+1}|X_n=x_n,\ldots,X_0=x_0)=P(X_{n+1}=x_{n+1}|X_n=x_n).
    \end{equation}
\end{definition}
Pour une chaîne de Markov, il n'est pas important de savoir l'historique pour prédire la futur : \( X_{n+1}\) est seulement déterminé par \( X_n\).

\begin{remark}
    Il existe une théorie des chaînes de Markov à temps continu ou avec \( E\) non dénombrable, mais ce n'est pas au programme.
\end{remark}

Nous notons
\begin{equation}
    p_n(x,y)=P(X_{n+1}=y|X_n=x)
\end{equation}
la \defe{probabilité de transition}{transition!probabilité} de la chaîne à l'instant \( n\). Si cette probabilité ne dépend pas de \( n\), nous disons que la chaîne de Markov est \defe{homogène}{homogène!chaîne de Markov}, et nous notons \( p(x,y)\) au lieu de \( p_n(x,y)\). Nous notons \( Q\) la matrice (éventuellement infinie) de transition\index{matrice!de transition}
\begin{equation}
    Q_{xy}=p(x,y).
\end{equation}
Nous avons
\begin{equation}
    \sum_{y\in E}p(x,y)=1
\end{equation}
parce que c'est la somme de toutes les transitions possibles en partant de \( x\). Notons aussi que \( p(x,y)\geq 0\).

\begin{definition}
    Une matrice dont tous les éléments sont positifs ou nuls et donc la somme de toutes les lignes sont \( 1\) est une \defe{matrice stochastique}{matrice!stochastique}.
\end{definition}

\begin{lemma}
    Si \( U\) est une matrice stochastique, alors il existe une chaîne de Markov dont la matrice de transition soit \( U\).
\end{lemma}

\begin{remark}
    La somme \( \sum_{x\in E}p(x,y)\) ne vaut pas spécialement \( 1\). Si les états \( x_1\) et \( x_2\) arrivent tous les deux en \( y\) de façon certaine, alors nous avons \( \sum_xp(x,y)\geq 2\). Il n'y a donc pas de limites aux sommes des colonnes.
\end{remark}

\begin{example}
    Nous considérons une fourmi qui se déplace dans un appartement à trois pièces \( A\), \( B\), \( C\). Supposons qu'à chaque minute, elle a une probabilité \( 1/3\) de rester dans la pièce et une probabilité \( 2/3\) de se déplacer. Le plan de l'appartement est
    \begin{equation}
        \xymatrix{%
        A \ar[r]      &  B\ar[r]&C
           }
    \end{equation}
    De la pièce \( A\) est est donc uniquement possible d'aller vers la pièce \( B\); de la \( B\) il est possible d'aller en \( A\) et en \( C\) et de la \( C\) il est uniquement possible d'aller en \( B\).

    La matrice de transition de cette chaîne de Markov est 
    \begin{equation}
        Q=\begin{pmatrix}
            1/3    &   2/3    &   0    \\
            1/3    &   1/3    &   1/3    \\
            0    &   2/3    &   1/3
        \end{pmatrix}
    \end{equation}
\end{example}

\begin{example}
    Si \( N_t\) est un processus de Poisson, alors les variables aléatoires \( X_n=N_n\) forment une chaîne de Markov.
\end{example}
% TODO : donner une raison.

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Marche aléatoire sur \texorpdfstring{$\eZ$}{\( \eZ\)}}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit \( (Y_n)\) une suite de variables aléatoires indépendantes et identiquement distribuées valant \( -1\) avec une probabilité \( p\) et \( 1\) avec une probabilité \( (1-p)\). La loi est
\begin{equation}
    Y_n\sim p\delta_{-1}+(1-p)\delta_{1}.
\end{equation}
Nous considérons la variable aléatoire
\begin{equation}
    X_n=X_0+\sum_{i=1}^nY_i
\end{equation}
où \( X_0\) est une variable aléatoire indépendante des \( Y_i\) à valeurs dans \( \eZ\). Nous vérifions à présent que \( X_n\) est une chaîne de Markov avec comme espace d'états \( E=\eZ\). Nous devons montrer que
\begin{equation}        \label{EqAVoirMarkovMAZ}
    P\big( X_{n+1}=x_{n+1}| X_n=x_n,\ldots,X_0=x_0 \big)=P\big( X_{n+1}=x_{n+1}|X_n=x_n \big).
\end{equation}
Étant donné que \( X_{n+1}=X_n+Y_{n+1}\), nous pouvons remplacer \( X_{n+1}=x_{n+1}\) par \( x_n+Y_{n+1}=x_{n+1}\) dans le membre de gauche de \ref{EqAVoirMarkovMAZ}.

L'événement 
\begin{equation}
    \{ X_n=x_n,\ldots,X_1=x_1,X_0=x_0 \}\subset\Omega
\end{equation}
est égal à l'événement
\begin{equation}
    \{ X_0=x_0,Y_1=x_1-x_0,Y_2=x_2-x_1,\ldots,Y_n=x_n-x_{n-1} \}.
\end{equation}
Cet événement est l'ensemble
\begin{equation}
    X_0^{-1}(x_0)\cap Y_1^{-1}(x_1-x_0)\cap\ldots\cap Y_{n}^{-1}(x_n-x_{n-1})
\end{equation}
qui est dans la tribu engendrée par les variables aléatoires \( X_0,(Y_i)_{i=1,\ldots,n}\). Le point délicat du raisonnement est de montrer que les événements \( A\) et \( B\) donnés par
\begin{subequations}
    \begin{align}
        A&=\{ Y_{n+1}=x_{n+1}-x_n \}\\
        B&=\{ X_0=x_0 \}\cap\bigcap_{i=1}^n \{ Y_i=x_i-x_{i-1}\}
    \end{align}
\end{subequations}
sont indépendants. Nous ne pouvons pas montrer directement que \( P(A\cap B)=P(A)P(B)\) parce que cela est la formule que nous voulons utiliser pour montrer que la chaîne est de Markov. Nous passons donc par les tribus :
\begin{subequations}        \label{subesqqsABtribsYYXzY}
    \begin{align}
        A&\in\sigma(Y_{n+1})\\
        B&\in\sigma(X_0,Y_1,\ldots,Y_n).
    \end{align}
\end{subequations}
Nous utilisons maintenant l'hypothèse d'indépendance des variables aléatoires \( X_0\) et \( Y_i\) pour conclure que les deux tribus des équations \eqref{subesqqsABtribsYYXzY} sont indépendantes. Les événements \( A\) et \( B\) sont par conséquent indépendants. De la même façon, l'événement \( A\) est indépendant de l'événement \( \{ X_n=x_n \}\). Nous avons donc successivement
\begin{subequations}
    \begin{align}
        P(X_{n+1}=x_{n+1}|Y_i=x_i-x_{i-1},X_0=x_0)&=P(Y_{n+1}=x_{n+1}-x_n)\\
        &=P(Y_{n+1}=x_{x+1}-x_n|X_n=x_n)\\
        &=P(Y_{n+1}=x_{n+1}-X_n|X_n=x_n)\\
        &=P(X_{n+1}=x_{n+1}|X_n=x_n).
    \end{align}
\end{subequations}
La chaîne est par conséquent de Markov.

La matrice de transition de cette chaîne de Markov est une matrice infinie «dans tous les sens» :
\begin{equation}
    p(x,y)=\begin{cases}
        p    &   \text{si y=x-1}\\
        (1-p)    &    \text{si y=x+1}\\
        0    &   \text{sinon}.
    \end{cases}
\end{equation}

\begin{proposition}
    Voici quelque propriétés des chaînes de Markov homogènes.
    \begin{enumerate}
        \item
            La probabilité d'une trajectoire donnée est
            \begin{equation}
                P(X_n=x_n,X_{n-1}=x_{n-1},\ldots,X_0=x_0)=p(x_{n-1},x_n)\dots p(x_0,x_1)P(X_0=x_0).
            \end{equation}
        \item
            La probabilité de transition «en \( n\) coups» est donnée par la puissance \( n\)ième de la matrice de transition :
            \begin{equation}
                P(X_n=x_n|X_0=x_0)=Q^n_{x_0,x_n}.
            \end{equation}
        \item
            Si l'espace des états \( E\) est fini, l'espérance d'une fonction bornée \( f\colon E\to \eR\) de l'état est donné par
            \begin{subequations}
                \begin{align}
                    E\big( f(X_{n+1})|X_n=x_n,\ldots,X_0=x_0 \big)&=E\big( f(X_{n+1})|X_n=x_n \big)\\
                    &=\sum_{y\in E}f(y)p(x_n,y).j
                \end{align}
            \end{subequations}
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Étant donné que \( P(A\cap B)=P(A|B)P(B)\), nous avons
            \begin{equation}
                P(X_n=x_n,\ldots,X_0=x_0)=P(X_n=x_n|X_{n-1}=x_{n-1},\ldots,X_0=x_0)P(X_{n-1}=x_{n-1},\ldots,X_0=x_0).
            \end{equation}
            Par la propriété de Markov, le premier facteur est
            \begin{equation}
                P(X_n=x_n|X_{n-1}=x_{n-1})=p(x_{n-1},x_n).
            \end{equation}
            Le reste est une récurrence sur \( n\).

        \item
            Montrons avec \( n=2\). En utilisant les divers points du théorème \ref{ThoBayesEtAutres}, nous avons
            \begin{subequations}
                \begin{align}
                    P(X_2=x_2|X_0=x_0)&=\sum_{y\in E}P(X_2=x_2,X_1=y|X_0=x_0)\\
                    &=\sum_{y\in E}P(X_2=x_2|X_1=y,X_0=x_0)P(X_1=y|X_0=x_0)\\
                    &=\sum_{y\in E}P(X_2=x_2|X_1=y)P(X_1=y|X_0=x_0)\\
                    &=\sum_{y\in E}p(x_2,y)p(y,x_0) \label{subEqyExdyyxz}\\
                    &=Q^2_{x_2,x_0}.
                \end{align}
            \end{subequations}
            Bien entendu ici la notion de produit matriciel doit être comprise de façon formelle lorsque \( E\) est infini.
            \begin{remark}
                Nous avons utilisé l'homogénéité de la chaîne de Markov au moment d'écrire l'expression \eqref{subEqyExdyyxz}. En principe nous aurions dû écrire \( p_2(y,x_2)p_1(x_0,y)\).
            \end{remark}
    \end{enumerate}
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Graphe de transition}
%---------------------------------------------------------------------------------------------------------------------------

Le \defe{graphe de transition}{graphe de transition} d'une chaîne de Markov est le graphe dont les sommets sont les éléments de l'espace des états de la chaîne et dont les sommets sont reliés par des arrêtes pondérées par la probabilité de transition correspondante.

\begin{definition}
    Une chaîne de Markov est \defe{irréductible}{irréductible!chaîne de Markov}\index{chaîne de Markov!irréductible} si pour tout \( x,y\in E\), il existe \( n\) tel que \( p^n(x,y)>0\) où 
    \begin{equation}
        p^n(x,y)=P(X_n=y|X_0=x).
    \end{equation}
    Le nombre \( n\) peut dépendre de \( x\) et \( y\).
\end{definition}

\begin{lemma}
    Une chaîne de Markov homogène est irréductible si et seulement si son graphe de transition est connexe.
\end{lemma}

\begin{proof}
    Pour chaque couple \( (x,y)\in E^2\) nous avons
    \begin{equation}
        \begin{aligned}[]
            p^n(x,y)&=\sum_{z_i\in E}P(X_n=y,X_{n-1}=z_{n-1},\ldots,X_1=z_1,X_0=x)\\
            &=\sum_{z_i}p(z_{n-1},y)p(z_{n-2},z_{n-1})\ldots p(z_1,z_2)p(x,z_1).
        \end{aligned}
    \end{equation}
    La positivité d'un des termes de la somme signifie que le graphe est connexe tandis que la positivité de \( p^n(x,y)\) signifie que la chaîne est irréductible.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{chaîne de Markov définie par récurrence}
%---------------------------------------------------------------------------------------------------------------------------

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Le cas général}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

\begin{proposition}     \label{PropqiMdHh}
    Soit \( X_0\) une variable aléatoire à valeurs dans $E$, un ensemble au plus dénombrable. Soit \( (Y_n)\) une suite de variables aléatoires réelles indépendantes et identiquement distribuées indépendantes de \( X_0\).

    Soit \( (X_n)\) la suite de variables aléatoires à valeurs dans \( E\) définie par récurrence selon la formule
    \begin{equation}
        X_{n+1}=G(X_n,Y_{n+1})
    \end{equation}
    où \( G\colon E\times \eR\to E\) est une fonction mesurable. Alors \( (X_n)\) est une chaîne de Markov.
\end{proposition}

\begin{proof}
    Soient \( x_0,\ldots, x_{n+1}\) des éléments de \( E\). Nous devons calculer la valeur de
    \begin{equation}
        P(X_{n+1}=x_{n+1}|X_n=x_n,\ldots, X_0=x_0).
    \end{equation}
    Commençons par préciser les espaces sur lesquels nos variable aléatoires sont définies. Nous avons
    \begin{equation}
        X_0\colon \Omega_0\to E
    \end{equation}
    et 
    \begin{equation}
        Y_i\colon \Omega\to \eR.
    \end{equation}
    La variable aléatoire \( X_1\) est donnée par
    \begin{equation}
        \begin{aligned}
            X_1\colon \Omega_0\times \Omega&\to E \\
            (\omega_0,\omega_1)&\mapsto G\big( X_0(\omega_0),Y_1(\omega_1) \big). 
        \end{aligned}
    \end{equation}
    La variable aléatoire \( X_2\) est 
    \begin{equation}
        \begin{aligned}
            X_2\colon \Omega_0\times \Omega^2&\to E \\
            (\omega_0,\omega_1,\omega_2)&\mapsto G\big( X_1(\omega_0,\omega_1),Y_2(\omega_2) \big)\\
                &\quad=G\Big( G\big( X_0(\omega_0),\omega_1 \big),Y_2(\omega_2) \Big)
        \end{aligned}
    \end{equation}
    et ainsi de suite.

    Considérons maintenant l'événement
    \begin{equation}
        \{ X_1=x_1,X_0=x_0 \}\subset \Omega_0\times \Omega.
    \end{equation}
    Il est donné explicitement par
    \begin{subequations}
        \begin{align}
            \{ X_1=x_1,X_0=x_0 \}&=\{ (\omega_0,\omega_1)\tq G\big( X_0(\omega_0),Y_1(\omega_1)=x_1,X_0(\omega_0)=x_0 \big) \}\\
            &=\{ (\omega_0,\omega_1)\tq G\big( x_0,Y_1(\omega_1)=x_1,X_0(\omega_0)=x_0 \big) \}\\
            &=\{ \omega_0\in \Omega_0\tq X_0(\omega_0)=x_0 \}\times \{ \omega_1\in \Omega\tq G\big( x_0,Y_1(\omega_1)=x_1 \big) \}.
        \end{align}
    \end{subequations}
    Le premier terme du produit cartésien est dans \( \sigma(X_0)\), tandis que le second est dans \( \sigma(Y_1)\). Étant donné la définition des tribus produit (définition \ref{DefTribProfGfYTuR}) nous avons
    \begin{equation}
        \{ X_1=x_1,X_0=x_0 \}\in\sigma(X_0,Y_1).
    \end{equation}
    Ce raisonnement se généralise immédiatement et nous trouvons que
    \begin{equation}
        \{ X_n=x_n,\ldots, X_0=x_0 \}\in\sigma(X_0,Y_1,\ldots, Y_n).
    \end{equation}
    Nous sommes donc à calculer
    \begin{subequations}
        \begin{align}
        \diamondsuit&=P(X_{n+1}=x_{n+1}|X_n=X_n,\ldots, X_0=X_0)\\
        &=P\big( \underbrace{G(x_n,Y_{n+1})=x_{n+1}}_{\in\sigma(Y_{n+1})}|\underbrace{X_n=x_n,\ldots, X_0=x_0}_{\in\sigma X_0,Y_1,\ldots, Y_n} \big).
        \end{align}
    \end{subequations}
    Les tribus \( \sigma(Y_{n+1})\) et \( \sigma(X_0,Y_1,\ldots, Y_n)\) étant indépendantes nous avons
    \begin{subequations}
        \begin{align}
            \diamondsuit&=P\big( G(x_n,Y_{n+1})=x_{n+1} \big)\\
            &=P\big( G(x_n,Y_{n+1})=x_{n+1}|X_n=X_n \big)       \label{jdvyUK}\\
            &=P\big( G(X_n,Y_{n+1})=x_{n+1}|X_n=x_n \big)\\
            &=P(X_{n+1}=x_{n+1}|X_n=x_n).
        \end{align}
    \end{subequations}
    Pour \eqref{jdvyUK} nous avons utilisé le fait que \( \sigma(Y_{n+1})\) est indépendante de \( \sigma(X_n)\). Nous avons prouvé que la chaîne était de Markov.
\end{proof}
Les probabilités de transition de la chaîne de Markov définie dans la proposition \ref{PropqiMdHh} sont
\begin{equation}
    P(X_1=y|X_0=x)=P\big( G(X_0,Y_1)=y|X_0=x_0 \big)=P\big( G(x_0,Y_1)=y \big).
\end{equation}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Exemple : la file de réparation de machines à laver}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Nous considérons un magasin de réparation d'électroménager. Durant le jour \( n\), un nombre aléatoire \( Z_{n}\) de machines en panne arrivent au magasin. Une machine est réparée chaque jour (aucune si le magasin est vide). Nous supposons que les \( Z_n\) soient indépendantes et identiquement distribuées, et nous posons \( X_n\), le nombre de machines en magasin le jour \( n\).

La loi d'avancement de \( X_n\) est
\begin{equation}
    X_{n+1}=\begin{cases}
        X_n+Z_n-1    &   \text{si \( X_n\neq 0\)}\\
        Z_n    &    \text{si \( X_n=0\)}.
    \end{cases}
\end{equation}
Cela est une chaîne de Markov en vertu de la proposition \ref{PropqiMdHh}. Ici la fonction est
\begin{equation}
    G(x,y)=x+y-\mtu_{x\neq 0}.
\end{equation}
Les probabilités de transitions sont 
\begin{equation}
    p(x,y)=\begin{cases}
        0    &   \text{si \( x\leq y-2\)}\\
        P(Z=0)    &    \text{si \( x=y-1\)}\\
        P(Z=k)&\text{si \( x=y+k-1\)}
    \end{cases}
\end{equation}
pour \( x\neq 0\).


\Exo{Model-0007}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Mesure invariante}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    Une mesure de probabilité \( \mu\) sur l'espace des états \( E\) d'une chaîne de Markov est \defe{invariante}{invariante!mesure!pour une chaîne de Markov} si pour tout \( x\in E\)
    \begin{equation}
        \mu(x)=\sum_{y\in E}p(y,x)\mu(y).
    \end{equation}
\end{definition}
\begin{remark}
    Une mesure invariante est une mesure de probabilité et nous noterons par abus $ \mu(x)$ pour $\mu(\{x\})$. Si \( A\subset E\) nous avons 
    \begin{equation}
        \mu(A)=\sum_{x\in A}\mu(x).
    \end{equation}
\end{remark}

\begin{remark}  \label{RemwcRRFZ}
    Une loi invariante associée à une chaîne de Markov est une loi associée à la matrice de transition de la chaîne, mais pas à la loi de $X_0$. Par conséquent nous pouvons tester si \( \mu\) est une mesure invariante pour une certaine chaîne de Markov $(X_k)$ en considérant la chaîne $(Y_k)$ avec $Y_k=X_k$ pour $k>0$ et $Y_0$ arbitraire.
\end{remark}

L'adjectif \emph{invariant} provient du lemme suivant.
\begin{lemma}       \label{LemUVMwbM}
    Soit \( (X_n)\) une chaîne de Markov telle que \( X_0\sim\mu\) où \( \mu\) est une mesure invariante sur l'espace des états. Alors \( X_k\sim \mu\) pour tout \( k\).
\end{lemma}

\begin{proof}
    Par hypothèse, \( P(X_0=x)=\mu(x)\). Ensuite nous avons
    \begin{subequations}
        \begin{align}
            P(X_1=y)&=\sum_{x\in E}P(X_1=y|X_0=x)P(X_0=x)\\
            &=\sum_xp(x,y)\mu(x)\\
            &=\mu(y).
        \end{align}
    \end{subequations}
    Par conséquent \( X_1\) suit également la loi \( \mu\). Par récurrence tous les états suivent cette même loi.
\end{proof}
Si les états d'une chaîne de Markov ont comme loi une mesure invariante, alors nous disons que la chaîne est \defe{stationnaire}{stationnaire!chaîne de Markov}.

\begin{remark}\label{RemcOEylF}
    Pour une chaîne de Markov stationnaire de loi invariante $\mu$ nous avons
    \begin{equation}
        \mu(x)=\sum_yp(y,x)\mu(y)
    \end{equation}
    et si l'ensemble \( E\) est fini cette équation signifie
    \begin{equation}
        \mu=Q\mu
    \end{equation}
    où \( Q\) est la matrice de transition de la chaîne de Markov.
\end{remark}


\begin{theorem}[Théorème ergodique]
    Une chaîne de Markov irréductible est positive récurrente si et seulement si elle accepte une mesure invariante. Cette mesure est invariante est alors unique et vérifie \( \mu=Q\mu\) où \( Q\) est la matrice de transition.
\end{theorem}

\begin{proof}
    Nous n'allons uniquement prouver le théorème ergodique dans le cas où \( E\) est fini. Soit \( (X_n)\) une chaîne de Markov récurrente positive; nous avons \( \pi(x)>0\) pour tout \( x\in E\). Nous allons montrer que \( \pi\) est une mesure invariante.

    Nous commençons par montrer que
    \begin{equation}
        \sum_{x\in E}\pi(x)=1.
    \end{equation}
    Pour cela nous reprenons la propriété de chaîne irréductible pour écrire
    \begin{equation}
        \pi(x)=\lim_{N\to \infty}\frac{1}{ N }\sum_{k=1}^N\mtu_{X_k=x}
    \end{equation}
    Étant donné que \( E\) est fini nous pouvons sommer sur \( x\in E\) et permuter la somme avec la limite :
    \begin{equation}
        \sum_{x\in E}\pi(x)=\lim_{N\to \infty} \frac{1}{ N }\sum_{k=1}^N\underbrace{\sum_{x\in E}\mtu_{X_k=x}}_{=1}.
    \end{equation}
    Nous nous retrouvons donc avec \( \lim_{N\to \infty} \frac{1}{ N }N=1\). La fonction \( \pi\) définit donc bien une mesure de probabilité sur \( E\).

    Nous montrons à présent que cette mesure est invariante, c'est à dire que
    \begin{equation}
        \pi(x)=\sum_{y\in E}p(y,x)\pi(y).
    \end{equation}
    Pour cela nous utilisons encore le théorème de la convergence dominée pour permuter la limite et l'intégrale dans
    \begin{equation}        \label{EqcKxNcL}
        \pi(x)=E(\pi(x))=\lim_{N\to \infty} \frac{1}{ N }\sum_{k=1}^N\underbrace{E\big( \mtu_{X_k=x} \big)}_{P(X_k=x)}=\lim_{N\to \infty} \frac{1}{ N }\sum_{k=1}^NP(X_{k+1}=x).
    \end{equation}
    La dernière égalité découle du fait que en divisant par \( N\) et en faisant tendre \( N\) vers l'infini, le fait d'enlever un terme à la somme ne change pas la valeur de la limite. Nous pouvons substituer dans \eqref{EqcKxNcL} la valeur
    \begin{equation}
        P(X_{k+1}=x)=\sum_{y\in E}p(y,x)P(X_k=y).
    \end{equation}
    Nous avons alors
    \begin{subequations}
        \begin{align}
            \pi(x)&=\lim_{N\to \infty} \frac{1}{ N }\sum_{k=1}^N\sum_{y\in E}p(y,x)P(X_k=y)\\
            &=\sum_{y\in E}p(y,x)\lim_{N\to \infty} \frac{1}{ N }\sum_{k=1}^NP(X_k=y)\\
            &=\sum_{y\in E}p(y,x)\pi(y),
        \end{align}
    \end{subequations}
    ce qui signifie que \( \pi\) est une mesure invariante. Notons que nous avons encore utilisé le fait que \( E\) soit fini pour permuter avec la limite.

    Il nous reste à montrer l'unicité de la mesure invariante sur la chaîne de Markov. Soit \( \mu\) une mesure invariante pour la chaîne de Markov $(X_k)$. Comme indiqué dans la remarque \ref{RemwcRRFZ} nous pouvons supposer que $X_0$ suit la loi \( \mu\). Par le lemme \ref{LemUVMwbM} nous avons \( P(X_k=x)=\mu(x)\) pour tout \( k\). Par conséquent
    \begin{equation}
        \pi(x)=\lim_{N\to \infty} \frac{1}{ N }\sum_{k=1}^NP(X_k=x)=\mu(x).
    \end{equation}
\end{proof}

\begin{theorem}[loi des grands nombres pour les chaîne de Markov]\index{loi des grands nombres!pour les chaînes de Markov}
    Soit \( (X_n)\) une chaîne de Markov irréductible acceptant une mesure invariante. Soit \( f\colon E\to \eR\) une fonction dans \( L^1(E,\mu)\). Alors nous avons
    \begin{equation}
        \frac{1}{ N }\sum_{k=1}^Nf(X_k)\stackrel{p.s.}{\longrightarrow}\sum_{x\in E}f(x)\mu(x).
    \end{equation}
\end{theorem}
En ce qui concerne les notations, l'hypothèse \( f\in L^1(E,\mu)\) signifie
\begin{equation}
    \sum_{x\in E}| f(x) |\mu(x)=\int_E| f(x) |d\mu(x)<\infty.
\end{equation}

\begin{proof}
    Nous prouvons le théorème dans le cas où \( E\) est fini. Si nous écrivons
    \begin{equation}
        f(X_k)=\sum_{y\in E}f(y)\mtu_{X_k=y},
    \end{equation}
    alors
    \begin{equation}
        \frac{1}{ N }\sum_{k=1}^Nf(X_k)=\sum_{y\in E}f(y)\frac{1}{ N }\sum_{k=1}^N\mtu_{X_k=y}.
    \end{equation}
    Étant donné que \( E\) est fini nous pouvons permuter les sommes et prendre la limite \( N\to\infty\):
    \begin{equation}
        \lim_{N\to \infty} \frac{1}{ N }\sum_kf(X_k)=\sum_{y\in E}f(y)\lim_{N\to \infty} \frac{1}{ N }\sum_{k=1}^N\mtu_{X_k=y}=\sum_{y\in E}f(y)\pi(y).
    \end{equation}
% TODO : il me semble que je dois encore terminer cette preuve.    
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Convergence vers l'équilibre}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Nous voudrions savoir sous quelles conditions la variable aléatoire \( X_n\) converge en loi vers quelque chose lorsque \( n\to \infty\). Une telle loi limite doit dépendre de la loi initiale\footnote{Lorsque la loi limite ne dépend pas de la loi initiale, nous disons que la chaîne de Markov est ergodique, nous y reviendrons.} comme le montre l'exemple de la chaîne de Markov
\begin{equation}
\xymatrix{%
A\ar@(dl,ul)^1  & C\ar[l]_{1/2}\ar[r]^{1/2} &  B\ar@(dr,ur)_1
   }
\end{equation}
Si \( X_0=C\), alors la loi limite est
\begin{equation}
    \frac{ 1 }{2}(\delta_A+\delta_B).
\end{equation}
Si par contre \( X_0=B\), la loi limite est \(\delta_B\). Notons que la chaîne de Markov proposée ici est irréductible.

Notons qu'il n'y a pas toujours de lois limite comme le montre l'exemple
\begin{equation}
    \xymatrix{%
        A\ar@<1ex>[r]^1  & B\ar@<1ex>[l]^1
       }
\end{equation}
avec \( X_0=A\). La loi en est
\begin{equation}
    X_k=\begin{cases}
        \delta_A    &   \text{si \( k\) est pair}\\
        \delta_B    &    \text{si \( k\) est impair}.
    \end{cases}
\end{equation}

\begin{lemma}
    Si nous avons une loi limite
    \begin{equation}    \label{EqmbQMAY}
        P(X_n=x)\to l(x),
    \end{equation}
    et que la chaîne est irréductible, alors nous avons \( l=\pi\).
\end{lemma}

\begin{proof}
    D'après la proposition \ref{PropjOjDux} nous avons
    \begin{equation}
        \frac{1}{ n }\sum_{k=1}^nP(X_k=x)\to \pi(x).
    \end{equation}
    Par le lemme \ref{LemyGjMqM} sur la moyenne de Cesaro et l'hypothèse \eqref{EqmbQMAY}, nous avons aussi
    \begin{equation}
        \frac{1}{ n }\sum_{k=1}^nP(X_k=x)\to l(x).
    \end{equation}
    Du coup \( \pi(x)=l(x)\).
\end{proof}

\begin{lemma}[\cite{MarkGuy}]
    Si \( \pi\) est une loi stationnaire et si \( x\) est un étant transient, alors \( \pi(x)=0\).
\end{lemma}
Ce lemme (qui peut être prouvé rigoureusement) est principalement dû au fait que la chaîne de Markov ne visite un état transitoire qu'un nombre fini de fois par la proposition \ref{PropEquivEPrecuequiv}\ref{ItemiMnGpD}.

\begin{definition}  \label{DefCxvOaT}
    Un état \( x\in E\) est \defe{apériodique}{état!apériodique}\index{apériodique!état d'une chaîne de Markov} si
    \begin{equation}
        \pgcd\{ n\geq 1\tq p^n(x,x)>0 \}=1.
    \end{equation}
\end{definition}
Mettons que tous les \( n\) tels que \( p^n(x,x)>0\) ont \( 2\) comme diviseur. L'état n'est alors pas apériodique, mais on voit que si \( X_0=x\), alors les états impairs ne peuvent pas être sur \( x\). Cela est une forme de périodicité.

Si un état est apériodique, il existe \( p\) et \( q\) premiers entre eux tels que \( p^p(x,x)\) et \( p^q(x,x)\) sont non nuls. En particulier pour tout \( n\in p\eN+q\eN\), \( P(X_n=x)\neq 0\). Par conséquent la proposition \ref{PropLAbRSE} nous indique qu'à partir d'un certain moment tous les \( X_k\) pourraient être \( x\).

L'état \( C\) de la chaîne de Markov suivante est apériodique :
\begin{equation}
    \xymatrix{%
    A\ar@<0.5ex>[rr]^1  && B\ar@<0.5ex>[ll]^{2/3}\ar[dl]^{1/3}\\
    &C\ar[ul]^1
       }
\end{equation}
En effet \( p^3(C,C)\neq 0\) par le chemin \( C\to A\to B\to C\) tandis que \( p^5(C,C)\neq 0\) également par le chemin \( C\to A\to B\to A\to B\to C\). Or \( \pgcd\{ 3,5 \}=1\).

\begin{proposition}[\cite{MarkGuy}]     \label{PropSaOysS}
    Soit \( (X_n)\), une chaîne de Markov irréductible. Un état \( x\) est apériodique si et seulement si il existe \( N\) tel que
    \begin{equation}
        p^k(x,x)=P(X_k=x|X_0=x)>0
    \end{equation}
    pour tout \( k\geq N\).
\end{proposition}

La proposition suivante va nous permettre de parler de \defe{chaîne apériodique}{chaîne de Markov!apériodique}\index{apériodique!chaîne de Markov}.
\begin{proposition}
    Si une chaîne de Markov est irréductible, alors un état est apériodique si et seulement si tous les états sont apériodiques.
\end{proposition}

\begin{proof}
    Soit \( x\) un état apériodique de la chaîne de Markov \( (X_n)_{n\in \eN}\). En vertu de la proposition \ref{PropSaOysS} il existe \( N_x\) tel que \( p^k(x,x)\neq 0\) pour tout \( k\geq N_x\). Soit \( y\in E\). Étant donné que la chaîne est irréductible, il existe \( r\) et \( s\) tels que\( p^r(x,y)>0\) et \( p^s(y,x)>0\). Nous avons
    \begin{equation}
        p^{k+r+s}(y,y)=P(X_{k+r+s}=y|X_0=y)\geq p^s(x,y)P(X_k=x|X_0=x)p^r(y,x).
    \end{equation}
    Si \( k\) est assez grand, cette quantité est strictement positive. Donc il suffit de prendre \( N_y=N_x+r+s\) pour savoir que \( y\) est également apériodique.
\end{proof}

Si \( E\) est fini et si la chaîne de Markov est irréductible, alors en posant \( N=\max_{x\in E}N(x)\), la matrice \( P^k\) a des éléments non nuls sur toute la diagonale pour tout \( k>N\). Ces éléments diagonaux ne sont autre que les \( p^k(x,x)\).

\begin{theorem}[Convergence en loi des chaîne de Markov]
    Si \( (X_n)\) est
    \begin{enumerate}
        \item
            irréductible,
        \item
            récurrente positive,
        \item
            apériodique,
    \end{enumerate}
    alors \( X_n\) converge en loi vers l'unique probabilité invariante \( \pi\) qui vérifie
    \begin{equation}
        \pi(x)=\sum_{u\in E}p(y,x)\pi(y)=\frac{1}{ E\big( T(x)|X_0=x \big) }.
    \end{equation}

    Cette convergence est indépendante de la loi de \( X_0\) et on a
    \begin{equation}
        P(X_n=x|X_0=y)\to_{n\to \infty} \pi(x).
    \end{equation}
\end{theorem}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Processus de Galton-Watson}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\index{processus!Galton-Watson}

Nous considérons une maladie et notons \( Z_n\) le nombre de malades à l'instant \( n\). Nous posons \( Z_0=1\) et
\begin{equation}        \label{EqBvILKj}
    Z_{n+1}=\begin{cases}
        0    &   \text{si \( Z_n=0\)}\\
        \sum_{i=1}^{Z_n}\xi_i^{(n)}    &    \text{sinon}
    \end{cases}
\end{equation}
où \( \xi_i^{(n)}\) est le nombre de personnes contaminées par le malade \(i\) à l'instant \( n\). Nous supposons que ces variables aléatoires sont indépendantes et identiquement distribuées et admettent un moment d'ordre \( 1\).

L'équation de propagation \ref{EqBvILKj} signifie que nous supposons qu'une personne malade à l'instant \( n\) n'est plus malade à l'instant \( n+1\). Par ailleurs les hypothèses d'indépendance signifient qu'à chaque instant, le nombre de personnes contaminées par le malade \( i\) est indépendant du nombre de personnes contaminées par le malade \( j\). De plus la façon dont la contamination se passe à l'instant \( n\) est indépendant de la façon dont la contamination se passe à l'instant \( m\). Ces hypothèses sont raisonnables tant que le nombre de personnes non contaminées est grand. À partir du moment où presque tout le monde est malade, l'approximation de Galton-Watson ne fonctionne plus.

Nous notons \( \xi\) la loi parente des \( \xi_i^{(n)}\). Ensuite nous considérons 
\begin{subequations}
    \begin{align}
        G(s)=E(s^{\xi})\\
        m=E(\xi)\\
        G_n(s)=E(s^{Z_n}).
    \end{align}
\end{subequations}

    Par le théorème de transfert (proposition \ref{PropintdPintdPXeR}) avec \( f(t)=s^t\). Ce que nous avons est
    \begin{equation}        \label{EqNRtXdC}
        G_n(s)=E\big( f(Z_n) \big)=\int_{\eR}s^xdP_{Z_n}(x)=\sum_{k=0}^{\infty}s^kP(Z_n=k)
    \end{equation}
    où l'intégrale s'est transformée en somme parce que la loi de \( Z_n\) est discrète : \( dP_{Z_n}\) est une somme de masses de Dirac. En particulier nous avons
    \begin{subequations}
        \begin{align}
            G_n(s)&=\sum_{k=0}^{\infty}s^kP(Z_n=k)\\
            G(0)&=P(Z_n=0)
        \end{align}
    \end{subequations}
    et
    \begin{equation}
        \eta=\lim_{n\to \infty} P(Z_n=0)=\lim_{n\to \infty} G_n(0).
    \end{equation}
    D'où l'intérêt d'étudier \( G_n\).

\begin{lemma}       \label{LemezrOiI}
    Pour tout \( n\in \eN^*\) et pour tout \( s\in\mathopen[ 0 , 1 \mathclose]\), nous avons
    \begin{equation}
        G_n(s)=\underbrace{G\circ G\circ\ldots\circ G(s)}_{\text{\( n\) fois}}.
    \end{equation}
\end{lemma}

\begin{proof}
    Pour \( n=1\), nous avons \( Z_1=\xi^{(1)}_1\) et donc
    \begin{equation}
        G_1(s)=E(s^{\xi})=G(s),
    \end{equation}
    comme il se doit.

    Si \( n\neq 1\) nous écrivons
    \begin{subequations}    \label{subEqsxhILKg}
        \begin{align}
            G_n(s)&=E(s^{Z_n})\\
            &=E\left( s^{\sum_{i=1}^{Z_{n-1}}\xi_i^{(n-1)}} \right)\\
            &=E\left( \sum_{k=0}^{\infty}\mtu_{\{ Z_{n-1}=k \}}s^{\sum_{i=1}^k\xi_i^{(n-1)}} \right).
        \end{align}
    \end{subequations}
    À ce niveau, nous voulons permuter la somme et l'espérance. Étant donné que le lemme est facile à vérifier pour \( s=1\), nous supposons \( s<1\). Du coup 
    \begin{equation}
        s^{\sum_{i=1}^k\xi_i^{(n-1)}}<1
    \end{equation}
    et ce qui se trouve dans l'espérance est majoré par
    \begin{equation}
        \sum_{k=0}^{\infty}\mtu_{Z_{n-1}=k}=1.
    \end{equation}
    La fonction constante \( 1\) est intégrable sur \( \Omega\) (ici nous utilisons à fond le fait que l'espace \( \Omega\) soit un espace de probabilité) et nous pouvons utiliser le théorème de convergence dominée de Lebesgue \ref{ThoockMHn} pour permuter la somme et l'intégrale. Nous continuons donc le calcul \eqref{subEqsxhILKg}:
    \begin{equation}
        G_n(s)=\sum_{k=0}^{\infty}E\left(  \mtu_{\{ Z_{n-1}=k \}}s^{\sum_{i=1}^k\xi_i^{(n-1)}}  \right).
    \end{equation}
    La tribu engendrée par la variable aléatoire \( \mtu_{\{ Z_{n-1}=k \}}\) est une fonction des variable aléatoires \( \xi_i^{(m)}\) avec \( m\leq n-2\) tandis que la variable aléatoire \( s^{\sum_{i=1}^k\xi_i^{(n-1)}}\) est une fonction des variable aléatoires \( \xi_{i}^{(n-1)}\). Par conséquent le lemme de regroupement \ref{LemHOjqqw} nous dit que ces variables aléatoires sont indépendantes, donc
    \begin{equation}
        G_n(s)=\sum_{k=0}^{\infty}\underbrace{E\big( \mtu_{\{ Z_{n-1}=k \}} \big)}_{=P(Z_{n-1}=k)}E\big( s^{\sum_{i=1}^{k}\xi_i^{(n-1)}} \big).
    \end{equation}
    Nous avons utilisé le fait que l'espérance d'une fonction indicatrice est la probabilité de l'événement.

    En ce qui concerne la puissance de \( s\), les événements \( \xi_i^{n-1}\) sont indépendants et suivent tous la même loi \( \xi\), donc
    \begin{equation}
        s^{\sum_{i=1}^{k}\xi_i^{(n-1)}}=\prod_{i=1}^ks^{\xi_i^{(n-1)}}
    \end{equation}
    et
    \begin{equation}
        E\big( \prod_{i=1}^ks^{\xi} \big)=E(s^{\xi})^k=G(s)^k.
    \end{equation}
    En mettant tout bout à bout,
    \begin{equation}
        G_n(s)=\sum_{k=1}^{\infty}P(Z_{n-1}=k)G(s)^j=G_{n-1}\big( G(s) \big).
    \end{equation}
\end{proof}

\begin{theorem}  \label{ThoJZnAOA}
    La probabilité d'extinction \( \eta\) est donnée par
    \begin{equation}
        \eta=P\left(\bigcup_{n\geq 1}(Z_n=0)\right)=\lim_{n\to \infty} P(Z_n=0).
    \end{equation}
    Ce nombre est la plus petite solution positive de l'équation \( G(s)=s\).

    De plus la classification des cas est comme suit.
    \begin{enumerate}
        \item
            Si \( P(\xi=0)=0\) alors \( \eta=0\).
        \item
            Si \( P(\xi=0)\neq 0\) alors
            \begin{enumerate}
                \item
                    si \( m\leq 1\) alors \( \eta=1\),
                \item
                    si \( m>1\) alors \( \eta\in\mathopen] 0 , 1 \mathclose[\).
            \end{enumerate}
    \end{enumerate}
\end{theorem}
Le cas \( m<1\) est dit \defe{sous-critique}{Galton-Watson!sous-critique}, le cas \( m=1\) est dit \defe{critique}{critique!Galton-Watson}. Le cas \( m>1\) est dit \defe{sur-critique}{Galton-Watson!sur-critique}.

\begin{proof}
    Commençons par prouver que \( G\) est une fonction continue. En utilisant la théorème de transfert comme pour l'équation \eqref{EqNRtXdC} nous trouvons que
    \begin{equation}    \label{EqQWTBfn}
        G(s)=E(s^{\xi})=\sum_{k=0}^{\infty}p_ks^k
    \end{equation}
    où nous avons noté \( p_k=P(\xi=k)\). Si \( r<1\), alors la suite \( p_kr^k\) est bornée, donc le critère d'Abel (\ref{LemmbWnFI}) nous indique que la série \eqref{EqQWTBfn} converge absolument et la théorie générale des séries entières conclu que la fonction \( G\) est en particulier dérivable terme à terme pour tout \( s\in\mathopen] -1 , 1 \mathclose[\). 

    \begin{subproof}

        \item[Le probabilité d'extinction est un point fixe de \( G\)]

    En utilisant la continuité de \( G\) en \( 0\) nous passons à la limite dans \( G_{n+1}(0)=G\big( G_n(0) \big)\) et nous obtenons
    \begin{equation}
        \eta=G(\eta),
    \end{equation}
    ce qui signifie que la probabilité d'extinction est un point fixe de \( G\).

        \item[\( \eta\) est le plus petit point fixe de \( G\)] 

    Nous démontrons maintenant que \( \eta\) est plus précisément le plus petit point fixe de \( G\) sur \( \mathopen[ 0 , 1 \mathclose]\). Nous allons effectuer cette partie en décomposant selon les valeurs de \( p_0\) et de \( p_1\).

    Au vu de l'écriture \eqref{EqQWTBfn}, si \( p_1=1\) alors \( G(s)=s\) pour tout \( s\in\mathopen[ 0 , 1 \mathclose]\). Mais dans ce cas nous savons par ailleurs que l'extinction est impossible.  Zéro est bien la plus petite solution de \( G(s)=s\).

    Supposons maintenant que \( p_1<1\) et \( p_0+p_1=1\). Alors \( G(s)=p_0+p_1s\) et \( s=1\) est l'unique solution. Mais vu que nous savons que \( \eta\) est solution, c'est que \( \eta=1\) et l'extinction est certaine. 

    Nous passons au cas général : \( p_0+p_1<1\). D'abord nous remarquons que \( s=1\) est solution parce que 
    \begin{equation}
        G(1)=p_0+p_1+\cdots=1.
    \end{equation}
    Remarquons aussi que dans ce cas \( s=0\) n'est plus solution.

    La fonction \( G\) est strictement convexe sur \( \mathopen[ 0 , 1 \mathclose]\) (parce que \( G''>0\)). Cela se voir en effectuant deux dérivations termes à termes (le rayon de convergence de la dérivée est le même que celui de la fonction). Cette stricte convexité entraine que l'équation \( G(s)=s\) a au maximum une autre solution que \( s=1\). Nous nommons \( s_0\) la plus petite solution dans \( \mathopen[ 0 , 1 \mathclose]\). Étant donné que \( G\) est croissante on a
    \begin{equation}
        G(0)\leq G(s_0)=s_0.
    \end{equation}
    En appliquant \( G\) à cette équation nous obtenons \( G\big( G(s_0) \big)\leq G(s_0)=s_0\) et en appliquant \( n\) fois,
    \begin{equation}
        G_n(0)\leq s_0.
    \end{equation}
    En passant à la limite, \( \eta\leq s_0\) mais \( \eta\) étant solution, nous avons \( \eta=s_0\). Nous avons donc prouvé que la probabilité d'extinction \( \eta\) est la plus petite solution de \( G(s)=s\).

\item[Classification des cas]

    Nous devons encore discuter les cas. Si \( P(\xi=0)=0\), alors \( p_0=0\) et \( G(0)=0\), ce qui signifie que \( s_0=\eta=0\) et l'extinction est impossible. 
    
    Nous passons au cas \( p_0\neq 0\). Si \( p_0+p_1=1\), alors \( m=p_1<1\) et nous avions déjà vu que dans le cas \( p_0+p_1=1\), la probabilité d'extinction est \( \eta=1\).

    Il nous reste à traiter le cas \( p_0+p_1<1\). Encore une fois, la courbe \( G\) est strictement convexe sur \( \mathopen[ 0 , 1 \mathclose]\) et elle est en particulier plus grande que sa tangente en \( s=1\), c'est à dire
    \begin{equation}
        G(s)>G'(1)(s-1)+G(1).
    \end{equation}
    Nous savons que \( G(1)=1\). En ce qui concerne \( G'(1)\), nous dérivons encore terme à termes :
    \begin{equation}
        G'(s)=\sum_{k=1}^{\infty}kp_ks^{k-1},
    \end{equation}
    donc
    \begin{equation}
        G'(1)=\sum_{k=1}^{\infty}kp_k=E(\xi)=m.
    \end{equation}
    Ce que nous avons donc est
    \begin{equation}
        G(s)>1+m(s-1).
    \end{equation}
    Nous nous particularisons au cas sous-critique (\( m\leq 1\)). En nous rappelant que \( s-1<0\),
    \begin{equation}
        G(s)>1+(s-1)=s,
    \end{equation}
    donc \( s=1\) est la plus petite solution et effectivement nous avons déjà vu que \( \eta=1\) dans ce cas.

    Si \( m>1\), alors on a
    \begin{equation}
        G(s)>1+m(s-1).
    \end{equation}
    Mais dire \( m>1\) revient à dire \( G'(1)>1\) et donc dans un voisinage de \( s=1\) on a
    \begin{equation}
        \frac{ G(s)-G(1) }{ s-1 }>1,
    \end{equation}
    ce qui implique que
    \begin{equation}
        G(s)<s-1+G(1)=s.
    \end{equation}
    Nous avons donc \( G(s)<s\) dans un voisinage de \( 1\). Mais \( G(0)-0=p_0>0\), donc la fonction \( f(s)=G(s)-s\) est positive en \( 0\) et négative proche de \( s=1\). Le théorème de la valeur intermédiaire nous indique alors qu'il existe un \( s\in \mathopen] 0 , 1 \mathclose[\) tel que \( f(s)=0\), c'est à dire tel que \( G(s)=s\).
    \end{subproof}
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Martingales}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    Si \( \tribA\) est une tribu, une \defe{filtration}{filtration} de \( \tribA\) est une suite croissante de sous-tribus \( \tribB_i\subseteq\tribB_{i+1}\subseteq\tribA\).
\end{definition}

\begin{definition}
    Une \defe{martingale}{martingale} adaptée à la filtration \( (\tribB_n)_{n\in \eN}\) est une suite de variables aléatoires \( M_n\in L^1(\Omega,\tribA,P)\) telle que
    \begin{enumerate}
        \item
            \( M_n\) est \( \tribB_n\)-mesurable,
        \item
            \( E(M_{n+1}|\tribB_n)=M_n\).
    \end{enumerate}
    Nous disons que la martingale \( (M_n)_{n\geq 1}\) est \defe{terminée}{terminée!martingale} si il existe \( M\in L^1(\Omega,\tribA,P)\) telle que \( M_n=E(M|\tribA_n)\) pour tout \( n>1\).
\end{definition}

\begin{example}
    Si \( M\in L^1(\Omega,\tribA,P)\) et si \( (\tribB_n)_{n\in \eN}\) est une filtration, nous pouvons considérer la martingale \( M_n=E(M|\tribB_n)\).
\end{example}

\begin{example}     \label{ExtFFKTr}
    Soit \( (X_i)_{i\geq 1}\) une suite de variables aléatoires indépendantes et centrées. On pose
    \begin{equation}
        S_n=X_1+\ldots +X_n
    \end{equation}
    et la filtration \( \tribB_n=\sigma(X_1,\ldots, X_n)\). Pour montrer que cela est une martingale, nous commençons par remarquer que
    \begin{equation}
        E(X_{n+1}|\tribB_n)=E(X_{n+1})=0
    \end{equation}
    par indépendance des tribus \( \tribB_n\) et \( \sigma(X_{n+1})\). Ici c'est le lemme \ref{LemxUZFPV} qui joue.

    Ensuite nous argumentons que \( E(X_1+\ldots +X_n|\tribB_n)=X_1+\ldots +X_n\). En effet d'une part \( X_1+\ldots +X_n\) est \( \tribB_n\)-mesurable et évidemment la condition intégrale de l'espérance conditionnelle est satisfaite.

    Plus généralement si \( X\) est une variable aléatoire et si \( \sigma(X)\subset\tribB\) alors \( E(X|\tribB)=X\).
\end{example}

\begin{lemma}   \label{LemqanhgJ}
    Soit \( (M_n)\) une martingales adaptée à la filtration \( (\tribF_n)\) et \( n\geq k\). Alors
    \begin{subequations}
        \begin{align}
            E(M_n|\tribF_k)&=M_k\\
            E(M_k|\tribF_n)&=M_k.
        \end{align}
    \end{subequations}
\end{lemma}

\begin{proof}
    La seconde relation revient seulement à dire que \( M_k\) est \( \tribF_n\)-mesurable, ce qui est évident parce que \( \tribF_k\subset\tribF_n\).

    Nous prouvons la première par récurrence (à l'envers) sur \( k\). D'abord si \( k=n\), l'égalité \( E(M_n|\tribF_n)=M_n\). Nous supposons maintenant que \( E(M_n|\tribF_k)=M_k\), et nous prouvons que \( E(M_n|\tribF_{k-1})=M_{k-1}\). Si \( B_{k-1}\in \tribF_{k-1}\), nous avons
    \begin{equation}
        \int_{B_{k-1}}M_{k-1}=\int_{B_{k-1}}M_{k}=\int_{B_{k-1}}M_n.
    \end{equation}
    La première égalité est la définition d'une martingale, et la seconde est l'hypothèse de récurrence.
\end{proof}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Convergence de martingales}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[\cite{GubinelliMartin,PMCmartinLP}]     \label{ThobysyWI}
    Soit \( (M_n)_{n\geq 0}\) une martingale bornée dans \( L^2(\Omega)\), c'est à dire telle que\index{martingale!bornée dans \( L^2(\Omega)\)}
    \begin{equation}
        \alpha=\sup_{n\geq 0}E(M_n^2)<\infty.
    \end{equation}
    Alors la suite \( M_n\) converge dans \( L^2(\Omega)\).
\end{theorem}

\begin{proof}
    Nous écrivons \( M_n\) en somme télescopique
    \begin{equation}
        M_n=M_0+\sum_{k=1}^n\Delta_k
    \end{equation}
    où \( \Delta_k=M_k-M_{k-1}\). Nous commençons par monter que les incréments sont orthogonaux au sens où \( E(\Delta_n\Delta_k)=0\). Pour \( n>k\), la variable aléatoire \( E\big( \Delta_n\Delta_k|\tribF_{n-1} \big)\) est la variable aléatoire \( \tribF_{n-1}\)-mesurable telle que
    \begin{equation}
        \int_{B_{n-1}}E\big( \Delta_n\Delta_k|\tribF_{n-1} \big)=\int_{B_{n-1}}\Delta_n\Delta_k
    \end{equation}
    pour tout \( B_{n-1}\in\tribF_{n-1}\). En particulier avec \( B_{n-1}=\Omega\) nous trouvons
    \begin{equation}
        E\Big( E\big( \Delta_n\Delta_k|\tribF_{n-1} \big)\Big)=E(\Delta_n\Delta_k)
    \end{equation}
    par la définition de l'espérance \eqref{EqdCBLst}. Par conséquent, en utilisant le lemme \ref{LemqanhgJ} nous avons\footnote{À ce niveau je crois qu'il y a une faute dans \cite{PMCmartinLP} qui conditionne par rapport à \( \tribF_n\).}
    \begin{equation}
        E(\Delta_n\Delta_k)=E\Big( E(\Delta_n\Delta_k|\tribF_{n-1}) \Big)=E\Big( \Delta_kE(\Delta_n|\tribF_{n-1}) \Big)=0
    \end{equation}
    parce que \( E(\Delta_n|\tribF_{n-1})=E(M_n|\tribF_{n-1})-E(M_{n-1}|\tribF_{n-1})=0\).

    Utilisant l'orthogonalité des incréments, nous avons
    \begin{equation}
        E(M_n^2)=E(M_0^2)+\sum_{k=1}^nE(\Delta_k^2).
    \end{equation}
    En prenant le supremum (par rapport à \( n\) des deux côtés),
    \begin{equation}
        E(M_0^2)+\sum_{k=1}^{\infty}E(\Delta_k^2)=\alpha<\infty.
    \end{equation}
    Cela prouve que la suite \( \sum_{k=1}^n\Delta_k\) converge dans \( L^2(\Omega)\). Nous en déduisons immédiatement que \( (M_n)\) est de Cauchy dans \( L^2(\Omega)\) parce que si \( k,l>n\), nous avons (en utilisant encore l'orthogonalité des incréments)
    \begin{equation}
        E\big( | M_k-M_l |^2 \big)=\sum_{i=k+1}^lE(\Delta_i^2)\leq\sum_{i=k+1}^{\infty}E(\Delta_i^2),
    \end{equation}
    qui tend vers zéro lorsque \( n\to\infty\).
\end{proof}

Le théorème suivant complète la conclusion du théorème \ref{ThobysyWI}.
\begin{theorem}[\cite{PMCmartinLP}] \label{ThofcttYW}
    Soit \( (M_n)_{n\in \eN}\) une martingale bornée dans \( L^2\). Alors \( (M_n)\) converge dans \( L^2(\Omega)\) et presque surement vers une même variable aléatoire \( M_{\infty}\) qui vérifie
    \begin{equation}        \label{EqmDMfZf}
        M_n=E(M_{\infty}|\tribF_n).
    \end{equation}
\end{theorem}

Notons en particulier que la variable aléatoire \( M_{\infty}\) est presque surement finie parce qu'en vertu de \eqref{EqmDMfZf} nous avons
\begin{equation}
    \int_{\Omega}M_{\infty}=\int_{\Omega}M_n<\infty.
\end{equation}


\begin{example}
    Soient des variables aléatoires indépendantes \( V_k\sim\dE(2^n\lambda)\) et la variable aléatoire somme
    \begin{equation}
        S_n=\sum_{k=1}^nV_k.
    \end{equation}
    Nous allons montrer que \( S_n\stackrel{p.s.}{\longrightarrow}X\) où \( X\) est une variable aléatoire presque surement finie. Nous posons
    \begin{equation}
        M_n=S_n-\sum_{k=1}^n\frac{1}{ 2^k\lambda }
    \end{equation}
    Cela est une martingale adaptée à la filtration \( \tribF_n=\sigma(V_1,\ldots, V_n)\) en vertu de l'exemple \ref{ExtFFKTr}. Nous montrons à présent qu'elle est bornée dans \( L^2(\Omega)\) au sens où \( \sum_{n\geq 1}E(M_n^2)<\infty\). Nous avons
    \begin{equation}
        E(M_n^2)=E\left( \big[ S_n-\sum_k\frac{1}{ 2^k\lambda } \big]^2 \right)=E\left( \big[ \sum_k(V_k-\frac{1}{ 2^k\lambda }) \big]^2 \right).
    \end{equation}
    La variable aléatoire \( V_k-1/2^k\lambda\) est une variable aléatoire centrée de variance \( 1/(2^k\lambda)^2\) (voir proposition \ref{PropTxGcWn}). Étant donné que \( M_n\) est centrée, \( \Var(M_n)=E(M_n^2)\) et nous avons
    \begin{equation}
        E(M_n^2)=\sum_{k=1}^n\Var\left( V_k-\frac{1}{ 2^k\lambda } \right)=\sum_{k=1}^n\frac{1}{ (2^k\lambda)^2 },
    \end{equation}
    cette dernière somme étant bornée par \( l=\sum_{k=1}^{\infty}\frac{1}{ (2^k\lambda)^2 }\), nous avons
    \begin{equation}
        E(M_n^2)\leq l
    \end{equation}
    avec \( l\) indépendant de \( n\). C'est pour cela que \( (M_n)_{n\in \eN}\) est une martingale bornée dans \( L^2(\Omega)\). Par le théorème \ref{ThofcttYW} nous avons \( M_n\to M_{\infty}\) et en faisant \( n\to \infty\) dans
    \begin{equation}
        S_n=M_n+\sum_{k=1}^n\frac{1}{ 2^k\lambda },
    \end{equation}
    nous trouvons
    \begin{equation}
        S_n\to M_{\infty}+\sum_{k=1}^{\infty}\frac{1}{ 2^k\lambda }=M_{\infty}+\frac{1}{ \lambda }
    \end{equation}
    qui est presque surement finie.
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Processus de Poisson}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecHxbtzQ}

\begin{definition}
    Une famille de variables aléatoires \( (N_t)_{t\geq 0}\) est une \defe{processus de Poisson}{processus!Poisson}\index{Poisson!processus} d'intensité \( \lambda\) si il existe une suite de variables aléatoires indépendantes et identiquement distribuées \( (T_k)_{k\in \eN}\) de loi \( \dE(\lambda)\) telles que 
    \begin{equation}
        N_t=\sup\{ n\geq 0\tq \sum_{k=1}^nT_k\leq t \}.
    \end{equation}
\end{definition}
Si nous posons \( S_n=\sum_{k=1}^nT_k\), alors nous avons une expression plus pratique pour \( N_t\) :
\begin{equation}
    N_t=\sum_{n=1}^{\infty}\mtu_{\{ S_n\leq t \}}.
\end{equation}
Nous avons par la proposition \ref{PropGMntiy} vu que \( N_t\sim\dP(\lambda t)\).


Pour chaque \( \omega\in \Omega\), la fonction \( t\mapsto N_t(\omega)\) est une fonction (pas du tout strictement) croissante à valeurs dans $\eN$. Cette fonction part de \( 0\) et fait un saut de taille \( 1\) après des intervalles de temps \( T_1(\infty)\), \( T_2(\omega)\), etc. Elle est continue à droite.

Nous avons les égalités d'événements suivantes qui sont pratiques :
\begin{subequations}
    \begin{align}
        \{ s<S_n\leq t \}=\{ N_t\geq n>N_s \}\\
        \{ N_t=n \}=\{ S_n\leq t\leq S_{n+1} \}.
    \end{align}
\end{subequations}

\begin{theorem}
    Les variables aléatoires \( (N_t)_{t\geq 0}\) est un processus de Poisson d'intensité \( \lambda\) si et seulement si elles vérifient les trois propriétés suivantes.
    \begin{description}
        \item[Accroissements indépendants] Pour tout choix \( 0<t_0<t_1<\ldots t_n\), les variables aléatoires \( N_{t_{i+1}}-N_{t_i}\) sont indépendantes. 
        \item[Accroissements stationnaires] Si \( 0<s<t\) et \( h>0\) alors
            \begin{equation}
                N_{t+h}-N_{s+h}\stackrel{\mL}{=}N_t-N_s,
            \end{equation}
            c'est à dire que les accroissements décalés suivent les mêmes lois.
        \item[Poisson] Pour tout \( t\) nous avons \( N_t\sim\dP(\lambda t)\).
    \end{description}
\end{theorem}
Une conséquence des accroissements stationnaires est que \( N_t-N_s\stackrel{\mL}{=}N_{t-s}-N_0=N_{t-s}\) parce que \( N_0=0\).

\begin{proposition}
    Si \( (N_t)\) est un processus de Poisson d'intensité \( \lambda\), alors 
    \begin{equation}
        \lim_{t\to \infty} N_t=+\infty
    \end{equation}
    presque surement. De plus
    \begin{equation}        \label{EqvaVYAs}
        \lim_{t\to \infty} \frac{ N_t }{ t }=\lambda
    \end{equation}
    presque surement.
\end{proposition}
La relation \eqref{EqvaVYAs} est appelée \defe{loi des grands nombres}{loi des grands nombres!processus de Poisson}.

\begin{proof}
    Par définition nous savons que
    \begin{equation}
        N_t=\sup\{ n\geq 0\tq S_n\leq t \}.
    \end{equation}
    Évidemment la fonction \( t\mapsto N_t\) est croissante, donc la limite
    \begin{equation}
        \lim_{t\to \infty} N_t(\omega)
    \end{equation}
    existe dans \( \mathopen[ 0 , \infty \mathclose]\). Nous pouvons nous restreindre à \( t\in \eN\) et considérer \( L(\omega)=\lim_{n\to \infty} N_n(\omega)\). Par somme télescopique avec \( N_0=0\),
    \begin{equation}    \label{EqfRpnfF}
        \frac{ N_n }{ n }=\frac{ \sum_{k=1}^n(N_k-N_{k-1}) }{ n }.
    \end{equation}
    Étant donné que le processus est de Poisson, les variables aléatoires \( (N_k-N_{k-1})_{k=1,\ldots, n}\) sont indépendantes et suivent toutes la loi de \( N_1-N_0\), c'est à dire la loi de \( N_1\). Encore par le fait que \( N_t\) soit de Poisson nous savons que \( N_1\sim\dP(\lambda)\). La loi des grands nombres (\ref{ThoefQyKZ}) appliquée aux variables aléatoires \( N_k-N_{k-1}\) nous dit que
    \begin{equation}
        \frac{ N_n }{ n }\stackrel{p.s.}{\longrightarrow}E(N_1)=\lambda>0.
    \end{equation}
    Du coup \( N_n\to \infty\) et \( L(\omega)=\infty\).

    Nous démontrons maintenant la loi des grands nombres pour les processus de Poisson. Étant donné que pour les entiers \( N_n/n\to \lambda\), pour les réels, si la limite existe, ça ne peut pas être autre chose. Si nous notons \( \bar t\) la partie entière de \( t\in \eR^+\),
    \begin{equation}
        \frac{ N_t }{ t }=\frac{ N_t-N_{\bar t} }{ t }+\frac{ N_{\bar t} }{ t }.
    \end{equation}
    Le second terme est relativement simple à traiter :
    \begin{equation}
        \frac{ N_{\bar t} }{ t }=\underbrace{\frac{ N_{\bar t} }{ \bar t }}_{\to \lambda}\cdot\underbrace{\frac{ \bar t }{ t }}_{\to 1}.
    \end{equation}
    où nous avons utilisé le premier point, \( \bar t\) étant entier. Pour le premier terme nous savons que \( t\mapsto N_t\) est croissante et donc que
    \begin{equation}
        \frac{ N_t-N_{\bar t} }{ t }\leq \frac{ N_{\bar t+1}-N_{\bar t}}{ t }=\frac{ N_{\bar t+1}-N_{\bar t} }{ \bar t+1 }\frac{ \bar t+1 }{ t }.
    \end{equation}
    Le second facteur tend vers \( 1\) lorsque \( t\to \infty\). Le premier s'écrit
    \begin{equation}    \label{eqtPgPpJ}
        \frac{ N_n-N_{n-1} }{ n }
    \end{equation}
    et tend vers zéro en tant que terme général de la série \eqref{EqfRpnfF} qui converge.
\end{proof}

\begin{proposition}
    La variable aléatoire \( N_t/t\) est un estimateur sans biais de \( \lambda\). De plus il converge vers \( \lambda\) en moyenne quadratique.
\end{proposition}

\begin{proof}
    Vu que \( N_t/t\to\lambda\) presque surement, la variable aléatoire \( N_t/t\) est un estimateur de \( \lambda\). Le fait qu'il soit sans biais a été fait dans l'exemple \ref{ExytNlTq}.

    D'autre part nous avons (voir théorème \ref{ThojDZjuj})
    \begin{equation}
        \Var\left( \frac{ N_t }{ t } \right)=\frac{1}{ t^2 }\Var(N_t)=\frac{ \lambda }{ t }.
    \end{equation}
    En appliquant la formule \( \Var(X)=E(X^2)-E(X)^2\) à \( X=N_t/t\) nous trouvons
    \begin{equation}
        E\left( \frac{ N_t^2 }{ t^2 } \right)=\frac{ \lambda }{ t }+\lambda^2.
    \end{equation}
    Cela montre que \( \frac{ N_t }{ t }\stackrel{L^2}{\longrightarrow}\lambda\).
\end{proof}

Pour le théorème central limite d'un processus de Poisson, nous visons un résultat du style de
\begin{equation}
    \frac{ \frac{1}{ n }\sum_iX_i-mn }{ \sigma\sqrt{n} }\stackrel{\hL}{\longrightarrow}\dN(0,1).
\end{equation}
Nous écrivons le théorème central limite pour le nombre de sauts que le processus de Poisson a connu en un temps \( t\). Le rôle de la moyenne empirique est joué par \( N_t\). Nous considérons avoir fait \emph{une seule expérience} qui a duré un temps \( t\). Donc le rôle de \( n\) est joué par \( 1\) (et non \( t\) comme on pourrait le croire). Pour le reste, le nombre de succès en un temps \( t\) d'une variable aléatoire exponentielle de paramètre \( \lambda\) est une variable aléatoire de Poisson de paramètre \( \lambda t\), en vertu de ce qui est raconté au point \ref{subsecPoissonetexpo}. C'est cela qui motive l'énoncé suivant.

\begin{theorem}[théorème central limite pour les processus de Poisson]\index{théorème!central limite!processus de Poisson}  \label{ThoCSuLLo}
    Si \( (N_t)_{t>0}\) est un processus de Poisson de paramètre \( \lambda\), alors nous avons
    \begin{equation}
        \frac{ N_t-\lambda t }{ \sqrt{\lambda t} }\stackrel{\hL}{\longrightarrow}\dN(0,1).
    \end{equation}
\end{theorem}

\begin{remark}
    Avant de nous lancer dans la démonstration, remarquons que si nous nous limitons à \( t\in \eN\), alors nous avons
    \begin{equation}
        \frac{ N_n-\lambda n }{ \sqrt{\lambda n} }=\frac{ \sum_{k=1}^n(N_k-N_{k-1})-\lambda n }{ \sqrt{\lambda n} }
    \end{equation}
    or par définition nous avons les égalités de lois
    \begin{equation}
        N_k-N_{k-1}\sim N_1\sim \dP(\lambda),
    \end{equation}
    donc
    \begin{equation}
        \frac{ S_n-\lambda n }{ \sqrt{\lambda n} }=\frac{ \frac{1}{ n }S_n-\lambda }{ \frac{ \sqrt{\lambda n} }{ n } }=\frac{ \frac{1}{ n }S_n-\lambda }{ \sqrt{\lambda}/\sqrt{n} },
    \end{equation}
    ce qui est exactement le théorème central limite pour une suite de lois de Poisson\footnote{Au fait près que nous devrions encore montrer que \( S_n\) est de carré intégrable.}.
\end{remark}

\begin{proof}
    Nous écrivons \( \bar t\) la partie entière de \( \bar t\) et nous décomposons :
    \begin{equation}
        \frac{ N_t-\lambda t }{ \sqrt{\lambda t} }=\underbrace{\frac{ N_t-N_{\bar t} }{ \sqrt{\lambda t} }}_A+\underbrace{\frac{ N_{\bar t}-\lambda \bar t }{ \sqrt{\lambda t} }}_B+\underbrace{\frac{ \lambda \bar t-\lambda t }{ \sqrt{\lambda t} }}_C.
    \end{equation}
    En ce qui concerne le terme \( B\), nous avons
    \begin{equation}
        B=\sqrt{\frac{ \bar t }{ t }}\frac{ N_{\bar t}-\lambda \bar t }{ \sqrt{\lambda \bar t} }\to\dN(0,1).
    \end{equation}
    Notons que nous utilisons le fait que si \( a_n\to 1\) (en tant que suite de nombre) et si \( X_n\to\dN(0,1)\) (limite en loi), alors \( a_nX_n\to \dN(0,1)\) en loi.

    Le terme \( C\) est également facile parce que \( \lambda \bar t-\lambda t\) est majoré en norme par \( \lambda\). Du coup
    \begin{equation}
        -\frac{ \lambda }{ \sqrt{\lambda t} }\leq C\leq \frac{ \lambda }{ \sqrt{\lambda t} }.
    \end{equation}
    Donc $\lim_{t\to \infty} C=0$.

    Reste à travailler sur \( A\). Vu que \( t\mapsto N_t\) est croissante, la différence \( N_t-N_{\bar t}\) est positive. Soit \( \eta>0\), nous avons
    \begin{equation}
        P(| A |>\eta)=P(N_t-N_{\bar t}>\sqrt{\lambda t}\eta)\leq P\big( N_{\bar t+1}-N_{\bar t}\geq \sqrt{\lambda t}\eta \big)=P(N_1\geq \eta\sqrt{\lambda t})
    \end{equation}
    parce que nous savons que $N_{\bar t+1}-N_{\bar t}\sim N_1\sim\dP(\lambda)$. En vertu des propriétés de la loi de Poisson,
    \begin{equation}
        \lim_{t\to \infty}P(N_1\geq \eta\sqrt{\lambda t})=0.
    \end{equation}
    En effet si \( Z\) est une variable aléatoire de Poisson de paramètre \( \lambda\) nous avons
    \begin{equation}
        P(Z>l)=\sum_{k=l}^{\infty}P(Z=k)= e^{-\lambda}\sum_{k=l}^{\infty}\frac{ \lambda^k }{ k! }.
    \end{equation}
    Nous reconnaissons la queue de série de \(  e^{\lambda}\), qui tend donc vers zéro lorsque \( l\to \infty\). Nous avons donc prouvé que
    \begin{equation}
        \lim_{t\to \infty} P\big( | A |>\eta \big)=0,
    \end{equation}
    c'est à dire la convergence en probabilité de \( A\) vers zéro.

    Nous avons montré que
    \begin{subequations}
        \begin{align}
            B+C&\stackrel{\hL}{\longrightarrow} U\sim\dN(0,1)\\
            A\stackrel{P}{\longrightarrow}0.
        \end{align}
    \end{subequations}
    Le lemme de Slutsky (\ref{LemgXDlhs}) nous avons une convergence du couple
    \begin{equation}
        (A,B+C)\stackrel{\hL}{\longrightarrow}(0,U).
    \end{equation}
    Utilisant le corollaire \ref{CorINgTPH}, nous trouvons la convergence en loi
    \begin{equation}
        A+(B+C)\stackrel{\hL}{\longrightarrow}0+U,
    \end{equation}
    ce qu'il fallait.
\end{proof}

\section{Quelque trucs sur la simulation}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Le théorème ergodique dit que
\begin{equation}
    \pi(x)=\lim_{N\to \infty} \frac{1}{ N }\sum_{k=1}^N\mtu_{X_k=x}.
\end{equation}
C'est avec cela qu'on calcule \( \pi(x)\) à partir d'une simulation de chaîne de Markov.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Le théorème central limite pour Markov}
%---------------------------------------------------------------------------------------------------------------------------

\begin{theorem}[Version allégée]
    Si \( (X_n)\) est irréductible et positive récurrente, alors pour toute fonction \( f\),
    \begin{equation}
        \frac{1}{ \sqrt{N} }\left[ \sum_{k=1}^N-N\int fd\pi \right]\stackrel{\hL}{\longrightarrow}\dN(0,\sigma^2)
    \end{equation}
    où \( \sigma^2\) dépend de la fonction \( f\) et de la chaîne de Markov.

   Ici, \( \int fd\pi=\sum_{x\in E}f(x)\pi(x)\).
\end{theorem}

Nous allons simuler la variable aléatoire
\begin{equation}
    Z=\frac{1}{ \sqrt{N} }\left[ \sum f(X_k)-N\sum_{x\in E} f(x)\pi(x) \right]
\end{equation}
et puis on va mettre sa réalisation dans un histogramme. Dans le cas où on prend \( f(i)=\mtu_{i=i_0}\), il y a de la simplification dans l'intégrale qui devient 
\begin{equation}
    Z=\frac{1}{ \sqrt{N} }\left[ \sum_{i=1}^N\mtu_{X_k=i_0}-N\pi(i_0) \right].
\end{equation}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Feuille 5}
%---------------------------------------------------------------------------------------------------------------------------

On pose 
\begin{equation}
    D_n=\sqrt{n}\sup_{x\in\eR}| F_n(x)-F(x) |.
\end{equation}
On en génère un milliers de fois \( D_n\), on note \( D_n^{(k)}\) ces réalisations, et on regarde ce que vaut
\begin{equation}
    \frac{1}{ 1000 }\sum_{k=1}^{1000}\mtu_{D_n^{(k)\geq c}}.
\end{equation}
Cela nous donne une approximation de
\begin{equation}
    P\big( \sqrt{n}\sup_{x\in\eR}| F_n(x)-F(x) |\geq c \big).
\end{equation}

Note que chacun des \( D_n^{(k)}\) demande de créer un nouveau vecteur \( Y_i\) de lois qu'on veut regarder. Par exemple de loi exponentielle.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Feuille 6}
%---------------------------------------------------------------------------------------------------------------------------

Pour créer une fonction qui renvoie \( i\) avec probabilité \( p_i\) pour \( i=1,2,3\), on peut faire
\begin{equation}
    U\sim\dU[0,1]
\end{equation}
et puis on a
\begin{subequations}
    \begin{align}
        P(U<p_0)&=p_0\\
        P(p_0<U<p_0+p_1)&=p_1\\
        P(p_0+p_1<U<p_2)&=p_2.
    \end{align}
\end{subequations}
Une façon de faire une loi uniforme \( [0,1]\) est de faire \info{rand}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Feuille 7}
%---------------------------------------------------------------------------------------------------------------------------

L'échantillon est \( (Y_1,\ldots, Y_n) \) et nous écrivons le vecteur
\begin{equation}
    Y=X\beta+\epsilon
\end{equation}
où \( Y\sim\dN(X\beta,\sigma^2\id)\) et \( \epsilon\sim\dN(0,\sigma^2\id)\). Nous utilisons le principe de maximum de vraisemblance. Soit \( (y_1,\ldots, y_n)\) un échantillon et
\begin{equation}
    P_{\theta}(y_1,\ldots, y_n)=\prod_i\frac{1}{ \sigma\sqrt{2\pi }}\exp\left[ -\frac{ 1 }{2}\left( \frac{ y_i-X_i^t\beta }{ \sigma } \right)^2 \right].
\end{equation}
L'astuce est de faire que \( y_i-X_i^t\beta\) est la \( i\)ième composante du vecteur \( Y-X\beta\) et donc la somme qui est dans l'exponentielle devient la norme de \( Y-X\beta\) :
\begin{equation}
    f_{\theta}(y_1,\ldots, y_n)=\left( \frac{1}{ \sigma\sqrt{2\pi} } \right)^n\exp\left[ -\frac{ 1 }{2}\| Y-X\beta \|^2 \right].
\end{equation}
On passe au logarithme et on dérive par rapport à \( \sigma^2\). Attention : la variable est \( \sigma^2\), donc la dérivée de \( \sigma^2\) est \( 1\) et non \( 2\sigma\). Bref, on trouve
\begin{equation}
    \sigma^2=\frac{1}{ 2n }\| U+X\beta \|.
\end{equation}
